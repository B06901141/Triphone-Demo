DRL_Lecture_1_-_Policy_Gradient_(Review)-0 各位同學大家好，那我們來上課吧
DRL_Lecture_1_-_Policy_Gradient_(Review)-1 那今天這一堂課，我們要講什麼呢？我們要講一個 policy gradient 的進階版
DRL_Lecture_1_-_Policy_Gradient_(Review)-2 這個技術呢，叫做 Proximal Policy Optimization (PPO)
DRL_Lecture_1_-_Policy_Gradient_(Review)-3 這個技術有多屌呢？這個技術呢，它是 default reinforcement learning algorithm at OpenAI
DRL_Lecture_1_-_Policy_Gradient_(Review)-4 是OpenAI 把這個方法，當作他們 default reinforcement learning algorithm
DRL_Lecture_1_-_Policy_Gradient_(Review)-5 所以今天假設你要 implement reinforcement learning，也許這是一個第一個你可以嘗試的方法，這是 PPO
DRL_Lecture_1_-_Policy_Gradient_(Review)-6 今天這門課呢，我們還是幫大家複習一下 Policy Gradient
DRL_Lecture_1_-_Policy_Gradient_(Review)-7 這部分也許有點快，所以如果你有問題的話，你就直接舉手打斷我
DRL_Lecture_1_-_Policy_Gradient_(Review)-8 接下來，我們會講說，原來的 policy gradient，是 On-Policy 的方法
DRL_Lecture_1_-_Policy_Gradient_(Review)-9 那怎麼從 on-policy 的方法，變到 off-policy 的方法，那等一下會講，on-policy 是什麼，off-policy 是什麼
DRL_Lecture_1_-_Policy_Gradient_(Review)-10 最後，從 on-policy 變成 off-policy 以後，再加一些 constrains，就變成了 PPO
DRL_Lecture_1_-_Policy_Gradient_(Review)-11 那在講這個技術之前，我們先看一下，Deep mind 跟 OpenAI，釋出來的 demo
DRL_Lecture_1_-_Policy_Gradient_(Review)-12 其實 deep mind 跟 OpenAI 各有一篇 PPO 的 paper
DRL_Lecture_1_-_Policy_Gradient_(Review)-13 那是 Deep mind 先發了，如果我沒有記錯時間的話，應該是在去年 7 月的月初，就發了 PPO 的 paper
DRL_Lecture_1_-_Policy_Gradient_(Review)-14 那在他們的 paper 裡面，其實有，他沒有說 PPO 不是他們 proposed
DRL_Lecture_1_-_Policy_Gradient_(Review)-15 他們 cited 一個 reference，這個 reference 是什麼呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-16 這個 reference 是 OpenAI 的某一個人，我記得是 John Schulman
DRL_Lecture_1_-_Policy_Gradient_(Review)-17 他在 Nice 的 tutorial 這樣，他在他的 tutorial 裡面有提到 PPO，但他還沒有寫 paper
DRL_Lecture_1_-_Policy_Gradient_(Review)-18 然後 deep mind 就先寫了 paper，然後 proposed PPO 這個方法
DRL_Lecture_1_-_Policy_Gradient_(Review)-19 然後在裡面是 cited Nice tutorial 影片的連結
DRL_Lecture_1_-_Policy_Gradient_(Review)-20 然後過了數日之後，可能 OpenAI 就發現說被 deep mind 搶先發走了
DRL_Lecture_1_-_Policy_Gradient_(Review)-21 他們也迅速的發了另外一片他們自己的 PPO 的 paper
DRL_Lecture_1_-_Policy_Gradient_(Review)-22 我們看一下 deep mind 的 demo
DRL_Lecture_1_-_Policy_Gradient_(Review)-23 這種讓機器人學走路的問題其實蠻難的，可以學起來，其實蠻厲害的
DRL_Lecture_1_-_Policy_Gradient_(Review)-24 這個是 OpenAI 的 demo，這個 demo 裡面，它也一樣控制一個機器人，這個機器人會在球場上
DRL_Lecture_1_-_Policy_Gradient_(Review)-25 去找紅色的球，吃紅色的球，然後會有白色的球一直攻擊它
DRL_Lecture_1_-_Policy_Gradient_(Review)-26 機器人被打倒以後，它會自己學著站起來，然後它去找紅色的球
DRL_Lecture_1_-_Policy_Gradient_(Review)-27 我覺得這個 demo 所 demo 的不只是 PPO，它 demo 其實是人生你知道嗎？以下部分省略
DRL_Lecture_1_-_Policy_Gradient_(Review)-28 那我們就來先複習一下 policy gradient，PPO 是 policy gradient 一個變形
DRL_Lecture_1_-_Policy_Gradient_(Review)-29 所以我們先講 policy gradient
DRL_Lecture_1_-_Policy_Gradient_(Review)-30 在 reinforcement learning 裡面呢有 3 個 components
DRL_Lecture_1_-_Policy_Gradient_(Review)-31 一個 actor，一個 environment，一個 reward function
DRL_Lecture_1_-_Policy_Gradient_(Review)-32 在我們作業 4-1 裡面呢，就是要做
DRL_Lecture_1_-_Policy_Gradient_(Review)-33 讓機器玩 video game，那這個時候你 actor 做的事情
DRL_Lecture_1_-_Policy_Gradient_(Review)-34 就是去操控，遊戲的搖桿，比如說向左向右，開火，等等，
DRL_Lecture_1_-_Policy_Gradient_(Review)-35 你的 environment 就是遊戲的主機，負責控制遊戲的畫面
DRL_Lecture_1_-_Policy_Gradient_(Review)-36 負責控制說，怪物要怎麼移動，你現在要看到什麼畫面，等等
DRL_Lecture_1_-_Policy_Gradient_(Review)-37 所謂的 reward function，就是決定，當你做什麼事情，發生什麼狀況的時候
DRL_Lecture_1_-_Policy_Gradient_(Review)-38 你可以得到多少分數，比如說殺一隻怪獸，得到20 分等等
DRL_Lecture_1_-_Policy_Gradient_(Review)-39 那同樣的概念，用在圍棋上也是一樣
DRL_Lecture_1_-_Policy_Gradient_(Review)-40 actor 就是 alpha Go，它要決定，下哪一個位置，那你的 environment 呢
DRL_Lecture_1_-_Policy_Gradient_(Review)-41 就是對手，你的 reward function 就是按照圍棋的規則，贏就是得一分，輸就是負一分等等
DRL_Lecture_1_-_Policy_Gradient_(Review)-42 那在 reinforcement 裡面啊，你要記得說 environment 跟 reward function，不是你可以控制的
DRL_Lecture_1_-_Policy_Gradient_(Review)-43 environment 跟 reward function，是在開始學習之前，就已經事先給定的
DRL_Lecture_1_-_Policy_Gradient_(Review)-44 你唯一能做的事情，是調整你的 actor
DRL_Lecture_1_-_Policy_Gradient_(Review)-45 調整你 actor 裡面的 policy
DRL_Lecture_1_-_Policy_Gradient_(Review)-46 使得它可以得到最大的 reward，你可以調的只有 actor
DRL_Lecture_1_-_Policy_Gradient_(Review)-47 environment 跟 reward function 是事先給定，你是不能夠去動它的
DRL_Lecture_1_-_Policy_Gradient_(Review)-48 那這個 actor 裡面，會有一個 policy，這個 policy 決定了 actor 的行為，
DRL_Lecture_1_-_Policy_Gradient_(Review)-49 那所謂的 policy 呢，就是給一個外界的輸入
DRL_Lecture_1_-_Policy_Gradient_(Review)-50 然後它會輸出 actor 現在應該要執行的行為
DRL_Lecture_1_-_Policy_Gradient_(Review)-51 那今天假設你是用 deep learning 的技術來做 reinforcement learning 的話
DRL_Lecture_1_-_Policy_Gradient_(Review)-52 那你的 policy，policy 我們一般寫成 pi，policy 就是一個 network
DRL_Lecture_1_-_Policy_Gradient_(Review)-53 那我們知道說，network 裡面，就有一堆參數，我們用 theta 來代表 pi 的參數
DRL_Lecture_1_-_Policy_Gradient_(Review)-54 今天你的這個 network，你的 policy 它是一個 network
DRL_Lecture_1_-_Policy_Gradient_(Review)-55 這個 network 的 input 它就是現在 machine 看到的東西
DRL_Lecture_1_-_Policy_Gradient_(Review)-56 如果讓 machine 打電玩的話，那 machine 看到的東西，就是遊戲的畫面
DRL_Lecture_1_-_Policy_Gradient_(Review)-57 當然讓 machine 看到什麼東西，會影響你現在 training
DRL_Lecture_1_-_Policy_Gradient_(Review)-58 到底好不好 train
DRL_Lecture_1_-_Policy_Gradient_(Review)-59 舉例來說，在玩遊戲的時候，也許你覺得遊戲的畫面，前後是相關的
DRL_Lecture_1_-_Policy_Gradient_(Review)-60 也許你覺得說，你應該讓你的 policy，看從遊戲初始
DRL_Lecture_1_-_Policy_Gradient_(Review)-61 到現在這個時間點，所有畫面的總和，你可能會覺得你要用到 RNN 來處理它
DRL_Lecture_1_-_Policy_Gradient_(Review)-62 不過這樣子，你會比較難處理就是了，那要讓
DRL_Lecture_1_-_Policy_Gradient_(Review)-63 你的 machine 你的 policy 看到什麼樣的畫面，這個是你自己決定的
DRL_Lecture_1_-_Policy_Gradient_(Review)-64 那等一下在講作業 4-1 的時候，助教會給你一些 tip，給你參考，讓你知道說
DRL_Lecture_1_-_Policy_Gradient_(Review)-65 給機器看到什麼樣的遊戲畫面，可能是比較有效的
DRL_Lecture_1_-_Policy_Gradient_(Review)-66 那在 output 的部分，輸出的就是今天機器要採取什麼樣的行為
DRL_Lecture_1_-_Policy_Gradient_(Review)-67 這邊這個是具體的例子，你的 policy 就是一個 network
DRL_Lecture_1_-_Policy_Gradient_(Review)-68 input 就是遊戲的畫面，那它通常就是由 pixels 所組成的
DRL_Lecture_1_-_Policy_Gradient_(Review)-69 那 output 就是看看說現在有那些選項
DRL_Lecture_1_-_Policy_Gradient_(Review)-70 是你可以去執行的，那你的 output layer 就有幾個 neurons
DRL_Lecture_1_-_Policy_Gradient_(Review)-71 假設你現在可以做的行為就是有 3 個，那你的 output layer 就是有 3 個 neurons
DRL_Lecture_1_-_Policy_Gradient_(Review)-72 每個 neuron 對應到一個可以採取的行為
DRL_Lecture_1_-_Policy_Gradient_(Review)-73 好，那 input 一個東西以後呢
DRL_Lecture_1_-_Policy_Gradient_(Review)-74 你的 network 就會給每一個可以採取的行為一個分數
DRL_Lecture_1_-_Policy_Gradient_(Review)-75 接下來，你把這個分數，當作是機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-76 那你的 actor 就是看這個機率的分布
DRL_Lecture_1_-_Policy_Gradient_(Review)-77 根據這個機率的分布，決定它要採取的行為
DRL_Lecture_1_-_Policy_Gradient_(Review)-78 比如說 70% 會走 left，20% 走 right，10% 開火，等等
DRL_Lecture_1_-_Policy_Gradient_(Review)-79 那這個機率分布不同，你的 actor 採取的行為，就會不一樣
DRL_Lecture_1_-_Policy_Gradient_(Review)-80 那這是 policy 的部分，它就是一個 network
DRL_Lecture_1_-_Policy_Gradient_(Review)-81 接下來用一個例子，具體的很快地說一下說，今天你的 actor 是怎麼樣跟環境互動的
DRL_Lecture_1_-_Policy_Gradient_(Review)-82 首先你的 actor 會看到一個遊戲畫面，這個遊戲畫面
DRL_Lecture_1_-_Policy_Gradient_(Review)-83 我們就用 s1 來表示它，它代表遊戲初始的畫面
DRL_Lecture_1_-_Policy_Gradient_(Review)-84 接下來你的 actor 看到這個遊戲的初始畫面以後
DRL_Lecture_1_-_Policy_Gradient_(Review)-85 根據它內部的 network，根據它內部的 policy
DRL_Lecture_1_-_Policy_Gradient_(Review)-86 它就會決定一個 action，那假設它現在決定的 action 是向右，
DRL_Lecture_1_-_Policy_Gradient_(Review)-87 那它決定完 action 以後，它就會得到一個  reward
DRL_Lecture_1_-_Policy_Gradient_(Review)-88 代表它採取這個 action 以後，它會得到多少的分數，
DRL_Lecture_1_-_Policy_Gradient_(Review)-89 那這邊我們把一開始的初始畫面，寫作 s1，我們把第一次執行的動作叫做 a1
DRL_Lecture_1_-_Policy_Gradient_(Review)-90 我們把第一次執行動作完以後得到的 reward，叫做 r1
DRL_Lecture_1_-_Policy_Gradient_(Review)-91 那不同的文件，其實有不同的定義，有人會覺得說
DRL_Lecture_1_-_Policy_Gradient_(Review)-92 這邊應該要叫做 r2，這個都可以，你自己看得懂就好
DRL_Lecture_1_-_Policy_Gradient_(Review)-93 那接下來就看到新的遊戲畫面，你的
DRL_Lecture_1_-_Policy_Gradient_(Review)-94 actor 決定一個的行為以後，就會看到一個新的遊戲畫面，這邊是 s2
DRL_Lecture_1_-_Policy_Gradient_(Review)-95 然後把這個 s2 輸入給 actor，這個 actor 決定要開火
DRL_Lecture_1_-_Policy_Gradient_(Review)-96 然後它可能殺了一隻怪，就得到五分，然後這個 process 就反覆的持續下去
DRL_Lecture_1_-_Policy_Gradient_(Review)-97 直到今天走到某一個 timestamp
DRL_Lecture_1_-_Policy_Gradient_(Review)-98 執行某一個 action，得到 reward 之後，這個 environment 決定這個遊戲結束了
DRL_Lecture_1_-_Policy_Gradient_(Review)-99 比如說，如果在這個遊戲裡面，你是控制綠色的船去殺怪，如果你被殺死的話
DRL_Lecture_1_-_Policy_Gradient_(Review)-100 遊戲就結束，或是你把所有的怪都清空，遊戲就結束了
DRL_Lecture_1_-_Policy_Gradient_(Review)-101 那一場遊戲，叫做一個 Episode
DRL_Lecture_1_-_Policy_Gradient_(Review)-102 把這個遊戲裡面，所有得到的 reward
DRL_Lecture_1_-_Policy_Gradient_(Review)-103 通通總合起來，就是 Total reward，那這邊用大 R 來表示它，
DRL_Lecture_1_-_Policy_Gradient_(Review)-104 那今天這個 actor 它存在的目的，就是想辦法去 maximize 它可以得到的 reward，
DRL_Lecture_1_-_Policy_Gradient_(Review)-105 那這邊是用圖像化的方式，來再跟大家說明一下，你的 environment，actor，還有 reward 之間的關係，
DRL_Lecture_1_-_Policy_Gradient_(Review)-106 首先，environment 其實它本身也是一個 function
DRL_Lecture_1_-_Policy_Gradient_(Review)-107 連那個遊戲的主機，你也可以把它看作是一個 function
DRL_Lecture_1_-_Policy_Gradient_(Review)-108 雖然它裡面不見得是 neural network，可能是 rule-based 的規則，但你可以把它看作是一個 function
DRL_Lecture_1_-_Policy_Gradient_(Review)-109 那這個 function，一開始就先吐出一個 state
DRL_Lecture_1_-_Policy_Gradient_(Review)-110 然後接下來呢，也就是遊戲的畫面
DRL_Lecture_1_-_Policy_Gradient_(Review)-111 接下來你的 actor 看到這個遊戲畫面 s1 以後，它吐出 a1
DRL_Lecture_1_-_Policy_Gradient_(Review)-112 然後接下來 environment，把這個 a1 當作它的輸入，然後它再吐出 s2，吐出新的遊戲畫面
DRL_Lecture_1_-_Policy_Gradient_(Review)-113 actor 看到新的遊戲畫面，又再決定新的行為 a2
DRL_Lecture_1_-_Policy_Gradient_(Review)-114 然後 environment 再看到 a2，再吐出 s3，那這個 process 就一直下去
DRL_Lecture_1_-_Policy_Gradient_(Review)-115 直到 environment 覺得說應該要停止為止
DRL_Lecture_1_-_Policy_Gradient_(Review)-116 在一場遊戲裡面，我們把 environment 輸出的 s
DRL_Lecture_1_-_Policy_Gradient_(Review)-117 跟 actor 輸出的行為 a，把這個 s 跟 a 全部串起來，叫做一個 Trajectory
DRL_Lecture_1_-_Policy_Gradient_(Review)-118 每一個 trajectory，你可以計算它發生的機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-119 假設現在 actor 的參數已經被給定了話，就是 theta
DRL_Lecture_1_-_Policy_Gradient_(Review)-120 根據這個 theta，你其實可以計算某一個 trajectory 發生的機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-121 你可以計算某一個回合，某一個 episode 裡面，發生這樣子狀況的機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-122 怎麼算呢，某一個 trajectory tao，在假設你 actor 的參數就是 theta 的情況下
DRL_Lecture_1_-_Policy_Gradient_(Review)-123 某一個 trajectory tao，它的機率就是這樣算的
DRL_Lecture_1_-_Policy_Gradient_(Review)-124 你先算說 environment 輸出 s1 的機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-125 再計算，根據 s1 執行 a1 的機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-126 是由你 policy 裡面的那個 network 參數 theta 所決定的，
DRL_Lecture_1_-_Policy_Gradient_(Review)-127 它是一個機率，因為我們之前有講過說
DRL_Lecture_1_-_Policy_Gradient_(Review)-128 你的 policy 的 network 它的 output 它其實是一個 distribution
DRL_Lecture_1_-_Policy_Gradient_(Review)-129 那你的 actor 是根據這個 distribution 去做 sample，決定現在實際上要採取的 action是哪一個
DRL_Lecture_1_-_Policy_Gradient_(Review)-130 接下來你這個 environment，根據，這邊畫是寫說根據 a1 產生 s2，那其實它是根據
DRL_Lecture_1_-_Policy_Gradient_(Review)-131 a1 跟 s1 產生 s2，因為你想說，s2 跟 s1 還是有關係的，下一個遊戲畫面，跟前一個遊戲畫面
DRL_Lecture_1_-_Policy_Gradient_(Review)-132 通常還是有關係的，至少要是連續的，所以這邊是給定前一個遊戲畫面 s1
DRL_Lecture_1_-_Policy_Gradient_(Review)-133 跟你現在 actor 採取的行為 a1，然後會產生 s2，
DRL_Lecture_1_-_Policy_Gradient_(Review)-134 這件事情它可能是機率，也可能不是機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-135 這個是就取決於那個 environment，就是那個主機它內部設定是怎樣
DRL_Lecture_1_-_Policy_Gradient_(Review)-136 看今天這個主機在決定，要輸出什麼樣的遊戲畫面的時候，有沒有機率，因為如果沒有機率的話
DRL_Lecture_1_-_Policy_Gradient_(Review)-137 那這個遊戲的每次的行為都一樣，你只要找到一條 path，就可以過關了，這樣感覺是蠻無聊的
DRL_Lecture_1_-_Policy_Gradient_(Review)-138 所以遊戲裡面，通常是還是有一些機率的
DRL_Lecture_1_-_Policy_Gradient_(Review)-139 你做同樣的行為，給同樣的給前一個畫面，下次產生的畫面其實不見得是一樣的，
DRL_Lecture_1_-_Policy_Gradient_(Review)-140 Process 就反覆繼續下去，你就可以計算說
DRL_Lecture_1_-_Policy_Gradient_(Review)-141 一個 trajectory s1,a1, s2, a2 它出現的機率有多大
DRL_Lecture_1_-_Policy_Gradient_(Review)-142 這邊只是把這個機率，把它寫出來而已
DRL_Lecture_1_-_Policy_Gradient_(Review)-143 那這個機率，取決於兩件事，一部分是 environment 本身的行為
DRL_Lecture_1_-_Policy_Gradient_(Review)-144 若 environment 的 function 它內部的參數或內部的規則長什麼樣子
DRL_Lecture_1_-_Policy_Gradient_(Review)-145 那這個部分，就這一項 p of s(t+1) given st,at
DRL_Lecture_1_-_Policy_Gradient_(Review)-146 代表的是 environment，這個 environment 這一項通常你是無法控制它的
DRL_Lecture_1_-_Policy_Gradient_(Review)-147 因為那個是人家寫好的，你不能控制它
DRL_Lecture_1_-_Policy_Gradient_(Review)-148 你能控制的是這個，你能控制的是 p theta of at given st，你就 given 一個 st
DRL_Lecture_1_-_Policy_Gradient_(Review)-149 你的 actor 要採取什麼樣的行為，at 這件事，會取決於你 actor 的參數
DRL_Lecture_1_-_Policy_Gradient_(Review)-150 你的 passed 參數 theta，所以這部分是 actor 可以自己控制的
DRL_Lecture_1_-_Policy_Gradient_(Review)-151 隨著 actor 的行為不同，每個同樣的 trajectory，它就會有不同的出現的機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-152 我們說在 reinforcement learning 裡面
DRL_Lecture_1_-_Policy_Gradient_(Review)-153 除了 environment 跟 actor 以外呢，還有第三個角色，叫做 reward function
DRL_Lecture_1_-_Policy_Gradient_(Review)-154 Reward function 做的事情就是，根據在某一個 state 採取的某一個 action
DRL_Lecture_1_-_Policy_Gradient_(Review)-155 決定說現在這個行為，可以得到多少的分數
DRL_Lecture_1_-_Policy_Gradient_(Review)-156 它是一個 function，給它 s1，a1，它告訴你得到 r1，給它 s2，a2，它告訴你得到 r2
DRL_Lecture_1_-_Policy_Gradient_(Review)-157 我們把所有的小 r 都加起來，我們就得到了大 R
DRL_Lecture_1_-_Policy_Gradient_(Review)-158 我們這邊寫做大 R of tao，代表說是
DRL_Lecture_1_-_Policy_Gradient_(Review)-159 某一個 trajectory tao，在某一場遊戲裡面，某一個 episode 裡面，我們會得到的大 R
DRL_Lecture_1_-_Policy_Gradient_(Review)-160 那今天我們要做的事情就是調整 actor 內部的參數 theta
DRL_Lecture_1_-_Policy_Gradient_(Review)-161 使得大 R 的值，越大越好
DRL_Lecture_1_-_Policy_Gradient_(Review)-162 但是實際上 reward，它並不只是一個 scalar
DRL_Lecture_1_-_Policy_Gradient_(Review)-163 reward 它其實是一個 random variable，這個大 R 其實是一個 random variable，為什麼呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-164 因為你的 actor 本身，在給定同樣的 state 會做什麼樣的行為，這件事情是有隨機性的
DRL_Lecture_1_-_Policy_Gradient_(Review)-165 你的 environment，在給定同樣的 action 要採取什麼樣的 observation
DRL_Lecture_1_-_Policy_Gradient_(Review)-166 要產生什麼樣的 observation，本身也是有隨機性的
DRL_Lecture_1_-_Policy_Gradient_(Review)-167 所以這個大 R 其實是一個 random variable
DRL_Lecture_1_-_Policy_Gradient_(Review)-168 你能夠計算的，是它的期望值
DRL_Lecture_1_-_Policy_Gradient_(Review)-169 你能夠計算的是說，在給定某一組參數 theta 的情況下
DRL_Lecture_1_-_Policy_Gradient_(Review)-170 我們會得到的這個大 R 的期望值，是多少
DRL_Lecture_1_-_Policy_Gradient_(Review)-171 那這個期望值是怎麼算的呢？這期望值的算法就是，窮舉所有可能的 trajectory
DRL_Lecture_1_-_Policy_Gradient_(Review)-172 窮舉所有可能的 trajectory tao，每一個 trajectory tao，它都有一個機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-173 比如說今天你的 theta 是一個很強的 model，那它都不會死
DRL_Lecture_1_-_Policy_Gradient_(Review)-174 那如果今天有一個 episode 是很快就死掉了，它的機率就很小
DRL_Lecture_1_-_Policy_Gradient_(Review)-175 如果有一個 episode 是都一直沒有死，那它的機率就很大
DRL_Lecture_1_-_Policy_Gradient_(Review)-176 那根據你的 theta，你可以算出某一個 trajectory tao 出現的機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-177 接下來你計算這個 tao 的 total reward 是多少
DRL_Lecture_1_-_Policy_Gradient_(Review)-178 把 total reward weighted by 這個 tao 出現的機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-179 summation over 所有的 tao，顯然就是期望值
DRL_Lecture_1_-_Policy_Gradient_(Review)-180 顯然就是 given 某一個參數你會得到的期望值
DRL_Lecture_1_-_Policy_Gradient_(Review)-181 或你會寫成這樣，從 p(theta) of tao 這個 distribution
DRL_Lecture_1_-_Policy_Gradient_(Review)-182 sample 一個 trajectory tao，然後計算 R of tao 的期望值，就是你的 expected reward
DRL_Lecture_1_-_Policy_Gradient_(Review)-183 那我們要做的事情，就是 maximize expected reward
DRL_Lecture_1_-_Policy_Gradient_(Review)-184 怎麼 maximize expected reward 呢？我們用的就是 gradient ascent
DRL_Lecture_1_-_Policy_Gradient_(Review)-185 因為我們是要讓它越大越好，所以不是 gradient decent，是 gradient ascent
DRL_Lecture_1_-_Policy_Gradient_(Review)-186 所以跟 gradient decent 唯一不同的地方就只是
DRL_Lecture_1_-_Policy_Gradient_(Review)-187 本來在 update 參數的時候，要減，現在變成加，這 gradient ascent
DRL_Lecture_1_-_Policy_Gradient_(Review)-188 然後這 gradient ascent 你就必須計算呢，R bar，這 expected reward，它的 gradient
DRL_Lecture_1_-_Policy_Gradient_(Review)-189 R bar 的 gradient 怎麼計算呢？這跟我上周講那個 GAN 產生做 sequence generation 的式子，其實是一模一樣的
DRL_Lecture_1_-_Policy_Gradient_(Review)-190 那所以這部分，我們會很快把它走過去
DRL_Lecture_1_-_Policy_Gradient_(Review)-191 我們說，R bar 我們取一個 gradient，這裡面只有 p theta
DRL_Lecture_1_-_Policy_Gradient_(Review)-192 是跟 theta 有關，所以 gradient 就放在 p theta 這個地方
DRL_Lecture_1_-_Policy_Gradient_(Review)-193 那 R 這個 reward function 啊，不需要是 differentiable，我們也可以解接下來的問題
DRL_Lecture_1_-_Policy_Gradient_(Review)-194 舉例來說，如果是在 GAN 裡面，你的這個 R 其實是一個 discriminator
DRL_Lecture_1_-_Policy_Gradient_(Review)-195 它就算是沒有辦法微分，也無所謂，你還是可以做接下來的運算
DRL_Lecture_1_-_Policy_Gradient_(Review)-196 接下來要做的事情，這個大家可能都看過很多次了，分子分母，上下同乘 p(theta) of tao
DRL_Lecture_1_-_Policy_Gradient_(Review)-197 然後接下來我就會告訴你說，後面這一項，p(theta) of tao 的 gradient 除以 p(theta) of tao
DRL_Lecture_1_-_Policy_Gradient_(Review)-198 其實就是這個 log p(theta) of tao，取 gradient
DRL_Lecture_1_-_Policy_Gradient_(Review)-199 或者是你其實之後就可以直接背一個公式
DRL_Lecture_1_-_Policy_Gradient_(Review)-200 就某一個 function f of x，你對它做 gradient 的話，就等於 f of x 乘上 gradient log f of x
DRL_Lecture_1_-_Policy_Gradient_(Review)-201 所以今天這邊有一個 gradient p(theta) of tao
DRL_Lecture_1_-_Policy_Gradient_(Review)-202 帶進這個公式裡面呢，這邊應該變成 p(theta) of tao 乘上 gradient log p(theta) of tao
DRL_Lecture_1_-_Policy_Gradient_(Review)-203 然後接下來呢，這邊又 summation over tao，然後又有把這個 R 跟這個 log 這兩項啊
DRL_Lecture_1_-_Policy_Gradient_(Review)-204 weighted by p(theta) of tao，
DRL_Lecture_1_-_Policy_Gradient_(Review)-205 那既然有 weighted by p(theta) of tao，它們就可以被寫成這個 expected 的形式
DRL_Lecture_1_-_Policy_Gradient_(Review)-206 也就是你從 p(theta) of tao 這個 distribution 裡面 sample tao 出來
DRL_Lecture_1_-_Policy_Gradient_(Review)-207 去計算 R of tao 乘上 gradient log p(theta) of tao
DRL_Lecture_1_-_Policy_Gradient_(Review)-208 然後把它對所有可能的 tao 呢，做 summation，就是這個 expected value
DRL_Lecture_1_-_Policy_Gradient_(Review)-209 然後接下來呢，這個 expected value 實際上你沒有辦法算，所以你是用 sample 的方式
DRL_Lecture_1_-_Policy_Gradient_(Review)-210 來 sample 一大堆的 tao，你 sample 大 N 筆 tao，然後每一筆呢，你都去計算它的這些 value
DRL_Lecture_1_-_Policy_Gradient_(Review)-211 然後把它全部加起來，最後你就得到你的 gradient
DRL_Lecture_1_-_Policy_Gradient_(Review)-212 你就可以去 update 你的參數，你就可以去 update 你的 agent
DRL_Lecture_1_-_Policy_Gradient_(Review)-213 那這邊呢，我們跳了一大步，這邊這個 p(theta) of tao，我們前面有講過 p(theta) of tao 是可以算的
DRL_Lecture_1_-_Policy_Gradient_(Review)-214 那 p(theta) of tao 裡面有兩項，一項是來自於 environment，一項是來自於你的 agent
DRL_Lecture_1_-_Policy_Gradient_(Review)-215 來自 environment 那一項，其實你根本就不能夠算它
DRL_Lecture_1_-_Policy_Gradient_(Review)-216 你對它做 gradient 是沒有用的，因為它跟 theta 是完全沒有任何關係的，所以你不需要對它做 gradient
DRL_Lecture_1_-_Policy_Gradient_(Review)-217 你真正會對它做 gradient 的，只有 log p(theta) of at given st 而已
DRL_Lecture_1_-_Policy_Gradient_(Review)-218 這個部分，其實你可以非常直觀的來理解它
DRL_Lecture_1_-_Policy_Gradient_(Review)-219 也就是在你 sample 到的 data 裡面，你 sample 到，在某一個 state st 要執行某一個 action at
DRL_Lecture_1_-_Policy_Gradient_(Review)-220 那如果在某一個 state st，執行某一個 action at
DRL_Lecture_1_-_Policy_Gradient_(Review)-221 最後導致整個 trajectory tao，就是這個 st 跟 at
DRL_Lecture_1_-_Policy_Gradient_(Review)-222 它是在整個 trajectory tao 的裡面的某一個 state and action 的 pair
DRL_Lecture_1_-_Policy_Gradient_(Review)-223 假設你在 st 執行 at，最後發現 tao 的 reward 是正的，那你就要增加這一項的機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-224 你就要增加在 st 執行 at 的機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-225 反之，在 st 執行 at 會導致整個 trajectory 的 reward 變成負的
DRL_Lecture_1_-_Policy_Gradient_(Review)-226 你就要減少這一項的機率，那這個概念就是怎麼簡單
DRL_Lecture_1_-_Policy_Gradient_(Review)-227 那因為我們在作業裡面要實作，那這個怎麼實作呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-228 你實作的方法就是這個樣子，你用 gradient ascent 的方法
DRL_Lecture_1_-_Policy_Gradient_(Review)-229 來 update 你的參數，所以你原來有一個參數 theta
DRL_Lecture_1_-_Policy_Gradient_(Review)-230 你把你的 theta 加上你的 gradient 這一項，那當然前面要有個 learning rate
DRL_Lecture_1_-_Policy_Gradient_(Review)-231 learning rate 其實也是要調的，你要用 ADAM、rmsprop 等等，還是要調一下
DRL_Lecture_1_-_Policy_Gradient_(Review)-232 那這 gradient 這一項怎麼來呢？gradient 這一項，就套下面這個公式，把它算出來
DRL_Lecture_1_-_Policy_Gradient_(Review)-233 那在實際上做的時候，要套下面這個公式，首先你要先收集一大堆的 s 跟 a 的 pair
DRL_Lecture_1_-_Policy_Gradient_(Review)-234 你還要知道這些 s 跟 a，如果實際上在跟環境互動的時候
DRL_Lecture_1_-_Policy_Gradient_(Review)-235 你會得到多少的 reward，所以這些資料，你要去收集起來，
DRL_Lecture_1_-_Policy_Gradient_(Review)-236 這些資料怎麼收集呢？你就要拿你的 agent，它的參數是 theta
DRL_Lecture_1_-_Policy_Gradient_(Review)-237 去跟環境做互動，也就是你拿你現在已經 train 好的那個 agent
DRL_Lecture_1_-_Policy_Gradient_(Review)-238 先去跟環境玩一下，先去跟那個遊戲互動一下，那互動完以後，你就會得到一大堆遊戲的紀錄
DRL_Lecture_1_-_Policy_Gradient_(Review)-239 你會記錄說，今天先玩了第一場
DRL_Lecture_1_-_Policy_Gradient_(Review)-240 在第一場遊戲裡面，我們在 state s1，採取 action a1，在 state s2，採取 action a2
DRL_Lecture_1_-_Policy_Gradient_(Review)-241 那要記得說其實今天玩遊戲的時候，是有隨機性的
DRL_Lecture_1_-_Policy_Gradient_(Review)-242 所以你的 agent 本身是有隨機性的，所以在同樣 state s1，不是每次都會採取 a1，所以你要記錄下來
DRL_Lecture_1_-_Policy_Gradient_(Review)-243 在 state s1，採取 a1，在 state s2，採取 a2，然後最後呢
DRL_Lecture_1_-_Policy_Gradient_(Review)-244 整場遊戲結束以後，得到的分數，是 R of tao(1)
DRL_Lecture_1_-_Policy_Gradient_(Review)-245 那你會 sample 到另外一筆 data，也就是另外一場遊戲，在另外一場遊戲裡面
DRL_Lecture_1_-_Policy_Gradient_(Review)-246 你在第一個 state 採取這個 action，在第二個 state 採取這個 action
DRL_Lecture_1_-_Policy_Gradient_(Review)-247 在第二個遊戲畫面採取這個 action，然後你 sample 到的，你得到的 reward 是 R of tao(2)
DRL_Lecture_1_-_Policy_Gradient_(Review)-248 你有了這些東西以後，你就去把這邊你 sample 到的東西
DRL_Lecture_1_-_Policy_Gradient_(Review)-249 帶到這個 gradient 的式子裡面，把 gradient 算出來
DRL_Lecture_1_-_Policy_Gradient_(Review)-250 也就是說你會做的事情是，把這邊的每一個 s 跟 a 的 pair
DRL_Lecture_1_-_Policy_Gradient_(Review)-251 拿進來，算一下它的 log probability
DRL_Lecture_1_-_Policy_Gradient_(Review)-252 你計算一下，在某一個 state，採取某一個 action 的 log probability，然後對它取 gradient
DRL_Lecture_1_-_Policy_Gradient_(Review)-253 然後這個 gradient 前面會乘一個 weight，這個 weight 是什麼？這個 weight 就是這場遊戲的 reward
DRL_Lecture_1_-_Policy_Gradient_(Review)-254 你有了這些以後，你就會去 update 你的 model，你 update 完你的 model 以後
DRL_Lecture_1_-_Policy_Gradient_(Review)-255 你回過頭來要重新再去收集你的 data
DRL_Lecture_1_-_Policy_Gradient_(Review)-256 然後再去收集你的 data，再 update model...
DRL_Lecture_1_-_Policy_Gradient_(Review)-257 那這邊要注意一下，一般 policy gradient，你 sample 的 data 就只會用一次
DRL_Lecture_1_-_Policy_Gradient_(Review)-258 你把這些 data sample 起來，然後拿去 update 參數，這些 data 就丟掉了
DRL_Lecture_1_-_Policy_Gradient_(Review)-259 再重新 sample data，才能夠再重新去 update 參數
DRL_Lecture_1_-_Policy_Gradient_(Review)-260 等一下我們會解決這個問題
DRL_Lecture_1_-_Policy_Gradient_(Review)-261 那接下來的就是實作的時候你會遇到的實作的一些細節
DRL_Lecture_1_-_Policy_Gradient_(Review)-262 實際上這個東西到底是怎麼實作的呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-263 因為到時候你要真的實作嘛，所以我們還是講一下
DRL_Lecture_1_-_Policy_Gradient_(Review)-264 這個東西到底實際上在用這個 deep learning 的 framework implement 的時候
DRL_Lecture_1_-_Policy_Gradient_(Review)-265 它是怎麼實作的呢，其實你的實作方法是這個樣子
DRL_Lecture_1_-_Policy_Gradient_(Review)-266 你要把它想成你就是在做一個分類的問題
DRL_Lecture_1_-_Policy_Gradient_(Review)-267 分類問題大家都會嘛，對不對，現在我們電機營都已經教大家用 TensorFlow
DRL_Lecture_1_-_Policy_Gradient_(Review)-268 implement MNIST classification，理論上每個人都會做 classification
DRL_Lecture_1_-_Policy_Gradient_(Review)-269 那在 classification 裡面就是 input 一個 image，就是做 MNIST input 一個 image
DRL_Lecture_1_-_Policy_Gradient_(Review)-270 然後 output 就是要決定說，是 10 個 class 裡面的哪一個
DRL_Lecture_1_-_Policy_Gradient_(Review)-271 所以那要怎麼做 classification，當然要收集一堆 training data
DRL_Lecture_1_-_Policy_Gradient_(Review)-272 你要有 input 跟 output 的 pair
DRL_Lecture_1_-_Policy_Gradient_(Review)-273 那今天在 reinforcement learning 裡面，在實作的時候
DRL_Lecture_1_-_Policy_Gradient_(Review)-274 你就把 state 當作是 classifier 的 input
DRL_Lecture_1_-_Policy_Gradient_(Review)-275 你就當作你是要做 image classification 的 problem
DRL_Lecture_1_-_Policy_Gradient_(Review)-276 只是現在的 class 不是說 image 裡面有什麼 objects
DRL_Lecture_1_-_Policy_Gradient_(Review)-277 現在的 class 是說，看到這張 image 我們要採取什麼樣的行為，每一個行為就叫做一個 class
DRL_Lecture_1_-_Policy_Gradient_(Review)-278 比如說第一個 class 叫做向左，第二個 class 叫做向右，第三個 class 叫做開火
DRL_Lecture_1_-_Policy_Gradient_(Review)-279 那這些訓練的資料是從哪裡來的呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-280 我們說你要做分類的問題，你要有 classified 的 input，跟它正確的 output
DRL_Lecture_1_-_Policy_Gradient_(Review)-281 這些訓練資料哪來呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-282 這些訓練資料，就是從 sampling 的 process 來的
DRL_Lecture_1_-_Policy_Gradient_(Review)-283 假設在 sampling 的 process 裡面，在某一個 state
DRL_Lecture_1_-_Policy_Gradient_(Review)-284 你 sample 到你要採取 action a，你就把這個 action a 當作是你的 ground truth
DRL_Lecture_1_-_Policy_Gradient_(Review)-285 你在這個 state，你 sample 到要向左
DRL_Lecture_1_-_Policy_Gradient_(Review)-286 本來向左這件事機率不一定是最高，因為你是 sample，它不一定機率最高
DRL_Lecture_1_-_Policy_Gradient_(Review)-287 假設你 sample 到向左，那接下來在 training 的時候
DRL_Lecture_1_-_Policy_Gradient_(Review)-288 你叫告訴 machine 說，調整 network 的參數，如果看到這個 state，你就向左
DRL_Lecture_1_-_Policy_Gradient_(Review)-289 在一般的 classification 的 problem 裡面
DRL_Lecture_1_-_Policy_Gradient_(Review)-290 其實你在 implement classification 的時候，你的 objective function
DRL_Lecture_1_-_Policy_Gradient_(Review)-291 你都會寫成，minimize cross entropy
DRL_Lecture_1_-_Policy_Gradient_(Review)-292 那其實 minimize cross entropy 就是 maximize log likelihood
DRL_Lecture_1_-_Policy_Gradient_(Review)-293 所以你今天在做 classification 的時候，你的 objective function
DRL_Lecture_1_-_Policy_Gradient_(Review)-294 你要去 maximize 或是 minimize 的對象，因為我們現在是 maximize likelihood
DRL_Lecture_1_-_Policy_Gradient_(Review)-295 所以其實是 maximize，你要 maximize 的對象，其實就長這樣子
DRL_Lecture_1_-_Policy_Gradient_(Review)-296 像這種 lost function，你在 TensorFlow 裡面，你 even 不用手刻，它都會有現成的 function 就是了，
DRL_Lecture_1_-_Policy_Gradient_(Review)-297 你就 call 個 function，它就會自動幫你算這樣子的東西
DRL_Lecture_1_-_Policy_Gradient_(Review)-298 然後接下來呢，你就 apply 計算 gradient 這件事，那你就可以把 gradient 計算出來，這是一般的分類問題
DRL_Lecture_1_-_Policy_Gradient_(Review)-299 那如果今天是 RL 的話，唯一不同的地方只是，你要記得在你原來的 loss 前面
DRL_Lecture_1_-_Policy_Gradient_(Review)-300 乘上一個 weight，這個 weight 是什麼？
DRL_Lecture_1_-_Policy_Gradient_(Review)-301 這個weight 是，今天在這個 state，採取這個 action 的時候，你會得到的 reward
DRL_Lecture_1_-_Policy_Gradient_(Review)-302 這個 reward 不是當場得到的 reward
DRL_Lecture_1_-_Policy_Gradient_(Review)-303 而是整場遊戲的時候得到的 reward，它並不是在 state s 採取 action a 的時候得到的 reward
DRL_Lecture_1_-_Policy_Gradient_(Review)-304 而是說，今天在 state s 採取 action a 的這整場遊戲裡面，你最後得到的 total reward 這個大 R
DRL_Lecture_1_-_Policy_Gradient_(Review)-305 你要把你的每一筆 training data，都 weighted by 這個大 R
DRL_Lecture_1_-_Policy_Gradient_(Review)-306 然後接下來，你就交給 TensorFlow 或 pyTorch 去幫你算 gradient，然後就結束了
DRL_Lecture_1_-_Policy_Gradient_(Review)-307 跟一般 classification 其實也沒太大的差別
DRL_Lecture_1_-_Policy_Gradient_(Review)-308 這邊有一些通常實作的時候，你也許用得上的 tip，一個就是你要 add 一個東西叫做 baseline
DRL_Lecture_1_-_Policy_Gradient_(Review)-309 所謂的 add baseline 是什麼意思呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-310 今天我們會遇到一個狀況是
DRL_Lecture_1_-_Policy_Gradient_(Review)-311 我們說這個式子，它直覺上的含意就是，假設 given state s 採取 action a
DRL_Lecture_1_-_Policy_Gradient_(Review)-312 會給你整場遊戲正面的 reward，那你就要增加它的機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-313 如果說今天在 state s 執行 action a，整場遊戲得到負的 reward，你就要減少這一項的機率
DRL_Lecture_1_-_Policy_Gradient_(Review)-314 但是我們今天很容易遇到一個問題是，很多遊戲裡面，它的 reward 總是正的
DRL_Lecture_1_-_Policy_Gradient_(Review)-315 就是說最高都是 0，比如說打乒乓球
DRL_Lecture_1_-_Policy_Gradient_(Review)-316 作業 4-1 是玩很簡單的就是碰的遊戲，你的分數就是介於 0-21 分間
DRL_Lecture_1_-_Policy_Gradient_(Review)-317 根本就不會得到負的分數，所以這個 R 總是正的
DRL_Lecture_1_-_Policy_Gradient_(Review)-318 所以假設你直接套用這個式子，你會發現說在 training 的時候
DRL_Lecture_1_-_Policy_Gradient_(Review)-319 你告訴 model 說，今天不管是什麼 action
DRL_Lecture_1_-_Policy_Gradient_(Review)-320 你都應該要把它的機率提升，這樣聽起來好像有點怪怪的
DRL_Lecture_1_-_Policy_Gradient_(Review)-321 在理想上，這麼做並不一定會有問題，為什麼呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-322 因為今天雖然說 R 總是正的
DRL_Lecture_1_-_Policy_Gradient_(Review)-323 但它正的量總是有大有小，你在玩乒乓球那個遊戲裡面，得到的 reward 總是正的
DRL_Lecture_1_-_Policy_Gradient_(Review)-324 但它是介於 0-21 分之間，所以有時候你是得到 20 分，有時候你採取某些 action 可能是得到 0 分
DRL_Lecture_1_-_Policy_Gradient_(Review)-325 採取某些 action 可能是得到 20 分
DRL_Lecture_1_-_Policy_Gradient_(Review)-326 假設你現在有 3 個 action a/b/c，可以執行，在某一個 state 有 3 個 action a/b/c，可以執行
DRL_Lecture_1_-_Policy_Gradient_(Review)-327 根據這個式子，你要把這 3 項的分數，機率，log probability 都拉高
DRL_Lecture_1_-_Policy_Gradient_(Review)-328 但是它們前面 weight 的這個 R，是不一樣的
DRL_Lecture_1_-_Policy_Gradient_(Review)-329 那麼前面 weight 的這個 R 是有大有小的，weight 小的
DRL_Lecture_1_-_Policy_Gradient_(Review)-330 它上升的就少，weight 多的，它上升的就大一點
DRL_Lecture_1_-_Policy_Gradient_(Review)-331 那因為今天這個 log probability，它是一個機率，所以，這三項的和，要是 0
DRL_Lecture_1_-_Policy_Gradient_(Review)-332 所以上升的少的，在做完 normalize 以後，它其實就是下降的，上升的多的，才會上升
DRL_Lecture_1_-_Policy_Gradient_(Review)-333 那這個是一個理想上的狀況，但是實際上，你千萬不要忘了，我們是在做 sampling
DRL_Lecture_1_-_Policy_Gradient_(Review)-334 就本來這邊應該是一個 expectation，summation over 所有可能的 s 跟 a 的 pair
DRL_Lecture_1_-_Policy_Gradient_(Review)-335 但是實際上你真正在學的時候，當然不可能是這麼做的，你只是 sample 了少量的 s 跟 a 的 pair 而已
DRL_Lecture_1_-_Policy_Gradient_(Review)-336 所以因為我們今天做的是 sampling，有一些 action 你可能從來都沒有 sample 到
DRL_Lecture_1_-_Policy_Gradient_(Review)-337 在某一個 state，雖然可以執行的 action 有 a/b/c，3 個
DRL_Lecture_1_-_Policy_Gradient_(Review)-338 但你可能只 sample 到 action b，你可能只 sample 到 action c，你沒有 sample 到 action a
DRL_Lecture_1_-_Policy_Gradient_(Review)-339 但現在所有 action 的 reward 都是正的，所以根據這個式子，今天它的每一項的機率都應該要上升
DRL_Lecture_1_-_Policy_Gradient_(Review)-340 但現在你會遇到的問題是，因為 a 沒有被 sample 到
DRL_Lecture_1_-_Policy_Gradient_(Review)-341 其它人的機率如果都要上升，那 a 的機率就下降，所以，a 可能不是一個不好的 action
DRL_Lecture_1_-_Policy_Gradient_(Review)-342 它只是沒被 sample 到，也就是運氣不好沒有被 sample 到
DRL_Lecture_1_-_Policy_Gradient_(Review)-343 但是只是因為它沒被 sample 到，它的機率就會下降，那這個顯然是有問題的
DRL_Lecture_1_-_Policy_Gradient_(Review)-344 要解決這個問題要怎麼辦呢？你會希望你的 reward 不要總是正的
DRL_Lecture_1_-_Policy_Gradient_(Review)-345 為了解決望你的 reward 不要總是正的這個問題
DRL_Lecture_1_-_Policy_Gradient_(Review)-346 你可以做的一個非常簡單的改變就是，把你的 reward 減掉一項叫做 b，這項 b 叫做 baseline
DRL_Lecture_1_-_Policy_Gradient_(Review)-347 你減掉這項 b 以後，就可以讓 R-b 小括號這一項，有正有負
DRL_Lecture_1_-_Policy_Gradient_(Review)-348 所以今天如果你得到的 reward 這個 R of tao(n)，這個 total reward 大於 b 的話
DRL_Lecture_1_-_Policy_Gradient_(Review)-349 就讓他的機率上升，如果這個 total reward 小於 b，就算它是負的，就算它是正的
DRL_Lecture_1_-_Policy_Gradient_(Review)-350 因為遊戲裡面不可能有負的，所以如果正的很小，也是不好的
DRL_Lecture_1_-_Policy_Gradient_(Review)-351 所以你就要讓這一項的機率下降
DRL_Lecture_1_-_Policy_Gradient_(Review)-352 如果今天 R of tao(n) 它小於 b 的話，你就要讓這個 state 採取這個 action 的分數下降
DRL_Lecture_1_-_Policy_Gradient_(Review)-353 那這個 b 怎麼設呢？你就隨便設，你就自己想個方法來設
DRL_Lecture_1_-_Policy_Gradient_(Review)-354 那一個最最簡單的做法就是，你把那個 tao(n) 的值，取絕對值
DRL_Lecture_1_-_Policy_Gradient_(Review)-355 喔不對，你把 tao(n) 的值，取 expectation，算一下 tao(n) 的平均值
DRL_Lecture_1_-_Policy_Gradient_(Review)-356 你就可以把它當作 b 來用，這是其中一種做法，你永遠可以想想看你有沒有其它的做法
DRL_Lecture_1_-_Policy_Gradient_(Review)-357 所以在實作上，你就是在 implement/training 的時候
DRL_Lecture_1_-_Policy_Gradient_(Review)-358 你會不斷的把 R of tao 的分數，把它不斷的記錄下來
DRL_Lecture_1_-_Policy_Gradient_(Review)-359 然後你會不斷的去計算 R of tao 的平均值，然後你會把你的這個平均值，當作你的 b 來用
DRL_Lecture_1_-_Policy_Gradient_(Review)-360 這樣就可以讓你在 training 的時候
DRL_Lecture_1_-_Policy_Gradient_(Review)-361 你今天，這個 gradient log probability 乘上前面這一項，是有正有負的，這個是第一個 tip
DRL_Lecture_1_-_Policy_Gradient_(Review)-362 第二個 tip 我們在 machine learning 那一門課沒有講到，前面那的東西在 machine learning 講過了
DRL_Lecture_1_-_Policy_Gradient_(Review)-363 今天我們要講在 machine learning 那一門課沒有講過的 tip，這個 tip 是這樣子
DRL_Lecture_1_-_Policy_Gradient_(Review)-364 今天你應該要給每一個 action，合適的 credit
DRL_Lecture_1_-_Policy_Gradient_(Review)-365 什麼意思呢，如果我們看今天下面這個式子的話
DRL_Lecture_1_-_Policy_Gradient_(Review)-366 我們原來會做的事情是，今天在某一個 state
DRL_Lecture_1_-_Policy_Gradient_(Review)-367 假設，你執行了某一個 action a
DRL_Lecture_1_-_Policy_Gradient_(Review)-368 它得到的 reward ，它前面乘上的這一項，就是 (R of tao)-b
DRL_Lecture_1_-_Policy_Gradient_(Review)-369 今天只要在同一個 Episode 裡面，在同一場遊戲裡面，所有的 state 跟 a 的 pair
DRL_Lecture_1_-_Policy_Gradient_(Review)-370 它都會 weighted by 同樣的 reward/term
DRL_Lecture_1_-_Policy_Gradient_(Review)-371 這件事情顯然是不公平的，因為在同一場遊戲裡面
DRL_Lecture_1_-_Policy_Gradient_(Review)-372 也許有些 action 是好的，也許有些 action 是不好的
DRL_Lecture_1_-_Policy_Gradient_(Review)-373 那假設最終的結果，整場遊戲的結果是好的，並不代表這個遊戲裡面每一個行為都是對的
DRL_Lecture_1_-_Policy_Gradient_(Review)-374 若是整場遊戲結果不好，但不代表遊戲裡面的所有行為都是錯的
DRL_Lecture_1_-_Policy_Gradient_(Review)-375 所以我們其實希望，可以給每一個不同的 action，前面都乘上不同的 weight
DRL_Lecture_1_-_Policy_Gradient_(Review)-376 那這個每一個 action 的不同 weight，它真正的反應了每一個 action，它到底是好還是不好
DRL_Lecture_1_-_Policy_Gradient_(Review)-377 那這邊就是舉了一個例子說，假設現在這個遊戲都很短，只會有 3-4 個互動
DRL_Lecture_1_-_Policy_Gradient_(Review)-378 在 sa 這個 state 執行 a1 這件事，得到 5 分
DRL_Lecture_1_-_Policy_Gradient_(Review)-379 在 sb 這個 state 執行 a2 這件事，得到 0 分
DRL_Lecture_1_-_Policy_Gradient_(Review)-380 在 sc 這個 state 執行 a3 這件事，得到 -2 分
DRL_Lecture_1_-_Policy_Gradient_(Review)-381 整場遊戲下來，你得到 +3 分，那今天你得到 +3 分
DRL_Lecture_1_-_Policy_Gradient_(Review)-382 代表在 state sb 執行 action a2 是好的嗎？並不見得代表 state sb 執行 a2 是好的
DRL_Lecture_1_-_Policy_Gradient_(Review)-383 因為這個正的分數，主要是來自於在一開始的時候 state sa 執行了 a1
DRL_Lecture_1_-_Policy_Gradient_(Review)-384 也許跟在 state sb 執行 a2 是沒有關係的
DRL_Lecture_1_-_Policy_Gradient_(Review)-385 也許在 state sb 執行 a2 反而是不好的，因為它導致你接下來會進入 state sc 執行 a3 被扣分
DRL_Lecture_1_-_Policy_Gradient_(Review)-386 所以今天整場遊戲得到的結果是好的，並不代表每一個行為都是對的
DRL_Lecture_1_-_Policy_Gradient_(Review)-387 如果按照我們剛才的講法，今天整場遊戲得到的分數是 3 分
DRL_Lecture_1_-_Policy_Gradient_(Review)-388 那到時候在 training 的時候，每一個 state 跟 action 的 pair，都會被乘上 +3
DRL_Lecture_1_-_Policy_Gradient_(Review)-389 在理想的狀況下，這個問題，如果你 sample 夠多
DRL_Lecture_1_-_Policy_Gradient_(Review)-390 就可以被解決，為什麼？因為假設你今天 sample 夠多
DRL_Lecture_1_-_Policy_Gradient_(Review)-391 在 state sb 執行 a2 的這件事情，被 sample 到很多次
DRL_Lecture_1_-_Policy_Gradient_(Review)-392 就某一場遊戲，在 state sb 執行 a2，你會得到 +3 分
DRL_Lecture_1_-_Policy_Gradient_(Review)-393 但在另外一場遊戲，在 state sb 執行 a2，你卻得到了 -7 分
DRL_Lecture_1_-_Policy_Gradient_(Review)-394 為什麼會得到 -7 分呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-395 因為在 state sb 執行 a2 之前，你在 state sa 執行 a2 得到 -5 分，那這 -5 分可能也不是
DRL_Lecture_1_-_Policy_Gradient_(Review)-396 中間這一項的錯，這 -5 分這件事可能也不是在 sb 執行 a2 的錯，這兩件事情，可能是沒有關係的，因為它先發生了
DRL_Lecture_1_-_Policy_Gradient_(Review)-397 這件事才發生，所以他們是沒有關係的
DRL_Lecture_1_-_Policy_Gradient_(Review)-398 在 state sb 執行 a2，它可能造成問題只有
DRL_Lecture_1_-_Policy_Gradient_(Review)-399 會在接下來 -2 分，而跟前面的 -5 分沒有關係的
DRL_Lecture_1_-_Policy_Gradient_(Review)-400 但是假設我們今天 sample 到這項的次數夠多
DRL_Lecture_1_-_Policy_Gradient_(Review)-401 把所有有發生這件事情的情況的分數通通都集合起來，那可能不是一個問題
DRL_Lecture_1_-_Policy_Gradient_(Review)-402 但現在的問題就是，我們 sample 的次數，是不夠多的
DRL_Lecture_1_-_Policy_Gradient_(Review)-403 那在 sample 的次數，不夠多的情況下，你就需要想辦法，給每一個 state 跟 action pair 合理的 credit
DRL_Lecture_1_-_Policy_Gradient_(Review)-404 你要給他合理的，你要讓大家知道它合理的 contribution，它實際上對這些分數的貢獻到底有多大
DRL_Lecture_1_-_Policy_Gradient_(Review)-405 那怎麼給它一個合理的 contribution 呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-406 一個做法是，我們今天在計算這個 pair，它真正的 reward 的時候
DRL_Lecture_1_-_Policy_Gradient_(Review)-407 不把整場遊戲得到的 reward 全部加起來
DRL_Lecture_1_-_Policy_Gradient_(Review)-408 我們只計算從這一個 action 執行以後
DRL_Lecture_1_-_Policy_Gradient_(Review)-409 所得到的 reward，因為這場遊戲在執行這個 action 之前發生的事情
DRL_Lecture_1_-_Policy_Gradient_(Review)-410 是跟執行這個 action 是沒有關係的，前面的事情都已經發生了
DRL_Lecture_1_-_Policy_Gradient_(Review)-411 那跟執行這個 action 是沒有關係的，所以在執行這個 action 之前
DRL_Lecture_1_-_Policy_Gradient_(Review)-412 得到多少 reward 都不能算是這個 action 的功勞
DRL_Lecture_1_-_Policy_Gradient_(Review)-413 跟這個 action 有關的東西，只有在執行這個 action 以後發生的所有的 reward
DRL_Lecture_1_-_Policy_Gradient_(Review)-414 把它總合起來，才是這個 action 它真正的 contribution
DRL_Lecture_1_-_Policy_Gradient_(Review)-415 才比較可能是這個 action 它真正的 contribution
DRL_Lecture_1_-_Policy_Gradient_(Review)-416 所以在這個例子裏面，在 state sb，執行 a2 這件事情
DRL_Lecture_1_-_Policy_Gradient_(Review)-417 也許它真正會導致你得到的分數，應該是 -2 分而不是 +3 分，因為前面的 +5 分
DRL_Lecture_1_-_Policy_Gradient_(Review)-418 並不是執行 a2 的功勞
DRL_Lecture_1_-_Policy_Gradient_(Review)-419 實際上執行 a2 以後，到遊戲結束前，你只有被扣 2 分而已，所以它應該是 -2
DRL_Lecture_1_-_Policy_Gradient_(Review)-420 那一樣的道理，今天執行 a2 實際上不應該是扣 7 分，因為前面扣 5 分，跟在 sb 這個 state 執行 a2 是沒有關係的
DRL_Lecture_1_-_Policy_Gradient_(Review)-421 在 sb 這個 state 執行 a2，真正導致的問題只有你接下來
DRL_Lecture_1_-_Policy_Gradient_(Review)-422 被扣兩分而已，所以也許在 sb 這個 state 執行 a2，你真正會導致的結果只有扣兩分而已
DRL_Lecture_1_-_Policy_Gradient_(Review)-423 那如果要把它寫成式子的話是什麼樣子呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-424 你本來前面的 weight，是 R  of tao，是整場遊戲的 reward 的總和
DRL_Lecture_1_-_Policy_Gradient_(Review)-425 那現在改一下，怎麼改呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-426 改成從某個時間 t 開始，假設這個 action 是在 t 這個時間點所執行的
DRL_Lecture_1_-_Policy_Gradient_(Review)-427 從 t 這個時間點，一直到遊戲結束
DRL_Lecture_1_-_Policy_Gradient_(Review)-428 所有 reward R 的總和，才真的代表這個 action，是好的，還是不好的
DRL_Lecture_1_-_Policy_Gradient_(Review)-429 這樣大家了解我的意思嗎？
DRL_Lecture_1_-_Policy_Gradient_(Review)-430 接下來再更進一步
DRL_Lecture_1_-_Policy_Gradient_(Review)-431 我們會把比較未來的 reward，做一個 discount
DRL_Lecture_1_-_Policy_Gradient_(Review)-432 為什麼我要把比較未來的 reward 做一個 discount 呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-433 因為今天雖然我們說，在某一個時間點，執行某一個 action，會影響接下來所有的結果
DRL_Lecture_1_-_Policy_Gradient_(Review)-434 有可能在某一個時間點執行的 action，接下來得到的 reward 都是這個 action 的功勞
DRL_Lecture_1_-_Policy_Gradient_(Review)-435 但是在比較真實的情況下，如果時間拖得越長，影響力就越小
DRL_Lecture_1_-_Policy_Gradient_(Review)-436 就是今天我在第二個時間點執行某一個 action
DRL_Lecture_1_-_Policy_Gradient_(Review)-437 哪我在第三個時間點得到 reward，那可能是再第二個時間點執行某個 action 的功勞
DRL_Lecture_1_-_Policy_Gradient_(Review)-438 但是在 100 個 timestamp 之後，又得到 reward，那可能就不是在第二個時間點執行某一個 action 得到的功勞
DRL_Lecture_1_-_Policy_Gradient_(Review)-439 所以我們實際上在做的時候，你會在你的 R 前面
DRL_Lecture_1_-_Policy_Gradient_(Review)-440 乘上一個 term 叫做 gamma，那 gamma 它是小於 1 的，它會設個 0.9 或 0.99
DRL_Lecture_1_-_Policy_Gradient_(Review)-441 那如果你今天的 R，它是越之後的 time stamp
DRL_Lecture_1_-_Policy_Gradient_(Review)-442 你這個 t prime 越大，這個 R 是越之後的 timestamp
DRL_Lecture_1_-_Policy_Gradient_(Review)-443 它前面就乘上越多次的 gamma
DRL_Lecture_1_-_Policy_Gradient_(Review)-444 就代表說現在在某一個 state st，執行某一個 action at 的時候
DRL_Lecture_1_-_Policy_Gradient_(Review)-445 真正它的 credit，其實是它之後，在執行這個 action 之後
DRL_Lecture_1_-_Policy_Gradient_(Review)-446 所有 reward 的總和，而且你還要乘上 gamma，這樣大家可以了解我的意思嗎？
DRL_Lecture_1_-_Policy_Gradient_(Review)-447 或是要舉一個很具體的例子的話，你就想成說，這是遊戲的第 1/2/3/4 回合
DRL_Lecture_1_-_Policy_Gradient_(Review)-448 那你在遊戲的第二回合，的某一個 st 你執行 at，它真正的 credit 到底是得到多分數呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-449 它真正的 credit 得到的分數應該是，假設你這邊得到 +1 分
DRL_Lecture_1_-_Policy_Gradient_(Review)-450 這邊得到 +3 分，這邊得到 -5 分
DRL_Lecture_1_-_Policy_Gradient_(Review)-451 它的真正的 credit，應該是 1 加上有一個 discount 的 credit 叫做 gamma
DRL_Lecture_1_-_Policy_Gradient_(Review)-452 乘上 3，再加上 gamma 的平方，乘上 -5
DRL_Lecture_1_-_Policy_Gradient_(Review)-453 以下部分因為老師聲音不清楚省略
DRL_Lecture_1_-_Policy_Gradient_(Review)-454 如果大家可以接受這樣子的話，實際上你 implement 就是這麼 implement 的
DRL_Lecture_1_-_Policy_Gradient_(Review)-455 那這個 b 呢，b 這個我們之後再講，它可以是 state-dependent 的
DRL_Lecture_1_-_Policy_Gradient_(Review)-456 事實上 b  它通常是一個 network estimate 出來的
DRL_Lecture_1_-_Policy_Gradient_(Review)-457 這還蠻複雜，它是一個 network 的 output，這個我們之後再講
DRL_Lecture_1_-_Policy_Gradient_(Review)-458 好，那把這個 R 減掉 b 這一項，這一項我們可以把它合起來
DRL_Lecture_1_-_Policy_Gradient_(Review)-459 我們統稱為 advantage function，我們這邊用 A 來代表 advantage function
DRL_Lecture_1_-_Policy_Gradient_(Review)-460 那這個 advantage function 呢，它是 dependent on s and a，我們就是要計算的是說
DRL_Lecture_1_-_Policy_Gradient_(Review)-461 在某一個 state s 採取某一個 action a 的時候，你的 advantage function 有多大
DRL_Lecture_1_-_Policy_Gradient_(Review)-462 然後這個 advantage function 它的上標是 theta，theta 是什麼意思呢？
DRL_Lecture_1_-_Policy_Gradient_(Review)-463 因為你實際上在算這一項的時候，你實際上在算這個 summation 的時候
DRL_Lecture_1_-_Policy_Gradient_(Review)-464 你會需要有一個 interaction 的結果嘛，對不對，你會需要有一個 model 去跟環境做 interaction，你才知道你接下來得到的 reward 會有多少
DRL_Lecture_1_-_Policy_Gradient_(Review)-465 而這個 theta 就是代表說，現在是用 theta 這個 model，跟環境去做 interaction
DRL_Lecture_1_-_Policy_Gradient_(Review)-466 然後你才計算出這一項，從時間 t 開始到遊戲結束為止，所有 R 的 summation
DRL_Lecture_1_-_Policy_Gradient_(Review)-467 把這一項減掉 b，然後這個就叫 advantage function
DRL_Lecture_1_-_Policy_Gradient_(Review)-468 那它的意義就是，現在假設，我們在某一個 state st，執行某一個 action at
DRL_Lecture_1_-_Policy_Gradient_(Review)-469 相較於其他可能的 action，它有多好
DRL_Lecture_1_-_Policy_Gradient_(Review)-470 它真正在意的不是一個絕對的好，
DRL_Lecture_1_-_Policy_Gradient_(Review)-471 而是說在同樣的 state 的時候
DRL_Lecture_1_-_Policy_Gradient_(Review)-472 是採取某一個 action at，相較於其它的 action
DRL_Lecture_1_-_Policy_Gradient_(Review)-473 它有多好，它是相對的好，不是絕對好，那麼今天會減掉一個 b 嘛
DRL_Lecture_1_-_Policy_Gradient_(Review)-474 減掉一個 baseline，所以這個東西是相對的好，不是絕對的好
DRL_Lecture_1_-_Policy_Gradient_(Review)-475 那這個 A 我們之後再講，它通常可以是由一個 network estimate 出來的
DRL_Lecture_1_-_Policy_Gradient_(Review)-476 那這個 network 叫做 critic，我們講到 Actor-Critic 的方法的時候，再講這件事情
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-0 今天的規劃是這樣，我們先講 PPO
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-1 這個是作業 4-1 用的到的東西，接下來講 Q-learning
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-2 這是作業 4-2 用到的東西，然後最後助教會來講一下作業 4-2
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-3 那上週我們做的事情，是複習了一下 policy gradient
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-4 那我們今天要講的事情，是 policy gradient 的一個變形
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-5 就是 PPO，那上周已經講過 PPO 是現在 OpenAI default reinforcement learning 的 algorithm
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-6 那在講 PPO 之前呢
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-7 我們要講 on-policy and off-policy 這兩種 training 方法的區別
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-8 那什麼是 on-policy 什麼是 off-policy 呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-9 我們知道在 reinforcement learning 裡面，我們要 learn 的就是一個 agent
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-10 那如果我們今天拿去跟環境互動的那個 agent
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-11 跟我們要 learn 的 agent 是同一個的話，這個叫做 on-policy
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-12 如果我們今天要 learn 的 agent
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-13 跟和環境互動的 agent 不是同一個的話，那這個叫做 off-policy
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-14 就如果比較擬人化的講法就是
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-15 如果今天要學習的那個 agent，它是一邊跟環境互動，一邊做學習
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-16 這個叫 on-policy
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-17 果它是在旁邊看別人玩，透過看別人玩，來學習的話，這個叫做 off-policy
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-18 為什麼我們會想要考慮 off-policy 這樣的選項呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-19 讓我們來想想看我們已經講過的 policy gradient
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-20 其實我們之前講的 policy gradient，它是 on-policy 還是 off-policy 的做法呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-21 它是 on-policy 的做法，為什麼？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-22 我們之前講說，在做 policy gradient 的時候呢
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-23 我們會需要有一個 agent，我們會需要有一個 policy
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-24 我們會需要有一個 actor，這個 actor 先去跟環境互動
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-25 去蒐集資料，蒐集很多的 tao
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-26 那根據它蒐集到的資料，會按照這個 policy gradient 的式子
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-27 去 update 你 policy 的參數，這個就是我們之前講過的 policy gradient
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-28 所以它是一個 on-policy 的 algorithm
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-29 你拿去跟環境做互動的那個 policy，跟你要 learn 的 policy 是同一個
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-30 那今天的問題是
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-31 我們之前有講過說，因為在這個 update 的式子裡面
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-32 其中有一項，你的這個 expectation，應該是對你現在的 policy theta
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-33 所 sample 出來的 trajectory tao，做 expectation
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-34 所以當你今天 update 參數以後，一旦你 update 了參數
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-35 從 theta 變成 theta prime，那這一個機率，就不對了
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-36 之前 sample 出來的 data，就變的不能用了
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-37 所以我們之前就有講過說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-38 policy gradient 是一個會花很多時間來 sample data algorithm
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-39 你會發現大多數時間都在 sample data，跟你的 agent 去跟環境做互動以後
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-40 接下來，就要 update 參數
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-41 每次 update 完參數一次，你只能 update 參數一次
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-42 你只能做一次 gradient decent，你只能 update 參數一次
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-43 接下來你就要重新再去 collect data，然後才能再次 update 參數
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-44 這顯然是非常花時間的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-45 所以我們現在想要從 on-policy 變成 off-policy 的好處就是
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-46 我們希望說現在我們可以用另外一個 policy，另外一個 actor theta prime 去跟環境做互動
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-47 用 theta prime collect 到的 data 去訓練 theta
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-48 假設我們可以用 theta prime collect 到的 data 去訓練 theta，意味著說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-49 我們可以把 theta prime collect 到的 data 用非常多次
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-50 你在做 gradient decent 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-51 其實是 gradient ascent，因為我要 optimize 某個數值，所以是 gradient ascent
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-52 在做 gradient ascent 的時候，我們可以執行那個 gradient ascent 好幾次
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-53 我們可以 update 參數好幾次，都只要用同一筆 data 就好了
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-54 因為假設現在 theta 有能力從另外一個 actor theta prime
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-55 它所 sample 出來的 data 來學習的話，那 theta prime 就只要 sample 一次
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-56 也許 sample 多一點的 data，讓 theta 去 update 很多次，這樣就會比較有效率
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-57 所以怎麼做呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-58 這邊就需要介紹一個 important sampling 的概念
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-59 那這個 important sampling 的概念不是只能用在 RL 上
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-60 它是一個 general 的想法，可以用在其他很多地方
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-61 我們先介紹這個 general 的想法
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-62 那假設現在你有一個 function f of x，那你要計算
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-63 從 t 這個 distribution
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-64 sample x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-65 再把 x 帶到，f 裡面，得到 f of x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-66 你要計算這個 f of x 的期望值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-67 那怎麼做呢？假設你今天沒有辦法對 p 這個 distribution 做積分的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-68 那你可以從 p 這個 distribution 去 sample 一些 data x(i)
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-69 那這一個期望值，這個 f of x 的期望值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-70 就等同是你 sample 到的 x(i)，把 x(i) 帶到 f of x 裡面
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-71 然後取它的平均值，就可以拿來近似這個期望值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-72 假設你知道怎麼從 p 這個 distribution 做 sample 的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-73 你要算這個期望值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-74 你只需要從 p 這個 distribution，做 sample 就好了
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-75 但是我們現在有另外一個問題
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-76 那等一下我們會更清楚知道說為什麼會有這樣的問題
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-77 我們現在的問題是這樣，我們沒有辦法從 p 這個 distribution 裡面
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-78 sample data
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-79 假設我們不能從 p sample data
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-80 我們只能從另外一個 distribution q 去 sample data
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-81 q 這個 distribution 可以是任何 distribution，不管它是怎麼樣的 distribution
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-82 在多數情況下，等一下討論的情況都成立
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-83 我們不能夠從 p 去 sample data
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-84 但我們可以從 q 去 sample x(i)
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-85 但我們從 q 去 sample x(i)，我們不能直接套這個式子
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-86 因為這邊是假設你的 x(i) 都是從 p sample 出來的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-87 你才能夠套這個式子
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-88 從 q sample 出來的 x(i) 套這個式
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-89 你也不會等於左邊這項期望值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-90 所以怎麼辦，做一個修正，這個修正是這樣子的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-91 期望值這一項，其實就是積分 f of x 乘上 p of x, dx
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-92 然後我們現在上下都同乘 q of x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-93 上下同乘 q of x 不會改變任何事
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-94 但是我們可以把這一個式子
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-95 寫成對 q 裡面所 sample 出來的 x，取期望值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-96 我們從 q 裡面，sample x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-97 然後再去計算 f of x 乘上 p of x 除以 q of x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-98 再去取期望值，然後左邊這一項，會等於右邊這一項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-99 要算左邊這一項，你要從 p 這個 distribution sample x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-100 但是要算右邊這一項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-101 你不是從 p 這個 distribution sample x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-102 你是從 q 這個 distribution sample x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-103 你從 q 這個 distribution sample x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-104 sample 出來之後，再帶入 f of x 乘上 p of x, q of x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-105 接下來你就可以計算左邊這項你想要算的期望值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-106 所以就算是我們不能從 p 裡面去 sample data
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-107 你想要計算這一項的期望值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-108 也是沒有問題的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-109 你只要能夠從 q 裡面去 sample data
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-110 可以帶這個式子，你就一樣可以計算
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-111 從 p 這個 distribution sample x 帶入 f 以後所算出來的期望值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-112 那這個兩個式子唯一不同的地方是說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-113 這邊是從 x 做 sample
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-114 這邊是從 q 做 sample
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-115 因為他是從 q 裡做 sample
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-116 所以 sample 出來的每一筆 data
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-117 你需要乘上一個 weight
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-118 修正這兩個 distribution 的差異
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-119 而這個 weight 就是 p of x 的值除以 q of x 的值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-120 所以 q of x 它是任何 distribution 都可以，這邊唯一的限制就是
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-121 你不能夠說 q 的機率是 0 的時候，p 的機率不為 0
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-122 不然這樣會沒有定義
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-123 假設 q 的機率是 0 的時候，p 的機率也都是 0 的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-124 那這樣 p 除以 q 是有定義的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-125 所以這個時候你就可以 apply important sampling 這個技巧
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-126 所以你就可以本來是從 p 做 sample，換成從 q 做 sample
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-127 這個跟我們剛才講得有什麼關係呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-128 剛才講的從 on-policy 變成 off-policy，有什麼關係呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-129 在繼續講之前，我們來看一下 important sampling 的 issue
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-130 雖然理論上你可以把 p 換成任何的 q
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-131 但是在實作上，並沒有那麼容易
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-132 實作上 p and q 還是不能夠差太多，如果差太多的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-133 會有一些問題
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-134 什麼樣的問題呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-135 雖然我們知道說，左邊這個式子，等於右邊這個式子
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-136 但你要不要想想看，如果今天左邊這個是 f of x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-137 它的期望值 distribution 是 p
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-138 這邊是 f of x 乘以 p 除以 q
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-139 的期望值，它的 distribution 是 q
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-140 我們現在如果不是算期望值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-141 而是算 various 的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-142 這兩個 various 你覺得它會一樣嗎？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-143 多數同學都覺得他們不會一樣，沒錯，它們確實是不一樣的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-144 兩個 random variable 它的 mean 一樣，並不代表它的 various 一樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-145 對不對，所以你今天可以實際算一下
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-146 f of x 這個 random variable，跟 f of x 乘以 p of x 除以 q of x，這個 random variable
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-147 他們的這個 various 是不是一樣的，可以計算一下，那我們知道 various of x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-148 這是個公式啊，various of x 是 x^2 的 expectation 減掉 (E of x)^2
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-149 那你就帶一下這個式子，所以今天這一項的 various
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-150 是這個樣子，這一項的 various
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-151 就套一下公式，寫成這樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-152 然後今天這一項啊
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-153 你其實是，可以做一些整理的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-154 這邊有一個 x^2
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-155 這邊有一個 p^2
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-156 這邊有一個 q^2
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-157 這邊又有一個 q
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-158 所以這邊有一個 f of x 的平方
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-159 然後有一個 p of x 的平方
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-160 有一個 q of x 的平方
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-161 但是前面呢
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-162 是對 q 取 expectation
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-163 所以 q 的 distribution 取 expectation
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-164 所以如果你要算積分的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-165 你就會把這個 q 呢
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-166 乘到前面去
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-167 算期望值其實是這樣算的，然後 q 就可以消掉了
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-168 然後你可以把這個 p 拆成兩項，然後就會變成是對 p 呢，取期望值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-169 那這個是左邊這一項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-170 那右邊這一項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-171 這一項，其實就寫在這邊
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-172 所以這一項其實就是這一項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-173 然後你把它平方就變成這樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-174 所以如果你比較這一項跟這一項的話，他們的差別在那裡，他們的差別在第一項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-175 是不同的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-176 第一項這邊多乘了 p 除以 q
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-177 這邊沒有乘 p 除以 q，這邊多乘了 p 除以 q
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-178 如果 p 除以 q 差距很大的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-179 這個時候，這一項的 various
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-180 就會很大
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-181 所以雖然理論上它的 expectation 一樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-182 也就是說，你只要對 p 這個 distribution sample  夠多次
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-183 q 這個 distribution sample  夠多次
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-184 你得到的結果會是一樣的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-185 但是假設你 sample 的次數不夠多
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-186 因為它們的 various 差距是很大的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-187 所以你就有可能得到非常大的差別
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-188 如果這個地方你聽的不是很懂的話，那沒有關係，這邊就是舉一個具體的例子告訴你說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-189 當 p and q 差距很大的時候，會發生什麼樣的問題
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-190 假設這個是 p 的 distribution
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-191 這個是 q 的 distribution
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-192 這個是 f of x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-193 那如果我們要計算 f of x 的期望值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-194 它的 distribution 是從 p 這個 distribution 做 sample
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-195 那顯然這一項是負的，對不對
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-196 因為 f of x 在這個區域
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-197 這個區域 p of x 的機率很高，所以要 sample 的話，都會 sample 到這個地方
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-198 而 f of x 在這個區域是負的，所以理論上這一項算出來會是負的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-199 接下來我們改成從 q 這邊做 sample
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-200 那因為 q 在右邊這邊的機率比較高
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-201 所以如果你 sample 的點不夠的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-202 那你可能都只 sample 到右側
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-203 如果你都只 sample 到右側的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-204 你會發現說，如果只 sample 到右側的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-205 算起來右邊這一項，搞不好還應該是正的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-206 對不對，你這邊 sample 到這些點
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-207 然後你去計算它們的 f of x p 除以 q
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-208 都是正的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-209 所以你 sample 到這些點
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-210 都是正的，所以你取期望值以後，也都是正的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-211 那為什麼會這樣，那是因為你 sample 的次數不夠多
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-212 因為假設你 sample 次數很少，你只能 sample 到右邊這邊
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-213 左邊這邊雖然機率很低，但也不是沒有可能
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-214 被 sample 到，假設你今天好不容易 sample 到左邊的點
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-215 因為左邊的點，p and q 是差很多的，這邊 p 很小，q 很大
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-216 今天 f of x 好不容易終於 sample 到一個負的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-217 這個負的就會被乘上一個非常巨大的 weight
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-218 就可以平衡掉剛才那邊
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-219 一直 sample 到 positive 的 value 的情況
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-220 eventually，你就可以算出這一項的期望值，終究還是負的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-221 但問題就是，這個前提是你要 sample 夠多次
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-222 這件事情才會發生，但有可能 sample 不夠
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-223 左式跟右式就有可能有很大的差距
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-224 所以這是 importance sampling 的問題
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-225 現在要做的事情就是，把 importance sampling 這件事
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-226 用在 off-policy 的 case
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-227 我要把 on-policy training 的 algorithm
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-228 改成 off-policy training 的 algorithm
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-229 那怎麼改呢
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-230 之前我們是看
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-231 我們是拿 theta 這個 policy
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-232 去跟環境做互動，sample 出 trajectory tao
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-233 然後計算中括號裡面這一項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-234 現在我們不根據 theta
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-235 我們不用 theta 去跟環境做互動
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-236 我們假設有另外一個 policy
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-237 另外一個 policy 它的參數
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-238 theta prime，它就是另外一個 actor
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-239 它的工作是他要去做 demonstration
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-240 它要去示範給你看
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-241 這個 theta prime 它的工作是要去示範給 theta 看
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-242 它去跟環境做互動，告訴
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-243 theta 說，它跟環境做互動會發生什麼事
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-244 然後，藉此來訓練 theta
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-245 我們要訓練的是 theta 這個 model
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-246 theta prime 只是負責做 demo 負責跟環境做互動
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-247 我們現在的 tao
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-248 它是從 theta prime sample 出來的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-249 不是從 theta sample 出來的，但我們本來要求的式子是這樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-250 但是我們實際上做的時候，是拿 theta prime 去跟環境做互動，所以 sample 出來的 tao
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-251 是從 theta prime sample 出來的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-252 這兩個 distribution 不一樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-253 但沒有關係，我們之前講過說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-254 假設你本來是從 p 做 sample
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-255 但你發現你不能夠從 p 做 sample
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-256 所以現在我們說我們不拿 theta 去跟環境做互動
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-257 所以不能跟 p 做 sample
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-258 你永遠可以把 p 換成另外一個 q
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-259 然後在後面這邊補上一個 importance weight
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-260 所以現在的狀況就是一樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-261 把 theta 換成 theta prime 以後
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-262 要在中括號裡面補上一個 importance weight
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-263 這個 importance weight 就是某一個 trajectory tao
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-264 它用 theta 算出來的機率
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-265 除以這個 trajectory tao
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-266 用 theta prime 算出來的機率
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-267 那這一項是很重要的，因為
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-268 今天你要 learn 的是 actor theta and theta prime 是不太一樣的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-269 theta prime 會遇到的狀況，會見到的情形
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-270 跟 theta 見到的情形，不見得是一樣的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-271 所以中間要做一個修正的項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-272 所以我們做了一下修正
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-273 現在的 data 不是從 theta prime sample 出來的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-274 是從 theta sample 出來的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-275 那我們從 theta 換成 theta prime 有什麼好處呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-276 我們剛才就講過說，因為現在跟環境做互動是 theta prime 而不是 theta
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-277 所以你今天 sample 出來的東西
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-278 跟 theta 本身是沒有關係的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-279 所以你就可以讓 theta prime 做互動 sample 一大堆的 data 以後
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-280 theta 可以 update 參數很多次
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-281 然後一直到 theta 可能 train 到一定的程度
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-282 update 很多次以後，theta prime 再重新去做 sample
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-283 這就是 on-policy 換成 off-policy 的妙用
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-284 那我們其實上周有講過說呢
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-285 實際上我們在做 policy gradient 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-286 我們並不是給一整個 trajectory tao 都一樣的分數
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-287 而是每一個 state/action 的 pair
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-288 我們會分開來計算
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-289 所以我們上周其實有講過說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-290 我們實際上 update 我們的 gradient 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-291 我們的式子是長這樣子的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-292 我們用 theta 這個 actor 去 sample 出 st 跟 at
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-293 sample 出 state 跟 action 的 pair
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-294 我們會計算這個 state 跟 action pair 它的 advantage，就是它有多好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-295 那我們上周有講過說，這一項呢
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-296 就是那個 accumulated 的 reward 減掉 bias
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-297 那如果你忘記的話也沒有關係，反正這一項就是估測出來的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-298 它要估測的是，現在在 state st 採取 action at
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-299 它是好的，還是不好的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-300 那接下來後面會乘上這個 log p(theta) at given st
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-301 也就是說如果這一項是正的，就要增加機率，這一項是負的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-302 就要減少機率
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-303 那我們現在用了 importance sampling 的技術把 on-policy 變成 off-policy
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-304 就從 theta 變成 theta prime，所以現在 st/at 它不是 theta 跟環境互動以後所 sample 到的 data
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-305 它是 theta prime，另外一個 actor 跟環境互動以後
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-306 所 sample 到的 data，但是拿來訓練我們要調整參數的那個 model theta
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-307 但是我們有講過說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-308 因為 theta prime 跟 theta 是不同的 model，所以你要做一個修正的項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-309 那這項修正的項，就是用 importance sampling 的技術
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-310 把 st/at 用 theta sample 出來的機率
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-311 除掉 st/at 用 theta prime sample 出來的機率
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-312 那這邊其實有一件事情我們需要稍微注意一下
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-313 這邊  A 有一個上標 theta 代表說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-314 這個是 actor theta 跟環境互動的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-315 所計算出來的 A
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-316 但是實際上我們今天從 theta 換到 theta prime 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-317 這一項，你其實應該改成 theta prime
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-318 而不是 theta，為什麼？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-319 這個 A 這一項是怎麼來的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-320 A 這一項是想要估測說現在在某一個 state
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-321 採取某一個 action 接下來
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-322 會得到 accumulated reward 的值減掉 base line
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-323 對不對，我們上周有講過，你怎麼估這一項 advantage，你就會看
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-324 在這個 state st，採取這個 action at
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-325 接下來會得到的 reward 的總和
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-326 再減掉 baseline，就是這一項，這上周有講過
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-327 之前是 theta 在跟環境做互動
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-328 所以你觀察到的是 theta 可以得到的 reward
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-329 但現在不是 theta 跟環境做互動，現在是 theta prime 在跟環境做互動
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-330 所以你得到的這個 advantage，其實是根據 theta prime 所 estimate 出來的 advantage
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-331 但我們現在先不要管那麼多，我們就假設這兩項可能是差不多的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-332 那接下來，st/at，這一件事情
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-333 你可以拆解成 st 的機率乘上 at given st 的機率
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-334 然後接下來這邊需要做一件事情是
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-335 我們假設當你的 model 是 theta 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-336 你看到 st 的機率
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-337 跟你的 model 是 theta prime 的時候，你看到 st 的機率
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-338 是差不多的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-339 你把它刪掉，因為它們是一樣的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-340 所以你可以把它刪掉，為什麼可以假設它是差不多的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-341 當然你可以找一些理由，舉例來說，會看到什麼 state
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-342 往往跟你會採取什麼樣的 action 是沒有太大的關係的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-343 比如說你玩不同的 Atari 的遊戲
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-344 那其實你看到的遊戲畫面都是差不多的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-345 所以也許不同的 theta 對 st 是沒有影響的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-346 但是有一個更直覺的理由就是
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-347 這一項到時候真的要你算，你會算嗎？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-348 你不覺得這項你不太能算嗎？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-349 因為想想看這項要怎麼算，這一項你還要說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-350 我有一個參數 theta，然後拿 theta 去跟環境做互動
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-351 算 st 出現的機率，這個你根本很難算
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-352 尤其是你如果 input 是 image 的話，同樣的 st 根本就不會出現第二次
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-353 所以你根本沒有辦法估這一項，所以乾脆就無視這個問題
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-354 但是 given st，接下來產生 at 這個 機率
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-355 你是會算的，這個很好算
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-356 你手上有 theta 這個參數，它就是個 network
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-357 你就把 st 帶進去，st 就是遊戲畫面，你ˋ把遊戲畫面帶進去
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-358 它就會告訴你某一個 state 的 at 機率是多少
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-359 對不對，我們說，我們其實有個 policy 的 network
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-360 把 st 帶進去，它會告訴我們每一個 at 的機率是多少
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-361 所以這一項你只要知道 theta 的參數，知道 theta prime 的參數，這個就可以算
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-362 但這一項，其實不太好算，所以你就說服自己，其實這一項不太會有影響，我們只管前面這個部分就好了
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-363 那所以現在我們得到一個新的 objective function，我們得到一個新的 objective function
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-364 這一項是 gradient，其實我們可以從 gradient 去反推原來的 objective function
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-365 怎麼從 gradient 去反推原來的 objective function 呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-366 這邊有一個公式，我們就背下來，f of x 的 gradient
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-367 等於 f of x 乘上 log f of x 的 gradient
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-368 這一項是個樣子的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-369 前面有 p theta 對不對
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-370 然後下面有一個 p theta prime
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-371 然後這邊有一個 function A，那邊呢有一個 gradient log p theta 這樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-372 然後我們看一下這邊，我們說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-373 這個 f of x 乘以 gradient log f of x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-374 p theta 就當作 f of x，這個是 f of x 這個是 gradient log f of x
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-375 這兩項合起來，就可以變成 gradient f of x，所以變成 gradient p theta
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-376 然後接下來我們要做的事情就是
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-377 這個是 gradient 的項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-378 我們要還原說原來沒有取 gradient 的樣子
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-379 那是什麼樣子？其實就是把這個 gradient 拿掉
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-380 就變成下面這個式子
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-381 所以實際上，當我們 apply importance sampling 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-382 我們要去 optimize 的那一個 objective function 長什麼樣子呢
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-383 我們要去 optimize 的那一個 objective function 就長這樣子，我們把它寫作
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-384 J(theta prime) follow theta，為什麼要這麼麻煩寫 J(theta prime) follow theta 呢
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-385 這個括號裡面那個 theta 代表我們要去 optimize 的那個參數
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-386 theta prime 代表什麼，theta prime 是說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-387 我們拿 theta prime 去做 demonstration
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-388 就是現在真正在跟環境互動的是 theta prime
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-389 因為 theta 是不跟環境做互動，是 theta prime 在跟環境互動
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-390 然後你用 theta prime 去跟環境做互動，sample 出 st/at 以後
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-391 那你要去計算 st 跟 at 的 advantage
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-392 然後你再去把它乘上 p(theta) at given st，再除掉 p(theta prime) at given st
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-393 那這兩項都是好算的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-394 這一項你是可以從 data 裡面
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-395 你可以從這個 sample 的結果裡面，去估測出來的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-396 所以這一整項，你是可以算的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-397 那我們實際上在 update 參數的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-398 就是按照上面這個式子 update 參數
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-399 現在我們做的事情，我們可以把 on-policy 換成 off-policy
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-400 但是我們會遇到的問題是
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-401 我們在前面講 importance sampling 的時候，我們說 importance sampling 有一個 issue
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-402 這個 issue 是什麼呢？其實你的 p theta 跟 p theta prime 不能差太多
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-403 差太多的話，importance sampling 結果就會不好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-404 如果 p theta 跟 p theta prime 差太多的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-405 你的這兩個 distribution 差太多的話，importance sampling 的結果就會不好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-406 所以怎麼避免它差太多呢？這個就是 PPO 在做的事情
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-407 PPO 你雖然你看它原始的 paper 或你看 PPO 的前身
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-408 TRPO 原始的 paper 的話，它裡面寫了很多的數學式
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-409 但它實際上做的事情式怎麼樣呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-410 它實際上做的事情就是這樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-411 我們原來在 off-policy 的方法裡面說，我們要 optimize 的是這個 objective function
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-412 但是我們又說這個 objective function 又牽涉到 importance sampling
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-413 在做 importance sampling 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-414 p theta 不能跟 p theta prime 差太多
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-415 你做 demonstration 的 model 不能夠跟真正的 model 差太多，差太多的話 importance sampling 的結果就會不好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-416 我們在 training 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-417 多加一個 constrain
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-418 這個 constrain 是什麼？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-419 這個 constrain 是 theta 跟 theta prime
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-420 這兩個 model 它們 output 的 action 的 KL diversions
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-421 就是簡單來說，這一項的意思就是要衡量說 theta 跟 theta prime 有多像
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-422 然後我們希望，在 training 的過程中
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-423 我們 learn 出來的 theta 跟 theta prime 越像越好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-424 因為 theta 如果跟 theta prime 不像的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-425 最後你做出來的結果，就會不好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-426 所以在 PPO 裡面呢
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-427 有兩個式子，一方面就是 optimize 你要得到的你本來要 optimize 的東西
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-428 但是再加一個 constrain
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-429 這個 constrain 就好像那個 regularization 的 term 一樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-430 就好像我們在做 machine learning 的時候不是有 L1/L2 的 regularization
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-431 這一項也很像 regularization，這樣 regularization 做的事情就是希望最後 learn 出來的 theta
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-432 不要跟 theta prime 太不一樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-433 那 PPO 有一個前身叫做 TRPO
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-434 TRPO 寫的式子是這個樣子的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-435 這一項，跟前面這一項是一樣的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-436 它唯一不一樣的地方是說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-437 這一個 constrain 擺的位置不一樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-438 這邊是直接把 constrain 放到你要 optimize 的那個式子裡面
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-439 然後接下來你就可以用 gradient ascent 的方法去 maximize 這個式子
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-440 但是如果是在 TRPO 的話，它是把 KL diversions 當作 constrain
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-441 他希望 theta 跟 theta prime 的 KL diversions
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-442 小於一個 delta
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-443 那你知道你在做那種 optimization
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-444 如果你是用 gradient based optimization 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-445 有 constrain 是很難處理的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-446 所以你會發現 PPO 上次助教有講很多，我相信大家應該都沒有聽懂
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-447 那個是很難處理的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-448 就是因為它是把這一個 KL diversions constrain 當做一個額外的 constrain
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-449 沒有放 objective 裡面，所以它很難算
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-450 所以如果你不想搬石頭砸自己的腳的話，你就用 PPO 不要用 TRPO
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-451 看文獻上的結果是，PPO 跟 TRPO 可能 performance 差不多，但是 PPO 在實作上，比 TRPO 容易的多
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-452 那這邊要注意一下，所謂的 KL diversions
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-453 到底指的是什麼？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-454 這邊我是直接把 KL diversions 當做一個 function，它吃的 input 是 theta 跟 theta prime
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-455 但我的意思並不是說把 theta/theta prime 當做一個 distribution
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-456 算這兩個 distribution 之間的距離，我不是這個意思
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-457 今天這個所謂的 theta 跟 theta prime 的距離
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-458 並不是參數上的距離，而是他們 behavior 上的距離
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-459 我不知道大家可不可以了解這中間的差異
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-460 就是假設你現在有一個 model
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-461 有一個 actor 它是 theta
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-462 你有另外一個 actor 它的參數是 theta prime
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-463 所謂參數上的距離就是你算這兩組參數有多像
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-464 我今天所講的不是參數上的距離，我今天所講的是它們行為上的距離
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-465 就是你先帶進去一個 state s
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-466 然後它不是會 output 一個 distribution 嗎？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-467 它會對這個 action 的 space output 一個 distribution
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-468 假設你有 3 個 actions
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-469 3 個可能 actions 就 output 3 個值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-470 那我們今天所指的 distance 是 behavior distance
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-471 也就是說，給同樣的 state 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-472 他們 output 的這個 action，這個 action 之間的差距
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-473 這兩個 actions 的 distribution 他們都是一個機率分布嘛
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-474 所以就可以計算這兩個機率分布的 KL diversions
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-475 把不同的 state 它們 output 的這兩個 distribution 的 KL diversions，平均起來
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-476 才是我這邊所指的這兩個 actor 間的 KL diversions
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-477 不知道大家聽不聽得懂我的意思
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-478 那你可能說那怎麼不直接算這個 theta 或 theta prime 之間的距離
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-479 甚至不要用 KL diversions 算
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-480 L1 跟 L2 的 node 也可以保證 theta 跟 theta prime 很接近啊
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-481 在做 reinforcement learning 的時候，之所以我們考慮的不是參數上的距離
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-482 而是 action 上的距離，是因為很有可能對 actor 來說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-483 參數的變化跟 action 的變化，不一定是完全一致的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-484 就有時候你參數小小變了一下，它可能 output 的行為就差很多
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-485 或是參數變很多，但 output 的行為可能沒什麼改變
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-486 所以我們真正在意的是這個 actor 它的行為上的差距
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-487 而不是它們參數上的差距
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-488 所以這裡要注意一下，在做 PPO 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-489 所謂的 KL diversions 並不是參數的距離
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-490 而是 action 的距離
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-491 那假設這個你沒有聽得很懂的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-492 我們等一下還有一個 PPO2，我們這個是 PPO1
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-493 它還是略為複雜的，我們來看一下 PPO1 的 algorithm
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-494 它就是這樣，它說，initial 一個 policy 的參數，theta(0)
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-495 然後在每一個 iteration 裡面呢，你要用
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-496 參數 theta(k)，theta(k) 怎麼來的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-497 那 theta(k) 就是你在前一個 training 的 iteration
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-498 得到的 actor 的參數，你用 theta(k) 去跟環境做互動
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-499 sample 到一大堆 state/action 的 pair
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-500 然後你根據 theta(k) 互動的結果，你也要估測一下
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-501 st 跟 at 這個 state/action pair它的 advantage
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-502 然後接下來，你就 apply PPO 的 optimization 的 formulation
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-503 但是跟原來的 policy gradient 不一樣，原來的 policy gradient 你只能 update 一次參數，update 完以後
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-504 你就要重新 sample data
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-505 但是現在不用，你拿 theta(k) 去跟環境做互動，sample 到這組 data 以後
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-506 你就努力去測 theta，你可以讓 theta update 很多次
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-507 想辦法去 maximize 你的 objective function
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-508 你讓 theta update 很多次
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-509 這邊 theta update 很多次沒有關係
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-510 因為我們已經有做 importance sampling
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-511 所以這些 experience，這些 state/action 的 pair 是從 theta(k) sample 出來的沒有關係
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-512 theta 可以 update 很多次，它跟 theta(k) 變得不太一樣也沒有關係，你還是可以照樣訓練 theta，那其實就說完了
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-513 在 PPO 的 paper 裡面
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-514 這邊還有一個 adaptive 的 KL diversions
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-515 因為這邊會遇到一個問題就是
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-516 這個 beta 要設多少
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-517 它就跟那個 regularization 一樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-518 regularization 前面也要乘一個 weight
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-519 所以這個 KL diversions 前面也要乘一個 weight，但是 beta 要設多少呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-520 所以有個動態調整 beta 的方法，這個調整方法也是蠻直觀的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-521 在這個直觀的方法裡面呢
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-522 你先設一個 KL diversions，你可以接受的最大值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-523 然後假設你發現說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-524 你 optimize 完這個式子以後
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-525 KL diversions 的項太大
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-526 那就代表說後面這個 penalize 的 term 沒有發揮作用
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-527 那就把 beta 調大
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-528 那另外你定一個 KL diversions 的最小值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-529 而且發現 optimize 完上面這個式子以後，ok
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-530 你得到 KL diversions 比最小值還要小
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-531 那代表後面這一項它的效果太強了
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-532 那你怕它都只弄後面這一項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-533 那 theta 跟 theta(k) 都一樣，這不是你要的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-534 所以你這個時候你叫要減少 beta
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-535 所以這個 beta 是可以動態調整的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-536 這個叫做 adaptive 的 KL penalty
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-537 所以實際上你在算這個 KL 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-538 也是有點麻煩的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-539 什麼有點麻煩呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-540 因為這個 KL 理論上你應該 sample 一大堆不同的 set
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-541 以下部分省略因為收音關係
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-542 如果你覺得這個很複雜，為什麼還要算 KL diversions 很複雜
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-543 有一個 PPO2，PPO2 它的式子我們就寫在這邊
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-544 要去 maximize 的 objective function 寫成這樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-545 它的式子裡面就沒有什麼 KL 了
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-546 這個式子看起來有點複雜，但實際 implement 就很簡單
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-547 我們來實際看一下說這個式子到底是什麼意思
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-548 這式子很複雜喔
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-549 這邊是 summation over state/action 的 pair，那沒有問題
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-550 這個是 minimize
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-551 就是說這邊有個大括號
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-552 這邊有個括號，這邊有個括號，這個括號裡面有兩項，這是第一項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-553 這個是第二項，min 這個 operator 做的事情是
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-554 第一項跟第二項裡面選比較小的那個
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-555 第一項比較單純，第二項比較複雜，第二項前面有個 clip function
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-556 clip 這個 function 是什麼意思呢？clip 這個function 的意思是說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-557 在括號裡面有 3 項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-558 如果第一項小於第二項的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-559 那就 output 1-epsilon
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-560 第一項如果大於第三項的話，那就 output 1+epsilon
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-561 那 epsilon 是一個 hyper parameter，你要 tune 的，比如說你就設 0.1 啊，設 0.2 啊
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-562 也就是說，假設這邊設 0.2 的話，就是說這個值如果算出來小於 0.8，那就當作 0.8
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-563 這個值如果算出來大於 1.2，那就當作 1.2
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-564 這個式子到底是什麼意思呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-565 我們先來解釋一下，我們來看一下這個，第二項這個就好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-566 我們先來看第二項這個算出來到底是什麼的東西
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-567 第二項這項算出來的意思是這樣
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-568 假設這個橫軸是 p(theta) 除以 p(theta k)
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-569 橫軸是第一項 p(theta) 除以 p(theta k)
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-570 縱軸是 clip 這個 function 它實際的輸出，那我們剛才講過說
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-571 如果 p(theta) 除以 p(theta k) 大於 1+epsilon
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-572 它輸出就是 1+epsilon
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-573 如果小於 1-epsilon 它輸出就是 1-epsilon
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-574 如果介於 1+epsilon 跟 1-epsilon 之間，就是輸入等於輸出
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-575 p(theta) 除以 p(theta k) 跟 clip function 輸出的關係，是這樣的一個關係
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-576 那接下來呢，我們就加入前面這一項
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-577 來看看前面這一項，到底在做什麼？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-578 前面這一項呢，其實就是綠色的這一條線，對不對
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-579 前面這一項，其實就是綠色的這一條線， p(theta) 除以 p(theta k)
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-580 就是綠色的這一條線
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-581 但是這兩項裡面，第一項跟第二項，也就是綠色的線
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-582 跟藍色的線中間，我們要取一個最小的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-583 假設今天前面乘上的這個 term A
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-584 它是大於 0 的話，取最小的結果，就是紅色的這一條線
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-585 反之，如果 A 小於 0 的話
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-586 那取紅色，取最小的以後，就得到紅色的這一條線
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-587 這一個結果，其實非常的直觀
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-588 這一個式子雖然看起來有點複雜，implement 起來是蠻簡單的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-589 想法也非常的直觀，因為這個式子想要做的事情
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-590 就是希望 p(theta) 跟 p(theta k)
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-591 也就是你拿來做 demonstration 的那個 model，跟你實際上 learn 的 model
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-592 最後在 optimize 以後
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-593 不要差距太大
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-594 那這個式子是怎麼讓它做到不要差距太大的呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-595 你要怎麼讓它做到不要差距太大呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-596 複習一下這橫軸的意思
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-597 就是 p(theta) 除以 p(theta k)
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-598 如果今天 A 大於 0
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-599 也就是某一個 state/action 的 pair 是好的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-600 那我們想要做的事情，當然是希望增加這個 state/action pair 的機率
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-601 也就是說，我們想要讓 p theta 越大越好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-602 但是我們覺得 p(theta) 越大越好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-603 沒有問題，但是，它跟這個 theta k 的比值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-604 不可以超過 1+ epsilon
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-605 如果超過 1+epsilon 的話，就沒有 benefit 了
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-606 紅色的線就是我們的 objective function
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-607 我們希望我們的 objective 越大越好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-608 當 p(theta) 比 p(theta k) 的比值
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-609 我們希望 p(theta) 越大越好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-610 但是 p(theta) 比 p(theta k) 的比值只要大過 1+epsilon，就沒有 benefit 了
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-611 所以今天在 train 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-612 p(theta) 只會被 train 到
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-613 比 p(theta k) 它們相除大 1+epsilon，它就會停止
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-614 那假設今天不幸的是，p(theta) 比 p(theta k) 還要小
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-615 那我們的目標是要讓 p(theta) 越大越好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-616 假設這個 advantage 是正的
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-617 我們當然希望 p(theta) 越大越好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-618 假設這個 action 是好的，我們當然希望這個 action 被採取的機率，越大越好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-619 所以假設 p(theta) 還比 p(theta k) 小
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-620 那就盡量把它挪大
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-621 但只要大到 1+epsilon 就好
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-622 那負的時候也是一樣，如果今天
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-623 某一個 state/action pair 是不好的，我們當然希望 p(theta) 把它減小
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-624 假設今天 p(theta) 比 p(theta k) 還大那你就要趕快盡量把它壓小
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-625 那壓到什麼樣就停止呢？
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-626 壓到 p(theta) 除以 p(theta k) 是 1-epsilon 的時候
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-627 就停了，就算了
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-628 就不要再壓得更小
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-629 那這樣的好處就是，你不會讓 p(theta) 跟 p(theta k) 差距太大
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-630 那要 implement 這個東西，其實對你來說可能不是太困難的事情
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-631 那最後這頁投影片呢
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-632 只是想要 show 一下
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-633 在文獻上，PPO 跟其它方法的比較
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-634 今天這邊有 show 的有這個 Actor-Critic 的方法
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-635 這邊有 A2C+TrustRegion 他們都是 actor-critic based 的方法
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-636 Actor-Critic 我們之後會講到
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-637 然後這邊有 PPO，PPO 是紫色線的方法
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-638 然後還有 TRPO
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-639 PPO 就是紫色的線
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-640 那你會發現在多數的 task 裡面，這邊每張圖就是某一個 RL 的任務
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-641 你會發現說在多數的 cases 裡面
DRL_Lecture_2_-__Proximal_Policy_Optimization_(PPO)-642 PPO 都是不錯的，不是最好的，就是第二好的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-0 Q-learning 是什麼呢？我們等一下，會簡單的介紹一下 Q-learning
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-1 那其實在 machine learning 那一門課裡面，其實也簡單的介紹過 Q-learning 了
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-2 那不過今天就算是再很快地複習一下，接下來呢
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-3 會講一些 Q-learning 的 tips
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-4 是之前沒有講過的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-5 然後會講說 Q-learning 怎麼樣用在 continuous 的 action 上
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-6 我們就先從 Q-learning 的簡介開始說，那我們說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-7 Q-learning 這種方法，它是 value-based 的方法，在value based 的方法裡面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-8 我們 learn 的並不是 policy，我們並不是直接 learn policy
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-9 我們要 learn 的是一個 critic
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-10 critic 並不直接採取行為，它想要做的事情是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-11 評價現在的行為有多好或者是有多不好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-12 這邊說的是 critic 並不直接採取行為
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-13 他是說我們假設有一個 actor pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-14 那 critic 的工作就是來評價這個 actor pi 他做得有多好，或者是有多不好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-15 舉例來說，有一種 actor 叫做 state value 的 function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-16 這個 state value function 的意思就是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-17 假設現在的 actor 叫做 pi，拿這個 pi 跟環境去做互動
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-18 拿 pi 去跟環境做互動的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-19 現在假設 pi 這個 actor，他看到了某一個 state s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-20 那如果在玩 Atari 遊戲的話，state s 是某一個畫面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-21 看到某一個畫面，某一個 state s 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-22 接下來一直玩到遊戲結束
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-23 累積的 reward 的期望值有多大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-24 accumulated reward 的 expectation 有多大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-25 所以 V(pi) 它是一個 function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-26 這個 function 它是吃一個 state
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-27 當作 input，然後它會 output 一個 scalar，這個 scalar 代表說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-28 現在 pi 這個 actor 它看到 state s 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-29 接下來預期到遊戲結束的時候，它可以得到多大的 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-30 我這邊就舉一個例子，假設你是玩 space invader 的話
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-31 也許這個 state 這個 s，這一個遊戲畫面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-32 你的 V(pi) of s 會很大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-33 因為接下來還有很多的怪可以殺，所以你會得到很大的分數
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-34 一直到遊戲結束的時候，你仍然有很多的分數可以吃
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-35 那在這個 case，也許你得到的 V(pi)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-36 就很小
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-37 因為一方面，剩下的怪也不多了
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-38 那再來就是現在因為那個防護罩這個紅色的東西防護罩已經消失了
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-39 所以可能很快就會死掉
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-40 所以接下來得到預期的 reward，就不會太大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-41 那這邊需要強調的一個點是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-42 當你在講這一個 critic 的時候，你一定要注意，critic 都是綁一個 actor 的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-43 就 critic 它並沒有辦法去憑空去 evaluate 一個 state 的好壞
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-44 而是它所 evaluate 的東西是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-45 在 given 某一個 state 的時候，假設我接下來互動的 actor 是 pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-46 那我會得到多少 reward，因為就算是給同樣的 state
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-47 你接下來的 pi 不一樣，你得到的 reward 也是不一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-48 舉例來說，在這個 case
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-49 雖然假設是一個正常的 pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-50 它可以殺很多怪，那假設他是一個很弱的 pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-51 他就站在原地不動，然後馬上就被射死了
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-52 那你得到的 V 還是很小
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-53 所以今天這個 critic output 值有多大，其實是取決於兩件事
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-54 一個是 state，另外一個其實是 actor
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-55 所以今天你的 critic 其實都要綁一個 actor
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-56 他是在衡量某一個 actor 的好壞
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-57 而不是 generally 衡量一個 state 的好壞
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-58 這邊有強調一下，你這個 critic output 是跟 actor 有關的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-59 你的 state value 其實是 depend on 你的 actor
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-60 當你的 actor 變的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-61 你的 state value function 的 output 其實也是會跟著改變的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-62 再來問題就是，怎麼衡量這一個 state value function 呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-63 怎麼衡量這一個 V(pi) of s 呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-64 有兩種不同的作法
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-65 那等一下會講說，像這種 critic
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-66 它是怎麼演變成可以真的拿來採取 action，這是等一下要講
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-67 我們現在要先問的是怎麼 estimate 這些 critic
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-68 那怎麼 estimate V(pi) of s 呢
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-69 有兩個方向，一個適用 Monte-Carlo MC based 的方法
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-70 如果是 MC based 的方法，他非常的直覺
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-71 他怎麼直覺呢，他就是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-72 你就讓 actor 去跟環境做互動
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-73 你要量 actor 好不好，你就讓 actor 去跟環境做互動，給 critic 看
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-74 然後，接下來 critic 就統計說，這個 actor 如果看到 state sa
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-75 他接下來 accumulated reward，會有多大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-76 如果它看到 state sb，他接下來 accumulated reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-77 會有多大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-78 但是因為實際上，你當然不可能把所有的 state 通通都掃過
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-79 不要忘了如果你是玩 Atari 遊戲的話，你的 state 可是 image 喔
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-80 你可是沒有辦法把所有的 state 通通掃過
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-81 所以實際上我們的 V(pi) of s，它是一個 network
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-82 對一個 network 來說，就算是 input state 是從來都沒有看過的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-83 它也可以想辦法估測一個 value 的值
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-84 怎麼訓練這個 network 呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-85 因為我們現在已經知道說，如果在 state sa
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-86 接下來的 accumulated reward 就是 Ga
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-87 也就是說，今天對這 value function 來說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-88 如果 input 是 state sa
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-89 正確的 output 應該是 Ga
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-90 如果 input state sb
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-91 正確的 output 應該是 value Gb
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-92 所以在 training 的時候，其實它就是一個 regression 的 problem
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-93 regression problem 大家應該都知道
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-94 胡亂 call 一下就有，它是一個 regression problem
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-95 你的 network 的 output 就是一個 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-96 你希望在 input sa 的時候，output value 跟 Ga 越近越好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-97 input sb 的時候，output value 跟 Gb 越近越好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-98 接下來把 network train 下去，就結束了
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-99 這是第一個方法，這是 MC based 的方法
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-100 那還有第二個方法是 Temporal-difference 的方法，這個是 TD based 的方法
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-101 那 TD based 的方法是什麼意思呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-102 在剛才那個 MC based 的方法
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-103 每次我們都要算 accumulated reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-104 也就是從某一個 state Sa，一直玩遊戲玩到遊戲結束的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-105 你會得到的所有 reward 的總和，我在前一個投影片裡面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-106 把它寫成 Ga 或 Gb
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-107 所以今天你要 apply MC based 的 approach
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-108 你必須至少把這個遊戲玩到結束
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-109 你才能夠估測 MC based 的 approach
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-110 但是有些遊戲非常的長
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-111 你要玩到遊戲結束才能夠 update network
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-112 你可能根本收集不到太多的資料，花的時間太長了
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-113 所以怎麼辦？有另外一種 TD based 的方法
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-114 TD based 的方法，不需要把遊戲玩到底
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-115 只要在遊戲的某一個情況，某一個 state st 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-116 採取 action at
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-117 得到 reward rt，跳到 state s(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-118 就可以 apply TD 的方法
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-119 怎麼 apply TD 的方法呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-120 這邊是基於以下這個式子，以下這個式子是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-121 我們知道說，在 state st
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-122 採取某一個 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-123 就是說我們現在有某一個 policy pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-124 假設我們現在用的是某一個 policy pi，在 state st
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-125 以後在 state st，它會採取 action at
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-126 給我們 reward rt
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-127 接下來進入 s(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-128 那就告訴我們說，這個 state s(t+1) 的 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-129 跟 state st 的 value，他們的中間差了一項 rt
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-130 因為你把 s(t+1) 得到的 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-131 加上這邊得到的 reward rt
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-132 就會等於 st 得到的 value，st 得到的 value，就是 rt 的 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-133 加上 s(t+1) 的 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-134 st 的 value 就是 rt 加上 s(t+1) 的 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-135 有了這個式子以後，你在 training 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-136 你要做的事情並不是真的直接去估測 V
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-137 而是希望你得到的結果，你得到的這個 V
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-138 可以滿足這個式子
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-139 也就是說你 training 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-140 會是這樣 train 的，你把 st 丟到 network 裡面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-141 因為 st 丟到 network 裡面會得到 V of st
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-142 你把 s(t+1) 丟到你的 value network 裡面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-143 會得到 V of s(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-144 這個式子告訴我們，V of st 減 V of s(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-145 它應該值是 rt
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-146 V of st 減 V of s(t+1)，它得到的值應該是 rt
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-147 然後按照這樣的 loss，希望他們兩個相減跟 rt 越接近越好的 loss
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-148 train 下去，update V 的參數
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-149 你就可以把 V function learn 出來
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-150 這邊是比較一下 MC 跟 TD 之間的差別
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-151 那 MC 跟 TD 它們有什麼樣的差別呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-152 MC 它最大的問題就是它的 various 很大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-153 因為今天我們在玩遊戲的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-154 它本身是有隨機性的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-155 所以 Ga 本身你可以想成它其實是一個 random 的 variable
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-156 因為你每次同樣走到 sa 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-157 最後你得到的 Ga，其實是不一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-158 對不對，你看到同樣的 state sa
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-159 最後玩到遊戲結束的時候，因為遊戲本身是有隨機性的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-160 你的玩遊戲的 model 本身搞不好也有隨機性
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-161 所以你每次得到的 Ga ，是不一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-162 那每一次得到 Ga 的差別，其實會很大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-163 為什麼它會很大呢？因為 Ga 其實是很多個不同的 step 的 reward 的和
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-164 假設你每一個 step 都會得到一個 reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-165 Ga 是從 state sa 開始，一直玩到遊戲結束
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-166 每一個 timestamp reward 的和
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-167 那舉例來說，我在右上角就列一個式子是說，假設本來只有 X
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-168 它的 various 是 var of X
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-169 但是你把某一個 variable 乘上 K 倍的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-170 它的 various 就會變成原來的 K 平方
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-171 所以現在這個 Ga 的 variance
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-172 相較於某一個 state 的 reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-173 它會是比較大的，Ga 的 variance 是比較大的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-174 Ga 的 variance 相較於某一個 state
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-175 你會得到的 reward variance 是比較大的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-176 現在，如果說用 TD 的話呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-177 用 TD 的話，你是要去 minimize 這樣的一個式子
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-178 在這中間會有隨機性的是 r
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-179 因為你在 st 就算你採取同一個 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-180 你得到的 reward 也不見得是一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-181 所以 r 其實也是一個 random variable
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-182 但這個 random variable 它的 variance
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-183 會比 Ga 還要小，因為 Ga 是很多 r 合起來
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-184 這邊只是某一個 r 而已，Ga 的 variance 會比較大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-185 r 的 variance 會比較小
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-186 但是這邊你會遇到的一個問題是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-187 你這個 V 不見得估的準
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-188 假設你的這個 V 估的是不準的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-189 那你 apply 這個式子 learn 出來的結果，其實也會是不準的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-190 所以今天 MC 跟 TD
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-191 它們是各有優劣，那等一下其實會講一個 MC 跟 TD 綜合的版本
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-192 今天其實 TD 的方法是比較常見的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-193 MC 的方法其實是比較少用的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-194 那這張圖是想要講一下
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-195 TD 跟 MC 的差異
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-196 這個圖想要說的是什麼呢？這個圖想要說的是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-197 假設我們現在有某一個 critic
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-198 它去觀察某一個 policy pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-199 跟環境互動搭著 episode 的結果
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-200 有一個 actor pi  它去跟環境互動了 8 次得到了 8 次玩遊戲的結果是這個樣子
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-201 接下來我們要這個 critic 去估測 state 的 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-202 那如果我們看 sb 這個 state 它的 value 是多少
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-203 sb 這個 state 在 8場遊戲裡面都有經歷過
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-204 然後在這 8 場遊戲裡面，其中有 6 場得到 reward 1
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-205 再有兩場得到 reward 0
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-206 所以如果你是要算期望值的話
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-207 state sb，就看到 state sb 以後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-208 得到的 reward，一直到遊戲結束的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-209 得到的 accumulated reward 期望值是 3/4，非常直覺
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-210 但是，不直覺的地方是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-211 sa 期望的 reward 到底應該是多少呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-212 這邊其實有兩個可能的答案
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-213 一個是 0，一個是 3/4
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-214 為什麼有兩個可能的答案呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-215 這取決於你用 MC 還是 TD
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-216 你用 MC 跟用 TD，你算出來的結果是不一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-217 假如你用 MC 的話
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-218 你用 MC 的話，你會發現說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-219 這個 sa 就出現一次，它就出現一次
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-220 看到 sa 這個 state
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-221 接下來 accumulated reward 是多少，就是 0
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-222 所以今天 sa 它的 expected reward 就是 0
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-223 但是如果你今天去看 TD 的話
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-224 TD 在計算的時候，它是要 update 下面這個式子
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-225 下面這個式子想要說的事情是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-226 因為我們在 state sa 得到 reward r=0 以後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-227 跳到 state sb
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-228 所以 state sb 的 reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-229 會等於 state sb 的 reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-230 加上在 state sa 它跳到 state sb 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-231 可能得到的 reward r
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-232 而這個可能得到的 reward r，它的值是多少？它的值是 0
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-233 而 sb expected reward 是多少呢？它的 reward 是 3/4
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-234 那 sa 呢它的 reward 應該是 3/4
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-235 有趣的地方是用 MC 跟TD 你估出來的結果
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-236 其實很有可能是不一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-237 就算今天你的 critic observed 到一樣的 training data
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-238 它最後估出來的結果，也不見得會是一樣
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-239 那為什麼會這樣呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-240 你可能問說，那一個比較對呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-241 其實就都對，對不對
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-242 因為今天在 sa 這邊
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-243 今天在第一個 trajectory
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-244 sa 它得到 reward 0 以後，再跳到 sb 也得到 reward 0
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-245 這邊有兩個可能，一個可能是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-246 sa 它就是一個帶賽的 state
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-247 所以只要有看到 sa 以後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-248 sb 就會拿不到 reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-249 有可能 sa 其實影響了 sb
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-250 如果是用 MC 的算法的話
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-251 它就會考慮這件事，它會把 sa 影響 sb 這件事，考慮進去
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-252 所以看到 sa 以後，接下來 sb 就得不到 reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-253 所以看到 sa 以後，期望的 reward 是 0
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-254 但是今天看到 sa 以後，sb 的 reward 是 0 這件事有可能只是一個巧合
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-255 就並不是 sa 所造成
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-256 並不是因為 sa 它是一個帶賽的 reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-257 而是因為說，sb 有時候就是會得到 reward 0
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-258 這只是單純運氣的問題，其實平常 sb 它會得到 reward 期望值是 3/4
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-259 跟 sa 是完全沒有關係的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-260 所以 sa 假設之後會跳到 sb
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-261 那其實得到的 reward 按照 TD 來算
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-262 應該是 3/4
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-263 所以不同的方法，它考慮了不同的假設
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-264 最後你其實是會得到不同的運算結果的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-265 那接下來我們要講的是另外一種 critic
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-266 這種 critic 叫做 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-267 它又叫做 state-action value function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-268 那我們剛才看到的那一個 state function，它的 input
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-269 就是一個 state
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-270 它是根據 state 去計算出，看到這個 state 以後的 expected accumulated reward 是多少
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-271 那這個 state-action value function 它的 input 不是 state
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-272 它是一個 state 跟action 的 pair
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-273 它的意思是說，在某一個 state，採取某一個 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-274 接下來假設我們都使用 actor pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-275 得到的 accumulated reward 它的期望值有多大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-276 那在講這個 Q-function 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-277 有一個會需要非常注意的問題是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-278 今天這個 actor pi，在看到 state s 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-279 它採取的 action，不一定是 a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-280 大家了解我的意思嗎？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-281 Q function 的假設是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-282 假設在 state s，強制採取 action a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-283 不管你現在考慮的這個 actor pi，它會不會採取 action a，這不重要
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-284 在 state s，強制採取 action a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-285 接下來，都用 actor pi 繼續玩下去
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-286 就只有在 state s，我們才強制一定要採取 action a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-287 接下來就進入自動模式，讓 actor pi 繼續玩下去，得到的 expected reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-288 才是 Q of sa
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-289 那假設你讓 pi 看到這個 state 的時候，它不見得要去採取 a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-290 我們只是強制手動讓它一定要去採取 action a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-291 那 Q function 有兩種寫法
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-292 一種寫法是你 input 就是 state 跟 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-293 那 output 就是一個 scalar，就跟那個 value function 是一樣
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-294 那還有另外一種寫法，也許是比較常見的寫法是這樣
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-295 你 input 一個 state s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-296 接下來你會 output 好幾個 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-297 假設你的 actor 是 discrete 的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-298 假設你 action 是 discrete 的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-299 你 action 就只有 3 個可能，往左往右或是開火
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-300 那今天你的這個 Q function output 的 3 個 values
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-301 就分別代表假設，a 是向左的時候的 Q value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-302 a 是向右的時候的 Q value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-303 還有 a 是開火的時候的 Q value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-304 那你要注意的事情是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-305 像這樣的 function 只有 discrete action 才能夠使用
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-306 如果你的 action 是無法窮舉的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-307 你只能夠用左邊這個式子，不能夠用右邊這個式子
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-308 這個是文獻上的結果，但是在作業 4-1 裡面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-309 其實作業 4-2 不是要玩這個，是玩打磚塊
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-310 但如果是作業 4-1 碰的遊戲
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-311 你去 estimate Q function 的話
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-312 看到的結果可能會像是這個樣子
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-313 這是什麼意思呢？他說假設現在在這個 state
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-314 上面這個畫面就是 state
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-315 在這個 state，那我們有 3 個 actions
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-316 3 個 action 就是原地不動，向上，向下
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-317 那假設是在這個 state
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-318 不管是採取這個 action，這個 action，還是這個 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-319 最後到遊戲結束的時候，得到的 expected reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-320 其實都差不了多少
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-321 因為球在這個地方，就算是你向下
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-322 接下來你其實應該還來的急救
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-323 所以今天不管是採取哪一個 action，就差不了太多
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-324 但假設現在這個球
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-325 這個乒乓球它已經反彈到很接近邊緣的地方
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-326 這個時候你採取向上
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-327 你才能得到 positive 的 reward，才接的到球
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-328 如果你是站在原地不動或向下的話
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-329 接下來你都會 miss 掉這個球
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-330 你得到的 reward 就會是負的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-331 這個 case 也是一樣，球很近了，所以就要向上
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-332 接下來，球被反彈回去，這時候採取那個 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-333 就都都沒有差了，這個是 state-action value 的一個例子
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-334 是在文獻上截下來的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-335 大家應該都知道說，deep reinforcement learning 最早的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-336 受到大家重視注意的一篇paper 就是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-337 deep mind 發表在 Nature 上的那個 paper
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-338 就是用 DQN 玩 Atari 可以痛電人類
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-339 那個是那篇 paper  上的一個圖
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-340 接下來要講的是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-341 雖然表面上我們 learn 一個 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-342 他只能拿來評估某一個 actor pi 的好壞
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-343 但是實際上只要有了這個 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-344 我們就可以做 reinforcement learning
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-345 其實有這個 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-346 我們就可以決定要採取哪一個 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-347 他的大原則是這樣
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-348 假設你有一個初始的 actor，也許一開始很爛，隨機的也沒有關係
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-349 初始的 actor 叫做 pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-350 那這個 pi 跟環境互動
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-351 會 collect data，接下來你 learn 一個 pi 這個 actor 的 Q value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-352 你去衡量一下 pi 這個 actor
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-353 他在某一個 state 強制採取某一個 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-354 接下來用 pi 這個 policy 會得到的 expected reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-355 那你可以用 TD 也可以用 MC 都是可以的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-356 你 learn 出一個 Q function 以後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-357 等一下我們接下來會細講的一個神奇的地方就是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-358 只要認得出某一個 policy pi 的 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-359 就保證你可以找到一個新的 policy
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-360 這個 policy 就做 pi prime
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-361 這一個 policy pi prime
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-362 他一定會比原來的 policy pi 還要好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-363 那等一下會定義說，什麼叫做好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-364 所以這邊神奇的地方是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-365 假設你只要有一個 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-366 你有某一個 policy pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-367 你根據那個 policy pi learn 出那 policy pi 的 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-368 接下來保證你可以找到一個新的 policy 叫做 pi prime
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-369 它一定會比 pi 還要好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-370 你今天找到一個新的 pi prime，一定會比 pi 還要好以後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-371 你把原來的 pi 用 pi prime 取代掉
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-372 再去找它的 Q，得到新的 Q 以後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-373 再去找一個更好的 policy
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-374 然後這個循環一直下去，你的 policy 就會越來越好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-375 今天這邊我們講完這一頁我們就下課
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-376 這一頁要講的是什麼呢？這一頁就是要講我們剛才講的到底是什麼
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-377 首先第一個要定義的是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-378 什麼叫做比較好？我們說 pi prime 一定會比 pi 還要好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-379 什麼叫做好呢？這邊所謂好的意思是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-380 對所有可能的 state s 而言
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-381 對同一個 state s 而言
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-382 pi 的 value function 一定會小於 pi prime 的 value function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-383 也就是說我們走到同一個 state s 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-384 如果拿 pi 繼續跟環境互動下去
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-385 我們得到的 reward 一定會小於用 pi prime 跟環境互動下去得到的 reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-386 所以今天不管在哪一個 state
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-387 你用 pi prime 去做 interaction
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-388 你得到的 expected reward 一定會比較大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-389 所以 pi prime 是比 pi 還要好的一個 policy
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-390 那有了這個 Q 以後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-391 怎麼找這個 pi prime 呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-392 這邊的構想非常的簡單
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-393 事實上這個 pi prime 是什麼？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-394 這個 pi prime 就是，如果你根據以下的這個式子去決定你的 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-395 根據以下的這個式子去決定你的 action 的步驟叫做 pi prime 的話
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-396 那這個 pi prime 一定會比 pi 還要好，在下一頁會有證明
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-397 這個是什麼意思呢，這個意思是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-398 假設你已經 learn 出 pi 的 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-399 今天在某一個 state s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-400 你把所有可能的 action a，都一一帶入這個 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-401 看看說那一個 a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-402 可以讓 Q function 的 value 最大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-403 那這一個 action，就是 pi prime 會採取的 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-404 那這邊要注意一下，今天 given 這個 state s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-405 我們剛才有講過 Q function 的定義
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-406 given 這個 state s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-407 你的 policy pi，並不一定會採取 action a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-408 今天是 given 某一個 state s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-409 強制採取 action a，用 pi 繼續互動下去
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-410 得到的 expected reward，才是這個 Q function 的定義
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-411 所以在 state s 裡面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-412 不一定會採取 action a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-413 我們強調一次，在 state s 裡面，不一定會採取 action a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-414 今天假設我們用這一個 pi prime
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-415 它在 state s 採取 action a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-416 跟 pi 所謂採取 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-417 是不一定會一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-418 然後 pi prime 所採取的 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-419 會讓他得到比較大的 reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-420 所以實際上，根本就沒有所謂一個 policy 叫做 pi prime
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-421 這個 pi prime 其實就是用 Q function 推出來的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-422 所以並沒有另外一個 network 決定 pi prime 怎麼 interaction
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-423 我們只要 Q 就好，有 Q 就可以找出 pi prime
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-424 但是這邊有另外一個問題是我們等一下會解決的就是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-425 在這邊要解一個 Arg Max 的 problem
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-426 所以 a 如果是 continuous 的就會有問題
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-427 如果是 discrete 的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-428 a 只有 3 個選項，一個一個帶進去，看誰的 Q 最大，沒有問題
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-429 但如果是 continuous 要解 Arg Max problem
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-430 你就會有問題，但這個是之後才會解決的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-431 下一頁投影片想要跟大家講的是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-432 為什麼用 Q theta 這個 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-433 所決定出來的 pi prime
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-434 一定會比 pi 還要好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-435 所以下一頁是要證這件事
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-436 那下一頁的證明，假設你覺得你沒有辦法 follow 的話
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-437 其實就算了，就只要記得這個結果
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-438 下一頁投影片要證的就是這樣，假設現在呢
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-439 我們有一個 policy 叫做 pi prime，它是由 Q(pi) 決定的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-440 我們要證說，對所有的 state s 而言
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-441 V(pi prime) 一定會比 V(pi) 還要大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-442 這件事怎麼證呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-443 這邊的式子是這樣，我們先把 V(pi) 寫出來
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-444 那 V(pi) 這個式子，會等於 Q(pi) of (s, pi of s)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-445 假設你在 state s 這個地方，你 follow pi 這個 actor
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-446 它會採取的 action，也就是 pi of s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-447 那你算出來的 Q(pi) 會等於 V(pi)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-448 之前 in general 而言，Q(pi) 不見得等於 V(pi) 是因為這一個 action 不見得是 pi of s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-449 但這個 action 如果是 pi of s 的話，Q(pi) 是等於 V(pi) 的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-450 今天 Q(pi) of (s, pi of s) 一定會小於等於 q(pi) of (s, a) a 取最大的那一個
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-451 對不對，因為這邊是某一個 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-452 這邊是所有 action 裡面可以讓 Q 最大的那個 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-453 所以今天這一項一定會比它大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-454 那我們知道說這一項是什麼，這一項就是 Q(pi) of (s, a)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-455 然後 a 是什麼，a 就是 pi prime of s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-456 因為今天 pi prime of s，它 output 的 a，就是可以讓 Q(pi) of s 最大的那一個
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-457 所以今天這個式子可以寫成 Q(pi) of (s, pi prime of s)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-458 這邊我們就知道 V(pi) of s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-459 它一定小於等於 Q(pi) of (s, pi prime of s)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-460 也就是說你在某一個 state
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-461 如果你按照 policy pi，一直做下去
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-462 你得到的 reward 一定會小於等於你在現在這個 state s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-463 你故意不按照 pi 所給你指示的方向
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-464 你故意按照 pi prime 的方向走一步，但之後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-465 只有第一步是按照 pi prime 的方向走
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-466 只有在 state s 這個地方，你才按照 pi prime 的方向走
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-467 pi prime 的指示走，但接下來你就按照 pi 的指示走
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-468 雖然只有一步之差，但是我們可以按照上面這個式子知道說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-469 這個時候你得到的 reward，只有一步之差
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-470 你得到的 reward 一定會比完全 follow pi 得到的 reward 還要大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-471 那接下來，eventually，你想要證的東西就是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-472 這一個 Q(pi) of (s, pi prime of s)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-473 會小於等於 V(pi prime) of s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-474 也就是說，只有一步之差，你會得到比較大的 reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-475 但假設每步都是不一樣的，每步通通都是 follow pi prime 而不是 pi 的話
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-476 那你得到的 reward 一定會更大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-477 就這樣，直覺上想起來是這樣子的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-478 如果你要用數學式把它寫出來的話，略嫌麻煩，但也沒有很難
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-479 只是比較繁瑣而已，怎麼寫呢？你可以這樣寫
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-480 Q pi 這個式子，它的意思就是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-481 我們在 state st
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-482 我們會採取 action at
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-483 接下來我們會得到 reward r(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-484 然後跳到 state s(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-485 這邊有一個地方我覺得我寫得不太好，我覺得這邊應該寫成 rt 跟我之前的 notation 感覺比較一致
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-486 但這邊寫成了 r(t+1)，其實這都是可以的，在文獻上有時候有人會說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-487 在 state st 採取 action at 得到 reward r(t+1)，有人會寫成 rt，但意思其實都是一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-488 在 state s，按照 pi prime 採取某一個 action at，得到 reward r(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-489 然後接下來跳到 state s(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-490 然後我們這邊是 state s(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-491 根據 pi 這個 actor 所估出來的 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-492 上面這個式子，等於下面這個式子
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-493 這邊要取一個期望值
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-494 因為在同樣的 state 採取同樣的 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-495 你得到的 reward 還有會跳到 state 不見得是一樣，所以這邊需要取一個期望值
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-496 這一項會小於等於下面這個式子，為什麼這一項會小於等於下面這個式子呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-497 因為我們上面已經講過說 V(pi) of s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-498 一定小於 Q(pi) of (s, pi prime)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-499 也就是這邊 V(pi) of s(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-500 一定會小於等於 Q(pi) of ( s(t+1), pi(prime) of s(t+1) )
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-501 也就是說，現在你一直 follow pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-502 跟某一步 follow pi prime，接下來都 follow pi，比起來
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-503 某一步 follow pi prime 得到的 reward 是比較大的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-504 這一個式子就可以寫成
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-505 就可以寫成，下面這個式子
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-506 因為 Q(pi) 這個東西可以寫成 r(t+2) + s(t+2) 的 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-507 然後接下來你再把這個式子
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-508 你再把 V(pi) 小於等於 Q(pi)  of (s, pi prime of s) 這件事情
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-509 再帶進去，然後一直算算算到底
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-510 算到 episode 結束
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-511 那你就知道說 V(pi) of s 會小於等於 V(pi prime) of s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-512 反正這邊假設你沒有辦法 follow 的話，總是想要告訴你的事情是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-513 你可以 estimate 某一個 policy 的 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-514 接下來你就一定可以找到另外一個 policy 叫做 pi prime
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-515 它一定比原來的 policy 還要更好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-516 我們講一下 Target Network，我們講一下接下來在 Q learning 裡面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-517 typically 你一定會用到的 tip
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-518 有幾個 tip，第一個你會用一個東西叫做 target network
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-519 什麼意思呢？我們在 learn Q function 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-520 你也會用到 TD 的概念
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-521 那怎麼用 TD 的概念呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-522 就是說你現在收集到一個 data，是說在 state st，你採取 action at 以後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-523 你得到 reward rt，然後跳到 state s(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-524 然後今天根據這個 Q function 你會知道說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-525 Q(pi) of (st, at) 跟 Q(pi) of ( s(t+1), pi of s(t+1) )，他們中間差了一項就是 rt
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-526 所以你在 learn 的時候，你會說我們有 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-527 input  st, at 得到的 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-528 跟 input s(t+1), pi(s(t+1) 得到的 value 中間
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-529 我們希望它差了一個 rt，這跟剛才講的 TD 的概念是一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-530 但是實際上在 learn 的時候，你會發現說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-531 這樣 in general 而言這樣的一個 function 並不好 learn
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-532 為什麼，因為假設你說這是一個 regression 的 problem
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-533 這是你 network 的 output，這是你的 target，你會發現你的  target 是會動的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-534 當然你要 implement 這樣的 training 其實也沒有問題
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-535 對不對，就是你在做 back propagation 的時候，這個 model 它的參數要不要 update
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-536 這個 model 參數也會被 update，當然是同一個 model
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-537 所以你會把兩個 update 的結果加在一起
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-538 他們是同一個 model，所以兩個 update 的結果會加在一起
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-539 但是實際上在做的時候，你的 training 會變得不太穩定
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-540 因為假設你把這個當作你 model 的 output，這個當作 target 的話
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-541 你會變成說你要去 fit 的 target，它是一直在變的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-542 這種一直在變的 target 的 training 其實是不太好 train 的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-543 所以實際上怎麼做呢？實際上你會把其中一個 Q
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-544 通常是你就選擇下面這個 Q
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-545 把它固定住
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-546 也就是說你在 training 的時候，你並不 update 這個 Q 的參數
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-547 你只 update 左邊這個 Q 的參數
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-548 而右邊這個 Q 的參數，它會被固定住
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-549 我們叫它 target network，它負責產生 target，所以叫做 target network
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-550 因為 target network 是固定的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-551 所以你現在得到的 target，也就是 rt 加上
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-552 Q(pi) of ( s(t+1), pi of s(t+1) ) 的值也會是固定的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-553 那我們只調左邊這個 network 的參數
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-554 那假設因為 target network 是固定的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-555 我們只調左邊 network 的參數
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-556 它就變成是一個 regression 的 problem
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-557 我們希望我們 model 的 output，它的值跟你的目標越接近越好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-558 你會 minimize 它的 mean square error
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-559 那你會 minimize 它們 L2 的 distance
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-560 那這個東西就是 regression
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-561 在實作上呢，你會把這個 Q update 好幾次以後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-562 再去把這個 target network 用 update 過的 Q
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-563 去把它替換掉
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-564 你在 train 的時候，先update 它好幾次
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-565 然後再把它替換掉
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-566 但它們兩個不要一起動，他們兩個一起動的話，你的結果會很容易壞掉
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-567 今天本來一開始這兩個 network 是一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-568 然後接下來在 train 的時候，你會把它 fix 住
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-569 然後你只調這個，你在做 gradient decent 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-570 只調左邊這個 network 的參數，那你可能 update 100 次以後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-571 才把這個參數，複製到右邊去，把它蓋過去
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-572 把它蓋過去以後，你這個 target 的 value，就變了
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-573 就好像說你今天本來在做一個 regression 的 problem
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-574 那你 train... 把這個 regression problem 的 loss 壓下去以後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-575 接下來你把這邊的參數把它 copy 過去以後
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-576 你的 target 就變掉了
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-577 你 output 的 target 就變掉了，那你接下來就要重新再 train
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-578 它其實不會變成 0，為什麼呢？因為首先它們的 input 是不一樣
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-579 同樣的 function，這邊的 input 是 st 跟 at
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-580 這邊 input 是 s(t+1) 跟在 s(t+1) 會採取的 action pi of s(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-581 因為 input 不一樣
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-582 所以它 output 的值會不一樣
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-583 所以光這一項跟這一項的值
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-584 就會不一樣，今天再加上 rt
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-585 所以他們的值就會更不一樣
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-586 但是你希望說你會把這兩項的值把它拉近
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-587 如果大家 ok 的話，這是第一個你會用到的 tip
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-588 第二個會用到的 tip 是 Exploration
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-589 我們剛才講說，當我們使用 Q function 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-590 我們的 policy 是怎麼樣，我們的 policy 完全 depend on 那個 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-591 看說 given 某一個 state，你就窮舉所有的 a，看那個 a 可以讓 Q value 最大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-592 它就是你採取的 policy
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-593 它就是採取的 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-594 那其實這個跟 policy gradient 不一樣，在做 policy gradient 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-595 我們的 output 其實是 stochastic 的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-596 對不對，我們 output 一個 action 的 distribution
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-597 根據這個 action 的 distribution 去做 sample，所以在 policy gradient 裡面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-598 你每次採取的 action 是不一樣的，是有隨機性的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-599 那像這種 Q function，如果你採取的 action 總是固定的會有什麼問題呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-600 你會遇到的問題就是，這不是一個好的收集 data 的方式
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-601 為什麼這不是一個好的收集 data 的方式呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-602 因為假設我們今天真的要估某一個，我這邊應該要
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-603 就是說某一個 state，你可以採取 action a1, a2, a3
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-604 我這邊應該寫 a1, a2, a3，但我忘了加
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-605 今天你要估測在某一個 state 採取某一個 action 會得到的 Q value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-606 你一定要在那一個 state，採取過那一個 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-607 你才估得出它的 value 對不對
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-608 如果你沒有在那個 state 採取過那個 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-609 你其實估不出那個 value 的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-610 當然如果是用 deep 的 network，就你的 Q function 其實是一個 network，這種情形可能會比較沒有那麼嚴重
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-611 但是 in general 而言，假設你 Q function 是一個 table
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-612 沒有看過的 state-action pair，它就是估不出值來
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-613 當然 network 也是會有一樣的問題就是，只是沒有那麼嚴重，但也會有一樣的問題
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-614 所以今天假設你在某一個 state，action a1, a2, a3 你都沒有採取過
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-615 那你估出來的 (s, a1) (s, a2) (s, a3) 的 Q value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-616 可能就都是一樣的，就都是一個初始值，比如說 0
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-617 但是今天假設你在 state s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-618 你 sample 過某一個 action a2 了
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-619 那 sample 到某一個 action a2
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-620 它得到的值是 positive 的 reward
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-621 那現在 Q of (s, a2)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-622 就會比其他的 action 都要好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-623 那我們說今天在採取 action 的時候，就看說誰的 Q value 最大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-624 就採取誰，所以之後你永遠都只會 sample 到 a2
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-625 其他的 action 就再也不會被做了，所以今天就會有問題
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-626 就好像說你進去一個餐廳吃飯，餐廳都有一個菜單，那其實你都很難選
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-627 你今天點了某一個東西以後，假說點了某一樣東西，比如說椒麻雞，你覺得還可以
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-628 接下來你每次去，就都會點椒麻雞，再也不會點別的東西了，那你就不知道說別的東西是不是會比椒麻雞好吃
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-629 這個是一樣的問題
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-630 那如果你今天沒有好的 exploration 的話，你在 training 的時候就會遇到這種問題
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-631 舉一個實際的例子，假設你今天是用 Q learning 來玩比如說slither.io
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-632 在玩 slither.io 你會有一個蛇
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-633 然後它在環境裡面就走來走去，然後就吃到星星，它就加分
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-634 那今天假設這個遊戲一開始，它採取往上走
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-635 然後就吃到那個星星，它就得到分數，它就知道說往上走是 positive
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-636 接下來他就再也不會採取往上走以外的 action 了
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-637 所以接下來就會變成每次遊戲一開始
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-638 它就往上衝，然後就死掉，再也做不了別的事
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-639 所以今天需要有 exploration 的機制
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-640 需要讓 machine 知道說，雖然 a2
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-641 根據之前 sample 的結果，好像是不錯的，但你至少偶爾也試一下 a1 跟 a3，搞不好他們更好也說不定
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-642 有兩個方法解這個問題，一個是 Epsilon Greedy
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-643 Epsilon Greedy 的意思是說，我們有
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-644 1-epsilon 的機率，通常 epsilon 就設一個很小的值，1-epsilon 可能是 90%
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-645 也就是 90% 的機率，完全按照 Q function 來決定 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-646 但是你有 10% 的機率是隨機的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-647 通常在實作上 epsilon 會隨著時間遞減
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-648 也就是在最開始的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-649 因為還不知道那個 action 是比較好的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-650 所以你會花比較大的力氣在做 exploration
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-651 那接下來隨著 training 的次數越來越多
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-652 已經比較確定說哪一個 Q 是比較好的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-653 你就會減少你的 exploration，你會把 epsilon 的值變小
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-654 主要根據 Q function 來決定你的 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-655 比較少做 random，這是 Epsilon Greedy
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-656 那還有另外一個方法叫做 Boltzmann Exploration
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-657 這個方法就比較像是 policy gradient
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-658 在 policy gradient 裡面我們說 network 的 output 是一個
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-659 根據 probability，根據 expect(ed) action space 上面的一個 probability distribution
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-660 再根據 probability distribution 去做 sample
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-661 那其實你也可以根據 Q value 去定一個 probability distribution
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-662 你可以說，假設某一個 action，它的 Q value 越大，代表它越好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-663 那我們採取這個 action 的機率就越高
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-664 但是某一個 action 它的 Q value 小
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-665 不代表我們不能 try try 看它好不好用
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-666 所以我們有時候也要 try try 那些 Q value 比較差的 action
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-667 那怎麼做呢，因為 Q value 它是有正有負的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-668 所以你要把它弄成一個機率，你可能就先取 exponential
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-669 然後再做 normalize
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-670 然後把 Q of (s, a) exponential，再做 normalize 以後的這個機率
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-671 就當作是你在決定 action 的時候 sample 的機率
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-672 Q 一開始嗎？其實在實作上，你那個 Q 是一個 network
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-673 所以你有點難知道說，今天在一開始的時候 network 的 output
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-674 到底會長怎麼樣子，但是其實你可以猜測說，假設你一開始沒有任何的 training data
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-675 你的參數是隨機的，那 given 某一個 state s
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-676 你的不同的 a output 的值
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-677 可能就是差不多的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-678 所以一開始 Q of (s, a) 應該會傾向於是 uniform
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-679 也就是在一開始的時候，你這個 probability distribution 算出來
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-680 它可能是比較 uniform 的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-681 假設今天你的值通通都是 1
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-682 你的值通通都是 2，你的值通通都是 100
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-683 靠這個式子，以後算出來的結果，會是一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-684 那還有第三個你會用的 tip，這個 tip 叫做 replay buffer
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-685 replay buffer 的意思是說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-686 現在我們會有某一個 policy pi 去跟環境做互動，然後它會去收集 data
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-687 我們會把所有的 data 放到一個 buffer 裡面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-688 那 buffer 裡面就排了很多 data，那你 buffer 設比如說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-689 5 萬，這樣它裡面可以存 5 萬筆資料
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-690 每一筆資料是什麼？每一筆資料就是記得說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-691 我們之前在某一個 state st
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-692 採取某一個 action at
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-693 接下來我們得到的 reward rt
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-694 然後接下來跳到 state s(t+1)，某一筆資料，就是這樣
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-695 那你用 pi 去跟環境互動很多次
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-696 把所有收集到的資料通通都放到這個 replay buffer 裡面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-697 這邊要注意的事情是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-698 這個 replay buffer 它裡面的 experience
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-699 可能是來自於不同的 policy
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-700 就你每次拿 pi 去跟環境互動的時候，你可能只互動 10,000 次
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-701 然後接下來你就更新你的 pi 了
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-702 但是你的這個 buffer 裡面可以放 5 萬筆資料
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-703 所以那 5 萬筆資料，它們可能是來自於不同的 policy
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-704 那這個 buffer 只有在它裝滿的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-705 才會把舊的資料丟掉
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-706 所以這個 buffer 裡面它其實裝了很多不同的 policy
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-707 所計算出來的不同的 policy 的 experiences
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-708 接下來你有了這個 buffer 以後，你做的事情
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-709 你是怎麼 train 這個 Q 的 model 呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-710 你是怎麼估 Q 的 function，你的做法是這樣
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-711 你會 iterative 去train 這個 Q function，在每一個 iteration 裡面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-712 你從這個 buffer 裡面，隨機挑一個 batch 出來
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-713 就跟一般的 network training 一樣，你從那個 training data set 裡面，去挑一個 batch 出來
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-714 你去 sample 一個 batch 出來，裡面有一把的 experiences
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-715 根據這把 experiences 去 update 你的 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-716 就跟我們剛才講那個 TD learning 要有一個 target network 是一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-717 你去 sample 一堆batch
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-718 sample 一個 batch 的 data，sample 一堆 experiences
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-719 然後再去 update 你的 Q function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-720 這邊其實有一個東西你可以稍微想一下，你會發現說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-721 實際上當我們這麼做的時候，它變成了一個 off policy 的做法
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-722 對不對，因為本來我們的 Q 是要觀察 pi 這個 action 它的 value
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-723 但實際上存在你的 replay buffer 裡面的這些experiences
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-724 不是通通來自於 pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-725 對不對，有些是過去其他的 pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-726 所遺留下來的 experience
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-727 因為你不會拿某一個 pi 就把整個 buffer 裝滿
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-728 然後拿去測 Q function，這個 pi 只是 sample 一些 data
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-729 塞到那個 buffer 裡面去，然後接下來就讓 Q 去 train
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-730 所以 Q 在 sample 的時候，它會 sample 到過去的一些資料
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-731 但是這麼做到底有什麼好處呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-732 這麼做有兩個好處，第一個好處
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-733 其實在做 reinforcement learning 的時候，往往最花時間的 step
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-734 是在跟環境做互動
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-735 train network 反而是比較快的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-736 因為你用 GPU train 其實很快，真正花時間的往往是在跟環境做互動
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-737 今天用 replay buffer，你可以減少跟環境做互動的次數
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-738 因為今天你在做 training 的時候，你的 experience 不需要通通來自於某一個 policy
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-739 一些過去的 policy 他所得到的 experience
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-740 可以放在 buffer 裡面被使用很多次
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-741 被反覆的再利用
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-742 這樣讓你的 sample 到 experience 的利用是比較 efficient
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-743 那還有另外一個理由是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-744 你記不記得我們說在 train network 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-745 其實我們希望一個 batch 裡面的 data
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-746 越 diverse 越好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-747 如果你的 batch 裡面的 data 通通都是同樣性質的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-748 你 train 下去，其實是容易壞掉的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-749 對不對，不知道大家有沒有這樣子的經驗
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-750 如果你 batch 裡面都是一樣的 data，你 train 的時候，performance 會比較差
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-751 我們希望 batch data 越 diverse 越好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-752 那如果你今天，你的這個 buffer 裡面的那些 experience
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-753 它通通來自於不同的 policy 的話
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-754 那你得到的結果
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-755 你 sample 到的一個 batch 裡面的 data
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-756 會是比較 diverse 的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-757 但是接下來你會問的一個問題是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-758 我們明明是要觀察 pi 的 value，我們要量的明明是 pi 的 value 啊
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-759 裡面混雜了一些不是 pi 的 experience
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-760 到底有沒有關係？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-761 一個很簡單的解釋，也許這些不同的 pi 也沒差那麼多
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-762 所以也沒有關係，但是你仔細想一項
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-763 這一件事情其實是沒有關係的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-764 這並不是因為過去的 pi 跟現在的 pi 很像，就算過去的 pi 沒有很像
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-765 其實也是沒有關係的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-766 這個留給大家回去想一下，為什麼會這個樣子
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-767 今天主要的原因是因為，我們並不是去 sample 一個 trajectory
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-768 我們只 sample 了一筆 experience
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-769 所以跟我們是不是 off policy 這件事是沒有關係的，就算是 off-policy
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-770 就算是這些 experience 不是來自於 pi
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-771 我們其實還是可以拿這些experience
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-772 來估測 Q(pi) of (s, a)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-773 這件事有點難解釋，不過你就記得說
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-774 replay buffer 這招其實是在理論上也是沒有問題的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-775 那這個就是 typical 的一般的正常的 Q learning 演算法
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-776 這個演算法是這樣
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-777 我們說我們需要一個 target network，先開始 initialize 的時候
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-778 你 initialize 2 個 network，一個 是 Q，一個是 Q hat
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-779 那其實 Q hat 就等於 Q，一開始這個 target Q-network，跟你原來的 Q network 是一樣的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-780 那在每一個 episode，就你拿你的 agent
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-781 你拿你的 actor 去跟環境做互動
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-782 那在每一次互動的過程中
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-783 你都會得到一個 state st
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-784 一個遊戲的畫面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-785 那你會採取某一個 action at
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-786 那怎麼知道採取那一個 action at 呢？你就根據你現在的 Q-function
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-787 但是記得你要有 exploration 的機制
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-788 比如說你用 Boltzmann exploration 或是 Epsilon Greedy的 exploration
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-789 也有一點 exploration 的機制
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-790 那接下來你得到 reward rt，然後跳到 state s(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-791 所以現在 collect 到一筆 data，這筆 data 是 st, at ,rt, s(t+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-792 結果這筆 data 就塞到你的 buffer 裡面去
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-793 那如果 buffer 滿的話，你就再把一些舊有的資料再把它丟掉
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-794 那接下來你就從你的 buffer 裡面去 sample data
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-795 那你 sample 到的是 si, ai, ri, s(i+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-796 這筆 data 跟你剛放進去的，不見得是同一筆
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-797 懂嗎？你把這筆 data 塞到 buffer 裡面
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-798 再到 buffer 裡面去抽 data，抽出來並不是同一筆
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-799 你只是可能抽到一個舊的，也是有可能的
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-800 那這邊另外要注意的是，我這邊notation 不太好寫
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-801 其實你 sample 出來不是一筆 data
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-802 你 sample 出來的是一個 batch 的 data
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-803 你 sample 一個 batch 出來，sample 一把 experiences 出來
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-804 你 sample 這一把 experience 以後，接下來你要做的事情就是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-805 計算你的 target
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-806 根據你 sample 出來的 data
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-807 假設你 sample 出這麼一筆 data
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-808 根據這筆 data 去算你的 target
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-809 你的 target 是什麼呢？
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-810 target 記得要用 target network，也就是 Q hat 來算
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-811 我們用 Q hat 來代表 target network
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-812 好那 target 是多少呢？ target 就是 ri 加上
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-813 Q hat of (s(i+1), a)，a 是什麼？a 就是
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-814 看說現在哪一個 a，可以讓 Q hat 的值最大
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-815 你就選那一個 a，因為我們在這邊，在 state s(i+1)
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-816 會採取的 action a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-817 其實就是那個可以讓 Q value 的值最大的 那一個 a
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-818 接下來我們要 update Q 的值
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-819 那就把它當作一個 regression 的 problem
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-820 希望 Q of (si, ai) 跟你的 target 越接近越好
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-821 然後今天假設這個 update 已經 update 了某一個數目的次
DRL_Lecture_3_-_Q-learning_(Basic_Idea)-822 比如說 c 次，你就設一個 100 c = 100，那你就把 Q hat 設成 Q，就這樣
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-0 接下來我們要講的是 train Q learning 的一些 tip
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-1 在作業裡面當然是會要求你 implement 某一個 tip
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-2 那你等一下就看看說，哪一個 tip 是你覺得最簡單的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-3 第一個要介紹的 tip，叫做 double DQN
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-4 那為什麼要有 double DQN 呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-5 因為在實作上，你會發現說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-6 Q value 往往是被高估的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-7 那下面這幾張圖是來自於 double DQN 的原始 paper
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-8 它想要顯示的結果就是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-9 Q value 往往是被高估的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-10 這邊就是有 4 個不同的小遊戲
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-11 那橫軸是 training 的時間
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-12 然後紅色這個鋸齒狀一直在變的線就是 Q function 所 estimate 出來的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-13 對不同的 state estimate 出來的平均 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-14 就有很多不同的 state，每個 state 你都 sample 一下
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-15 然後算它們的 Q value，把它們平均起來
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-16 這是紅色這一條線
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-17 它在 training 的過程中會改變
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-18 但它是不斷上升的，為什麼它不斷上升，因為很直覺
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-19 不要忘了 Q function 是 depend on 你的 policy 的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-20 你今天在 learn 的過程中你的 policy 越來越強
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-21 所以你得到 Q 的 value 會越來越大
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-22 在某一個 state，同一個 state，你得到 expected reward 會越來越大
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-23 所以 general 而言，這個值都是上升的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-24 但是它說，這是 Q network 估測出來的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-25 接下來你真的去算它，那怎麼真的去算，很容易啊，你有那個 policy
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-26 然後真的去玩那個遊戲
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-27 你就可以估說，你就可以真的去算說，就玩很多次，玩個 1 百萬次
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-28 然後就去真的估說，在某一個 state，你會得到的 Q value，到底有多少
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-29 你會得到說在某一個 state，採取某一個 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-30 你接下來會得到 accumulated reward 的總和是多少
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-31 那你會發先說，實際的值，跟估測出來的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-32 估測出來的值是遠比實際的值大
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-33 在每一個遊戲都是這樣，都大很多
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-34 所以他今天要 propose double DQN 的方法就是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-35 它可以讓估測的值跟實際的值是比較接近的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-36 那我們還沒有講 double DQN 的方法，但我們先看它的結果
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-37 藍色的會鋸齒狀的線是 double DQN 的 Q network 所估測出來的 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-38 藍色的是真正的 Q value，你會發現他們是比較接近的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-39 但還有另外一個有趣可以觀察的點就是說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-40 你會發現用 double DQN 所估出來的 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-41 就是說估測出來的都不用管它，用 network 估測出來的就不用管它
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-42 那比較沒有參考價值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-43 但是如果是真正的 accumulated reward
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-44 用 double DQN 得出來真正的 accumulated reward
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-45 在這 3 個 case，都是比原來的 DQN 高的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-46 代表 double DQN learn 出來那個 policy 比較強
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-47 所以它實際上得到的 reward 是比較大的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-48 雖然說看那個 Q network 的話
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-49 一般的 DQN 的 Q network 虛張聲勢
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-50 高估了自己會得到的 reward
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-51 但實際上它得到的 reward 是比較低的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-52 那接下來要講的第一個問題就是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-53 為什麼 Q value 總是被高估了呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-54 這個是有道理的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-55 因為想想看，我們實際上在做的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-56 我們是要讓左邊這個式子
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-57 跟右邊我們這個 target，越接近越好
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-58 那你會發現說，target 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-59 很容易一不小心就被設得太高
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-60 為什麼 target 的值很容易一不小心就被設得太高呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-61 因為你想想看，在算這個 target 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-62 我們實際上在做的事情是說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-63 看哪一個 a 它可以得到最大的 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-64 就把它加上去，就變成我們的 target
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-65 所以今天假設有某一個 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-66 它得到的值是被高估的，舉例來說，我們現在有 4 個 actions
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-67 那本來其實它們得到的值都是差不多的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-68 他們得到的 reward 都是差不多的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-69 但是在 estimate 的時候，那畢竟是個 network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-70 所以 estimate 的時候是有誤差的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-71 所以假設今天是第一個 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-72 它被高估了，假設綠色的東西代表是被高估的量
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-73 它被高估了，那這個 target 就會選這個 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-74 然後就會選這個高估的 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-75 來加上 rt，來當作你的 target
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-76 如果第 4 個 action 被高估了
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-77 那就會選第 4 個 action 來加上 rt
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-78 來當作你的 target value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-79 所以你總是會選那個 Q value 被高估的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-80 你總是會選那個 reward 被高估的 action 當作這個 max 的結果
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-81 去加上 rt 當作你的 target
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-82 所以你的 target 總是太大，對不對
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-83 那怎麼解決這 target 總是太大的問題呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-84 那 double DQN 它的設計是這個樣子的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-85 在 double DQN 裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-86 選 action 的 Q function
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-87 跟算 value 的 Q function，不是同一個
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-88 今天在原來的 DQN 裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-89 你窮舉所有的 a
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-90 把每一個 a 都帶進去，看哪一個 a 可以給你的 Q value 最高
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-91 那你就把那個 Q value 加上 rt
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-92 但是在 double DQN 裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-93 你有兩個 Q network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-94 第一個 Q network，決定那一個 action 的 Q value 最大
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-95 你用第一個 Q network 去帶入所有的 a
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-96 去看看哪一個 Q value 最大
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-97 然後你決定你的 action 以後
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-98 實際上你的 Q value 是用 Q prime 所算出來的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-99 這樣子有什麼好處呢？為什麼這樣就可以避免 over estimate 的問題呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-100 因為今天假設我們有兩個 Q function
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-101 假設第一個 Q function 它高估了它現在選出來的 action a
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-102 那沒關係，只要第二個 Q function Q prime
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-103 它沒有高估這個 action a 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-104 那你算出來的，就還是正常的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-105 那今天假設反過來是 Q prime 高估了某一個 action 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-106 那也沒差，因為反正只要前面這個 Q 不要選那個 action 出來
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-107 就沒事了
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-108 這個就跟行政跟立法是分立的概念
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-109 是一樣的，這樣大家了解嗎？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-110 這一個 Q，它只能夠提案
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-111 它不能夠執行，Q 負責提案，它負責選 a
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-112 Q prime 負責執行，它負責算出 Q value 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-113 所以今天就算是前面這個 Q，做了不好的提案
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-114 它選的 a 是被高估的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-115 只要後面 Q prime 不要高估這個值就好了
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-116 那就算 Q prime 會高估某個 a 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-117 只要前面這個 Q 不提案那個 a
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-118 算出來的值就不會被高估了
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-119 所以這個就是 double DQN 神奇的地方
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-120 然後你可能會說，哪來兩個 Q 跟 Q prime 呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-121 哪來兩個 network 呢？其實在實作上
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-122 你確實是有兩個 Q value 的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-123 因為一個就是你真正在 update 的 Q
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-124 另外一個就是 target 的 Q network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-125 就是你其實有兩個 Q network，一個是 target 的 Q network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-126 一個是真正你會 update 的 Q network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-127 所以在 double DQN 裡面，你的實作方法會是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-128 你拿真正的 Q network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-129 你會 update 參數的那個 Q network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-130 去選 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-131 然後你拿 target 的 network，那個固定住不動的 network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-132 去算 value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-133 而那 double DQN 相較於原來的 DQN 的更動是最少的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-134 它幾乎沒有增加任何的運算量，你看連新的 network 都不用
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-135 因為你原來就有兩個 network 了
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-136 你唯一要做的事情只有，本來你在找最大的 a 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-137 你在決定這個 a 要放哪一個的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-138 你是用 Q prime 來算
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-139 你是用 freeze 的 那個 network 來算
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-140 你是用 target network 來算
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-141 現在改成用另外一個會 update 的 Q network 來算
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-142 這個應該是改一行 code 就可以解決了，就這樣
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-143 所以假設今天，等一下會講
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-144 好幾個 tip，假如你今天只選一個 tip 的話
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-145 正常人都是 implement double DQN
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-146 所以這個就是輕易的就可以 implement
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-147 這是第一個你可以嘗試的 tip
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-148 那第二個 tip，叫做 dueling 的 DQN
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-149 dueling DQN 是什麼呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-150 其實 dueling DQN 也蠻好做的，相較於原來的 DQN
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-151 它唯一的差別是改了 network 的架構
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-152 等一下你聽了如果覺得
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-153 聽了有點沒有辦法跟上的話，你就要記住一件事
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-154 dueling DQN 它唯一做的事情
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-155 是改 network 的架構
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-156 我們說 Q network 就是 input state
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-157 output 就是每一個 action 的 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-158 dueling DQN 唯一做的事情，是改了 network 的架構
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-159 其它的演算法，你都不要去動它
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-160 那 dueling DQN 它是怎麼改了 network 的架構呢
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-161 它是這樣說的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-162 本來的 DQN 就是直接output Q value 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-163 現在這個 dueling 的 DQN 就是下面這個 network 的架構
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-164 它不直接 output Q value 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-165 它是怎麼做的？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-166 它在做的時候，它分成兩條 path 去運算
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-167 第一個 path，它算出一個 scalar
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-168 那這個 scalar 我們叫做 V of s
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-169 因為它跟 input s 是有關係，所以叫做 V of s，
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-170 V of s 是一個 scalar
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-171 那下面這個呢，它會 output 另外一個 vector
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-172 這個 vector 叫做 A of (s, a)
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-173 那下面這個 vector，它是每一個 action 都有一個 value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-174 然後你再把這兩個東西加起來，就得到你的 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-175 如果你覺得這樣不夠具體的話
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-176 實際上做的事情就是這樣
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-177 以下為白板解說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-178 但是接下來你要問的問題就是，這麼改有什麼好？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-179 所以接下來就是想要講一下，說這麼改有什麼好呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-180 那我們假設說，原來的 Q of (s, a)
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-181 它其實就是一個 table
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-182 對不對？我們假設 state 是 discrete 的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-183 那實際上 state 不是 discrete 的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-184 那為了說明方便，我們假設就是只有 4  個不同的 state
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-185 只有3 個不同的 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-186 所以 Q of (s, a) 你可以看作是一個 table
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-187 那我們說 Q of (s, a) 等於 V of s 加上 A of (s, a)
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-188 那 V of s 是對不同的 state 它都有一個值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-189 A of (s, a) 它是對不同的 state，不同的 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-190 都有一個值，那你把這個 V 的值加到 A 的每一個 column
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-191 V 的值加到 A 的每一個 column
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-192 V 的值加到 A 的每一個 column
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-193 就會得到 Q 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-194 把 2+1，2+(-1)，2+0，就得到 3，1，2，以此類推
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-195 所以你就把這個東西，給它加上去
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-196 加上去，加上去，就得到上面這個
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-197 你把 V 加上 A，就得到 Q
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-198 那今天假設說，你在 train network 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-199 你現在的 target 是希望，這一個值變成 4
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-200 這一個值變成 0
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-201 但是你實際上能更動的，並不是 Q 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-202 你的 network 更動的是 V 跟 A 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-203 根據 network 的參數，V 跟 A 的值 output 以後
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-204 就直接把它們加起來，所以其實不是更動 Q 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-205 然後在 learn network 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-206 假設你希望這邊的值，這個 3 增加 1 變成 4
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-207 這個 -1 增加 1 變成 0
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-208 最後你在 train network 的時候，network 可能會選擇說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-209 我們就不要動這個 A 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-210 就動 V 的值，把 V 的值，從 0 變成 1
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-211 那你把 0 變成 1 有什麼好處呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-212 這個時候你會發現說，本來你只想動這兩個東西的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-213 那你會發現說，這個第三個值也動了
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-214 所以有可能說你在某一個 state
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-215 你明明只 sample 到這 2 個 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-216 你沒 sample 到第三個 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-217 但是你其實也可以更動到第三個 action 的 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-218 那這樣的好處就是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-219 你就變成你不需要把所有的 state action pair 都 sample 過
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-220 你可以用比較 efficient 的方式
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-221 去 estimate Q value 出來
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-222 因為有時候你 update 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-223 不一定是 update 下面這個 table
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-224 而是只 update 了 V of s
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-225 但 update V of s 的時候，只要一改
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-226 所有的值就會跟著改
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-227 這是一個比較有效率的方法，去使用你的 data
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-228 這個是 Dueling DQN 可以帶給我們的好處
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-229 那可是接下來有人就會問說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-230 真的會這麼容易嗎？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-231 會不會最後 learn 出來的結果是說，反正 machine 就學到說我們也不要管什麼 V 了，V 就永遠都是 0
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-232 然後反正 A 就等於 Q
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-233 那你就沒有得到任何 Dueling DQN 可以帶給你的好處，就變成跟原來的 DQN 一模一樣
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-234 所以為了避免這個問題，實際上你會對下面這個 A
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-235 下一些 constrain，你要給 A 一些 constrain
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-236 讓 update A 其實比較麻煩
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-237 讓 network 傾向於 會想要去用 V 來解問題
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-238 舉例來說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-239 你可以看原始的文件，它有不同的 constrain 啦
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-240 那一個最直覺的 constrain 是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-241 你必須要讓這個 A 的每一個 column 的和都是 0
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-242 每一個 column 的值的和都是 0
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-243 所以看我這邊舉的的例子，我的 column 的和都是 0
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-244 那如果這邊 column 的和都是 0
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-245 這邊這個 V 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-246 你就可以想成是上面 Q 的每一個 column 的平均值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-247 這個平均值，加上這些值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-248 才會變成是 Q 的 value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-249 所以今天假設你發現說你在 update 參數的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-250 你是要讓整個 row 一起被 update
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-251 你就不會想要 update 這邊
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-252 因為你不會想要 update Ａ這個 matrix
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-253 因為 A 這個 matrix 的每一個 column 的和都要是 0
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-254 所以你沒有辦法說，讓這邊的值，通通都 +1
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-255 這件事是做不到的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-256 因為它的 constrain 就是你不可以你的和永遠都是要 0
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-257 所以不可以都 +1
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-258 這時候就會強迫 network 去 update V 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-259 然後讓你可以用比較有效率的方法，去使用你的 data
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-260 那實作上怎麼做呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-261 所以實作上我們剛才說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-262 你要給這個 A 一個 constrain
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-263 那所以在實際 implement 的時候，你會這樣 implement
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-264 假設現在你的 network 的 output 是，7 3 2 好了
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-265 就舉個例子，假設你有 3 個 actions
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-266 然後在這邊 output 的 vector 是 7 3 2
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-267 你在把這個 A 跟這個 B 加起來之前
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-268 先加一個 normalization
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-269 就好像做那個 layer normalization 一樣
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-270 加一個 normalization，這個 normalization 做的事情
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-271 就是把 7+3+2 加起來等於 12，12/3 = 4
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-272 然後把這邊通通減掉 4，變成 3, -1, 2
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-273 再把  3, -1, 2 加上 1.0，得到最後的 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-274 然後這個東西啊，就是 network 的一部分
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-275 這樣你聽得懂嗎？這個 normalization 的這個 step
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-276 就是 network 的其中一部分，在 train 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-277 你從這邊也是一路 back propagate 回來的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-278 只是 normalization 這一個地方，是沒有參數的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-279 它是沒有參數，它就是一個 normalization 的 operation
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-280 那它可以放到 network 裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-281 跟 network 的其他部分 jointly trained
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-282 這樣 A 就會有比較大的 constrain
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-283 這樣 network 就會給它一些 benefit，傾向於去 update V 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-284 這個是 Dueling DQN
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-285 那其實還有很多技巧可以用，這邊我們就比較快的帶過去
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-286 有一個技巧叫做 Prioritized Replay
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-287 Prioritized Replay 是什麼意思呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-288 我們原來在 sample data 去 train 你的 Q-network 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-289 你是 uniformly 地從 experience buffer
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-290 從 buffer 裡面去 sample data
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-291 你就是 uniform 地去 sample 每一筆 data
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-292 那這樣不見得是最好的，因為也許有一些 data 比較重要呢
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-293 你做不好的那些 data
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-294 就假設有一些 data，你之前有 sample 過
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-295 你發現說那一筆 data 的 TD error，所謂 TD error 就是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-296 你的 network 的 output 跟 target 之間的差距
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-297 你的 TD error 特別大
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-298 那這些 data 代表說你在 train network 的時候，你是比較 train 不好的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-299 那既然比較 train 不好，那你就應該給它比較大的機率被 sample 到
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-300 所以這樣在 training 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-301 才會考慮那些 train 不好的 training data 多次一點
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-302 這個非常的直覺
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-303 那詳細的式子呢
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-304 你再去看一下 paper，那其實等一下助教在講 prioritized replay 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-305 會講更多東西，因為實際上在做 prioritized replay 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-306 你還不只會更改 sampling 的 process
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-307 你還會因為更改了 sampling 的 process
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-308 你會更改 update 參數的方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-309 那這個我們就留給助教講就好
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-310 所以 prioritized replay 其實並不只是改變了 sample data 的 distribution 這麼簡單
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-311 你也會改 training process，這個我們就不細講
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-312 那另外一個可以做的方法是，你可以 balance MC 跟 TD
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-313 我們剛才講說 MC 跟 TD 的方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-314 他們各自有各自的優劣
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-315 我們怎麼在 MC 跟 TD 裡面取得一個平衡呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-316 那我們的做法是這樣
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-317 在 TD 裡面，你只需要存
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-318 在某一個 state st
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-319 採取某一個 action at
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-320 得到 reward rt，還有接下來跳到那一個 state s(t+1)
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-321 但是我們現在可以不要只存一個 step 的 data
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-322 我們存大 N 個 step 的 data
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-323 我們記錄在 st 採取 at，得到 rt，會跳到什麼樣 st
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-324 一直紀錄到在第 N 個 step 以後
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-325 在 s(t+N) 採取 a(t+N) 得到 reward r(t+N)
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-326 跳到 s(t+N+1) 的這個經驗，通通把它存下來
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-327 實際上你今天在做 update 的時候，在做你 Q network learning 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-328 你的 learning 的方法會是這樣，你 learning 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-329 你是要讓 Q of (st, at)
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-330 跟你的 target value 越接近越好
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-331 而你的 target value 是什麼呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-332 你的 target value 是會把從時間 t
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-333 一直到 t+N 的 N 個 reward 通通都加起來
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-334 然後你現在 Q hat 所計算的，不是 s(t+1)
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-335 而是 s(t+N+1)
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-336 你會把大 N 個 step 以後的 state 丟進來
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-337 去計算大 N 個 step 以後，你會得到的 reward
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-338 再加上 multi-step 的 reward
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-339 然後希望你的 target value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-340 跟這個 multi-step reward 越接近越好
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-341 那你會發現說這個方法，它就是 MC 跟 TD 的結合
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-342 因為它就有 MC 的好處跟壞處
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-343 也有 TD 的好處跟壞處
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-344 那如果看它的這個好處的話
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-345 因為我們現在 sample 了比較多的 step
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-346 之前是只 sample 了一個 step，所以某一個 step 得到的 data 是 real 的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-347 接下來都是 Q value 估測出來的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-348 現在 sample 比較多 step，sample 大 N 個 step
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-349 才估測 value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-350 所以估測的部分所造成的影響
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-351 就會比較輕微，當然它的壞處就跟 MC 的壞處一樣
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-352 因為你的 r 比較多項
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-353 你把大 N 項的 r 加起來
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-354 你的 variance 就會比較大，但是你可以去調這個 N 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-355 去在 variance 跟不精確的 Q 之間呢
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-356 取得一個平衡
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-357 那這個就是一個 hyper parameter，你要調這個大 N 到底是多少
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-358 你是要多 sample 三步
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-359 還是多 sample 五步
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-360 這個就跟 network structure 是一樣
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-361 是一個你需要自己調一下的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-362 那還有其他的技術
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-363 有一個技術是要 improve 這個 exploration 這件事
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-364 我們之前講的 Epsilon Greedy 這樣的 exploration
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-365 它是在 action 的 space 上面加 noise
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-366 但是有另外一個更好的方法叫做 Noisy Net
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-367 它是在參數的 space 上面加 noise
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-368 什麼意思，Noisy Net 的意思是說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-369 每一次在一個 episode 開始的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-370 在你要跟環境互動的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-371 你就把你的 Q function 拿出來
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-372 那 Q function 裡面其實就是一個 network 嘛，就變成你把那個 network 拿出來
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-373 在 network 的每一個參數上面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-374 加上一個 Gaussian noise
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-375 那你就把原來的 Q function，變成 Q tilde
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-376 因為 Q hat 已經用過，Q hat 是那個 target network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-377 我們用 Q tilde 來代表一個 Noisy Q function
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-378 那我們把每一個參數都可能都加上一個 Gaussian noise
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-379 你就得到一個新的 network 叫做 Q tilde
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-380 那這邊要注意的事情是，我們每次在 sample 在 sample noise 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-381 要注意在每一個 episode 開始的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-382 我們才 sample network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-383 這樣大家了解我意思嗎？每個 episode 開始的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-384 開始跟環境互動之前，我們就 sample network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-385 接下來你就會用這個固定住的 noisy network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-386 去玩這個遊戲直到遊戲結束
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-387 你才重新再去 sample 新的 noise
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-388 那這個方法神奇的地方就是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-389 OpenAI 跟 Deep mind 又在同時間 propose 一模一樣的方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-390 通通都 publish 在 ICLR 2018，兩篇 paper 的方法就是一樣的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-391 不一樣的地方是，他們用不同的方法，去加 noise
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-392 那我記得那個 OpenAI 加的方法好像比較簡單
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-393 他就直接加一個 Gaussian noise，就結束了
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-394 就你把每一個參數，每一個 weight
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-395 都加一個 Gaussian noise 就結束了
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-396 然後 Deep mind 他們做比較複雜
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-397 他們的 noise 是由一組參數控制的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-398 也就是說 network 可以自己決定說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-399 它那個 noise 要加多大
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-400 但是概念就是一樣的，總之你就是把你的 Q function
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-401 的裡面的那個 network 加上一些 noise
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-402 把它變得有點不一樣，跟原來的 Q function 不一樣
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-403 然後拿去跟環境做互動
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-404 那兩篇 paper 裡面都有強調說，你這個參數啊
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-405 雖然會加 noise，但在同一個 episode 裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-406 你的參數就是固定的，你是在換 episode，玩第二場新的遊戲的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-407 你才會重新 sample noise，在同一場遊戲裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-408 就是同一個 noisy Q network，在玩那一場遊戲
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-409 這件事非常重要，為什麼這件事非常重要呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-410 因為這是導致了為什麼在 network 這個方法 Noisy Net
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-411 跟原來的 Epsilon Greedy 或是其他
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-412 在 action 做 sample 方法本質上的差異
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-413 有什麼樣本質上的差異呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-414 在原來 sample 的方法，比如說 Epsilon Greedy 裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-415 就算是給同樣的 state
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-416 你的 agent 採取的 action，也不一定是一樣的，對不對
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-417 因為你是用 sample 決定的，given 同一個 state
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-418 你如果 sample 到說喔，要根據 Q function 的 network
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-419 你會得到一個 action，你 sample 到 random
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-420 你會採取另外一個 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-421 所以 given 同樣的 state，如果你今天是用 Epsilon Greedy 的方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-422 它得到的 action，是不一樣的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-423 但是你想想看，實際上你的 policy
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-424 並不是這樣運作的啊
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-425 在一個真實世界的 policy，給同樣的 state
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-426 他應該會有同樣的回應
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-427 而不是給同樣的 state，它其實有時候吃 Q function
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-428 然後有時候又是隨機的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-429 所以這是一個比較奇怪的，不正常的 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-430 是在真實的情況下不會出現的 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-431 但是如果你是在 Q function 上面去加 noise 的話，就不會有這個情形
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-432 因為如果你今天在 Q function 上加 noise
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-433 在 Q function 的 network 的參數上加 noise
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-434 那在整個互動的過程中，在同一個 episode 裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-435 它的 network 的參數總是固定的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-436 所以看到同樣的 state，或是相似的 state
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-437 就會採取同樣的 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-438 那這個是比較正常的，那在 paper 裡面有說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-439 這個叫做 state dependent exploration
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-440 也就是說你雖然會做 explore 這件事，但是你的 explore 是跟 state 有關係的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-441 看到同樣的 state，你就會採取同樣的 exploration 的方式
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-442 也就是說你在 explore 你的環境的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-443 你是用一個比較 consistent 一致的方式
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-444 去測試這個環境
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-445 也就是上面你是 noisy 的 action，你只是隨機亂試
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-446 但是如果你是在參數下加 noise
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-447 那在同一個 episode 裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-448 裡面你的參數是固定的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-449 那你就是有系統地在嘗試
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-450 每次會試說，在某一個 state，我都向左試試看
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-451 然後再下一次在玩這個同樣遊戲的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-452 看到同樣的 state，你就說我再向右試試看
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-453 你是有系統地在 explore 這個環境
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-454 Demo
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-455 還有另外一個東西，這個東西叫做 Distributional Q-function
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-456 我們就不講它的細節，只告訴你他的大概念
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-457 那 Distributional Q-function 我覺得還蠻有道理的，但是它沒有紅起來
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-458 你就發現說沒有太多人真的在實作的時候用這個技術
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-459 可能一個原因就是，是因為他不好實作
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-460 它怎麼實作，它的意思是什麼？它說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-461 我們說 Q function 到底是什麼意思啊
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-462 我們說 Q function 是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-463 accumulated reward 的期望值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-464 所以我們算出來的這個 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-465 它其實是一個期望值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-466 也就是說實際上我在某一個 state 採取某一個 action 的時候，因為環境是有隨機性
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-467 在某一個 state 採取某一個 action 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-468 實際上我們把所有的 reward 玩到遊戲結束的時候所有的 reward，進行一個統計
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-469 你其實得到的是一個 distribution
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-470 也許在 reward 得到 0 的機率很高
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-471 在 -10 的機率比較低，在 +10 的機率比較低
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-472 但是它是一個 distribution
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-473 那這個 Q value 代表的值是說我們對這一個 distribution 算它的 mean
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-474 才是這個 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-475 我們算出來是 expected accumulated reward
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-476 所以這 accumulated reward 是一個 distribution
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-477 對它取 expectation，對它取 mean
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-478 你得到了 Q value，但是有趣的地方是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-479 不同的 distribution，他們其實可以有同樣的 mean
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-480 也許真正的 distribution 是這個樣子
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-481 它算出來的 mean 跟這個 distribution 算出來的 mean，其實是一樣的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-482 但他們背後所代表的 distribution，其實是不一樣的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-483 所以今天假設我們只用一個 expected 的 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-484 來代表整個 reward 的話
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-485 其實可能是有一些 information 是 loss 的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-486 你沒有辦法 model reward 的 distribution
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-487 所以今天 Distributional Q function 它想要做的事情是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-488 model distribution
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-489 所以怎麼做？在原來的 Q function 裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-490 假設你只能夠採取 a1, a2, a3, 3 個 actions
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-491 那你就是 input 一個 state，output 3 個 values
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-492 3 個 values 分別代表 3 個actions 的 Q value
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-493 但是這個 Q value，是一個 distribution 的期望值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-494 所以今天 Distributional Q function，它的 ideas 就是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-495 何不直接 output 那個 distribution
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-496 但是要直接 output 一個 distribution 也不知道怎麼做嘛，對不對
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-497 但實際上的做法是說，假設 distribution 的值就分佈在某一個 range 裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-498 比如說 -10 到 10，那把 -10 到 10 中間
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-499 拆成一個一個的 bin，拆成一個一個的長條圖
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-500 拆成一個一個的 bin，舉例來說，在這個例子裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-501 每一個 action，對我們把 reward 的 space 就拆成 5 個 bin
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-502 拆成 5 個 bin，拆成 5 個 bin
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-503 詳細一點的作法就是，假設 reward 可以拆成 5 個 bin 的話
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-504 今天你的 Q function 的 output，是要預測說在某一個 bin 裡面
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-505 你在某一個 state，採取某一個 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-506 你得到的 reward，落在某一個 bin 裡面的機率
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-507 所以其實這邊的機率的和，這些綠色的 bar 的和應該是 1
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-508 它的和應該是 1，它的和應該是 1，它的高度代表說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-509 在某一個 state，採取某一個 action 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-510 它落在某一個 bin 的機率，這邊綠色的代表 action 1
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-511 紅色的代表 action 2，藍色的代表 action 3
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-512 所以今天你就可以真的用 Q function 去 estimate a1 的 distribution
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-513 a2 的 distribution，a3 的 distribution
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-514 那實際上在做 testing 的時候，我們還是要選某一個 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-515 去執行嘛，那選哪一個 action 呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-516 實際上在做的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-517 他還是選這個 mean 最大的那個 action 去執行
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-518 但是假設我們今天可以 model distribution 的話
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-519 除了選 mean 最大的以外
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-520 也許在未來你可以有更多其他的運用
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-521 舉例來說，你可以考慮它的 distribution 長什麼樣子
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-522 若 distribution variance 很大
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-523 代表說採取這個 action
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-524 雖然 mean 可能平均而言很不錯
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-525 但也許風險很高
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-526 你可以 train一個 network 它是可以規避風險的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-527 就在 2 個 action mean 都差不多的情況下
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-528 也許他可以選一個風險比較小的 action 來執行
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-529 這是 Distributional Q function 的好處
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-530 那細節怎麼 train 這樣的 Q network，我們就不講
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-531 你只要記得說反正 Q network 有辦法 output 一個 distribution 就對了
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-532 我們可以不只是估測 mean 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-533 我們不只是估測得到的期望 reward mean 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-534 我們其實是可以估測一個 distribution 的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-535 Demo
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-536 那最後跟大家講的是一個叫做 rainbow 的技術
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-537 這個 rainbow 它的技術是什麼呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-538 rainbow 這個技術就是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-539 把剛才所有的方法都綜合起來就變成 rainbow 啊
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-540 因為剛才每一個方法，就是有一種自己的顏色
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-541 把所有的顏色通通都合起來，就變成 rainbow
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-542 我們算算看是不是真的有所有的方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-543 結果我仔細算一下，不是才 6 種方法而已嗎？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-544 為什麼你會變成是 7 色的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-545 也許它把原來的 DQN 也算是一種方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-546 那我們來看看這些不同的方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-547 這個灰色這一條，它這個縱軸就是
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-548 這個橫軸是你 training process
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-549 縱軸是玩了 10 幾個 ATARI 小遊戲的平均的分數的和
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-550 但它取的是 median 的分數
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-551 為什麼是取 median 不是直接取平均呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-552 因為它說每一個小遊戲的分數，其實差很多
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-553 如果你取平均的話，到時候某幾個遊戲就 dominate 你的結果
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-554 所以它取 median 的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-555 那這個如果你是一般的 DQN
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-556 就灰色這一條線，就沒有很強
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-557 那如果是你換 noisy DQN，就強很多
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-558 然後如果這邊每一個單一顏色的線是代表說只用某一個方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-559 那紫色這一條線是 DDQN double DQN，DDQN 還蠻有效的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-560 你換 DDQN 就從灰色這條線跳成紫色這一條線
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-561 然後 Prioritized DQN， Dueling DQN
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-562 還有 Distributional DQN 都蠻強的，它們都差不多很強的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-563 那這邊有個 A3C
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-564 A3C 其實是 Actor-Critic 的方法，這我們下週會講
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-565 那單純的 A3C 看起來是比 DQN 強的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-566 那發現這邊怎麼沒有 Multi step 的方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-567 他們講的 Multi step 的方法就 balance TD 跟 MC
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-568 我猜是因為 A3C 本身內部就有做 Multi step 的方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-569 所以他可能覺得說有 implement A3C 就算是有 implement
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-570 Multi step 的方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-571 所以可以把這個 A3C 的結果想成是 Multi step 的方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-572 最後其實這些方法他們本身之間是沒有衝突的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-573 所以全部都用上去
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-574 就變成七彩的一個方法
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-575 就叫做 rainbow，然後它很高這樣
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-576 這是 rainbow 的第一張圖，這是下一張圖
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-577 這張圖要說的是什麼呢？這張圖要說的事情是說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-578 在 rainbow 這個方法裡面，如果我們每次拿掉其中一個技術
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-579 到底差多少，因為現在是把所有的方法通通倒在一起
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-580 發現說進步很多，但會不會有些方法其實是沒用的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-581 所以看看說，每一個方法哪些方法特別有用，哪些方法特別沒用
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-582 所以這邊的虛線就是，拿掉某一種方法以後的結果
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-583 那你發現說，黃色，拿掉 Multi time step 掉很多
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-584 rainbow 是彩色這一條，拿掉 Multi time step，馬上就掉到這裡
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-585 掉到這裡，然後拿掉 Prioritized replay，也馬上就掉下來
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-586 拿掉這個 distribution，它也就掉下來
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-587 那這邊有一個有趣的地方是說，在開始的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-588 distribution 訓練的方法跟其他方法速度差不多
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-589 但是如果你拿掉 distribution 的時候，你的訓練不會變慢
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-590 但是你最後 performance，最後會收斂在比較差的地方
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-591 然後拿掉 Noisy Net，performance 也是差一點
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-592 拿掉 Dueling 也是差一點，那發現拿掉 Double
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-593 沒什麼用這樣子，你就拿掉 Double 沒什麼差
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-594 所以看來全部到再一起的時候，Double 是比較沒有影響的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-595 那其實在 paper 裡面有給一個 make sense 的解釋，他說
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-596 其實當你有用 Distributional DQN的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-597 本質上就不會 over estimate 你的 reward
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-598 因為我們之所以用 Double 的理由是因為，害怕會 over estimate reward 嘛
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-599 over estimate 因為，避免 over estimate reward 才加了 double DQN
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-600 那在 paper 裡面有講說，如果有做 Distributional DQN
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-601 就比較不會有 over estimate 的結果，事實上他有真的算了一下發現説
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-602 它其實多數的狀況
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-603 是 under estimate reward 的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-604 所以會變成 Double DQN 沒有用
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-605 那為什麼做Distributional DQN，不會 over estimate reward
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-606 反而會 under estimate reward 呢？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-607 因為可能是說，現在這個 distributional DQN
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-608 我們不是說它 output 的是一個 distribution 的 range 嗎？
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-609 所以你 output 的那個 range 啊，不可能是無限寬的
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-610 你一定是設一個 range，比如說我最大 output range 就是從 -10 到 10
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-611 那假設今天得到的 reward 超過 10 怎麼辦？是 100 怎麼辦，就當作沒看到這件事
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-612 所以會變成說，reward 很極端的值，很大的值
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-613 其實是會被丟掉的，所以變成說你今天用 Distributional DQN 的時候
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-614 你不會有 over estimate 的現象
DRL_Lecture_4_-_Q-learning_(Advanced_Tips)-615 反而有 under estimate 的傾向就是了
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-0 今天的規劃是這樣，我們剩下一點點 Q learning 的部分把它講完
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-1 接下來要講 Actor-critic
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-2 講一些 Actor-critic 之後，之後要講說假設在 reinforcement learning 有碰到
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-3 reward 非常 sparse 的情況怎麼辦
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-4 然後最後會講說，如果完全沒有 reward 的時候
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-5 要怎麼辦，完全沒有 reward 的狀況叫做 Imitation learning
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-6 今天首先跟大家繼續講一下 Q-learning
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-7 那其實跟 policy gradient based 方法比起來
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-8 Q learning 其實是比較穩的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-9 policy gradient 其實是沒有太多遊戲是玩得起來的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-10 你想想看我們作業 4-1，是要做 Pong
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-11 4-2 是要做 Breaking
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-12 那會什麼 4-1 不用 Breaking 呢？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-13 因為 policy gradient 在 Breaking 上面是做不太起來的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-14 所以 policy gradient 其實比較不穩，尤其在沒有 PPO 之前
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-15 你很難用 policy gradient 做什麼事情
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-16 Q learning 相對而言是比較穩的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-17 可以看最早 Deep reinforcement learning 受到大家注意
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-18 最早 deep mind 的 paper 拿 deep reinforcement learning 來玩 Atari 的遊戲
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-19 用的就是 Q-learning
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-20 那我覺得 Q-learning 比較容易，比較好train 的一個理由是
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-21 我們說在 Q-learning 裡面，你只要能夠 estimate 出Q-function
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-22 就保證你一定可以找到一個比較好的 policy
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-23 也就是你只要能夠 estimate 出 Q-function
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-24 就保證你可以 improve 你的 policy
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-25 而 estimate Q function 這件事情，是比較容易的，為什麼？因為她就是一個 regression 的 problem
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-26 在這個 regression 的 problem 裡面，你可以輕易地知道說
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-27 你現在的 model learn 的是不是越來越好
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-28 你只要看那個 regression 的 loss 有沒有下降，你就知道說你的 model learn 的好不好
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-29 所以 estimate Q function 相較於 learn 一個 policy
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-30 是比較容易的，然後你只要 estimate Q function
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-31 就可以保證說，你現在一定會得到比較好的 policy
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-32 所以一般而言 Q learning 是比較容易操作
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-33 那 Q learning 有什麼問題呢？他一個最大的問題是
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-34 他不太容易處理 continuous action
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-35 很多時候你的 action 是 continuous 的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-36 什麼時候你的 action 會是 continuous 的呢？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-37 在作業裡面，我們都只玩 Atari 的遊戲
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-38 你的 agent 只需要決定
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-39 比如說上下左右，這種 action 是 discrete 的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-40 那很多時候你的 action 是 continuous 的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-41 舉例來說假設你的 agent 要做的事情是開自駕車
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-42 它要決定說它方向盤要左轉幾度，右轉幾度，這是 continuous 的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-43 假設你的 agent 是一個機器人
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-44 它的每一個 action 對應到的就是它的，假設它身上有 50 個 關節
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-45 它的每一個 action 就對應到它身上的這 50 個關節的角度
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-46 而那些角度，也是 continuous 的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-47 所以很多時候你的 action，並不是一個 discrete 的東西
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-48 它是一個 vector，這個 vector 裡面
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-49 它的每一個 dimension 都有一個對應的 value
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-50 都是 real number，它是 continuous 的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-51 假設你的 action 是 continuous 的時候
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-52 做 Q learning 就會有困難
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-53 為什麼呢？因為我們說在做 Q-learning 裡面
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-54 一個很重要的一步是
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-55 你要能夠解這個 optimization 的 problem
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-56 你 estimate 出Q function，Q of (s, a) 以後
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-57 必須要找到一個 a，它可以讓 Q of (s, a) 最大
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-58 假設 a 是 discrete 的，那 a 的可能性都是有限的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-59 舉例來說 Atari 的小遊戲裡面，a 就是上下左右跟開火
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-60 它是有限的，你可以把每一個可能的 action 都帶到 Q 裡面
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-61 算它的 Q value
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-62 但是假如 a 是 continuous 的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-63 你會很麻煩這個 continuous 的 action
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-64 你無法窮舉所有可能 continuous action 試試看那一個 continuous action 可以讓 Q 的 value 最大
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-65 所以怎麼辦呢？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-66 在概念上，反正我們就是要能夠解這個問題
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-67 但是怎麼解這個問題呢？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-68 就有各種不同的 solution
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-69 第一個 solution 是
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-70 假設你不知道怎麼解這個問題，因為 a 是很多的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-71 a 是沒有辦法窮舉的，怎麼辦，用 sample 的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-72 sample 出大 N 個 可能的 a
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-73 一個一個帶到 Q function 裡面
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-74 那看誰最快？這個方法其實也不會太不  efficient，因為其實你真的在運算的時候
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-75 你會用 GPU，所以你一次會把 N 個 continuous action
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-76 都丟到 Q function 裡面
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-77 一次得到 N 個 Q value，然後看誰最大
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-78 那當然這個不是一個 非常精確的做法
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-79 因為你真的沒有辦法做太多的 sample，所以你 estimate 出來的 Q value
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-80 你最後決定的 action，可能不是非常的精確，這是第一個 solution
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-81 那第二個 solution 是什麼呢？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-82 今天既然我們要解的是一個 optimization 的 problem
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-83 你會不會解這種 optimization 的 problem 呢？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-84 你其實是會的，因為你其實可以用 gradient decent 的方法
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-85 來解這個 optimization 的 problem
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-86 我們現在其實是要 maximize 我們的 objective function
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-87 我們是要 maximize 一個東西，所以這是 gradient decent
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-88 這是 gradient ascent，我的意思是一樣的，我相信大家都很清楚
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-89 你就把 a 當作是你的 parameter
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-90 然後你要找一組 a 去 maximize 你的 Q function
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-91 那你就用 gradient ascent 去 update a 的 value
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-92 最後看看你能不能找到一個 a 去 maximize 你的 Q function
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-93 也就是你的 objective function
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-94 當然這樣子你會遇到的問題，就是
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-95 global maximum 的問題，就不見得能夠真的找到最 optimal 的結果
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-96 而且這個運算量顯然很大，因為你要 iterative 的去 update 你的 a
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-97 我們 train 一個 network 就很花時間了
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-98 今天如果你是用這樣子 gradient ascent 的方法呢
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-99 來處理這個 continuous 的 problem，等於是你每次要決定要 take 哪一個 action 的時候
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-100 你都還要做一次 train network 的 process
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-101 這個顯然運算量是很大的，這是第二個 solution
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-102 那第三個 solution 呢
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-103 這是有點神奇，第三個 solution 是
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-104 特別 design 一個network 的架構
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-105 特別 design 你的 Q function
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-106 使得解那個 arg max 的 problem
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-107 變得非常容易
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-108 也就是這邊的 Q function 不是一個 general 的 Q function
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-109 特別設計一下它的樣子
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-110 讓你要找哪一個 a 可以讓這個 Q function 最大的時候，非常容易
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-111 那這邊是一個例子，這邊有我們的 Q function
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-112 然後這個 Q function 它的作法是這樣
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-113 input 你的 state s，通常它就是一個 image，它可以用一個向量，或是一個 matrix 來表示
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-114 input 這個 s，這個 Q function 會 output 3 個東西
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-115 它會 output mu of s，這是一個 vector
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-116 它會 output sigma of s，這個 sigma of s，是一個 matrix
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-117 它會 output V of s，V of s，是一個 scalar
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-118 output 這 3 個東西以後，我們知道 Q function 其實是吃一個 s 跟 a
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-119 然後決定一個 value，對不對
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-120 Q function 意思是說在某一個 state，take 某一個 action 的時候，你 expected 的 reward 有多大
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-121 到目前為止這個 Q function 只吃 s，它還沒有吃 a 進來
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-122 a 在那裡呢，a 在當這個 Q function 吐出 mu
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-123 sigma 跟 V 的時候
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-124 我們才把 s 引入，用 a 跟這 3 個東西互相作用一下
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-125 你才算出最終的 Q value
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-126 a 怎麼和這 3 個東西互相作用呢？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-127 它的作用方法就寫在下面這個地方
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-128 所以實際上 Q of (s, a)，你的 Q function 的運作方式是
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-129 先 input s，讓你得到 mu
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-130 sigma 跟 V
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-131 然後再 input a
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-132 然後接下來的計算方法是把 a 跟 mu 相減
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-133 注意一下 a 現在是 continuous 的 action
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-134 所以它也是一個 vector，假設你現在是要操作機器人的話，這個 vector 的每一個 dimension
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-135 可能就對應到機器人的某一個關節，它的數值
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-136 就是那關節的角度
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-137 所以 a 是一個 vector
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-138 把 a 的這個 vector
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-139 減掉 mu 的這個 vector，取 transpose
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-140 所以它是一個橫的 vector，它是倒下來的 vector
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-141 sigma 是一個 matrix
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-142 然後 a 減掉 mu of s
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-143 這兩個都是 vector，減掉以後還是一個豎的 vector
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-144 然後接下來你把這一個 vector
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-145 乘上這個 matrix，再乘上這個 vector
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-146 你得到的是什麼？你得到是一個 scalar 對不對？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-147 把這個 scalar 再加上 V of s
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-148 得到另外一個 scalar
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-149 這一個數值就是你的 Q of (s, a)，就是你的 Q value
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-150 那接下來假設我們的 Q of (s, a) 定義成這個樣子
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-151 我們要怎麼找到一個 a
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-152 去 maximize 這個 Q value 呢？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-153 其實這個 solution 非常簡單
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-154 因為我們把 formulation 寫成這樣
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-155 那什麼樣的 a，可以讓這一個 Q function 最終的值，最大呢？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-156 因為這邊這一項，a 減 mu 乘上 sigma
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-157 再乘上 a 減 mu 這一項一定是正的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-158 然後前面乘上一個負號
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-159 所以第一項
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-160 就假設我們不要看這個負號的話
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-161 第一項這個值越小，你最終的這個 Q value 就越大
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-162 因為我們是把 V 減掉第一項
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-163 所以第一項
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-164 假設不要看這個負號的話，第一項的值越小
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-165 最後的 Q value 就最大，越大
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-166 怎麼讓第一項的值最小呢？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-167 你直接把 a 帶 mu
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-168 讓它變成 0，就會讓第一項的值最小
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-169 你這個問題問得很好
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-170 假設它是不是正定的是嗎？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-171 你說如果它不是正定的話，它有可能會出現 negative 的 value
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-172 對，其實有假設它是正定的，這邊少講一個東西
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-173 因為你知道這個東西，就像是那個 Gaussian distribution
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-174 所以 mu 就是 Gaussian 的 mean
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-175 sigma 就是 Gaussian 的 various
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-176 但是 various 是一個 positive definite 的 matrix
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-177 所以其實怎麼樣讓這個 sigma，一定是 positive definite 的 matrix 呢？
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-178 其實在 Q(pi) 裡面
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-179 它不是直接 output sigma
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-180 就如果直接 output 一個 sigma，它可能不見得是 positive definite 的 matrix
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-181 它其實是 output 一個 matrix
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-182 然後再把那個 matrix 跟另外一個 matrix
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-183 做 transpose 相乘，然後可以確保它是 positive definite 的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-184 所以這邊確實是漏講了一塊，感謝你有特別提出來
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-185 這邊要強調的點就是說，實際上它不是直接output 一個 matrix
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-186 你再去那個 paper 裡面 check 一下，它的 trick
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-187 它可以保證說 sigma 是  positive definite 的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-188 所以今天前面這一項
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-189 因為 sigma 是 positive definite，所以它一定是正的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-190 所以現在怎麼讓它值最小呢？你就把 a 帶 mu of s，
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-191 你把 a 帶 mu of s 以後呢
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-192 你可以讓 Q 的值最大
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-193 所以這個 problem 就解了，所以今天假設要你 arg max 這個東西
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-194 雖然 in general 而言，若 Q 是一個 general function，你很難算，但是我們這邊
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-195 philosophically design 了 Q 這個 function
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-196 所以 a 只要設 mu of s
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-197 我們就得到 maximum 的 value
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-198 你在解這個 arg max 的 problem 的時候
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-199 就變得非常的容易
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-200 所以其實 Q learning 也不是不能夠用在 continuous path
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-201 是可以用的，只是就是有一些侷限，就是你的 function 就是不能夠隨便亂設，它必須有一些限制
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-202 那第 4 招是什麼，第 4 招就是不要用 Q-learning
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-203 用 Q learning 處理 continuous 的 action 還是比較麻煩
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-204 那到目前為止，我們講了 policy based 的方法
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-205 我們講了 PPO，這是上上周講的
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-206 上周講了 value based 的方法，也就是 Q learning
DRL_Lecture_5_-_Q-learning_(Continuous_Action)-207 但是這兩者其實是可以結合在一起的，也就是 Actor-Critic 的方法
DRL_Lecture_6_-_Actor-Critic-0 接下來要跟大家講一下
DRL_Lecture_6_-_Actor-Critic-1 Actor-Critic，要跟大家講一下 Actor-Critic
DRL_Lecture_6_-_Actor-Critic-2 那在 Actor-Critic 裡面，最知名的方法就是 A3C
DRL_Lecture_6_-_Actor-Critic-3 Asynchronous Advantage Actor-Critic
DRL_Lecture_6_-_Actor-Critic-4 那等一下就要跟大家介紹一下，這個 A3C 是什麼？
DRL_Lecture_6_-_Actor-Critic-5 如果去掉前面這個 Asynchronous 這個字眼啊
DRL_Lecture_6_-_Actor-Critic-6 只有 Advantage Actor-Critic，就叫做 A2C
DRL_Lecture_6_-_Actor-Critic-7 那如果前面加了 Asynchronous 這個字眼呢
DRL_Lecture_6_-_Actor-Critic-8 變成 Asynchronous Advantage Actor-Critic，就變成 A3C
DRL_Lecture_6_-_Actor-Critic-9 那我們等一下會來講一下這個東西是什麼
DRL_Lecture_6_-_Actor-Critic-10 那我們很快複習一下 policy gradient
DRL_Lecture_6_-_Actor-Critic-11 在 policy gradient 裡面，我們是怎麼說的呢？
DRL_Lecture_6_-_Actor-Critic-12 在 policy gradient 裡面我們說我們在 update 我們的 policy 的參數 θ的時候
DRL_Lecture_6_-_Actor-Critic-13 我們是用了以下這個式子，來算出我們的 gradient
DRL_Lecture_6_-_Actor-Critic-14 那我們說以下這個式子其實是還蠻直覺的
DRL_Lecture_6_-_Actor-Critic-15 以下這個式子在說什麼呢？
DRL_Lecture_6_-_Actor-Critic-16 以下這個式子是在說，我們先讓 agent 去跟環境互動一下
DRL_Lecture_6_-_Actor-Critic-17 然後我們知道我們在某一個 state, s
DRL_Lecture_6_-_Actor-Critic-18 採取了某一個 action, a
DRL_Lecture_6_-_Actor-Critic-19 那我們可以計算出在某一個 state , s
DRL_Lecture_6_-_Actor-Critic-20 採取了某一個 action, a 的機率
DRL_Lecture_6_-_Actor-Critic-21 接下來，我們去計算說
DRL_Lecture_6_-_Actor-Critic-22 從這一個 state 採取這個 action, a 之後
DRL_Lecture_6_-_Actor-Critic-23 accumulated reward 有多大
DRL_Lecture_6_-_Actor-Critic-24 從這個時間點開始，在某一個 state, s
DRL_Lecture_6_-_Actor-Critic-25 採取了某一個 action, a 之後，到遊戲結束
DRL_Lecture_6_-_Actor-Critic-26 到互動結束為止，我們到底 collect 了多少的 reward
DRL_Lecture_6_-_Actor-Critic-27 那我們把這些 reward
DRL_Lecture_6_-_Actor-Critic-28 從時間 t 到時間大 T 的 reward 通通加起來
DRL_Lecture_6_-_Actor-Critic-29 那有時候我們會在前面
DRL_Lecture_6_-_Actor-Critic-30 乘一個 discount factor
DRL_Lecture_6_-_Actor-Critic-31 因為我們之前也有講過說
DRL_Lecture_6_-_Actor-Critic-32 離現在這個時間點比較久遠的 action
DRL_Lecture_6_-_Actor-Critic-33 它可能是跟現在這個 action 比較沒有關係的
DRL_Lecture_6_-_Actor-Critic-34 所以我們會給它乘一個 discount 的 factor
DRL_Lecture_6_-_Actor-Critic-35 可能設 0.9 或 0.99
DRL_Lecture_6_-_Actor-Critic-36 那我們接下來還說，我們會減掉一個 bias
DRL_Lecture_6_-_Actor-Critic-37 減掉一個 baseline b，減掉一個值 b
DRL_Lecture_6_-_Actor-Critic-38 減掉這個值 b 的目的，是希望括號這裡面這一項
DRL_Lecture_6_-_Actor-Critic-39 是有正有負的，那如果括號裡面這一項是正的
DRL_Lecture_6_-_Actor-Critic-40 那我們就要增加在這個 state 採取這個 action 的機率
DRL_Lecture_6_-_Actor-Critic-41 如果括號裡面是負的
DRL_Lecture_6_-_Actor-Critic-42 我們就要減少在這個 state 採取這個 action 的機率
DRL_Lecture_6_-_Actor-Critic-43 這個是我們之前都講過的東西
DRL_Lecture_6_-_Actor-Critic-44 那我們把這個 accumulated reward
DRL_Lecture_6_-_Actor-Critic-45 從這個時間點採取 action, a
DRL_Lecture_6_-_Actor-Critic-46 一直到遊戲結束為止會得到的 reward
DRL_Lecture_6_-_Actor-Critic-47 用 G 來表示它
DRL_Lecture_6_-_Actor-Critic-48 那這個 b 呢？就是一個 baseline
DRL_Lecture_6_-_Actor-Critic-49 它是要確保説括號裡面的值有時候是正的
DRL_Lecture_6_-_Actor-Critic-50 有時候是負的
DRL_Lecture_6_-_Actor-Critic-51 好那但是問題是 G 這個值啊
DRL_Lecture_6_-_Actor-Critic-52 它其實是非常的 unstable 的
DRL_Lecture_6_-_Actor-Critic-53 為什麼會說 G 這個值是非常的 unstable 的呢
DRL_Lecture_6_-_Actor-Critic-54 因為你想想看，這個互動的 process
DRL_Lecture_6_-_Actor-Critic-55 其實本身是有隨機性的
DRL_Lecture_6_-_Actor-Critic-56 所以我們在某一個 state, s，採取某一個 action, a
DRL_Lecture_6_-_Actor-Critic-57 然後計算 accumulated reward
DRL_Lecture_6_-_Actor-Critic-58 每次算出來的結果，都是不一樣的，對不對？
DRL_Lecture_6_-_Actor-Critic-59 所以 G 其實是一個 random variable
DRL_Lecture_6_-_Actor-Critic-60 給同樣的 state, s，給同樣的 action, a
DRL_Lecture_6_-_Actor-Critic-61 G 它可能有一個固定的 distribution
DRL_Lecture_6_-_Actor-Critic-62 但我們是採取 sample 的方式
DRL_Lecture_6_-_Actor-Critic-63 我們在某一個 state, s，採取某一個 action, a
DRL_Lecture_6_-_Actor-Critic-64 然後玩到底，我們看看說我們會得到多少的 reward
DRL_Lecture_6_-_Actor-Critic-65 我們就把這個東西當作 G
DRL_Lecture_6_-_Actor-Critic-66 所以我們只有對 G
DRL_Lecture_6_-_Actor-Critic-67 把 G 想成是一個 random variable 的話
DRL_Lecture_6_-_Actor-Critic-68 我們實際上做的事情是
DRL_Lecture_6_-_Actor-Critic-69 對這個 G 做一些 sample
DRL_Lecture_6_-_Actor-Critic-70 然後拿這些 sample 的結果，去 update 我們的參數
DRL_Lecture_6_-_Actor-Critic-71 但實際上在某一個 state, s 採取某一個 action, a
DRL_Lecture_6_-_Actor-Critic-72 接下來會發生什麼事，它本身是有隨機性的
DRL_Lecture_6_-_Actor-Critic-73 雖然說有個固定的 distribution
DRL_Lecture_6_-_Actor-Critic-74 但它本身是有隨機性的，而這個 random variable
DRL_Lecture_6_-_Actor-Critic-75 它的 variance，可能會非常的巨大
DRL_Lecture_6_-_Actor-Critic-76 就你在同一個 state 採取同一個 action
DRL_Lecture_6_-_Actor-Critic-77 你最後得到的結果，可能會是天差地遠的
DRL_Lecture_6_-_Actor-Critic-78 那今天假設我們可以 sample 足夠的次數
DRL_Lecture_6_-_Actor-Critic-79 我們在每次 update 參數之前
DRL_Lecture_6_-_Actor-Critic-80 我們都可以 sample 足夠的次數
DRL_Lecture_6_-_Actor-Critic-81 那其實沒有什麼問題，但問題就是
DRL_Lecture_6_-_Actor-Critic-82 我們每次做 policy gradient
DRL_Lecture_6_-_Actor-Critic-83 每次 update 參數之前都要做一些 sample
DRL_Lecture_6_-_Actor-Critic-84 這個 sample 的次數，其實是不可能太多的
DRL_Lecture_6_-_Actor-Critic-85 我們只能夠做非常少量的 sample
DRL_Lecture_6_-_Actor-Critic-86 那如果你今天正好 sample 到差的結果
DRL_Lecture_6_-_Actor-Critic-87 比如說你正好 sample 到 G = 100
DRL_Lecture_6_-_Actor-Critic-88 正好 sample 到 G = -10
DRL_Lecture_6_-_Actor-Critic-89 那顯然你的結果會是很差的
DRL_Lecture_6_-_Actor-Critic-90 所以接下來我們要問的問題是
DRL_Lecture_6_-_Actor-Critic-91 能不能讓這整個 training process
DRL_Lecture_6_-_Actor-Critic-92 變得比較 stable 一點，我們能不能夠直接估測
DRL_Lecture_6_-_Actor-Critic-93 G 這個 random variable 的期望值
DRL_Lecture_6_-_Actor-Critic-94 我們在 state, s 採取 action, a 的時候
DRL_Lecture_6_-_Actor-Critic-95 我們直接想辦法用一個 network
DRL_Lecture_6_-_Actor-Critic-96 去估測在 state s 採取 action a 的時候
DRL_Lecture_6_-_Actor-Critic-97 你的 G 的期望值，如果這件事情是可行的
DRL_Lecture_6_-_Actor-Critic-98 那之後 training 的時候
DRL_Lecture_6_-_Actor-Critic-99 就用期望值來代替 sample 的值
DRL_Lecture_6_-_Actor-Critic-100 那這樣會讓 training 變得比較 stable
DRL_Lecture_6_-_Actor-Critic-101 從期望值來代替 sample 的值
DRL_Lecture_6_-_Actor-Critic-102 那怎麼拿期望值代替 sample 的值呢？
DRL_Lecture_6_-_Actor-Critic-103 這邊就需要引入 value based 的方法
DRL_Lecture_6_-_Actor-Critic-104 value based 的方法我們介紹的就是 Q learning
DRL_Lecture_6_-_Actor-Critic-105 那在講 Q learning 的時候我們說
DRL_Lecture_6_-_Actor-Critic-106 有兩種 functions，有兩種 critics
DRL_Lecture_6_-_Actor-Critic-107 第一種 critic 我們寫作 V，它的意思是說
DRL_Lecture_6_-_Actor-Critic-108 假設我們現在 actor 是 pi
DRL_Lecture_6_-_Actor-Critic-109 那我們拿 pi 去跟環境做互動
DRL_Lecture_6_-_Actor-Critic-110 當今天我們看到 state, s 的時候
DRL_Lecture_6_-_Actor-Critic-111 接下來 accumulated reward 的期望值有多少
DRL_Lecture_6_-_Actor-Critic-112 還有另外一個 critic，叫做 pi
DRL_Lecture_6_-_Actor-Critic-113 pi 是吃 s 跟 a 當作 input
DRL_Lecture_6_-_Actor-Critic-114 他的意思是說，在 state s 採取 action a
DRL_Lecture_6_-_Actor-Critic-115 接下來都用 actor pi 來跟環境進行互動
DRL_Lecture_6_-_Actor-Critic-116 那 accumulated reward 的期望值，是多少
DRL_Lecture_6_-_Actor-Critic-117 那這個都是我們看過的圖啦，V input s
DRL_Lecture_6_-_Actor-Critic-118 output 一個 scalar
DRL_Lecture_6_-_Actor-Critic-119 Q inputs，然後它會給每一個 a 呢
DRL_Lecture_6_-_Actor-Critic-120 都 assign 一個 Q value
DRL_Lecture_6_-_Actor-Critic-121 那這個 estimate 的時候
DRL_Lecture_6_-_Actor-Critic-122 你可以用 TD 也可以用 MC
DRL_Lecture_6_-_Actor-Critic-123 TD 會比較穩， 然後用 MC 比較精確
DRL_Lecture_6_-_Actor-Critic-124 那接下來我們要做的事情其實，就是你完全可以想見
DRL_Lecture_6_-_Actor-Critic-125 G 這個 random variable
DRL_Lecture_6_-_Actor-Critic-126 它的期望值到底是什麼呢？
DRL_Lecture_6_-_Actor-Critic-127 其實 G 的 random variable 的期望值
DRL_Lecture_6_-_Actor-Critic-128 正好就是 Q 這樣子，因為這個就是 Q 的定義
DRL_Lecture_6_-_Actor-Critic-129 Q 的定義就是，在某一個 state, s
DRL_Lecture_6_-_Actor-Critic-130 採取某一個 action, a
DRL_Lecture_6_-_Actor-Critic-131 假設我們現在的 policy，就是 pi 的情況下
DRL_Lecture_6_-_Actor-Critic-132 會得到的 reward 的期望值
DRL_Lecture_6_-_Actor-Critic-133 accumulated reward 的期望值有多大
DRL_Lecture_6_-_Actor-Critic-134 而這個東西就是 G 的期望值
DRL_Lecture_6_-_Actor-Critic-135 這個為什麼會這樣，因為這個就是 Q 的定義
DRL_Lecture_6_-_Actor-Critic-136 Q function 的定義
DRL_Lecture_6_-_Actor-Critic-137 其實就是 accumulated reward 的期望值
DRL_Lecture_6_-_Actor-Critic-138 就是 G 的期望值
DRL_Lecture_6_-_Actor-Critic-139 所以，我們現在要做的事情就是
DRL_Lecture_6_-_Actor-Critic-140 這一項假設我們要用期望值來代表的話
DRL_Lecture_6_-_Actor-Critic-141 然後把 Q function 套在這裡，就結束了
DRL_Lecture_6_-_Actor-Critic-142 那我們就可以 Actor 跟 Critic 這兩個方法，把它結合起來
DRL_Lecture_6_-_Actor-Critic-143 講到這裡，大家有問題要問的嗎 ?
DRL_Lecture_6_-_Actor-Critic-144 這個其實很直覺，就是應該要這麼做
DRL_Lecture_6_-_Actor-Critic-145 那接下來 baseline 呢，baseline 你就用
DRL_Lecture_6_-_Actor-Critic-146 當然 baseline 你有很多不同的方法去加啦
DRL_Lecture_6_-_Actor-Critic-147 但通常一個常見的做法是，你用 value function
DRL_Lecture_6_-_Actor-Critic-148 來表示 baseline，所謂 value function 的意思就是說
DRL_Lecture_6_-_Actor-Critic-149 假設現在 policy 是 pi，在某一個 state, s
DRL_Lecture_6_-_Actor-Critic-150 一直 interact 到遊戲結束
DRL_Lecture_6_-_Actor-Critic-151 那你 expected 的 reward 有多大
DRL_Lecture_6_-_Actor-Critic-152 那 V 呢沒有 involve action，然後 Q 有 involve action
DRL_Lecture_6_-_Actor-Critic-153 那其實 V 它會是 Q 的期望值
DRL_Lecture_6_-_Actor-Critic-154 所以你今天把 Q，減掉 V
DRL_Lecture_6_-_Actor-Critic-155 你的括號裡面這一項，就會是有正有負的
DRL_Lecture_6_-_Actor-Critic-156 所以我們現在很直覺的
DRL_Lecture_6_-_Actor-Critic-157 我們就把原來在 Actor-Critic
DRL_Lecture_6_-_Actor-Critic-158 你的原來在 policy gradient 裡面
DRL_Lecture_6_-_Actor-Critic-159 括號這一項，換成了 Q function 的 value
DRL_Lecture_6_-_Actor-Critic-160 減掉 V function 的 value，就是這樣，就結束了
DRL_Lecture_6_-_Actor-Critic-161 那接下來呢，其實你可以就單純的這麼實作
DRL_Lecture_6_-_Actor-Critic-162 但是如果你這麼實作的話，他有一個缺點是
DRL_Lecture_6_-_Actor-Critic-163 你要 estimate 2 個 networks，而不是一個 network
DRL_Lecture_6_-_Actor-Critic-164 你要 estimate Q 這個 network
DRL_Lecture_6_-_Actor-Critic-165 你也要 estimate V 這個 network
DRL_Lecture_6_-_Actor-Critic-166 那你現在就有兩倍的風險
DRL_Lecture_6_-_Actor-Critic-167 你有estimate 估測不準的風險就變成兩倍
DRL_Lecture_6_-_Actor-Critic-168 所以我們和不只估測一個 network 就好了呢？
DRL_Lecture_6_-_Actor-Critic-169 事實上在這個 Actor-Critic 方法裡面
DRL_Lecture_6_-_Actor-Critic-170 你可以只估測 V 這個 network
DRL_Lecture_6_-_Actor-Critic-171 你可以把 Q 的值，用 V 的值來表示，什麼意思呢？
DRL_Lecture_6_-_Actor-Critic-172 現在其實 Q of (s, a) 呢，它可以寫成
DRL_Lecture_6_-_Actor-Critic-173 r + V of s 的期望值
DRL_Lecture_6_-_Actor-Critic-174 其實 Q of (s, a) 呢，可以寫成，r + V of s 的期望值
DRL_Lecture_6_-_Actor-Critic-175 那今天我們直接假設說
DRL_Lecture_6_-_Actor-Critic-176 當然這個 r 這個本身，它是一個 random variable
DRL_Lecture_6_-_Actor-Critic-177 就是你今天在 state s，你採取了 action a
DRL_Lecture_6_-_Actor-Critic-178 接下來你會得到什麼樣的 reward
DRL_Lecture_6_-_Actor-Critic-179 其實是不確定的，這中間其實是有隨機性的
DRL_Lecture_6_-_Actor-Critic-180 所以小 r 呢，它其實是一個 random variable
DRL_Lecture_6_-_Actor-Critic-181 所以要把右邊這個式子
DRL_Lecture_6_-_Actor-Critic-182 取期望值它才會等於 Q function
DRL_Lecture_6_-_Actor-Critic-183 但是，我們現在把期望值這件事情去掉
DRL_Lecture_6_-_Actor-Critic-184 就當作左式等於右式
DRL_Lecture_6_-_Actor-Critic-185 就當作 Q function 等於， r 加上 state value function
DRL_Lecture_6_-_Actor-Critic-186 然後接下來我們就可以把這個 Q function
DRL_Lecture_6_-_Actor-Critic-187 用 r + V 取代掉
DRL_Lecture_6_-_Actor-Critic-188 我們就可以把 Q function，用 r + V 取代掉
DRL_Lecture_6_-_Actor-Critic-189 所以本來是 Q of (st, at) 減掉 V of  st
DRL_Lecture_6_-_Actor-Critic-190 現在就換成把 Q，換成 r + V，然後這邊是 s(t+1)
DRL_Lecture_6_-_Actor-Critic-191 因為這邊這個式子的意思是說
DRL_Lecture_6_-_Actor-Critic-192 你在 state s 採取 action a
DRL_Lecture_6_-_Actor-Critic-193 接下來你會得到，reward r，然後呢
DRL_Lecture_6_-_Actor-Critic-194 跳到 state, s(t+1)
DRL_Lecture_6_-_Actor-Critic-195 但是你會得到什麼樣的 reward r
DRL_Lecture_6_-_Actor-Critic-196 跟跳到什麼樣的 state, s(t+1)
DRL_Lecture_6_-_Actor-Critic-197 它本身是有隨機性的
DRL_Lecture_6_-_Actor-Critic-198 所以我們要在前面取一個期望值
DRL_Lecture_6_-_Actor-Critic-199 左式才會等於右式
DRL_Lecture_6_-_Actor-Critic-200 那我們現在把期望值這件事情拿掉
DRL_Lecture_6_-_Actor-Critic-201 就直接說左式等於右式
DRL_Lecture_6_-_Actor-Critic-202 然後把這個式子直接套進去這樣
DRL_Lecture_6_-_Actor-Critic-203 這樣大家可以了解我的意思嗎？
DRL_Lecture_6_-_Actor-Critic-204 大家可以了解我的意思嗎？
DRL_Lecture_6_-_Actor-Critic-205 講到這一邊，大家有沒有問題要問的
DRL_Lecture_6_-_Actor-Critic-206 有沒有問題要問的
DRL_Lecture_6_-_Actor-Critic-207 如果大家可以接受這個想法
DRL_Lecture_6_-_Actor-Critic-208 因為這個其實也是很直覺啦
DRL_Lecture_6_-_Actor-Critic-209 因為我們說 Q function 是什麼意思
DRL_Lecture_6_-_Actor-Critic-210 Q function 的意思就是在 state s 採取 action a 的時候
DRL_Lecture_6_-_Actor-Critic-211 接下來會得到 reward 的期望值
DRL_Lecture_6_-_Actor-Critic-212 那接下來會得到 reward 的期望值怎麼算呢？
DRL_Lecture_6_-_Actor-Critic-213 在 state, s 採取 action, a
DRL_Lecture_6_-_Actor-Critic-214 接下來會發生的事情就是
DRL_Lecture_6_-_Actor-Critic-215 你會得到 reward rt，然後跳到 state, s(t+1)
DRL_Lecture_6_-_Actor-Critic-216 我們現在在 state, st
DRL_Lecture_6_-_Actor-Critic-217 然後我們採取 action, at
DRL_Lecture_6_-_Actor-Critic-218 然後我們想要知道說，接下來會得到多少 reward
DRL_Lecture_6_-_Actor-Critic-219 那接下來會發生什麼事呢？
DRL_Lecture_6_-_Actor-Critic-220 接下來你會得到 reward rt，然後跳到 state, s(t+1)
DRL_Lecture_6_-_Actor-Critic-221 那在 state s 採取 action a 得到的 reward
DRL_Lecture_6_-_Actor-Critic-222 其實就是等於接下來得到 reward rt
DRL_Lecture_6_-_Actor-Critic-223 加上從 state, s(t+1) 開始
DRL_Lecture_6_-_Actor-Critic-224 得到接下來所有 reward 的總和
DRL_Lecture_6_-_Actor-Critic-225 而從 state, s(t+1) 開始
DRL_Lecture_6_-_Actor-Critic-226 得到接下來所有 reward 的總和
DRL_Lecture_6_-_Actor-Critic-227 就是 V of s(t+1)
DRL_Lecture_6_-_Actor-Critic-228 那在 state, st 採取 action, at 以後得到的 reward, rt
DRL_Lecture_6_-_Actor-Critic-229 就寫在這個地方
DRL_Lecture_6_-_Actor-Critic-230 所以這兩項加起來，會等於 Q function
DRL_Lecture_6_-_Actor-Critic-231 那為什麼前面要取期望值呢？
DRL_Lecture_6_-_Actor-Critic-232 因為你在 st 採取 action, at 會得到什麼樣的 reward
DRL_Lecture_6_-_Actor-Critic-233 跳到什麼樣的 state 這件事情
DRL_Lecture_6_-_Actor-Critic-234 本身是有隨機性的，不見得是你的 model 可以控制的
DRL_Lecture_6_-_Actor-Critic-235 為了要把這隨機性考慮進去
DRL_Lecture_6_-_Actor-Critic-236 前面你必須加上期望值
DRL_Lecture_6_-_Actor-Critic-237 但是我們現在把這個期望值拿掉就說他們兩個是相等的
DRL_Lecture_6_-_Actor-Critic-238 把這個東西塞進去，把 Q 替換掉
DRL_Lecture_6_-_Actor-Critic-239 這樣的好處就是，你不需要再 estimate Q 了
DRL_Lecture_6_-_Actor-Critic-240 你只需要 estimate V 就夠了
DRL_Lecture_6_-_Actor-Critic-241 你只要 estimate 一個 network 就夠了
DRL_Lecture_6_-_Actor-Critic-242 你不需要 estimate 2 個 network
DRL_Lecture_6_-_Actor-Critic-243 你只需要 estimate 一個 network 就夠了
DRL_Lecture_6_-_Actor-Critic-244 但這樣的壞處是什麼呢？
DRL_Lecture_6_-_Actor-Critic-245 這樣你引入了一個隨機的東西
DRL_Lecture_6_-_Actor-Critic-246 就是 r 現在，它是有隨機性的
DRL_Lecture_6_-_Actor-Critic-247 它是一個 random variable
DRL_Lecture_6_-_Actor-Critic-248 但是這個 random variable，相較於剛才的 G
DRL_Lecture_6_-_Actor-Critic-249 accumulated reward 可能還好
DRL_Lecture_6_-_Actor-Critic-250 因為他是某一個 step 會得到的 reward
DRL_Lecture_6_-_Actor-Critic-251 而 Q 是所有未來的 step 會得到的 reward 的總和
DRL_Lecture_6_-_Actor-Critic-252 說錯不是 Q，是 G
DRL_Lecture_6_-_Actor-Critic-253 G 是所有未來會得到的 reward 的總和
DRL_Lecture_6_-_Actor-Critic-254 G variance 比較大，r 雖然也有一些 variance
DRL_Lecture_6_-_Actor-Critic-255 但它的 variance 會比 G 還要小
DRL_Lecture_6_-_Actor-Critic-256 所以把原來 variance 比較大的 G
DRL_Lecture_6_-_Actor-Critic-257 換成現在只有 variance 比較小的 r 這件事情也是合理的
DRL_Lecture_6_-_Actor-Critic-258 那如果你不相信的話
DRL_Lecture_6_-_Actor-Critic-259 如果你覺得說什麼期望值拿掉不相信的話
DRL_Lecture_6_-_Actor-Critic-260 那我就告訴你原始的 A3C paper
DRL_Lecture_6_-_Actor-Critic-261 它試了各式各樣的方法
DRL_Lecture_6_-_Actor-Critic-262 最後做出來就是這個最好這樣
DRL_Lecture_6_-_Actor-Critic-263 當然你可能說，搞不好 estimate Q 跟 V 也都 estimate 很好
DRL_Lecture_6_-_Actor-Critic-264 那我給你的答案就是做實驗的時候，最後結果就是這個最好
DRL_Lecture_6_-_Actor-Critic-265 所以，後來大家都用這個
DRL_Lecture_6_-_Actor-Critic-266 所以那這整個流程呢，就是這樣
DRL_Lecture_6_-_Actor-Critic-267 因為前面這個式子叫做 Advantage function
DRL_Lecture_6_-_Actor-Critic-268 就是前面這個式子，叫做 advantage function
DRL_Lecture_6_-_Actor-Critic-269 所以這整個方法就叫 Advantage Actor-Critic
DRL_Lecture_6_-_Actor-Critic-270 那整個流程是這樣子的
DRL_Lecture_6_-_Actor-Critic-271 我們現在先有一個 pi
DRL_Lecture_6_-_Actor-Critic-272 然後pi，有個初始的 actor
DRL_Lecture_6_-_Actor-Critic-273 那去跟環境做互動，先收集資料
DRL_Lecture_6_-_Actor-Critic-274 在每一個 policy gradient 收集資料以後
DRL_Lecture_6_-_Actor-Critic-275 你就要拿去 update 你的 policy
DRL_Lecture_6_-_Actor-Critic-276 但是在 actor critic 方法裡面呢
DRL_Lecture_6_-_Actor-Critic-277 你不是直接拿你的那些資料，去 update 你的 policy
DRL_Lecture_6_-_Actor-Critic-278 你先拿這些資料去 estimate 出
DRL_Lecture_6_-_Actor-Critic-279 你的 value function，那假設你是用別的方法
DRL_Lecture_6_-_Actor-Critic-280 你有時候可能也需要 estimate Q function
DRL_Lecture_6_-_Actor-Critic-281 那我們這邊是 A3C
DRL_Lecture_6_-_Actor-Critic-282 我們只需要 value function 就好
DRL_Lecture_6_-_Actor-Critic-283 我們不需要 Q function 只需要 value function 就好
DRL_Lecture_6_-_Actor-Critic-284 那有了 value function
DRL_Lecture_6_-_Actor-Critic-285 那 actor value function 過程
DRL_Lecture_6_-_Actor-Critic-286 你可以用 TD，也可以用 MC
DRL_Lecture_6_-_Actor-Critic-287 那你 estimate 出 value function 以後
DRL_Lecture_6_-_Actor-Critic-288 接下來，你再 based on value function
DRL_Lecture_6_-_Actor-Critic-289 套用下面這個式子去 update 你的 pi
DRL_Lecture_6_-_Actor-Critic-290 然後你有了新的 pi 以後，再去跟環境互動
DRL_Lecture_6_-_Actor-Critic-291 再收集新的資料，去 estimate 你的 value function
DRL_Lecture_6_-_Actor-Critic-292 然後再用新的 value function，去 update 你的 policy
DRL_Lecture_6_-_Actor-Critic-293 去 update 你的 actor
DRL_Lecture_6_-_Actor-Critic-294 整個 actor-critic 的 algorithm，就是這麼運作的
DRL_Lecture_6_-_Actor-Critic-295 那這個 implement Actor-Critic 的時候
DRL_Lecture_6_-_Actor-Critic-296 有兩個幾乎一定會用的 tip，第一個 tip 是
DRL_Lecture_6_-_Actor-Critic-297 我們現在說，我們其實要 estimate 的 network 有兩個
DRL_Lecture_6_-_Actor-Critic-298 一個是 D，就不用 estimate Q 了
DRL_Lecture_6_-_Actor-Critic-299 我們只要 estimate V function
DRL_Lecture_6_-_Actor-Critic-300 而另外一個需要 estimate 的 network
DRL_Lecture_6_-_Actor-Critic-301 是 policy 的 network，也就是你的 actor
DRL_Lecture_6_-_Actor-Critic-302 那這兩個 network
DRL_Lecture_6_-_Actor-Critic-303 那個 V 那個 network 它是 input 一個 state
DRL_Lecture_6_-_Actor-Critic-304 output 一個 scalar
DRL_Lecture_6_-_Actor-Critic-305 然後 actor 這個 network，它是 input 一個 state
DRL_Lecture_6_-_Actor-Critic-306 output 就是一個 action 的 distribution
DRL_Lecture_6_-_Actor-Critic-307 假設你的  action 是 discrete 不是 continuous 的話
DRL_Lecture_6_-_Actor-Critic-308 如果是 continuous 的話，它也是一樣
DRL_Lecture_6_-_Actor-Critic-309 如果是 continuous 的話
DRL_Lecture_6_-_Actor-Critic-310 就只是 output 一個 continuous 的 vector
DRL_Lecture_6_-_Actor-Critic-311 我想大家應該知道我的意思
DRL_Lecture_6_-_Actor-Critic-312 那這邊是舉 discrete 的例子
DRL_Lecture_6_-_Actor-Critic-313 但是 continuous 的 case，其實也是一樣的
DRL_Lecture_6_-_Actor-Critic-314 input 一個 state
DRL_Lecture_6_-_Actor-Critic-315 然後他要決定你現在要 take 那一個 action
DRL_Lecture_6_-_Actor-Critic-316 那這兩個 network，這個 actor 跟你的 critic
DRL_Lecture_6_-_Actor-Critic-317 跟你的 value function，它們的 input 都是 s
DRL_Lecture_6_-_Actor-Critic-318 所以他們前面幾個 layer，其實是可以 share 的
DRL_Lecture_6_-_Actor-Critic-319 尤其是假設你今天是玩 ATARI 遊戲
DRL_Lecture_6_-_Actor-Critic-320 或者是你玩的是那種什麼 3D 遊戲
DRL_Lecture_6_-_Actor-Critic-321 那 input 都是 image
DRL_Lecture_6_-_Actor-Critic-322 那 input 那個 image 都非常複雜
DRL_Lecture_6_-_Actor-Critic-323 那 image 很大張
DRL_Lecture_6_-_Actor-Critic-324 通常你前面都會用一些 CNN 來處理
DRL_Lecture_6_-_Actor-Critic-325 把那些 image 抽成 high level 的 information
DRL_Lecture_6_-_Actor-Critic-326 把那個 pixel level 到 high level information 這件事情
DRL_Lecture_6_-_Actor-Critic-327 其實對 actor 跟 critic 來說可能是可以共用的
DRL_Lecture_6_-_Actor-Critic-328 所以通常你會讓這個 actor 跟 critic 的前面幾個 layer 是 shared
DRL_Lecture_6_-_Actor-Critic-329 你會讓 actor 跟 critic 的前面幾個 layer 共用同一組參數
DRL_Lecture_6_-_Actor-Critic-330 那這一組參數可能是 CNN
DRL_Lecture_6_-_Actor-Critic-331 先把 input 的 pixel，變成比較high level 的資訊
DRL_Lecture_6_-_Actor-Critic-332 然後再給 actor 去決定說它要採取什麼樣的行為
DRL_Lecture_6_-_Actor-Critic-333 給這個 critic，給 value function
DRL_Lecture_6_-_Actor-Critic-334 去計算 expected 的 return
DRL_Lecture_6_-_Actor-Critic-335 也就是 expected reward
DRL_Lecture_6_-_Actor-Critic-336 那另外一個事情是
DRL_Lecture_6_-_Actor-Critic-337 我們一樣需要那個 exploration
DRL_Lecture_6_-_Actor-Critic-338 我們一樣需要 exploration 的機制
DRL_Lecture_6_-_Actor-Critic-339 那我們之前已經有講過
DRL_Lecture_6_-_Actor-Critic-340 在講這個 Q function 的時候呢
DRL_Lecture_6_-_Actor-Critic-341 在講 Q learning 的時候呢
DRL_Lecture_6_-_Actor-Critic-342 我們有講過 exploration 這件事，是很重要的
DRL_Lecture_6_-_Actor-Critic-343 那今天在做 Actor-Critic 的時候呢
DRL_Lecture_6_-_Actor-Critic-344 有一個常見的 exploration 的方法
DRL_Lecture_6_-_Actor-Critic-345 是你會對你的 pi 的 output 的這個 distribution
DRL_Lecture_6_-_Actor-Critic-346 下一個 constrain
DRL_Lecture_6_-_Actor-Critic-347 這個 constrain 是希望這個 distribution 的 entropy
DRL_Lecture_6_-_Actor-Critic-348 不要太小
DRL_Lecture_6_-_Actor-Critic-349 希望這個 distribution 的 entropy 可以大一點
DRL_Lecture_6_-_Actor-Critic-350 也就是希望不同的 action
DRL_Lecture_6_-_Actor-Critic-351 它的被採用的機率，平均一點
DRL_Lecture_6_-_Actor-Critic-352 這樣在 testing 的時候
DRL_Lecture_6_-_Actor-Critic-353 他才會多嘗試各種不同的 action
DRL_Lecture_6_-_Actor-Critic-354 才會把這個環境探索的比較好
DRL_Lecture_6_-_Actor-Critic-355 explorer 的比較好，才會得到比較好的結果
DRL_Lecture_6_-_Actor-Critic-356 這個是 advantage 的 Actor-Critic
DRL_Lecture_6_-_Actor-Critic-357 那接下來什麼東西是 A3C 呢？
DRL_Lecture_6_-_Actor-Critic-358 因為這個 reinforcement learning 它的一個問題
DRL_Lecture_6_-_Actor-Critic-359 就是它很慢
DRL_Lecture_6_-_Actor-Critic-360 那怎麼增加訓練的速度呢？
DRL_Lecture_6_-_Actor-Critic-361 這個可以講到火影忍者
DRL_Lecture_6_-_Actor-Critic-362 就是有一次鳴人說，他想要在一周之內打敗曉
DRL_Lecture_6_-_Actor-Critic-363 所以要加快修行的速度
DRL_Lecture_6_-_Actor-Critic-364 他老師就教他一個方法
DRL_Lecture_6_-_Actor-Critic-365 這個方法是說你只要用隱分身進行同樣修行
DRL_Lecture_6_-_Actor-Critic-366 那兩個一起修行的話呢？
DRL_Lecture_6_-_Actor-Critic-367 經驗值累積的速度就會變成2倍
DRL_Lecture_6_-_Actor-Critic-368 所以，鳴人就開了 1000 個隱分身，開始修行了
DRL_Lecture_6_-_Actor-Critic-369 這個其實就是 Asynchronous Advantage Actor-Critic也就是 A3C 這個方法的精神
DRL_Lecture_6_-_Actor-Critic-370 所以 A3C 這個方法他的精神就是
DRL_Lecture_6_-_Actor-Critic-371 同時開很多個 worker
DRL_Lecture_6_-_Actor-Critic-372 那每一個 worker 其實就是一個隱分身
DRL_Lecture_6_-_Actor-Critic-373 那最後這些隱分身會把所有的經驗
DRL_Lecture_6_-_Actor-Critic-374 通通集合在一起
DRL_Lecture_6_-_Actor-Critic-375 這個 A3C 是怎麼運作的呢？
DRL_Lecture_6_-_Actor-Critic-376 首先，當然這個你可能自己實作的時候
DRL_Lecture_6_-_Actor-Critic-377 你如果沒有很多個 CPU，你可能也是不好做啦
DRL_Lecture_6_-_Actor-Critic-378 反正就是講一講
DRL_Lecture_6_-_Actor-Critic-379 作業也沒有強制你一定要做 A3C 就是了
DRL_Lecture_6_-_Actor-Critic-380 你可以 implement A2C 就好
DRL_Lecture_6_-_Actor-Critic-381 那 A3C 是這樣子，一開始有一個 global network
DRL_Lecture_6_-_Actor-Critic-382 那我們剛才有講過說
DRL_Lecture_6_-_Actor-Critic-383 其實 policy network 跟 value network 是 tie 在一起的
DRL_Lecture_6_-_Actor-Critic-384 他們的前幾個 layer 會被 tie 一起
DRL_Lecture_6_-_Actor-Critic-385 我們有一個 global network
DRL_Lecture_6_-_Actor-Critic-386 他們有包含 policy 的部分
DRL_Lecture_6_-_Actor-Critic-387 有包含 value 的部分
DRL_Lecture_6_-_Actor-Critic-388 假設他的參數就是 θ1
DRL_Lecture_6_-_Actor-Critic-389 那接下來每一個 worker 做的事情
DRL_Lecture_6_-_Actor-Critic-390 你會開很多個 worker
DRL_Lecture_6_-_Actor-Critic-391 那每一個 worker 就用一張 CPU 去跑
DRL_Lecture_6_-_Actor-Critic-392 比如你就開 8 個 worker 那你至少 8 張 CPU
DRL_Lecture_6_-_Actor-Critic-393 那第一個 worker 呢
DRL_Lecture_6_-_Actor-Critic-394 就去跟 global network 進去把它的參數 copy 過來
DRL_Lecture_6_-_Actor-Critic-395 每一個  work 要工作前就把他的參數 copy 過來
DRL_Lecture_6_-_Actor-Critic-396 接下來你就去跟環境做互動
DRL_Lecture_6_-_Actor-Critic-397 那每一個 actor 去跟環境做互動的時候
DRL_Lecture_6_-_Actor-Critic-398 為了要 collect 到比較 diverse 的 data
DRL_Lecture_6_-_Actor-Critic-399 所以舉例來說如果是走迷宮的話
DRL_Lecture_6_-_Actor-Critic-400 可能每一個 actor 它出生的位置起始的位置都會不一樣
DRL_Lecture_6_-_Actor-Critic-401 這樣他們才能夠收集到比較多樣性的 data
DRL_Lecture_6_-_Actor-Critic-402 每一個 actor 就自己跟環境做互動
DRL_Lecture_6_-_Actor-Critic-403 互動完之後，你就會計算出 gradient
DRL_Lecture_6_-_Actor-Critic-404 那計算出 gradient 以後
DRL_Lecture_6_-_Actor-Critic-405 你要拿 gradient 去 update 你的參數
DRL_Lecture_6_-_Actor-Critic-406 我在想說，這 gradient 的三角形的方向怎麼好像怪怪的
DRL_Lecture_6_-_Actor-Critic-407 這個應該是倒三角形啦
DRL_Lecture_6_-_Actor-Critic-408 不過沒有關係，大家知道我的意思就好，這是倒三角形
DRL_Lecture_6_-_Actor-Critic-409 那你就計算一下你的 gradient
DRL_Lecture_6_-_Actor-Critic-410 然後你就拿你的 gradient 呢
DRL_Lecture_6_-_Actor-Critic-411 去 update global network 的參數
DRL_Lecture_6_-_Actor-Critic-412 就是這個 worker，它算出 gradient 以後
DRL_Lecture_6_-_Actor-Critic-413 就把 gradient 傳回給中央的控制中心
DRL_Lecture_6_-_Actor-Critic-414 然後中央的控制中心，就會拿這個 gradient
DRL_Lecture_6_-_Actor-Critic-415 去 update 原來的參數
DRL_Lecture_6_-_Actor-Critic-416 但是要注意一下，所有的 actor
DRL_Lecture_6_-_Actor-Critic-417 都是平行跑的，就每一個 actor 就是各做各的
DRL_Lecture_6_-_Actor-Critic-418 互相之間就不要管彼此，就是各做各的
DRL_Lecture_6_-_Actor-Critic-419 所以每個人都是去要了一個參數以後
DRL_Lecture_6_-_Actor-Critic-420 做完，它就把它的參數傳回去
DRL_Lecture_6_-_Actor-Critic-421 做完就把參數傳回去
DRL_Lecture_6_-_Actor-Critic-422 所以，當今天第一個 worker 做完
DRL_Lecture_6_-_Actor-Critic-423 想要把參數傳回去的時候
DRL_Lecture_6_-_Actor-Critic-424 本來它要的參數是 θ1
DRL_Lecture_6_-_Actor-Critic-425 等它要把 gradient 傳回去的時候
DRL_Lecture_6_-_Actor-Critic-426 可能別人 已經把原來的參數覆蓋掉，變成 θ2了
DRL_Lecture_6_-_Actor-Critic-427 但是沒有關係，就不要在意這種細節，
DRL_Lecture_6_-_Actor-Critic-428 它一樣會把這個 gradient 就覆蓋過去就是了
DRL_Lecture_6_-_Actor-Critic-429 這個 Asynchronous actor-critic 就是這麼做的
DRL_Lecture_6_-_Actor-Critic-430 那這個就是 A3C 啦，那在講 A3C 之後
DRL_Lecture_6_-_Actor-Critic-431 我們要講另外一個方法叫做
DRL_Lecture_6_-_Actor-Critic-432 Pathwise Derivative Policy Gradient，
DRL_Lecture_6_-_Actor-Critic-433 那這個方法很神奇，它可以想成是 Q learning
DRL_Lecture_6_-_Actor-Critic-434 解 continuous action 的一種特別的方法
DRL_Lecture_6_-_Actor-Critic-435 那它也可以想成是一種，特別的 Actor-Critic 的方法
DRL_Lecture_6_-_Actor-Critic-436 所以如果用棋靈王來比喻的話
DRL_Lecture_6_-_Actor-Critic-437 你想想看阿光是一個 actor
DRL_Lecture_6_-_Actor-Critic-438 佐為是一個 critic，阿光落某一子以後呢
DRL_Lecture_6_-_Actor-Critic-439 佐為會說如果是一般的 Actor-Critic
DRL_Lecture_6_-_Actor-Critic-440 他會告訴他說這時候不應該下小馬步飛
DRL_Lecture_6_-_Actor-Critic-441 他會告訴你，你現在採取的這一步
DRL_Lecture_6_-_Actor-Critic-442 算出來的 value 到底是好還是不好
DRL_Lecture_6_-_Actor-Critic-443 但這樣就結束了，他只告訴你説好還是不好
DRL_Lecture_6_-_Actor-Critic-444 因為一般的這個 Actor-Critic 裡面那個 critic
DRL_Lecture_6_-_Actor-Critic-445 就是 input state 或 input state 跟 action 的 pair
DRL_Lecture_6_-_Actor-Critic-446 然後給你一個 value，然後就結束了
DRL_Lecture_6_-_Actor-Critic-447 所以對 actor 來說它只知道說現在，它做的這個行為
DRL_Lecture_6_-_Actor-Critic-448 到底是好還是不好
DRL_Lecture_6_-_Actor-Critic-449 但是，如果是在剛才講的這個
DRL_Lecture_6_-_Actor-Critic-450 Pathwise derivative policy gradient 裡面
DRL_Lecture_6_-_Actor-Critic-451 這個 critic 會直接告訴 actor 説
DRL_Lecture_6_-_Actor-Critic-452 採取什麼樣的 action，才是好的
DRL_Lecture_6_-_Actor-Critic-453 所以今天佐為不只是告訴阿光說
DRL_Lecture_6_-_Actor-Critic-454 這個時候不要下小馬步飛，同時還告訴阿光說
DRL_Lecture_6_-_Actor-Critic-455 這個時候應該要下大馬步飛
DRL_Lecture_6_-_Actor-Critic-456 所以，這個就是
DRL_Lecture_6_-_Actor-Critic-457 Pathwise Derivative Policy Gradient
DRL_Lecture_6_-_Actor-Critic-458 要告訴我們的 critic
DRL_Lecture_6_-_Actor-Critic-459 等一下會講得更清楚一點
DRL_Lecture_6_-_Actor-Critic-460 critic 會直接引導 actor 做什麼樣的 action
DRL_Lecture_6_-_Actor-Critic-461 才是可以得到比較大的 value 的
DRL_Lecture_6_-_Actor-Critic-462 那如果今天從這個 Q learning 的觀點來看
DRL_Lecture_6_-_Actor-Critic-463 我們之前說，Q learning 的一個問題是
DRL_Lecture_6_-_Actor-Critic-464 你沒有辦法在用 Q learning 的時候
DRL_Lecture_6_-_Actor-Critic-465 考慮 continuous vector
DRL_Lecture_6_-_Actor-Critic-466 其實也不是完全沒辦法，就是不容易比較麻煩
DRL_Lecture_6_-_Actor-Critic-467 比較沒有 general solution
DRL_Lecture_6_-_Actor-Critic-468 那今天我們其實可以說
DRL_Lecture_6_-_Actor-Critic-469 我們怎麼解這個 optimization problem 呢？
DRL_Lecture_6_-_Actor-Critic-470 我們用一個 actor 來解這個 optimization 的 problem
DRL_Lecture_6_-_Actor-Critic-471 所以我們本來在 Q learning 裡面
DRL_Lecture_6_-_Actor-Critic-472 如果是一個 continuous action
DRL_Lecture_6_-_Actor-Critic-473 我們要解這個 optimization problem
DRL_Lecture_6_-_Actor-Critic-474 但是現在這個 optimization problem
DRL_Lecture_6_-_Actor-Critic-475 由 actor 來解，我們假設 actor 就是一個 solver
DRL_Lecture_6_-_Actor-Critic-476 這個 solver 他的工作就是，給你 state, s
DRL_Lecture_6_-_Actor-Critic-477 然後他就去解解解告訴我們說，那一個 action
DRL_Lecture_6_-_Actor-Critic-478 可以給我們最大的 Q value
DRL_Lecture_6_-_Actor-Critic-479 這是從另外一個觀點來看
DRL_Lecture_6_-_Actor-Critic-480 Pathwise derivative policy gradient 這件事情
DRL_Lecture_6_-_Actor-Critic-481 那這個說法，你有沒有覺得非常的熟悉呢？
DRL_Lecture_6_-_Actor-Critic-482 我們在講 GAN 的時候，不是也講過一個說法
DRL_Lecture_6_-_Actor-Critic-483 我們說，我們 learn 一個 discriminator
DRL_Lecture_6_-_Actor-Critic-484 它是要 evaluate 東西好不好
DRL_Lecture_6_-_Actor-Critic-485 discriminator 要自己生東西，非常的困難，那怎麼辦？
DRL_Lecture_6_-_Actor-Critic-486 因為要解一個 Arg Max 的 problem
DRL_Lecture_6_-_Actor-Critic-487 非常的困難，所以怎麼辦
DRL_Lecture_6_-_Actor-Critic-488 用 generator 來生，所以今天的概念其實是一樣的
DRL_Lecture_6_-_Actor-Critic-489 Q 就是那個 discriminator
DRL_Lecture_6_-_Actor-Critic-490 要根據這個 discriminator 決定 action 非常困難
DRL_Lecture_6_-_Actor-Critic-491 怎麼辦？
DRL_Lecture_6_-_Actor-Critic-492 另外 learn 一個 network
DRL_Lecture_6_-_Actor-Critic-493 來解這個 optimization problem
DRL_Lecture_6_-_Actor-Critic-494 這個東西就是 actor
DRL_Lecture_6_-_Actor-Critic-495 所以，今天是從兩個不同的觀點
DRL_Lecture_6_-_Actor-Critic-496 其實是同一件事，從兩個不同的觀點來看
DRL_Lecture_6_-_Actor-Critic-497 一個觀點是說，原來的 Q learning 我們可以加以改進
DRL_Lecture_6_-_Actor-Critic-498 怎麼改進呢？我們 learn 一個 actor 來決定 action
DRL_Lecture_6_-_Actor-Critic-499 以解決 Arg Max 不好解的問題
DRL_Lecture_6_-_Actor-Critic-500 或換句話說，或是另外一個觀點是
DRL_Lecture_6_-_Actor-Critic-501 原來的 actor-critic 的問題是
DRL_Lecture_6_-_Actor-Critic-502 critic 並沒有給 actor 足夠的資訊
DRL_Lecture_6_-_Actor-Critic-503 它只告訴它好或不好，沒有告訴它說什麼樣叫好
DRL_Lecture_6_-_Actor-Critic-504 那現在有新的方法可以直接告訴 actor 説，什麼樣叫做好
DRL_Lecture_6_-_Actor-Critic-505 那我們就實際講一下它的 algorithm
DRL_Lecture_6_-_Actor-Critic-506 那其實蠻直覺的
DRL_Lecture_6_-_Actor-Critic-507 就假設我們 learn 了一個 Q function
DRL_Lecture_6_-_Actor-Critic-508 假設我們 learn 了一個 Q function
DRL_Lecture_6_-_Actor-Critic-509 Q function 就是 input s 跟 a，output 就是 Q of (s, a)
DRL_Lecture_6_-_Actor-Critic-510 那接下來呢，我們要 learn 一個 actor
DRL_Lecture_6_-_Actor-Critic-511 這個 actor 的工作是什麼
DRL_Lecture_6_-_Actor-Critic-512 這個 actor 的工作就是，解這個 Arg Max 的 problem
DRL_Lecture_6_-_Actor-Critic-513 這個 actor 的工作，就是 input 一個 state, s
DRL_Lecture_6_-_Actor-Critic-514 希望可以 output 一個 action a
DRL_Lecture_6_-_Actor-Critic-515 這個 action a 被丟到 Q function 以後
DRL_Lecture_6_-_Actor-Critic-516 它可以讓 Q of (s, a) 的值，越大越好
DRL_Lecture_6_-_Actor-Critic-517 那實際上在 train 的時候
DRL_Lecture_6_-_Actor-Critic-518 你其實就是把 Q 跟 actor 接起來
DRL_Lecture_6_-_Actor-Critic-519 變成一個比較大的 network
DRL_Lecture_6_-_Actor-Critic-520 Q 是一個 network，input s 跟 a，output 一個 value
DRL_Lecture_6_-_Actor-Critic-521 那 actor 它在 training 的時候
DRL_Lecture_6_-_Actor-Critic-522 它要做的事情就是 input s，output a
DRL_Lecture_6_-_Actor-Critic-523 把 a 丟到 Q 裡面
DRL_Lecture_6_-_Actor-Critic-524 希望 output 的值越大越好
DRL_Lecture_6_-_Actor-Critic-525 在 train 的時候會把 Q 跟 actor 直接接起來
DRL_Lecture_6_-_Actor-Critic-526 當作是一個大的 network
DRL_Lecture_6_-_Actor-Critic-527 然後你會 fix 住 Q 的參數
DRL_Lecture_6_-_Actor-Critic-528 只去調 actor 的參數
DRL_Lecture_6_-_Actor-Critic-529 就用 gradient ascent 的方法
DRL_Lecture_6_-_Actor-Critic-530 去 maximize Q 的 output
DRL_Lecture_6_-_Actor-Critic-531 這個東西你有沒有覺得很熟悉呢？
DRL_Lecture_6_-_Actor-Critic-532 這其實就是一個 GAN 對不對？
DRL_Lecture_6_-_Actor-Critic-533 這就是一個 GAN 這樣子
DRL_Lecture_6_-_Actor-Critic-534 這就是 conditional GAN
DRL_Lecture_6_-_Actor-Critic-535 Q 就是 discriminator
DRL_Lecture_6_-_Actor-Critic-536 但在 reinforcement learning 就是 critic
DRL_Lecture_6_-_Actor-Critic-537 actor 在 GAN 裡面它就是 generator
DRL_Lecture_6_-_Actor-Critic-538 其實他們就是同一件事情
DRL_Lecture_6_-_Actor-Critic-539 那我們來看一下這個
DRL_Lecture_6_-_Actor-Critic-540 Pathwise derivative policy gradient 的演算法
DRL_Lecture_6_-_Actor-Critic-541 一開始你會有一個 actor
DRL_Lecture_6_-_Actor-Critic-542 你會有一個 actor pi，它去跟環境互動
DRL_Lecture_6_-_Actor-Critic-543 然後，你可能會要它去 estimate Q value
DRL_Lecture_6_-_Actor-Critic-544 estimate 完 Q value 以後，你就把 Q value 固定
DRL_Lecture_6_-_Actor-Critic-545 只去那個 actor，actor learning 的方向
DRL_Lecture_6_-_Actor-Critic-546 就是希望它的 output，希望這個 actor 採取
DRL_Lecture_6_-_Actor-Critic-547 就我們假設這個 Q 估得是很準的
DRL_Lecture_6_-_Actor-Critic-548 它真的知道說
DRL_Lecture_6_-_Actor-Critic-549 今天在某一個 state 採取什麼樣的 action
DRL_Lecture_6_-_Actor-Critic-550 會真的得到很大的 value
DRL_Lecture_6_-_Actor-Critic-551 接下來就 learn 這個 actor
DRL_Lecture_6_-_Actor-Critic-552 actor 在 given s 的時候，它採取了 a
DRL_Lecture_6_-_Actor-Critic-553 可以讓最後 Q function 算出來的 value 越大越好
DRL_Lecture_6_-_Actor-Critic-554 你用這個 criteria，去 update 你的 actor pi
DRL_Lecture_6_-_Actor-Critic-555 然後接下來有新的 pi 再去跟環境做互動
DRL_Lecture_6_-_Actor-Critic-556 然後再 estimate Q，然後再得到新的 pi
DRL_Lecture_6_-_Actor-Critic-557 去 maximize Q 的 output
DRL_Lecture_6_-_Actor-Critic-558 那其實本來在 Q learning 裡面
DRL_Lecture_6_-_Actor-Critic-559 你用得上的技巧，在這邊也幾乎都用得上
DRL_Lecture_6_-_Actor-Critic-560 比如說 replay buffer，exploration 等等
DRL_Lecture_6_-_Actor-Critic-561 這些都用得上
DRL_Lecture_6_-_Actor-Critic-562 我們再講得更具體一點
DRL_Lecture_6_-_Actor-Critic-563 這個是原來 Q learning 的 algorithm
DRL_Lecture_6_-_Actor-Critic-564 這個是我們上週看過的
DRL_Lecture_6_-_Actor-Critic-565 你有一個 Q function，Q
DRL_Lecture_6_-_Actor-Critic-566 那你會有另外一個 target 的 Q function
DRL_Lecture_6_-_Actor-Critic-567 叫做 Q hat
DRL_Lecture_6_-_Actor-Critic-568 這我們之前有講過說你會有
DRL_Lecture_6_-_Actor-Critic-569 另外一個 target 的 Q function， 叫做 Q hat
DRL_Lecture_6_-_Actor-Critic-570 然後在每一次 training
DRL_Lecture_6_-_Actor-Critic-571 在每一個 episode 裡面
DRL_Lecture_6_-_Actor-Critic-572 在每一個 episode 的每一個 timestamp 裡面
DRL_Lecture_6_-_Actor-Critic-573 你會看到一個 state, st
DRL_Lecture_6_-_Actor-Critic-574 你會 take 某一個 action, at
DRL_Lecture_6_-_Actor-Critic-575 那至於 take 哪一個 action
DRL_Lecture_6_-_Actor-Critic-576 是由 Q function 所決定的
DRL_Lecture_6_-_Actor-Critic-577 因為解一個 Arg Max 的 problem
DRL_Lecture_6_-_Actor-Critic-578 如果是 discrete 的話沒有問題
DRL_Lecture_6_-_Actor-Critic-579 你就看說哪一個 a 可以讓 Q 的 value 最大
DRL_Lecture_6_-_Actor-Critic-580 就 take 那一個 action
DRL_Lecture_6_-_Actor-Critic-581 那你需要加一些 exploration
DRL_Lecture_6_-_Actor-Critic-582 這樣 performance 才會好
DRL_Lecture_6_-_Actor-Critic-583 你會得到 reward, rt
DRL_Lecture_6_-_Actor-Critic-584 跳到新的 state, s(t+1)
DRL_Lecture_6_-_Actor-Critic-585 你會把 st, at, rt, s(t+1) 塞到你的 buffer 裡面去
DRL_Lecture_6_-_Actor-Critic-586 你會從你的 buffer 裡面 sample 一個 batch 的 data
DRL_Lecture_6_-_Actor-Critic-587 這個 batch data 裡面，可能某一筆是 si, ai, ri, s(i+1)
DRL_Lecture_6_-_Actor-Critic-588 接下來你會算一個 target
DRL_Lecture_6_-_Actor-Critic-589 這個 target 叫做 y，y 是什麼？
DRL_Lecture_6_-_Actor-Critic-590 y 是 ri 加上你拿你的 target Q function 過來
DRL_Lecture_6_-_Actor-Critic-591 拿你的 Q function 過來
DRL_Lecture_6_-_Actor-Critic-592 去計算 target 的 Q function
DRL_Lecture_6_-_Actor-Critic-593 input 那一個 a 的時候，它的 value 會最大
DRL_Lecture_6_-_Actor-Critic-594 你把這個 target Q function 算出來的 Q value 跟 r 加起來
DRL_Lecture_6_-_Actor-Critic-595 你就得到你的 target y
DRL_Lecture_6_-_Actor-Critic-596 然後接下來你怎麼 learn 你的 Q 呢？
DRL_Lecture_6_-_Actor-Critic-597 你就希望你的 Q function
DRL_Lecture_6_-_Actor-Critic-598 在帶 si 跟 ai 的時候，跟 y 越接近越好
DRL_Lecture_6_-_Actor-Critic-599 這是一個 regression 的 problem，最後呢
DRL_Lecture_6_-_Actor-Critic-600 每 t 個 step，你要把 Q hat 用 Q 替代掉
DRL_Lecture_6_-_Actor-Critic-601 接下來我們把它改成
DRL_Lecture_6_-_Actor-Critic-602 Pathwise Derivative  Policy Gradient
DRL_Lecture_6_-_Actor-Critic-603 那怎麼做，這邊就是只要做四個改變就好
DRL_Lecture_6_-_Actor-Critic-604 第一個改變是，你要把 Q 換成 pi
DRL_Lecture_6_-_Actor-Critic-605 本來是用 Q 來決定在 state, st
DRL_Lecture_6_-_Actor-Critic-606 產生那一個 action, at
DRL_Lecture_6_-_Actor-Critic-607 現在是直接用 pi
DRL_Lecture_6_-_Actor-Critic-608 我們 learn 了一個 actor
DRL_Lecture_6_-_Actor-Critic-609 這個 actor，我們不用再解 Arg Max 的 problem 了
DRL_Lecture_6_-_Actor-Critic-610 我們直接 learn 了一個 actor
DRL_Lecture_6_-_Actor-Critic-611 這個 actor input st
DRL_Lecture_6_-_Actor-Critic-612 就會告訴我們應該採取哪一個 at
DRL_Lecture_6_-_Actor-Critic-613 所以本來 input st，採取哪一個 at，是 Q 決定的
DRL_Lecture_6_-_Actor-Critic-614 在 Pathwise Derivative  Policy Gradient 裡面
DRL_Lecture_6_-_Actor-Critic-615 我們會直接用 pi 來決定，這是第一個改變
DRL_Lecture_6_-_Actor-Critic-616 第二個改變是，本來這個地方是要計算在 s(t+1)
DRL_Lecture_6_-_Actor-Critic-617 根據你的 policy，採取某一個 action a
DRL_Lecture_6_-_Actor-Critic-618 會得到多少的 Q value
DRL_Lecture_6_-_Actor-Critic-619 那你會採取的 action a
DRL_Lecture_6_-_Actor-Critic-620 就是看說哪一個 action a 可以讓 Q hat 最大
DRL_Lecture_6_-_Actor-Critic-621 你就會採取那個 action a
DRL_Lecture_6_-_Actor-Critic-622 這就是你為什麼把式子寫成這樣
DRL_Lecture_6_-_Actor-Critic-623 那現在因為我們其實不好解這個 Arg Max 的 problem
DRL_Lecture_6_-_Actor-Critic-624 所以 Arg Max problem
DRL_Lecture_6_-_Actor-Critic-625 其實現在就是由 policy pi 來解了
DRL_Lecture_6_-_Actor-Critic-626 所以我們就直接把 s(t+1)
DRL_Lecture_6_-_Actor-Critic-627 帶到 policy pi 裡面
DRL_Lecture_6_-_Actor-Critic-628 那你就會知道說
DRL_Lecture_6_-_Actor-Critic-629 現在 given s(t+1)
DRL_Lecture_6_-_Actor-Critic-630 那一個 action 會給我們最大的 Q value
DRL_Lecture_6_-_Actor-Critic-631 那你在這邊就會 take 那一個 action
DRL_Lecture_6_-_Actor-Critic-632 那這邊還有另外一件事情要講一下
DRL_Lecture_6_-_Actor-Critic-633 我們原來在 Q function 裡面
DRL_Lecture_6_-_Actor-Critic-634 我們說，有兩個 Q network
DRL_Lecture_6_-_Actor-Critic-635 一個是真正的 Q network
DRL_Lecture_6_-_Actor-Critic-636 另外一個是，target Q network
DRL_Lecture_6_-_Actor-Critic-637 那實際上你在 implement 這個 algorithm 的時候
DRL_Lecture_6_-_Actor-Critic-638 你也會有兩個 actor
DRL_Lecture_6_-_Actor-Critic-639 你會有一個真正要 learn 的 actor pi
DRL_Lecture_6_-_Actor-Critic-640 你會有一個 target actor pi hat
DRL_Lecture_6_-_Actor-Critic-641 這個原理就跟那個
DRL_Lecture_6_-_Actor-Critic-642 為什麼要有 target Q network 一樣
DRL_Lecture_6_-_Actor-Critic-643 我們在算 target value 的時候
DRL_Lecture_6_-_Actor-Critic-644 我們並不希望它一直的變動
DRL_Lecture_6_-_Actor-Critic-645 所以我們會有一個 target 的 actor
DRL_Lecture_6_-_Actor-Critic-646 跟一個 target 的 Q function
DRL_Lecture_6_-_Actor-Critic-647 那它們平常的參數，就是固定住的
DRL_Lecture_6_-_Actor-Critic-648 這樣可以讓你的這個 target
DRL_Lecture_6_-_Actor-Critic-649 它的 value 不會一直的變化
DRL_Lecture_6_-_Actor-Critic-650 所以本來到底是要用哪一個 action a
DRL_Lecture_6_-_Actor-Critic-651 你會看說哪一個 action a
DRL_Lecture_6_-_Actor-Critic-652 可以讓 Q hat 最大，
DRL_Lecture_6_-_Actor-Critic-653 但是，現在
DRL_Lecture_6_-_Actor-Critic-654 現在因為那一個 action a 可以讓 Q hat 最大這件事情
DRL_Lecture_6_-_Actor-Critic-655 已經被直接用那個 policy 取代掉了
DRL_Lecture_6_-_Actor-Critic-656 所以我們要知道哪一個 action a 可以讓 Q hat 最大
DRL_Lecture_6_-_Actor-Critic-657 就直接把那個 state 帶到 pi hat 裡面
DRL_Lecture_6_-_Actor-Critic-658 看它得到哪一個 a，就用那一個 a，
DRL_Lecture_6_-_Actor-Critic-659 那一個 a 就是會讓 Q hat of (s, a)
DRL_Lecture_6_-_Actor-Critic-660 他的值最大的那個 a 這樣
DRL_Lecture_6_-_Actor-Critic-661 講到這邊大家可以接受嗎？
DRL_Lecture_6_-_Actor-Critic-662 其實跟原來的這個 Q learning 也是沒什麼不同
DRL_Lecture_6_-_Actor-Critic-663 只是原來你要解 Arg Max 的地方
DRL_Lecture_6_-_Actor-Critic-664 你如果有一個 Max a 的地方
DRL_Lecture_6_-_Actor-Critic-665 通通都用 policy 取代掉就是了
DRL_Lecture_6_-_Actor-Critic-666 那這個是第二個不同
DRL_Lecture_6_-_Actor-Critic-667 第三個不同就是
DRL_Lecture_6_-_Actor-Critic-668 之前只要 learn Q，現在你多 learn 一個 pi
DRL_Lecture_6_-_Actor-Critic-669 那 learn pi 的時候的方向是什麼呢？
DRL_Lecture_6_-_Actor-Critic-670 learn pi 的目的，就是為了 Maximize Q function
DRL_Lecture_6_-_Actor-Critic-671 希望你得到的這個 actor
DRL_Lecture_6_-_Actor-Critic-672 它可以讓你的 Q function output 越大越好
DRL_Lecture_6_-_Actor-Critic-673 這個跟 learn GAN 裡面的 generator 的概念
DRL_Lecture_6_-_Actor-Critic-674 其實是一樣的
DRL_Lecture_6_-_Actor-Critic-675 第四個 step，就跟原來的 Q function 一樣
DRL_Lecture_6_-_Actor-Critic-676 你要把 target 的 Q network 取代掉
DRL_Lecture_6_-_Actor-Critic-677 你現在也要把 target policy 取代掉
DRL_Lecture_6_-_Actor-Critic-678 那其實確實 GAN 跟 Actor-Critic 的方法是非常類似的
DRL_Lecture_6_-_Actor-Critic-679 那我們這邊就不細講
DRL_Lecture_6_-_Actor-Critic-680 你可以去找到一篇 paper 叫做
DRL_Lecture_6_-_Actor-Critic-681 Connecting Generative Adversarial Network and Actor-Critic Method
DRL_Lecture_6_-_Actor-Critic-682 那知道 GAN 跟 Actor-Critic 非常像有什麼幫助呢？
DRL_Lecture_6_-_Actor-Critic-683 一個很大的幫助就是
DRL_Lecture_6_-_Actor-Critic-684 GAN 跟 Actor-Critic 都是以難 train 而聞名的
DRL_Lecture_6_-_Actor-Critic-685 所以，在文獻上就會收集 develop 的各式各樣的方法
DRL_Lecture_6_-_Actor-Critic-686 告訴你說怎麼樣可以把 GAN train 起來
DRL_Lecture_6_-_Actor-Critic-687 怎麼樣可以把 Actor-Critic train 起來
DRL_Lecture_6_-_Actor-Critic-688 但是因為做 GAN 跟 Actor-Critic 的人
DRL_Lecture_6_-_Actor-Critic-689 其實是兩群人啊
DRL_Lecture_6_-_Actor-Critic-690 所以這篇 paper 裡面就列出說在 GAN 上面
DRL_Lecture_6_-_Actor-Critic-691 有哪些技術是有人做過的
DRL_Lecture_6_-_Actor-Critic-692 在 Actor-Critic 上面，有哪些技術是有人做過的
DRL_Lecture_6_-_Actor-Critic-693 但是也許在 GAN 上面有試過的技術
DRL_Lecture_6_-_Actor-Critic-694 你可以試著 apply 在 Actor-Critic 上
DRL_Lecture_6_-_Actor-Critic-695 在 Actor-Critic 上面做過的技術
DRL_Lecture_6_-_Actor-Critic-696 你可以試著 apply 在 GAN 上面
DRL_Lecture_6_-_Actor-Critic-697 看看 work 不 work
DRL_Lecture_6_-_Actor-Critic-698 這個就是 Actor-Critic 和 GAN 之間的關係
DRL_Lecture_6_-_Actor-Critic-699 可以帶給我們的一個好處
DRL_Lecture_6_-_Actor-Critic-700 那這個其實就是 Actor-Critic
DRL_Lecture_6_-_Actor-Critic-701 講到這裡大家有沒有問題要問的
DRL_Lecture_7_-_Sparse_Reward-0 稍微講一下，sparse reward 的 problem，然後再下課
DRL_Lecture_7_-_Sparse_Reward-1 我們稍微講一下 sparse reward problem，然後再下課
DRL_Lecture_7_-_Sparse_Reward-2 那個 sparse reward 是什麼意思呢？
DRL_Lecture_7_-_Sparse_Reward-3 就是實際上當我們在 learn agent
DRL_Lecture_7_-_Sparse_Reward-4 用 reinforcement learning learn agent 的時候
DRL_Lecture_7_-_Sparse_Reward-5 多數的時候 agent 都是沒有辦法得到 reward 的
DRL_Lecture_7_-_Sparse_Reward-6 那在沒有辦法得到 reward 的情況下
DRL_Lecture_7_-_Sparse_Reward-7 對 agent 來說它的訓練是非常困難的
DRL_Lecture_7_-_Sparse_Reward-8 假設説舉例來說，假設你今天要訓練一個機器手臂
DRL_Lecture_7_-_Sparse_Reward-9 然後桌上有一個螺絲釘跟螺絲起子
DRL_Lecture_7_-_Sparse_Reward-10 那你要訓練他用螺絲起子把螺絲釘栓進去
DRL_Lecture_7_-_Sparse_Reward-11 那這個很難，為什麼？因為你知道一開始你的 agent
DRL_Lecture_7_-_Sparse_Reward-12 它是什麼都不知道的
DRL_Lecture_7_-_Sparse_Reward-13 它唯一能夠做不同的 action 的原因
DRL_Lecture_7_-_Sparse_Reward-14 是因為 exploration
DRL_Lecture_7_-_Sparse_Reward-15 舉例來說你在做 Q learning 的時候
DRL_Lecture_7_-_Sparse_Reward-16 你會有一些隨機性
DRL_Lecture_7_-_Sparse_Reward-17 讓它去採取一些過去沒有採取過的 action
DRL_Lecture_7_-_Sparse_Reward-18 那你要隨機到說它把螺絲起子撿起來
DRL_Lecture_7_-_Sparse_Reward-19 再把螺絲栓進去，然後就會得到 reward 1
DRL_Lecture_7_-_Sparse_Reward-20 這件事情是永遠不可能發生的
DRL_Lecture_7_-_Sparse_Reward-21 所以，你會發現說
DRL_Lecture_7_-_Sparse_Reward-22 不管今天你的 actor 它做了什麼事情
DRL_Lecture_7_-_Sparse_Reward-23 它得到 reward 永遠都是 0
DRL_Lecture_7_-_Sparse_Reward-24 對它來說不管採取什麼樣的 action
DRL_Lecture_7_-_Sparse_Reward-25 都是一樣糟或者是一樣的好
DRL_Lecture_7_-_Sparse_Reward-26 所以，它最後什麼都不會學到
DRL_Lecture_7_-_Sparse_Reward-27 所以，今天如果你環境中的 reward 非常的 sparse
DRL_Lecture_7_-_Sparse_Reward-28 那這個 reinforcement learning 的問題
DRL_Lecture_7_-_Sparse_Reward-29 就會變得非常的困難
DRL_Lecture_7_-_Sparse_Reward-30 但是對人類來說，人類很厲害
DRL_Lecture_7_-_Sparse_Reward-31 人類可以在非常 sparse 的 reward 上面去學習
DRL_Lecture_7_-_Sparse_Reward-32 就我們的人生通常多數的時候我們就只是活在那裡
DRL_Lecture_7_-_Sparse_Reward-33 都沒有得到什麼 reward 或者是 penalty
DRL_Lecture_7_-_Sparse_Reward-34 但是，人還是可以採取各種各式各樣的行為
DRL_Lecture_7_-_Sparse_Reward-35 所以，一個真正厲害的人工智慧
DRL_Lecture_7_-_Sparse_Reward-36 它應該能夠在 sparse reward 的情況下
DRL_Lecture_7_-_Sparse_Reward-37 也學到要怎麼跟這個環境互動
DRL_Lecture_7_-_Sparse_Reward-38 所以，接下來我想要跟大家很快的
DRL_Lecture_7_-_Sparse_Reward-39 非常簡單的介紹
DRL_Lecture_7_-_Sparse_Reward-40 就是一些handle sparse reward 的方法
DRL_Lecture_7_-_Sparse_Reward-41 在這個作業 4-3 裡面
DRL_Lecture_7_-_Sparse_Reward-42 除了要做 Actor-Critic 以外
DRL_Lecture_7_-_Sparse_Reward-43 那會要求大家做 Actor-Critic
DRL_Lecture_7_-_Sparse_Reward-44 然後比較一下說
DRL_Lecture_7_-_Sparse_Reward-45 你現在的  Actor-Critic  如果做在 4-2 上，有沒有比較好
DRL_Lecture_7_-_Sparse_Reward-46 如果做在 4-1 上，有沒有比較好
DRL_Lecture_7_-_Sparse_Reward-47 那當然是希望應該是要有比較好
DRL_Lecture_7_-_Sparse_Reward-48 那接下來會希望你 implement 某一個方法
DRL_Lecture_7_-_Sparse_Reward-49 來 improve 你的 model，
DRL_Lecture_7_-_Sparse_Reward-50 那這個方法是什麼呢？你就自己決定
DRL_Lecture_7_-_Sparse_Reward-51 你不一定要 implement 很難的方法
DRL_Lecture_7_-_Sparse_Reward-52 你就看你自己想要做什麼
DRL_Lecture_7_-_Sparse_Reward-53 舉例來說，我們等一下會講一些
DRL_Lecture_7_-_Sparse_Reward-54 handle sparse reward 的方法
DRL_Lecture_7_-_Sparse_Reward-55 你就看看說裡面有沒有是你用得上的
DRL_Lecture_7_-_Sparse_Reward-56 我們會講 imitation 類的方法
DRL_Lecture_7_-_Sparse_Reward-57 看你要不要做一下 imitation learning
DRL_Lecture_7_-_Sparse_Reward-58 你可以給你的 agent 一些示範
DRL_Lecture_7_-_Sparse_Reward-59 看看它能不能夠學得比較好
DRL_Lecture_7_-_Sparse_Reward-60 等等就想一個你喜歡的方法
DRL_Lecture_7_-_Sparse_Reward-61 來 improve 你的 model
DRL_Lecture_7_-_Sparse_Reward-62 那怎麼解決 sparse reward 的這件事情呢？
DRL_Lecture_7_-_Sparse_Reward-63 我們等一下會講三個方向
DRL_Lecture_7_-_Sparse_Reward-64 我們先講第一個方向
DRL_Lecture_7_-_Sparse_Reward-65 第一個方向是常容易理解的
DRL_Lecture_7_-_Sparse_Reward-66 第一個方向叫做 reward shaping
DRL_Lecture_7_-_Sparse_Reward-67 reward shaping 是什麼意思呢？
DRL_Lecture_7_-_Sparse_Reward-68 reward shaping 的意思是說
DRL_Lecture_7_-_Sparse_Reward-69 環境有一個固定的 reward
DRL_Lecture_7_-_Sparse_Reward-70 它是真正的 reward
DRL_Lecture_7_-_Sparse_Reward-71 但是我們為了引導 machine
DRL_Lecture_7_-_Sparse_Reward-72 為了引導 agent
DRL_Lecture_7_-_Sparse_Reward-73 讓他學出來的結果是我們要的樣子
DRL_Lecture_7_-_Sparse_Reward-74 這個 developer，就是我們人類
DRL_Lecture_7_-_Sparse_Reward-75 刻意的去設計了一些 reward
DRL_Lecture_7_-_Sparse_Reward-76 來引導我們的 agent
DRL_Lecture_7_-_Sparse_Reward-77 舉例來說，如果是把人類當作 agent
DRL_Lecture_7_-_Sparse_Reward-78 小孩當作一個 agent 的話
DRL_Lecture_7_-_Sparse_Reward-79 那一個小孩，他可以 take 兩個 actions
DRL_Lecture_7_-_Sparse_Reward-80 一個 action 是他可以出去玩
DRL_Lecture_7_-_Sparse_Reward-81 那他出去玩的話
DRL_Lecture_7_-_Sparse_Reward-82 在下一秒鐘它會得到 reward 1
DRL_Lecture_7_-_Sparse_Reward-83 但是他可能在月考的時候，成績會很差
DRL_Lecture_7_-_Sparse_Reward-84 所以，在 100 個小時之後呢
DRL_Lecture_7_-_Sparse_Reward-85 他會得到 reward -100
DRL_Lecture_7_-_Sparse_Reward-86 然後，他也可以決定他要唸書
DRL_Lecture_7_-_Sparse_Reward-87 然後在下一個時間，因為他沒有出去玩
DRL_Lecture_7_-_Sparse_Reward-88 所以他覺得很不爽，所以他得到 reward -1
DRL_Lecture_7_-_Sparse_Reward-89 但是在 100 個小時後，他可以得到 reward 100
DRL_Lecture_7_-_Sparse_Reward-90 但是，對一個小孩來說
DRL_Lecture_7_-_Sparse_Reward-91 他可能就會想要 take play
DRL_Lecture_7_-_Sparse_Reward-92 而不是 take study
DRL_Lecture_7_-_Sparse_Reward-93 因為，今天我們雖然說
DRL_Lecture_7_-_Sparse_Reward-94 我們計算的是 accumulated reward
DRL_Lecture_7_-_Sparse_Reward-95 但是，也許對小孩來說
DRL_Lecture_7_-_Sparse_Reward-96 他的 discount factor 很大這樣
DRL_Lecture_7_-_Sparse_Reward-97 所以，未來的 reward
DRL_Lecture_7_-_Sparse_Reward-98 他就不太在意未來的 reward
DRL_Lecture_7_-_Sparse_Reward-99 而且也許因為他是一個小孩
DRL_Lecture_7_-_Sparse_Reward-100 他還沒有很多 experience
DRL_Lecture_7_-_Sparse_Reward-101 所以，他的 Q function estimate 是非常不精準的
DRL_Lecture_7_-_Sparse_Reward-102 所以要他去 estimate 很遙遠以後
DRL_Lecture_7_-_Sparse_Reward-103 會得到的 accumulated reward
DRL_Lecture_7_-_Sparse_Reward-104 他其實是預測不出來的
DRL_Lecture_7_-_Sparse_Reward-105 所以怎麼辦呢？這時候大人就要引導他
DRL_Lecture_7_-_Sparse_Reward-106 怎麼引導呢？就騙他說
DRL_Lecture_7_-_Sparse_Reward-107 如果你坐下來唸書我就給你吃一個棒棒糖
DRL_Lecture_7_-_Sparse_Reward-108 所以，對他來說
DRL_Lecture_7_-_Sparse_Reward-109 下一個時間點會得到的 reward 就變成是 positive 的
DRL_Lecture_7_-_Sparse_Reward-110 所以他就覺得說，也許 take 這個 study 是比 play 好的
DRL_Lecture_7_-_Sparse_Reward-111 雖然實際上這並不是真正的 reward
DRL_Lecture_7_-_Sparse_Reward-112 而是其他人去騙他的 reward
DRL_Lecture_7_-_Sparse_Reward-113 告訴他說你採取這個 action是好的
DRL_Lecture_7_-_Sparse_Reward-114 所以，我給你一個 reward
DRL_Lecture_7_-_Sparse_Reward-115 雖然這個不是環境真正的 reward
DRL_Lecture_7_-_Sparse_Reward-116 所以 reward shaping 的概念是一樣的
DRL_Lecture_7_-_Sparse_Reward-117 簡單來說，就是你自己想辦法 design 一些 reward
DRL_Lecture_7_-_Sparse_Reward-118 他不是環境真正的 reward
DRL_Lecture_7_-_Sparse_Reward-119 有在玩 ATARI 遊戲裡面
DRL_Lecture_7_-_Sparse_Reward-120 真的 reward 是那個遊戲的主機給你的 reward
DRL_Lecture_7_-_Sparse_Reward-121 但是你自己去設一些 reward
DRL_Lecture_7_-_Sparse_Reward-122 好引導你的 machine
DRL_Lecture_7_-_Sparse_Reward-123 做你想要它做的事情，舉例來說這個例子是
DRL_Lecture_7_-_Sparse_Reward-124 Example demo
DRL_Lecture_7_-_Sparse_Reward-125 所以，接下來就是介紹各種你可以自己加進去
DRL_Lecture_7_-_Sparse_Reward-126 然後，In general 看起來是有用的 reward
DRL_Lecture_7_-_Sparse_Reward-127 舉例來說，一個技術是
DRL_Lecture_7_-_Sparse_Reward-128 給 machine 加上 curiosity
DRL_Lecture_7_-_Sparse_Reward-129 給它加上好奇心
DRL_Lecture_7_-_Sparse_Reward-130 所以叫 curiosity driven 的 reward
DRL_Lecture_7_-_Sparse_Reward-131 那這個是我們之前講 Actor-Critic 的時候看過的圖
DRL_Lecture_7_-_Sparse_Reward-132 所以，我們有一個 reward function
DRL_Lecture_7_-_Sparse_Reward-133 它給你某一個 state
DRL_Lecture_7_-_Sparse_Reward-134 給你某一個 action
DRL_Lecture_7_-_Sparse_Reward-135 它就會評斷說
DRL_Lecture_7_-_Sparse_Reward-136 在這個 state 採取這個 action 得到多少的 reward
DRL_Lecture_7_-_Sparse_Reward-137 那我們當然是希望 total reward 越大越好
DRL_Lecture_7_-_Sparse_Reward-138 那在 curiosity driven 的這種技術裡面
DRL_Lecture_7_-_Sparse_Reward-139 你會加上一個新的 reward function
DRL_Lecture_7_-_Sparse_Reward-140 這個新的 reward function 叫做 ICM
DRL_Lecture_7_-_Sparse_Reward-141 Intrinsic curiosity module
DRL_Lecture_7_-_Sparse_Reward-142 它就是要給機器加上好奇心
DRL_Lecture_7_-_Sparse_Reward-143 這個 ICM，它會吃 3 個東西
DRL_Lecture_7_-_Sparse_Reward-144 它會吃 state, s1，它會吃 action, a1 跟 state, s2
DRL_Lecture_7_-_Sparse_Reward-145 根據 s1, a1, a2，它會 output 另外一個 reward
DRL_Lecture_7_-_Sparse_Reward-146 我們這邊叫做 r1(i)
DRL_Lecture_7_-_Sparse_Reward-147 那你最後你的 total reward，對 machine 來說
DRL_Lecture_7_-_Sparse_Reward-148 total reward 並不是只有 r 而已，還有 r(i)
DRL_Lecture_7_-_Sparse_Reward-149 它不是只有把所有的 r 都加起來
DRL_Lecture_7_-_Sparse_Reward-150 他把所有 r(i) 加起來當作 total reward
DRL_Lecture_7_-_Sparse_Reward-151 所以，它在跟環境互動的時候
DRL_Lecture_7_-_Sparse_Reward-152 它不是只希望 r 越大越好
DRL_Lecture_7_-_Sparse_Reward-153 它還同時希望 r(i) 越大越好
DRL_Lecture_7_-_Sparse_Reward-154 它希望從 ICM 的 module 裡面
DRL_Lecture_7_-_Sparse_Reward-155 得到的 reward 越大越好
DRL_Lecture_7_-_Sparse_Reward-156 那這個 ICM 呢
DRL_Lecture_7_-_Sparse_Reward-157 它就代表了一種 curiosity
DRL_Lecture_7_-_Sparse_Reward-158 那怎麼設計這個 ICM？讓它變成一種
DRL_Lecture_7_-_Sparse_Reward-159 讓他有類似這種好奇心的功能呢？
DRL_Lecture_7_-_Sparse_Reward-160 這個是最原始的設計
DRL_Lecture_7_-_Sparse_Reward-161 這個設計是這樣
DRL_Lecture_7_-_Sparse_Reward-162 我們說 curiosity module 就是 input 3 個東西
DRL_Lecture_7_-_Sparse_Reward-163 input 現在的 state
DRL_Lecture_7_-_Sparse_Reward-164 input 在這個 state 採取的 action
DRL_Lecture_7_-_Sparse_Reward-165 然後接下來 input 下一個 state, s(t+1)
DRL_Lecture_7_-_Sparse_Reward-166 然後接下來會 output 一個 reward, r(i)
DRL_Lecture_7_-_Sparse_Reward-167 那這個 r(i) 怎麼算出來的呢？
DRL_Lecture_7_-_Sparse_Reward-168 在 ICM 裡面，你有一個 network
DRL_Lecture_7_-_Sparse_Reward-169 這個 network 會 take a(t) 跟 s(t)
DRL_Lecture_7_-_Sparse_Reward-170 然後呢，去 output s(t+1) hat
DRL_Lecture_7_-_Sparse_Reward-171 也就是這個 network 做的事情
DRL_Lecture_7_-_Sparse_Reward-172 是根據 a(t) 跟 s(t)
DRL_Lecture_7_-_Sparse_Reward-173 去 predict 接下來我們會看到的 s(t+1) hat
DRL_Lecture_7_-_Sparse_Reward-174 你會根據現在的 state
DRL_Lecture_7_-_Sparse_Reward-175 跟在現在這個 state 採取的 action
DRL_Lecture_7_-_Sparse_Reward-176 我們有另外一個 network 去預測
DRL_Lecture_7_-_Sparse_Reward-177 接下來會發生什麼事
DRL_Lecture_7_-_Sparse_Reward-178 接下來再看說，machine 自己的預測
DRL_Lecture_7_-_Sparse_Reward-179 這個 network 自己的預測
DRL_Lecture_7_-_Sparse_Reward-180 跟真實的情況像不像
DRL_Lecture_7_-_Sparse_Reward-181 越不像，那越不像那得到的 reward 就越大
DRL_Lecture_7_-_Sparse_Reward-182 所以今天這個 reward 呢
DRL_Lecture_7_-_Sparse_Reward-183 它的意思是說，如果今天未來的 state
DRL_Lecture_7_-_Sparse_Reward-184 越難被預測的話，那得到的 reward 就越大
DRL_Lecture_7_-_Sparse_Reward-185 這就是鼓勵 machine 去冒險
DRL_Lecture_7_-_Sparse_Reward-186 現在採取這個 action，未來會發生什麼事
DRL_Lecture_7_-_Sparse_Reward-187 越沒有辦法預測的話
DRL_Lecture_7_-_Sparse_Reward-188 那這個 action 的 reward 就大
DRL_Lecture_7_-_Sparse_Reward-189 所以，machine 如果有這樣子的 ICM
DRL_Lecture_7_-_Sparse_Reward-190 它就會傾向於採取一些風險比較大的 action
DRL_Lecture_7_-_Sparse_Reward-191 它想要去探索未知的世界
DRL_Lecture_7_-_Sparse_Reward-192 它想要去看看說，假設某一個 state
DRL_Lecture_7_-_Sparse_Reward-193 是它沒有辦法預測
DRL_Lecture_7_-_Sparse_Reward-194 假設它沒有辦法預測未來會發生什麼事
DRL_Lecture_7_-_Sparse_Reward-195 它會特別去想要採取那種 state
DRL_Lecture_7_-_Sparse_Reward-196 可以增加 machine exploration 的能力
DRL_Lecture_7_-_Sparse_Reward-197 那這邊這個 network 1
DRL_Lecture_7_-_Sparse_Reward-198 其實是另外 train 出來的
DRL_Lecture_7_-_Sparse_Reward-199 大家瞭解我的意思嗎？
DRL_Lecture_7_-_Sparse_Reward-200 這個 training 的時候
DRL_Lecture_7_-_Sparse_Reward-201 你這個 network 1，它就是
DRL_Lecture_7_-_Sparse_Reward-202 在 training 的時候
DRL_Lecture_7_-_Sparse_Reward-203 你會給它 at, st, s(t+1)
DRL_Lecture_7_-_Sparse_Reward-204 然後讓這個 network 1 去學說 given at, st
DRL_Lecture_7_-_Sparse_Reward-205 怎麼 predict s(t+1) hat
DRL_Lecture_7_-_Sparse_Reward-206 apply 到 agent 互動的時候
DRL_Lecture_7_-_Sparse_Reward-207 這個 ICM module，其實要把它 fix 住
DRL_Lecture_7_-_Sparse_Reward-208 那大家知道我的意思的話
DRL_Lecture_7_-_Sparse_Reward-209 其實，這一整個想法裡面
DRL_Lecture_7_-_Sparse_Reward-210 是有一個問題的，這個問題是什麼呢？
DRL_Lecture_7_-_Sparse_Reward-211 這個問題是，某一些 state
DRL_Lecture_7_-_Sparse_Reward-212 它很難被預測，並不代表它就是好的
DRL_Lecture_7_-_Sparse_Reward-213 它就應該要去被嘗試的
DRL_Lecture_7_-_Sparse_Reward-214 舉例來說，俄羅斯輪盤的結果也是沒有辦法預測的
DRL_Lecture_7_-_Sparse_Reward-215 並不代表說，人應該每天去玩俄羅斯輪盤這樣子
DRL_Lecture_7_-_Sparse_Reward-216 所以，今天光是告訴 machine，鼓勵 machine 去冒險
DRL_Lecture_7_-_Sparse_Reward-217 是不夠的，因為如果光是只有這個 network 的架構
DRL_Lecture_7_-_Sparse_Reward-218 machine 只知道說什麼東西它無法預測
DRL_Lecture_7_-_Sparse_Reward-219 如果在某一個 state 採取某一個 action
DRL_Lecture_7_-_Sparse_Reward-220 它無法預測接下來結果
DRL_Lecture_7_-_Sparse_Reward-221 它就會採取那個 action
DRL_Lecture_7_-_Sparse_Reward-222 但並不代表這樣的結果一定是好的
DRL_Lecture_7_-_Sparse_Reward-223 舉例來說，可能在某個遊戲裡面
DRL_Lecture_7_-_Sparse_Reward-224 背景會有風吹草動，會有樹葉飄動
DRL_Lecture_7_-_Sparse_Reward-225 那也許樹葉飄動這件事情，是很難被預測的
DRL_Lecture_7_-_Sparse_Reward-226 對 machine 來說它在某一個 state 什麼都不做
DRL_Lecture_7_-_Sparse_Reward-227 看著樹葉飄動
DRL_Lecture_7_-_Sparse_Reward-228 然後，發現這個樹葉飄動是沒有辦法預測的
DRL_Lecture_7_-_Sparse_Reward-229 接下來它就會一直站在那邊，看樹葉飄動
DRL_Lecture_7_-_Sparse_Reward-230 所以說，光是有好奇心是不夠的
DRL_Lecture_7_-_Sparse_Reward-231 還要讓它知道說，什麼事情是真正重要的
DRL_Lecture_7_-_Sparse_Reward-232 那怎麼讓 machine 真的知道說什麼事情是真正重要的
DRL_Lecture_7_-_Sparse_Reward-233 而不是讓他只是一直看樹葉飄動呢？
DRL_Lecture_7_-_Sparse_Reward-234 你要加上另外一個 module
DRL_Lecture_7_-_Sparse_Reward-235 我們要 learn 一個 feature 的 extractor
DRL_Lecture_7_-_Sparse_Reward-236 這個黃色的格子代表 feature extractor
DRL_Lecture_7_-_Sparse_Reward-237 它是 input 一個 state
DRL_Lecture_7_-_Sparse_Reward-238 然後 output 一個 feature vector
DRL_Lecture_7_-_Sparse_Reward-239 代表這個 state，那我們現在期待的是
DRL_Lecture_7_-_Sparse_Reward-240 這個 feature extractor 可以做的事情是把那種
DRL_Lecture_7_-_Sparse_Reward-241 沒有意義的畫面
DRL_Lecture_7_-_Sparse_Reward-242 state 裡面沒有意義的東西把它濾掉
DRL_Lecture_7_-_Sparse_Reward-243 比如說風吹草動，白雲的飄動，樹葉的飄動這種
DRL_Lecture_7_-_Sparse_Reward-244 沒有意義的東西直接把它濾掉
DRL_Lecture_7_-_Sparse_Reward-245 我們希望這個 feature extractor 可以做到這個功能
DRL_Lecture_7_-_Sparse_Reward-246 那我們等一下再講說，這件事情是怎麼做到的
DRL_Lecture_7_-_Sparse_Reward-247 假設這個 feature extractor
DRL_Lecture_7_-_Sparse_Reward-248 真的可以把無關緊要的東西，濾掉以後
DRL_Lecture_7_-_Sparse_Reward-249 那我們的 network 1 實際上做的事情是
DRL_Lecture_7_-_Sparse_Reward-250 給它一個 actor
DRL_Lecture_7_-_Sparse_Reward-251 給他一個 state s1 的 feature representation
DRL_Lecture_7_-_Sparse_Reward-252 讓它預測，state, s(t+1) 的 feature representation
DRL_Lecture_7_-_Sparse_Reward-253 然接下來我們再看說，這個預測的結果
DRL_Lecture_7_-_Sparse_Reward-254 跟真正的 state, s(t+1) 的 feature representation 像不像
DRL_Lecture_7_-_Sparse_Reward-255 越不像，reward 就越大
DRL_Lecture_7_-_Sparse_Reward-256 接下來的問題就是
DRL_Lecture_7_-_Sparse_Reward-257 怎麼 learn 這個 feature extractor 呢？
DRL_Lecture_7_-_Sparse_Reward-258 讓這個 feature extractor 它可以把無關緊要的事情濾掉呢？
DRL_Lecture_7_-_Sparse_Reward-259 這邊的 learn 法就是
DRL_Lecture_7_-_Sparse_Reward-260 learn 另外一個 network 2
DRL_Lecture_7_-_Sparse_Reward-261 這個 network 2 啊
DRL_Lecture_7_-_Sparse_Reward-262 它是吃這兩個 vector 當做 input
DRL_Lecture_7_-_Sparse_Reward-263 然後接下來它要 predict action a 是什麼
DRL_Lecture_7_-_Sparse_Reward-264 然後它希望呢這個 action a
DRL_Lecture_7_-_Sparse_Reward-265 跟真正的 action a，越接近越好
DRL_Lecture_7_-_Sparse_Reward-266 我突然發現我這個地方其實寫得沒有很對，你有發現嗎？
DRL_Lecture_7_-_Sparse_Reward-267 這裡這個 a 跟 a hat，我應該要反過來吧
DRL_Lecture_7_-_Sparse_Reward-268 預測出來的東西我們用 hat 來表示
DRL_Lecture_7_-_Sparse_Reward-269 真正的東西沒有 hat，應該是這樣感覺比較對
DRL_Lecture_7_-_Sparse_Reward-270 所以 at 跟 at hat 應該是反過來的
DRL_Lecture_7_-_Sparse_Reward-271 所以這個 network 2，它會 output 一個 action
DRL_Lecture_7_-_Sparse_Reward-272 就根據 state, st 的 feature 跟 state, s(t+1) 的 feature
DRL_Lecture_7_-_Sparse_Reward-273 它 output 説，從 state, st，跳到 state, s(t+1)
DRL_Lecture_7_-_Sparse_Reward-274 要採取哪一個 action，才能夠做到
DRL_Lecture_7_-_Sparse_Reward-275 那希望這個action 跟真正的 action，越接近越好
DRL_Lecture_7_-_Sparse_Reward-276 那加上這個 network 的好處就是
DRL_Lecture_7_-_Sparse_Reward-277 因為這兩個東西要拿去預測 action
DRL_Lecture_7_-_Sparse_Reward-278 所以，今天我們抽出來的 feature
DRL_Lecture_7_-_Sparse_Reward-279 就會變成是跟 action
DRL_Lecture_7_-_Sparse_Reward-280 跟預測 action 這件事情是有關的
DRL_Lecture_7_-_Sparse_Reward-281 所以，假設是一些無聊的東西
DRL_Lecture_7_-_Sparse_Reward-282 是跟 machine 本身採取的 action 無關的東西
DRL_Lecture_7_-_Sparse_Reward-283 風吹草動或是白雲飄過去
DRL_Lecture_7_-_Sparse_Reward-284 是 machine 自己要採取的 action 無關的東西
DRL_Lecture_7_-_Sparse_Reward-285 那就會被濾掉
DRL_Lecture_7_-_Sparse_Reward-286 就不會被放在抽出來的 vector representation 裡面
DRL_Lecture_7_-_Sparse_Reward-287 那剛才講的是 reward shaping 的方法
DRL_Lecture_7_-_Sparse_Reward-288 那接下來我們要講 Curriculum learning 這件事情
DRL_Lecture_7_-_Sparse_Reward-289 那 Curriculum learning 不是
DRL_Lecture_7_-_Sparse_Reward-290 reinforcement learning 所獨有的概念
DRL_Lecture_7_-_Sparse_Reward-291 那其實在很多 machine learning
DRL_Lecture_7_-_Sparse_Reward-292 尤其是 deep learning 裡面
DRL_Lecture_7_-_Sparse_Reward-293 你都會用到 Curriculum learning 的概念，舉例來說
DRL_Lecture_7_-_Sparse_Reward-294 所謂 Curriculum learning 的意思是説
DRL_Lecture_7_-_Sparse_Reward-295 你為機器的學習啊，做規劃
DRL_Lecture_7_-_Sparse_Reward-296 你給他餵 training data 的時候，是有順序的
DRL_Lecture_7_-_Sparse_Reward-297 那通常都是由簡單到難，就好比說，
DRL_Lecture_7_-_Sparse_Reward-298 假設你今天要交一個小朋友作微積分，
DRL_Lecture_7_-_Sparse_Reward-299 他做錯就打他一巴掌
DRL_Lecture_7_-_Sparse_Reward-300 可是他永遠都不會做對，太難了
DRL_Lecture_7_-_Sparse_Reward-301 你要先教他 99 乘法，然後才教他微積分
DRL_Lecture_7_-_Sparse_Reward-302 打死他，他都學不起來這樣
DRL_Lecture_7_-_Sparse_Reward-303 所以很難，所以 Curriculum learning 的意思就是在教機器的時候
DRL_Lecture_7_-_Sparse_Reward-304 從簡單的題目，教到難的題目
DRL_Lecture_7_-_Sparse_Reward-305 那如果不是 reinforcement learning
DRL_Lecture_7_-_Sparse_Reward-306 一般在 train deep network 的時候
DRL_Lecture_7_-_Sparse_Reward-307 你有時候也會這麼做，舉例來說，在 train RNN 的時候
DRL_Lecture_7_-_Sparse_Reward-308 已經有很多的文獻，都 report 説
DRL_Lecture_7_-_Sparse_Reward-309 你給機器先看短的 sequence
DRL_Lecture_7_-_Sparse_Reward-310 再慢慢給它長的 sequence，通常可以學得比較好
DRL_Lecture_7_-_Sparse_Reward-311 那用在 reinforcement learning 裡面
DRL_Lecture_7_-_Sparse_Reward-312 你就是要幫機器規劃一下它的課程
DRL_Lecture_7_-_Sparse_Reward-313 從最簡單的到最難的， 舉例來說
DRL_Lecture_7_-_Sparse_Reward-314 在 Facebook 玩 VizDoom 的 agent 裡面
DRL_Lecture_7_-_Sparse_Reward-315 Facebook 那個 VizDoom 的 agent 據說蠻強的
DRL_Lecture_7_-_Sparse_Reward-316 他們在參加這個 VizDoom 的比賽
DRL_Lecture_7_-_Sparse_Reward-317 機器的 VizDoom 比賽是得第一名的
DRL_Lecture_7_-_Sparse_Reward-318 他們是有為機器規劃課程的
DRL_Lecture_7_-_Sparse_Reward-319 先從課程 0 一直上到課程 7
DRL_Lecture_7_-_Sparse_Reward-320 在這個課程裡面
DRL_Lecture_7_-_Sparse_Reward-321 那些怪就是有不同的 speed 跟 health
DRL_Lecture_7_-_Sparse_Reward-322 怪物的速度跟血量是不一樣的
DRL_Lecture_7_-_Sparse_Reward-323 所以，在越進階的課程裡面
DRL_Lecture_7_-_Sparse_Reward-324 怪物的速度越快，然後他的血量越多
DRL_Lecture_7_-_Sparse_Reward-325 在 paper 裡面也有講說
DRL_Lecture_7_-_Sparse_Reward-326 如果直接上課程 7，machine 是學不起來的
DRL_Lecture_7_-_Sparse_Reward-327 你就是要從課程 0 一路玩上去
DRL_Lecture_7_-_Sparse_Reward-328 這樣 machine 才學得起來
DRL_Lecture_7_-_Sparse_Reward-329 所以，再拿剛才的這個
DRL_Lecture_7_-_Sparse_Reward-330 把藍色的板子放到柱子上的時間
DRL_Lecture_7_-_Sparse_Reward-331 怎麼讓機器一直從簡單學到難呢？
DRL_Lecture_7_-_Sparse_Reward-332 也許一開始你讓機器初始的時候
DRL_Lecture_7_-_Sparse_Reward-333 它的板子就已經不在柱子上了
DRL_Lecture_7_-_Sparse_Reward-334 這個時候，你要做的事情只有
DRL_Lecture_7_-_Sparse_Reward-335 這個時候，機器要做的事情只有把藍色的板子壓下去
DRL_Lecture_7_-_Sparse_Reward-336 就結束了，這比較簡單，它應該很快就學的會
DRL_Lecture_7_-_Sparse_Reward-337 它只有往上跟往下這兩個選擇嘛
DRL_Lecture_7_-_Sparse_Reward-338 往下，就得到 reward 就結束了，他也不知道學的是甚麼
DRL_Lecture_7_-_Sparse_Reward-339 這邊是把板子挪高一點
DRL_Lecture_7_-_Sparse_Reward-340 挪高一點，所以它有時候會很笨的往上拉，就拿出來了
DRL_Lecture_7_-_Sparse_Reward-341 如果它這個學得會的話，這個也比較有機會學得會
DRL_Lecture_7_-_Sparse_Reward-342 假設它現在學的到說，只要板子接近柱子
DRL_Lecture_7_-_Sparse_Reward-343 它就可以把這個板子壓下去的話
DRL_Lecture_7_-_Sparse_Reward-344 接下來，你再讓它學更 general 的 case
DRL_Lecture_7_-_Sparse_Reward-345 先讓一開始，板子離柱子遠一點
DRL_Lecture_7_-_Sparse_Reward-346 然後，板子放到柱子上面的時候
DRL_Lecture_7_-_Sparse_Reward-347 它就會知道把板子壓下去
DRL_Lecture_7_-_Sparse_Reward-348 這個就是 Curriculum Learning 的概念
DRL_Lecture_7_-_Sparse_Reward-349 當然 Curriculum learning 這邊有點 ad hoc
DRL_Lecture_7_-_Sparse_Reward-350 就是你需要人，當作老師去為機器設計它的課程
DRL_Lecture_7_-_Sparse_Reward-351 那有一個比較 general 的方法叫做
DRL_Lecture_7_-_Sparse_Reward-352 Reverse Curriculum Generation
DRL_Lecture_7_-_Sparse_Reward-353 你可以用一個比較通用的方法
DRL_Lecture_7_-_Sparse_Reward-354 來幫機器設計課程
DRL_Lecture_7_-_Sparse_Reward-355 這個比較通用的方法是怎麼樣呢？
DRL_Lecture_7_-_Sparse_Reward-356 假設你現在一開始有一個 state, sg
DRL_Lecture_7_-_Sparse_Reward-357 這是你的 gold state，也就是最後最理想的結果
DRL_Lecture_7_-_Sparse_Reward-358 如果是拿剛才那個板子和柱子的例子的話
DRL_Lecture_7_-_Sparse_Reward-359 就把板子放到柱子裡面
DRL_Lecture_7_-_Sparse_Reward-360 這樣子叫做 gold state
DRL_Lecture_7_-_Sparse_Reward-361 你就已經完成了，或者你讓機器去抓東西
DRL_Lecture_7_-_Sparse_Reward-362 你訓練一個機器手臂抓東西
DRL_Lecture_7_-_Sparse_Reward-363 抓到東西以後叫做 gold state
DRL_Lecture_7_-_Sparse_Reward-364 那接下來你根據你的 gold state
DRL_Lecture_7_-_Sparse_Reward-365 去找其他的 state，這些其他的 state
DRL_Lecture_7_-_Sparse_Reward-366 跟 gold state 是比較接近的
DRL_Lecture_7_-_Sparse_Reward-367 舉例來說，如果是讓機器抓東西的例子裡面
DRL_Lecture_7_-_Sparse_Reward-368 你的機器手臂可能還沒有抓到東西
DRL_Lecture_7_-_Sparse_Reward-369 假裝這些跟 gold state 很近的 state 我們叫做 s1
DRL_Lecture_7_-_Sparse_Reward-370 你的機械手臂還沒有抓到東西
DRL_Lecture_7_-_Sparse_Reward-371 但是，它離 gold state 很近，那這個叫做 s1
DRL_Lecture_7_-_Sparse_Reward-372 至於什麼叫做近，這個就麻煩
DRL_Lecture_7_-_Sparse_Reward-373 就是 case dependent，你要根據你的 task
DRL_Lecture_7_-_Sparse_Reward-374 來 design 説怎麼從 sg sample 出 s1
DRL_Lecture_7_-_Sparse_Reward-375 如果是機械手臂的例子，可能就比較好想
DRL_Lecture_7_-_Sparse_Reward-376 其他例子可能就比較難想
DRL_Lecture_7_-_Sparse_Reward-377 接下來呢，你再從這些 state 1 開始做互動
DRL_Lecture_7_-_Sparse_Reward-378 看它能不能夠達到 gold state, sg
DRL_Lecture_7_-_Sparse_Reward-379 那每一個 state，你跟環境做互動的時候
DRL_Lecture_7_-_Sparse_Reward-380 你都會得到一個 reward, R
DRL_Lecture_7_-_Sparse_Reward-381 接下來，我們把 reward 特別極端的 case 去掉
DRL_Lecture_7_-_Sparse_Reward-382 reward 特別極端的 case 的意思就是說
DRL_Lecture_7_-_Sparse_Reward-383 那些 case 它太簡單，或者是太難
DRL_Lecture_7_-_Sparse_Reward-384 就 reward 如果很大，代表說這個 case 太簡單了
DRL_Lecture_7_-_Sparse_Reward-385 就不用學了，因為機器已經會了
DRL_Lecture_7_-_Sparse_Reward-386 它可以得到很大的 reward
DRL_Lecture_7_-_Sparse_Reward-387 那 reward 如果太小代表這個 case 太難了
DRL_Lecture_7_-_Sparse_Reward-388 依照機器現在的能力這個課程太難了
DRL_Lecture_7_-_Sparse_Reward-389 它學不會，所以就不要學這個
DRL_Lecture_7_-_Sparse_Reward-390 所以只找一些 reward 適中的 case
DRL_Lecture_7_-_Sparse_Reward-391 那當然什麼叫做適中，這個就是你要調的參數嘛
DRL_Lecture_7_-_Sparse_Reward-392 找一些 reward 適中的 case
DRL_Lecture_7_-_Sparse_Reward-393 接下來，再根據這些 reward 適中的 case
DRL_Lecture_7_-_Sparse_Reward-394 再去 sample 出更多的 state，更多的 state
DRL_Lecture_7_-_Sparse_Reward-395 就假設你一開始，你的東西在這裡
DRL_Lecture_7_-_Sparse_Reward-396 你機械手臂在這邊，可以抓的到以後
DRL_Lecture_7_-_Sparse_Reward-397 接下來，就再離遠一點，看看能不能夠抓得到
DRL_Lecture_7_-_Sparse_Reward-398 又抓的到以後，再離遠一點
DRL_Lecture_7_-_Sparse_Reward-399 看看能不能抓得到，這個方法很直覺
DRL_Lecture_7_-_Sparse_Reward-400 但是，它是一個有用的方法就是了
DRL_Lecture_7_-_Sparse_Reward-401 特別叫做 Reverse Curriculum learning
DRL_Lecture_7_-_Sparse_Reward-402 那剛才講的是 Curriculum learning
DRL_Lecture_7_-_Sparse_Reward-403 就是你要為機器規劃它學習的順序
DRL_Lecture_7_-_Sparse_Reward-404 那最後一個要跟大家講的 tip
DRL_Lecture_7_-_Sparse_Reward-405 叫做 Hierarchical Reinforcement learning
DRL_Lecture_7_-_Sparse_Reward-406 有階層式的 reinforcement learning
DRL_Lecture_7_-_Sparse_Reward-407 因為它說從 gold state 去反推
DRL_Lecture_7_-_Sparse_Reward-408 就是說你原來的目標是長這個樣子
DRL_Lecture_7_-_Sparse_Reward-409 我們從我們的目標去反推，所以這個叫做 reverse
DRL_Lecture_7_-_Sparse_Reward-410 接下來要講階層式的 Reinforcement learning
DRL_Lecture_7_-_Sparse_Reward-411 所謂階層式的 Reinforcement learning 是說
DRL_Lecture_7_-_Sparse_Reward-412 我們有好幾個 agent
DRL_Lecture_7_-_Sparse_Reward-413 然後，有一些 agent 負責比較 high level 的東西
DRL_Lecture_7_-_Sparse_Reward-414 它負責訂目標，然後它訂完目標以後
DRL_Lecture_7_-_Sparse_Reward-415 再分配給其他的 agent，去把它執行完成
DRL_Lecture_7_-_Sparse_Reward-416 那這樣的想法其實也是很合理的
DRL_Lecture_7_-_Sparse_Reward-417 因為我們知道說，我們人啊
DRL_Lecture_7_-_Sparse_Reward-418 在一生之中，我們並不是時時刻刻都在做決定
DRL_Lecture_7_-_Sparse_Reward-419 舉例來說，假設你想要寫一篇 paper
DRL_Lecture_7_-_Sparse_Reward-420 那你會先想說我要寫一篇 paper 的時候
DRL_Lecture_7_-_Sparse_Reward-421 我要做那些 process，就是說我先想個梗這樣子
DRL_Lecture_7_-_Sparse_Reward-422 然後想完梗以後，你還要跑個實驗
DRL_Lecture_7_-_Sparse_Reward-423 跑完實驗以後，你還要寫
DRL_Lecture_7_-_Sparse_Reward-424 寫完以後呢，你還要這個去發表這樣子
DRL_Lecture_7_-_Sparse_Reward-425 那每一個動作下面又還會再細分
DRL_Lecture_7_-_Sparse_Reward-426 比如說，怎麼跑實驗呢？
DRL_Lecture_7_-_Sparse_Reward-427 你要先 collect data，collect 完 data 以後
DRL_Lecture_7_-_Sparse_Reward-428 你要再 label，你要弄一個 network
DRL_Lecture_7_-_Sparse_Reward-429 然後又 train 不起來，要 train 很多次
DRL_Lecture_7_-_Sparse_Reward-430 然後，重新 design network 架構好幾次
DRL_Lecture_7_-_Sparse_Reward-431 最後才把 network train 起來
DRL_Lecture_7_-_Sparse_Reward-432 所以，我們要完成一個很大的 task 的時候
DRL_Lecture_7_-_Sparse_Reward-433 我們並不是從非常底層的那些 action，開始想起
DRL_Lecture_7_-_Sparse_Reward-434 我們其實是有個 plan
DRL_Lecture_7_-_Sparse_Reward-435 我們先想說，如果要完成這個最大的任務
DRL_Lecture_7_-_Sparse_Reward-436 那接下來要拆解成哪些小任務
DRL_Lecture_7_-_Sparse_Reward-437 每一個小任務要再怎麼拆解成，小小的任務
DRL_Lecture_7_-_Sparse_Reward-438 這個是我們人類做事情的方法
DRL_Lecture_7_-_Sparse_Reward-439 舉例來說，叫你直接寫一本書可能很困難
DRL_Lecture_7_-_Sparse_Reward-440 但叫你先把一本書拆成好幾個章節
DRL_Lecture_7_-_Sparse_Reward-441 每個章節拆成好幾段，每一段又拆成好幾個句子，
DRL_Lecture_7_-_Sparse_Reward-442 每一個句子又拆成好幾個詞彙
DRL_Lecture_7_-_Sparse_Reward-443 這樣你可能就比較寫得出來
DRL_Lecture_7_-_Sparse_Reward-444 這個就是階層式的 Reinforcement learning 的概念
DRL_Lecture_7_-_Sparse_Reward-445 這邊是隨便舉一個好像可能不恰當的例子
DRL_Lecture_7_-_Sparse_Reward-446 就是假設校長跟教授跟研究生通通都是 agent
DRL_Lecture_7_-_Sparse_Reward-447 那今天假設我們的 reward 就是
DRL_Lecture_7_-_Sparse_Reward-448 只要進入百大就可以得到 reward 這樣
DRL_Lecture_7_-_Sparse_Reward-449 假設進入百大的話，校長就要提出願景
DRL_Lecture_7_-_Sparse_Reward-450 告訴其他的 agent 説
DRL_Lecture_7_-_Sparse_Reward-451 現在你要達到什麼樣的目標
DRL_Lecture_7_-_Sparse_Reward-452 那校長的願景可能就是說
DRL_Lecture_7_-_Sparse_Reward-453 教授每年都要發三篇期刊，然後接下來
DRL_Lecture_7_-_Sparse_Reward-454 這些 agent 都是有階層式的，所以上面的 agent
DRL_Lecture_7_-_Sparse_Reward-455 他的 action 他所提出的動作
DRL_Lecture_7_-_Sparse_Reward-456 他不真的做事，他的動作就是提出願景這樣
DRL_Lecture_7_-_Sparse_Reward-457 那他把他的願景傳給下一層的 agent
DRL_Lecture_7_-_Sparse_Reward-458 下一層的 agent 就把這個願景吃下去
DRL_Lecture_7_-_Sparse_Reward-459 如果他下面還有其他人的話，它就會提出新的願景
DRL_Lecture_7_-_Sparse_Reward-460 比如說，校長要教授發期刊
DRL_Lecture_7_-_Sparse_Reward-461 但是其實教授自己也是不做實驗的
DRL_Lecture_7_-_Sparse_Reward-462 所以，教授也只能夠叫下面的苦命研究生做實驗
DRL_Lecture_7_-_Sparse_Reward-463 所以教授就提出願景，就做出實驗的規劃
DRL_Lecture_7_-_Sparse_Reward-464 然後研究生才是真的去執行這個實驗的人
DRL_Lecture_7_-_Sparse_Reward-465 然後，真的把實驗做出來
DRL_Lecture_7_-_Sparse_Reward-466 最後大家就可以得到 reward
DRL_Lecture_7_-_Sparse_Reward-467 這個例子其實有點差啦
DRL_Lecture_7_-_Sparse_Reward-468 為什麼說這個例子其實有點差呢？
DRL_Lecture_7_-_Sparse_Reward-469 因為真實的情況是，校長其實是不會管這些事情的
DRL_Lecture_7_-_Sparse_Reward-470 校長並不會管教授有沒有發期刊
DRL_Lecture_7_-_Sparse_Reward-471 而且發期刊跟進入百大其實關係也不大
DRL_Lecture_7_-_Sparse_Reward-472 而且更退一步說好了，我們現在是沒有校長的
DRL_Lecture_7_-_Sparse_Reward-473 所以，現在顯然這個就不是指台大
DRL_Lecture_7_-_Sparse_Reward-474 所以，這是一個虛構的故事
DRL_Lecture_7_-_Sparse_Reward-475 我隨便亂編的，沒有很恰當
DRL_Lecture_7_-_Sparse_Reward-476 那現在是這樣子的，在 learn 的時候
DRL_Lecture_7_-_Sparse_Reward-477 其實每一個 agent 都會 learn
DRL_Lecture_7_-_Sparse_Reward-478 那他們的整體的目標，就是要達成
DRL_Lecture_7_-_Sparse_Reward-479 就是要達到最後的 reward
DRL_Lecture_7_-_Sparse_Reward-480 那前面的這些 agent，他提出來的 actions
DRL_Lecture_7_-_Sparse_Reward-481 就是願景這樣
DRL_Lecture_7_-_Sparse_Reward-482 你如果是玩遊戲的話
DRL_Lecture_7_-_Sparse_Reward-483 他提出來的就是，我現在想要產生這樣的遊戲畫面
DRL_Lecture_7_-_Sparse_Reward-484 然後，下面的能不能夠做到這件事情
DRL_Lecture_7_-_Sparse_Reward-485 上面的人就是提出願景
DRL_Lecture_7_-_Sparse_Reward-486 但是，假設他提出來的願景
DRL_Lecture_7_-_Sparse_Reward-487 是下面的 agent 達不到的，那就會被討厭
DRL_Lecture_7_-_Sparse_Reward-488 舉例來說，教授對研究生
DRL_Lecture_7_-_Sparse_Reward-489 都一直逼迫研究生做一些很困難的實驗
DRL_Lecture_7_-_Sparse_Reward-490 研究生都做不出來的話
DRL_Lecture_7_-_Sparse_Reward-491 研究生就會跑掉，所以他就會得到一個 penalty
DRL_Lecture_7_-_Sparse_Reward-492 所以t，如果今天下層的 agent
DRL_Lecture_7_-_Sparse_Reward-493 他沒有辦法達到上層 agent 所提出來的 goal 的話
DRL_Lecture_7_-_Sparse_Reward-494 上層的 agent 就會被討厭
DRL_Lecture_7_-_Sparse_Reward-495 它就會得到一個 negative reward
DRL_Lecture_7_-_Sparse_Reward-496 所以，他要避免提出那些願景是底下的 agent 所做不到的
DRL_Lecture_7_-_Sparse_Reward-497 那每一個 agent 他都是吃
DRL_Lecture_7_-_Sparse_Reward-498 上層的 agent 所提出來的願景
DRL_Lecture_7_-_Sparse_Reward-499 當作輸入，然後決定他自己要產生甚麼輸出
DRL_Lecture_7_-_Sparse_Reward-500 決定他自己要產生什麼輸出
DRL_Lecture_7_-_Sparse_Reward-501 但是你知道說，就算你看到
DRL_Lecture_7_-_Sparse_Reward-502 上面的的願景說，叫你做這一件事情
DRL_Lecture_7_-_Sparse_Reward-503 你最後也不見得，做得到這一件事情，對不對？
DRL_Lecture_7_-_Sparse_Reward-504 假設，本來教授目標是要寫期刊
DRL_Lecture_7_-_Sparse_Reward-505 但是不知道怎麼回事，他就要變成一個 YouTuber
DRL_Lecture_7_-_Sparse_Reward-506 這個 paper 裡面的 solution，我覺得非常有趣
DRL_Lecture_7_-_Sparse_Reward-507 給大家做一個參考
DRL_Lecture_7_-_Sparse_Reward-508 這其實本來的目標是要寫期刊，但卻變成 YouTuber
DRL_Lecture_7_-_Sparse_Reward-509 那怎麼辦呢 ?
DRL_Lecture_7_-_Sparse_Reward-510 把原來的願景改成變成 YouTuber
DRL_Lecture_7_-_Sparse_Reward-511 就結束了，這樣子
DRL_Lecture_7_-_Sparse_Reward-512 在 paper 裡面就是這麼做的，為甚麼這麼做呢 ?
DRL_Lecture_7_-_Sparse_Reward-513 因為雖然本來的願景是要寫期刊
DRL_Lecture_7_-_Sparse_Reward-514 但是，後來變成 YouTube
DRL_Lecture_7_-_Sparse_Reward-515 難道這些動作都浪費了嗎 ?
DRL_Lecture_7_-_Sparse_Reward-516 不是，這些動作是沒有被浪費的
DRL_Lecture_7_-_Sparse_Reward-517 我們就假設說，本來的願景
DRL_Lecture_7_-_Sparse_Reward-518 其實就是要成為 YouTuber
DRL_Lecture_7_-_Sparse_Reward-519 那你就知道說，成為 YouTuber 要怎做了
DRL_Lecture_7_-_Sparse_Reward-520 這個細節我們就不講了，你自己去研究一下 paper
DRL_Lecture_7_-_Sparse_Reward-521 這個是階層式 RL，可以做得起來的 tip
DRL_Lecture_7_-_Sparse_Reward-522 那這個是真實的例子
DRL_Lecture_7_-_Sparse_Reward-523 真實的例子，就是給大家參考一下
DRL_Lecture_7_-_Sparse_Reward-524 實際上呢，這裡面就做了一些比較簡單的遊戲
DRL_Lecture_7_-_Sparse_Reward-525 這個是走迷宮，藍色是 agent
DRL_Lecture_7_-_Sparse_Reward-526 藍色的 agent 要走走走，走到黃色的目標
DRL_Lecture_7_-_Sparse_Reward-527 這邊也是
DRL_Lecture_7_-_Sparse_Reward-528 這個單擺要碰到黃色的球
DRL_Lecture_7_-_Sparse_Reward-529 那願景是甚麼呢？在這個 task 裡面
DRL_Lecture_7_-_Sparse_Reward-530 它只有兩個 agent 啦，只有下面的一個
DRL_Lecture_7_-_Sparse_Reward-531 最底層的 agent 負責執行，決定說要怎麼走
DRL_Lecture_7_-_Sparse_Reward-532 還有一個上層的 agent，負責提出願景
DRL_Lecture_7_-_Sparse_Reward-533 雖然，實際上你 general 而言可以用很多層
DRL_Lecture_7_-_Sparse_Reward-534 但是，paper 我看那個實驗，主要是這樣子
DRL_Lecture_7_-_Sparse_Reward-535 那今天這個例子是說
DRL_Lecture_7_-_Sparse_Reward-536 粉紅色的這個點，代表的就是願景
DRL_Lecture_7_-_Sparse_Reward-537 上面這個 agent，它告訴藍色的這個 agent 說
DRL_Lecture_7_-_Sparse_Reward-538 你現在的第一個目標是先走到這個地方
DRL_Lecture_7_-_Sparse_Reward-539 藍色的 agent 走到以後，再說你的新的目標是走到這裡
DRL_Lecture_7_-_Sparse_Reward-540 藍色的 agent 再走到以後，新的目標在這裡
DRL_Lecture_7_-_Sparse_Reward-541 接下來又跑到這邊
DRL_Lecture_7_-_Sparse_Reward-542 然後，最後希望藍色的 agent 就可以走到黃色的
DRL_Lecture_7_-_Sparse_Reward-543 這個位置，這邊也是一樣，就是
DRL_Lecture_7_-_Sparse_Reward-544 粉紅色的這個點，代表的是目標
DRL_Lecture_7_-_Sparse_Reward-545 代表的是上層的 agent 所提出來的願景，所以
DRL_Lecture_7_-_Sparse_Reward-546 這個 agent 先擺到這邊
DRL_Lecture_7_-_Sparse_Reward-547 接下來，新的願景又跑到這邊，所以它又擺到這裡
DRL_Lecture_7_-_Sparse_Reward-548 然後，新的願景又跑到上面
DRL_Lecture_7_-_Sparse_Reward-549 然後又擺到上面，最後就走到黃色的位置了
DRL_Lecture_7_-_Sparse_Reward-550 這個就是 hierarchical 的 Reinforcement Learning
DRL_Lecture_8_-_Imitation_Learning-0 那剛才講的是說
DRL_Lecture_8_-_Imitation_Learning-1 在 reward 非常 sparse 的情況下
DRL_Lecture_8_-_Imitation_Learning-2 要怎麼做
DRL_Lecture_8_-_Imitation_Learning-3 我們接下來要講的是 Imitation learning
DRL_Lecture_8_-_Imitation_Learning-4 Imitation learning 就更進一步討論的問題是
DRL_Lecture_8_-_Imitation_Learning-5 假設我們今天連 reward 都沒有
DRL_Lecture_8_-_Imitation_Learning-6 那要怎麼辦才好呢？
DRL_Lecture_8_-_Imitation_Learning-7 這個 Imitation learning 又叫做
DRL_Lecture_8_-_Imitation_Learning-8 learning by demonstration
DRL_Lecture_8_-_Imitation_Learning-9 或者叫做 apprenticeship learning
DRL_Lecture_8_-_Imitation_Learning-10 apprenticeship 這個字是那個學徒的意思
DRL_Lecture_8_-_Imitation_Learning-11 那在這 Imitation learning 裡面
DRL_Lecture_8_-_Imitation_Learning-12 你的 setup 是這樣子
DRL_Lecture_8_-_Imitation_Learning-13 你有一些 expert 的 demonstration
DRL_Lecture_8_-_Imitation_Learning-14 那 machine 也可以跟環境互動
DRL_Lecture_8_-_Imitation_Learning-15 但它沒有辦法從環境裡面得到任何的 reward
DRL_Lecture_8_-_Imitation_Learning-16 他只能夠看著 expert 的 demonstration
DRL_Lecture_8_-_Imitation_Learning-17 來學習什麼是好，什麼是不好
DRL_Lecture_8_-_Imitation_Learning-18 那你說為甚麼有時候，我們沒有辦法從環境得到 reward
DRL_Lecture_8_-_Imitation_Learning-19 其實，多數的情況，我們都沒有辦法
DRL_Lecture_8_-_Imitation_Learning-20 真的從環境裡面得到非常明確的 reward
DRL_Lecture_8_-_Imitation_Learning-21 舉例來說，因為如果今天是棋類遊戲
DRL_Lecture_8_-_Imitation_Learning-22 或者是電玩，你有非常明確的 reward
DRL_Lecture_8_-_Imitation_Learning-23 但是其實多數的任務，都是沒有 reward 的
DRL_Lecture_8_-_Imitation_Learning-24 舉例來說，你說開自駕車
DRL_Lecture_8_-_Imitation_Learning-25 雖然說自駕車，我們都知道撞死人不好
DRL_Lecture_8_-_Imitation_Learning-26 但是，撞死人應該扣多少分數，這個你沒有辦法訂出來
DRL_Lecture_8_-_Imitation_Learning-27 撞死人的分數，跟撞死一個動物的分數顯然是不一樣的
DRL_Lecture_8_-_Imitation_Learning-28 但你也不知道要怎麼訂
DRL_Lecture_8_-_Imitation_Learning-29 這個問題很難，你根本不知道要怎麼訂 reward
DRL_Lecture_8_-_Imitation_Learning-30 或是 chat bot 也是一樣
DRL_Lecture_8_-_Imitation_Learning-31 今天機器跟人聊天，聊得怎麼樣算是好
DRL_Lecture_8_-_Imitation_Learning-32 聊得怎麼樣算是不好
DRL_Lecture_8_-_Imitation_Learning-33 你也無法決定
DRL_Lecture_8_-_Imitation_Learning-34 所以很多 task，你是根本就沒有辦法訂出reward 的
DRL_Lecture_8_-_Imitation_Learning-35 但是，雖然沒有辦法訂出 reward
DRL_Lecture_8_-_Imitation_Learning-36 但是，收集 expert 的 demonstration 是可能可以做到的
DRL_Lecture_8_-_Imitation_Learning-37 舉例來說，在自駕車裡面
DRL_Lecture_8_-_Imitation_Learning-38 雖然，你沒有辦法訂出自駕車的 reward
DRL_Lecture_8_-_Imitation_Learning-39 但收集很多人類開車的紀錄
DRL_Lecture_8_-_Imitation_Learning-40 這件事情是可行的
DRL_Lecture_8_-_Imitation_Learning-41 在 chat bot 裡面，你可能沒有辦法收集到太多
DRL_Lecture_8_-_Imitation_Learning-42 你可能沒有辦法真的定義什麼叫做好的對話
DRL_Lecture_8_-_Imitation_Learning-43 什麼叫做不好的對話
DRL_Lecture_8_-_Imitation_Learning-44 但是，收集很多人的對話當作範例
DRL_Lecture_8_-_Imitation_Learning-45 這一件事情，也是可行的
DRL_Lecture_8_-_Imitation_Learning-46 所以，今天 Imitation learning
DRL_Lecture_8_-_Imitation_Learning-47 其實他的使用性非常高，假設
DRL_Lecture_8_-_Imitation_Learning-48 你今天有一個狀況是，你不知道該怎麼定義 reward
DRL_Lecture_8_-_Imitation_Learning-49 但是你可以收集到  expert 的 demonstration
DRL_Lecture_8_-_Imitation_Learning-50 你可以收集到一些範例的話
DRL_Lecture_8_-_Imitation_Learning-51 你可以收集到一些很厲害的 agent
DRL_Lecture_8_-_Imitation_Learning-52 比如說人跟環境實際上的互動的話
DRL_Lecture_8_-_Imitation_Learning-53 那你就可以考慮 Imitation learning 這個技術
DRL_Lecture_8_-_Imitation_Learning-54 那在 Imitation learning  裡面
DRL_Lecture_8_-_Imitation_Learning-55 我們等一下，就是介紹兩個方法
DRL_Lecture_8_-_Imitation_Learning-56 第一個叫做 Behavior Cloning
DRL_Lecture_8_-_Imitation_Learning-57 第二個叫做 Inverse Reinforcement Learning
DRL_Lecture_8_-_Imitation_Learning-58 或者又叫做 Inverse Optimal Control
DRL_Lecture_8_-_Imitation_Learning-59 我們先來講 Behavior Cloning
DRL_Lecture_8_-_Imitation_Learning-60 其實 Behavior Cloning跟 Supervised learning 是一模一樣的
DRL_Lecture_8_-_Imitation_Learning-61 舉例來說，我們以自駕車為例
DRL_Lecture_8_-_Imitation_Learning-62 今天，你可以收集到人開自駕車的所有資料
DRL_Lecture_8_-_Imitation_Learning-63 比如說，人類的駕駛跟收集人的行車記錄器
DRL_Lecture_8_-_Imitation_Learning-64 看到這樣子的 observation 的時候
DRL_Lecture_8_-_Imitation_Learning-65 人會決定向前，機器就採取跟人一樣的行為
DRL_Lecture_8_-_Imitation_Learning-66 也採取向前，也踩個油門就結束了
DRL_Lecture_8_-_Imitation_Learning-67 這個就叫做 Behavior Cloning
DRL_Lecture_8_-_Imitation_Learning-68 expert 做甚麼，人就做一模一樣的事
DRL_Lecture_8_-_Imitation_Learning-69 不是人做一樣的事，機器就做一模一樣的事
DRL_Lecture_8_-_Imitation_Learning-70 expert 做甚麼，機器就做一模一樣的事
DRL_Lecture_8_-_Imitation_Learning-71 那怎麼讓機器學會跟 expert 一模一樣的行為呢？
DRL_Lecture_8_-_Imitation_Learning-72 就把它當作一個 Supervised learning 的問題
DRL_Lecture_8_-_Imitation_Learning-73 你去收集很多自駕車
DRL_Lecture_8_-_Imitation_Learning-74 你去收集很多行車紀錄器
DRL_Lecture_8_-_Imitation_Learning-75 然後，再收集人在那個情境下會採取什麼樣的行為
DRL_Lecture_8_-_Imitation_Learning-76 你知道說人在state, s1  會採取 action, a1
DRL_Lecture_8_-_Imitation_Learning-77 人在state, s2  會採取 action , a2
DRL_Lecture_8_-_Imitation_Learning-78 人在 state, s3  會採取 action,  a3
DRL_Lecture_8_-_Imitation_Learning-79 接下來，你就 learn 一個 network
DRL_Lecture_8_-_Imitation_Learning-80 這個 network 就是你的 actor
DRL_Lecture_8_-_Imitation_Learning-81 他 input si 的時候
DRL_Lecture_8_-_Imitation_Learning-82 你就希望他的 output 是 ai，就這樣結束了
DRL_Lecture_8_-_Imitation_Learning-83 他就是一個非常單純的 Supervised learning 的 problem
DRL_Lecture_8_-_Imitation_Learning-84 Behavior Cloning 雖然非常簡單
DRL_Lecture_8_-_Imitation_Learning-85 但是，他有甚麼樣的問題呢？
DRL_Lecture_8_-_Imitation_Learning-86 他的問題是，今天如果你只收集 expert 的資料
DRL_Lecture_8_-_Imitation_Learning-87 你可能看過的 observation 會是非常 limited
DRL_Lecture_8_-_Imitation_Learning-88 舉例來說，假設你要 learn 一部自駕車
DRL_Lecture_8_-_Imitation_Learning-89 自駕車就是要過這個彎道
DRL_Lecture_8_-_Imitation_Learning-90 那如果是 expert  的話
DRL_Lecture_8_-_Imitation_Learning-91 你找人來，不管找多少人來
DRL_Lecture_8_-_Imitation_Learning-92 他就是把車，順著這個紅線就開過去了
DRL_Lecture_8_-_Imitation_Learning-93 但是，今天假設你的 agent 很笨
DRL_Lecture_8_-_Imitation_Learning-94 他今天開著開著，不知道怎麼回事
DRL_Lecture_8_-_Imitation_Learning-95 就開到撞牆了
DRL_Lecture_8_-_Imitation_Learning-96 他永遠不知道撞牆這種狀況要怎麼處理
DRL_Lecture_8_-_Imitation_Learning-97 為甚麼？因為 taring data 裡面從來沒有撞過牆呀
DRL_Lecture_8_-_Imitation_Learning-98 所以，他根本就不知道撞牆這一種 case
DRL_Lecture_8_-_Imitation_Learning-99 要怎麼處理
DRL_Lecture_8_-_Imitation_Learning-100 或是打電玩，電玩也是一樣
DRL_Lecture_8_-_Imitation_Learning-101 讓機器，讓人去玩 Mario
DRL_Lecture_8_-_Imitation_Learning-102 那可能 expert 非常強
DRL_Lecture_8_-_Imitation_Learning-103 他從來不會跳不上水管
DRL_Lecture_8_-_Imitation_Learning-104 所以，機器根本不知道跳不上水管時要怎麼處理
DRL_Lecture_8_-_Imitation_Learning-105 人從來不會跳不上水管
DRL_Lecture_8_-_Imitation_Learning-106 但是機器今天如果跳不上水管時，就不知道要怎麼處理
DRL_Lecture_8_-_Imitation_Learning-107 所以，今天光是做 Behavior Cloning 是不夠的
DRL_Lecture_8_-_Imitation_Learning-108 但是，只觀察 expert 的行為是不夠的
DRL_Lecture_8_-_Imitation_Learning-109 需要一個招數，這個招數叫作 Data aggregation
DRL_Lecture_8_-_Imitation_Learning-110 我們會希望收集更多樣性的 data
DRL_Lecture_8_-_Imitation_Learning-111 而不是只有收集 expert 所看到的 observation
DRL_Lecture_8_-_Imitation_Learning-112 我們會希望能夠收集 expert 在各種極端的情況下
DRL_Lecture_8_-_Imitation_Learning-113 他會採取什麼樣的行為
DRL_Lecture_8_-_Imitation_Learning-114 如果以自駕車為例的話，那就是這樣
DRL_Lecture_8_-_Imitation_Learning-115 假設一開始，你的 actor 叫作 π1
DRL_Lecture_8_-_Imitation_Learning-116 然後接下來呢，你讓 π1
DRL_Lecture_8_-_Imitation_Learning-117 真的去開這個車
DRL_Lecture_8_-_Imitation_Learning-118 但是，車上坐了一個 expert
DRL_Lecture_8_-_Imitation_Learning-119 這個 expert 會不斷的告訴，不斷的做紀錄說
DRL_Lecture_8_-_Imitation_Learning-120 如果，今天在這個情境裡面
DRL_Lecture_8_-_Imitation_Learning-121 我會怎麼樣開
DRL_Lecture_8_-_Imitation_Learning-122 所以，今天 π1，machine 自己開自己的
DRL_Lecture_8_-_Imitation_Learning-123 但是 expert 會不斷地表示他的想法
DRL_Lecture_8_-_Imitation_Learning-124 比如說，在這個時候，expert 可能說，那就往前走
DRL_Lecture_8_-_Imitation_Learning-125 這個時候，expert 可能就會說往右轉
DRL_Lecture_8_-_Imitation_Learning-126 但是，π1 是不管 expert 的指令的
DRL_Lecture_8_-_Imitation_Learning-127 所以，他會繼續去撞牆，然後
DRL_Lecture_8_-_Imitation_Learning-128 expert 雖然說要一直往右轉
DRL_Lecture_8_-_Imitation_Learning-129 但是不管他怎麼下指令都是沒有用的
DRL_Lecture_8_-_Imitation_Learning-130 π1 會自己做自己的事情
DRL_Lecture_8_-_Imitation_Learning-131 因為我們要做的紀錄的是說，今天 expert
DRL_Lecture_8_-_Imitation_Learning-132 在 π1 看到這種 observation 的情況下
DRL_Lecture_8_-_Imitation_Learning-133 他會做甚麼樣的反應
DRL_Lecture_8_-_Imitation_Learning-134 那這個方法顯然是有一些問題的
DRL_Lecture_8_-_Imitation_Learning-135 因為每次你開一次自駕車
DRL_Lecture_8_-_Imitation_Learning-136 都會犧牲一個人這樣
DRL_Lecture_8_-_Imitation_Learning-137 那你用這個方法，你犧牲一個 expert 以後
DRL_Lecture_8_-_Imitation_Learning-138 你就會得到說，人類在這樣子的 state 下
DRL_Lecture_8_-_Imitation_Learning-139 在快要撞牆的時候，會採取甚麼樣的反應
DRL_Lecture_8_-_Imitation_Learning-140 再把這個 data 拿去 train 新的 π2
DRL_Lecture_8_-_Imitation_Learning-141 這個 process 就反覆繼續下去
DRL_Lecture_8_-_Imitation_Learning-142 這個方法就叫做 Data aggregation
DRL_Lecture_8_-_Imitation_Learning-143 那 Behavior Cloning 這件事情
DRL_Lecture_8_-_Imitation_Learning-144 會有甚麼的樣的 issue
DRL_Lecture_8_-_Imitation_Learning-145 他的 issue 是說，今天機器會完全 copy expert 的行為
DRL_Lecture_8_-_Imitation_Learning-146 不管今天 expert 的行為，有沒有道理
DRL_Lecture_8_-_Imitation_Learning-147 就算沒有道理，沒有什麼用的
DRL_Lecture_8_-_Imitation_Learning-148 這是 expert 本身的習慣
DRL_Lecture_8_-_Imitation_Learning-149 機器也會硬把它記下來
DRL_Lecture_8_-_Imitation_Learning-150 其實這是 Big Bang Theory 的一段
DRL_Lecture_8_-_Imitation_Learning-151 所以，其實機器也是一樣
DRL_Lecture_8_-_Imitation_Learning-152 機器就是你教他甚麼，他就硬學起來
DRL_Lecture_8_-_Imitation_Learning-153 不管那個東西到底是不是值得的學的
DRL_Lecture_8_-_Imitation_Learning-154 那如果今天機器確實可以記住
DRL_Lecture_8_-_Imitation_Learning-155 所有 expert 的行為
DRL_Lecture_8_-_Imitation_Learning-156 那也許還好，為甚麼呢？
DRL_Lecture_8_-_Imitation_Learning-157 因為如果 expert 這麼做，有些行為是多餘的
DRL_Lecture_8_-_Imitation_Learning-158 但是沒有問題，在機器假設他的行為可以完全仿造 expert 行為
DRL_Lecture_8_-_Imitation_Learning-159 那也就算了，那他是跟 expert  一樣的好
DRL_Lecture_8_-_Imitation_Learning-160 只是做一些多餘的事
DRL_Lecture_8_-_Imitation_Learning-161 但是問題就是，他畢竟是一個 machine
DRL_Lecture_8_-_Imitation_Learning-162 他是一個 network，network 的 capacity 是有限的
DRL_Lecture_8_-_Imitation_Learning-163 我們知道說，今天就算給 network training data
DRL_Lecture_8_-_Imitation_Learning-164 他在 training data 上得到的正確率，往往也不是 100
DRL_Lecture_8_-_Imitation_Learning-165 他有些事情，他是學不起來
DRL_Lecture_8_-_Imitation_Learning-166 這個時候，什麼該學，什麼不該學
DRL_Lecture_8_-_Imitation_Learning-167 就變得很重要
DRL_Lecture_8_-_Imitation_Learning-168 舉例來說，以剛才的例子而言
DRL_Lecture_8_-_Imitation_Learning-169 在學習一個中文的時候
DRL_Lecture_8_-_Imitation_Learning-170 你看到你的老師，他有語音
DRL_Lecture_8_-_Imitation_Learning-171 他也有行為
DRL_Lecture_8_-_Imitation_Learning-172 他也有知識，但是今天其實只有語音部分是重要的
DRL_Lecture_8_-_Imitation_Learning-173 知識的部分是不重要的
DRL_Lecture_8_-_Imitation_Learning-174 也許 machine 他只能夠學一件事
DRL_Lecture_8_-_Imitation_Learning-175 也許他就只學到了語音
DRL_Lecture_8_-_Imitation_Learning-176 那沒有問題，如果他今天只學到了手勢
DRL_Lecture_8_-_Imitation_Learning-177 那這樣子就有問題了
DRL_Lecture_8_-_Imitation_Learning-178 所以，今天讓機器學習什麼東西是需要 copy
DRL_Lecture_8_-_Imitation_Learning-179 什麼東西是不需要copy，這件事情是重要的
DRL_Lecture_8_-_Imitation_Learning-180 而單純的 Behavior Cloning
DRL_Lecture_8_-_Imitation_Learning-181 其實就沒有把這件事情學進來
DRL_Lecture_8_-_Imitation_Learning-182 因為機器唯一做的事情只是複製 expert 所有的行為而已
DRL_Lecture_8_-_Imitation_Learning-183 他並不知道哪些行為是重要
DRL_Lecture_8_-_Imitation_Learning-184 是對接下來有影響的
DRL_Lecture_8_-_Imitation_Learning-185 哪些行為是不重要的
DRL_Lecture_8_-_Imitation_Learning-186 所以，接下來是沒有影響的
DRL_Lecture_8_-_Imitation_Learning-187 舉例來說 ，有一個人
DRL_Lecture_8_-_Imitation_Learning-188 他想要變得跟賈伯斯一樣的厲害
DRL_Lecture_8_-_Imitation_Learning-189 然後，就去讀了賈伯斯的傳記以後
DRL_Lecture_8_-_Imitation_Learning-190 就統計一下賈伯斯的人格特質
DRL_Lecture_8_-_Imitation_Learning-191 舉例來說，賈伯斯的特質有勤奮
DRL_Lecture_8_-_Imitation_Learning-192 有創造力，還有穿得很邋遢
DRL_Lecture_8_-_Imitation_Learning-193 脾氣很暴躁
DRL_Lecture_8_-_Imitation_Learning-194 然後，他就覺得說這些東西太多了
DRL_Lecture_8_-_Imitation_Learning-195 我沒有辦法每一件都學，也許就學其中一件這樣子
DRL_Lecture_8_-_Imitation_Learning-196 比如說，學習脾氣很暴躁，這樣就好
DRL_Lecture_8_-_Imitation_Learning-197 然後就什麼都沒有辦法做成
DRL_Lecture_8_-_Imitation_Learning-198 Behavior Cloning 就是這樣的道理
DRL_Lecture_8_-_Imitation_Learning-199 那 Behavior Coning 還有甚麼樣的問題呢？
DRL_Lecture_8_-_Imitation_Learning-200 在做 Behavior Cloning  的時候
DRL_Lecture_8_-_Imitation_Learning-201 這個你的 training data 跟 testing data
DRL_Lecture_8_-_Imitation_Learning-202 其實是 mismatch 的
DRL_Lecture_8_-_Imitation_Learning-203 我們剛才其實是有講到這個樣子的 issue
DRL_Lecture_8_-_Imitation_Learning-204 那我們可以用這個 Data aggregation 的方法來稍微解決這個問題
DRL_Lecture_8_-_Imitation_Learning-205 那這樣子的問題到底是甚麼樣的意思呢？
DRL_Lecture_8_-_Imitation_Learning-206 這樣的問題是，我們在 training 跟 testing 的時候
DRL_Lecture_8_-_Imitation_Learning-207 我們的 data distribution 其實是不一樣
DRL_Lecture_8_-_Imitation_Learning-208 因為我們知道在 Reinforcement learning 裡面
DRL_Lecture_8_-_Imitation_Learning-209 有一個特色是你的 action 會採取
DRL_Lecture_8_-_Imitation_Learning-210 會影響到接下來所看到的 gain，對不對
DRL_Lecture_8_-_Imitation_Learning-211 我們是先有 state, s1，然後再看到 action a1
DRL_Lecture_8_-_Imitation_Learning-212 action, a1 其實會決定接下來你看到甚麼樣的 state, s2
DRL_Lecture_8_-_Imitation_Learning-213 所以，在 Reinforcement learning 裡面
DRL_Lecture_8_-_Imitation_Learning-214 一個很重要的特徵，就是
DRL_Lecture_8_-_Imitation_Learning-215 你採取了 action 會影響你接下來所看到的 state
DRL_Lecture_8_-_Imitation_Learning-216 那今天如果我們做了 Behavior Cloning 的話
DRL_Lecture_8_-_Imitation_Learning-217 做 Behavior Cloning 的時候
DRL_Lecture_8_-_Imitation_Learning-218 我們只能夠觀察到 expert  的一堆 state 跟 action 的 pair
DRL_Lecture_8_-_Imitation_Learning-219 我們可以觀察到 expert  的一堆 state 跟 action 的 pair
DRL_Lecture_8_-_Imitation_Learning-220 然後，我們今天希望說我們可以 learn 一個 π
DRL_Lecture_8_-_Imitation_Learning-221 我們 learn 出來的，假設叫做 π* 好了
DRL_Lecture_8_-_Imitation_Learning-222 我們希望這一個 π* 跟 π\head 越接近越好
DRL_Lecture_8_-_Imitation_Learning-223 如果 π* 確實可以跟 π\head 一模一樣的話
DRL_Lecture_8_-_Imitation_Learning-224 那這個時侯，你 training 的時候看到的 state
DRL_Lecture_8_-_Imitation_Learning-225 跟 testing 的時候所看到的 state 會是一樣
DRL_Lecture_8_-_Imitation_Learning-226 因為，雖然 action 會影響我們看到的 state
DRL_Lecture_8_-_Imitation_Learning-227 假設兩個 policy 都一模一樣，在同一個 state 都會採取同樣的 action
DRL_Lecture_8_-_Imitation_Learning-228 那你接下來所看到的 state 都會是一樣
DRL_Lecture_8_-_Imitation_Learning-229 但是問題就是
DRL_Lecture_8_-_Imitation_Learning-230 你很難 learn 到讓你的 learn 出來的 π
DRL_Lecture_8_-_Imitation_Learning-231 跟 expert 的 π 一模一樣
DRL_Lecture_8_-_Imitation_Learning-232 expert 可是一個人，network 要跟人一模一樣，感覺很難吧
DRL_Lecture_8_-_Imitation_Learning-233 今天你的 π* 如果跟 π\head 有一點誤差
DRL_Lecture_8_-_Imitation_Learning-234 這個誤差也許在一般 Supervised  learning problem 裡面
DRL_Lecture_8_-_Imitation_Learning-235 每一個 example 都是 independent 的，也許還好
DRL_Lecture_8_-_Imitation_Learning-236 但是，今天假設 Reinforcement learning 的 problem
DRL_Lecture_8_-_Imitation_Learning-237 你可能在某個地方，就是失之毫釐，差之千里
DRL_Lecture_8_-_Imitation_Learning-238 你可能在某個地方，也許你的 machine 沒有辦法
DRL_Lecture_8_-_Imitation_Learning-239 完全複製 expert 的行為
DRL_Lecture_8_-_Imitation_Learning-240 它只複製了一點點，差了一點點
DRL_Lecture_8_-_Imitation_Learning-241 也許最後得到的結果，就會差很多這樣
DRL_Lecture_8_-_Imitation_Learning-242 所以，今天這個 Behavior Cloning 的方法
DRL_Lecture_8_-_Imitation_Learning-243 並不能夠完全解決 Imatation learning 這件事情
DRL_Lecture_8_-_Imitation_Learning-244 所以接下來，就有另外一個比較好的做法
DRL_Lecture_8_-_Imitation_Learning-245 叫做 Inverse Reinforcement Learning
DRL_Lecture_8_-_Imitation_Learning-246 這個方法一聽名字就覺得非常地長
DRL_Lecture_8_-_Imitation_Learning-247 有 Reinforcement Learning
DRL_Lecture_8_-_Imitation_Learning-248 現在有 Inverse Reinforce Learning
DRL_Lecture_8_-_Imitation_Learning-249 為甚麼叫 Inverse Reinforce Learning
DRL_Lecture_8_-_Imitation_Learning-250 因為原來的 Reinforce Learning 裡面
DRL_Lecture_8_-_Imitation_Learning-251 也就是有一個環境
DRL_Lecture_8_-_Imitation_Learning-252 跟你互動的環境，然後你有一個 reward function
DRL_Lecture_8_-_Imitation_Learning-253 然後根據環境跟 reward function
DRL_Lecture_8_-_Imitation_Learning-254 透過 Reinforce Learning 這個技術
DRL_Lecture_8_-_Imitation_Learning-255 你會找到一個 actor，你會 learn 出一個 optimal actor
DRL_Lecture_8_-_Imitation_Learning-256 但是 Inverse Reinforce Learning 剛好是相反的
DRL_Lecture_8_-_Imitation_Learning-257 你今天沒有 reward function
DRL_Lecture_8_-_Imitation_Learning-258 你只有一堆 expert 的 demonstration
DRL_Lecture_8_-_Imitation_Learning-259 但是你還是有環境的，IRL 的做法是說
DRL_Lecture_8_-_Imitation_Learning-260 假設我們現在有一堆 expert 的 demonstration
DRL_Lecture_8_-_Imitation_Learning-261 他這邊我們用這個 τ\head 來代表 expert  的demonstration
DRL_Lecture_8_-_Imitation_Learning-262 如果今天是在玩電玩的話
DRL_Lecture_8_-_Imitation_Learning-263 每一個 τ 就是一個很會玩電玩的人
DRL_Lecture_8_-_Imitation_Learning-264 他玩一場遊戲的紀錄，如果是自駕車的話
DRL_Lecture_8_-_Imitation_Learning-265 就是人開自駕車的紀錄
DRL_Lecture_8_-_Imitation_Learning-266 如果是用人開車的紀錄，這一邊就是 expert 的 demonstration
DRL_Lecture_8_-_Imitation_Learning-267 每一個 τ 是 一個 trajectory
DRL_Lecture_8_-_Imitation_Learning-268 把所有 trajectory expert demonstration 收集起來
DRL_Lecture_8_-_Imitation_Learning-269 然後，使用 Inverse Reinforcement Learning 這個技術
DRL_Lecture_8_-_Imitation_Learning-270 使用 Inverse Reinforcement Learning 技術的時候機器是可以跟環境互動的
DRL_Lecture_8_-_Imitation_Learning-271 但是，他得不到 reward
DRL_Lecture_8_-_Imitation_Learning-272 他的 reward 必須要從 expert 那邊推論出來
DRL_Lecture_8_-_Imitation_Learning-273 現在有了環境，有了 expert demonstration 以後
DRL_Lecture_8_-_Imitation_Learning-274 去反推出 reward function 長甚麼樣子
DRL_Lecture_8_-_Imitation_Learning-275 之前 Reinforcement learning 是由 reward function
DRL_Lecture_8_-_Imitation_Learning-276 反推出什麼樣的 action，actor 是最好的
DRL_Lecture_8_-_Imitation_Learning-277 Inverse Reinforcement Learning 是反過來
DRL_Lecture_8_-_Imitation_Learning-278 我們有 expert 的 demonstration
DRL_Lecture_8_-_Imitation_Learning-279 我們相信他是不錯的
DRL_Lecture_8_-_Imitation_Learning-280 然後去反推
DRL_Lecture_8_-_Imitation_Learning-281 expert 既然做這樣的行為
DRL_Lecture_8_-_Imitation_Learning-282 那實際的 reward function 到底長甚麼樣子
DRL_Lecture_8_-_Imitation_Learning-283 我就反推說，expert 是因為甚麼樣的 reward function
DRL_Lecture_8_-_Imitation_Learning-284 才會採取這些行為
DRL_Lecture_8_-_Imitation_Learning-285 你今天有了reward function以後
DRL_Lecture_8_-_Imitation_Learning-286 接下來，你就可以套用一般的Reinforcement learning 的方法
DRL_Lecture_8_-_Imitation_Learning-287 去找出 optimal actor
DRL_Lecture_8_-_Imitation_Learning-288 所以，Inverse Reinforcement Learning
DRL_Lecture_8_-_Imitation_Learning-289 裡面是先找出 reward function
DRL_Lecture_8_-_Imitation_Learning-290 找出 reward function 以後
DRL_Lecture_8_-_Imitation_Learning-291 再去實際上用 Reinforcement Learning 找出 optimal actor
DRL_Lecture_8_-_Imitation_Learning-292 有人可能就會問說
DRL_Lecture_8_-_Imitation_Learning-293 把 Reinforcement Learning，把這個 reward function learn 出來
DRL_Lecture_8_-_Imitation_Learning-294 到底相較於原來的 Reinforcement Learning有甚麼樣好處
DRL_Lecture_8_-_Imitation_Learning-295 一個可能的好處是
DRL_Lecture_8_-_Imitation_Learning-296 也許 reward function 是比較簡單的
DRL_Lecture_8_-_Imitation_Learning-297 也許，雖然這個 actor
DRL_Lecture_8_-_Imitation_Learning-298 這個 expert 他的行為非常複雜
DRL_Lecture_8_-_Imitation_Learning-299 也許簡單的 reward function
DRL_Lecture_8_-_Imitation_Learning-300 就可以導致非常複雜的行為
DRL_Lecture_8_-_Imitation_Learning-301 一個例子就是
DRL_Lecture_8_-_Imitation_Learning-302 也許人類本身的 reward function 就只有活著這樣
DRL_Lecture_8_-_Imitation_Learning-303 每多活一秒，你就加一分
DRL_Lecture_8_-_Imitation_Learning-304 但是，人類有非常複雜的行為
DRL_Lecture_8_-_Imitation_Learning-305 但是這些複雜的行為，都只是圍繞著
DRL_Lecture_8_-_Imitation_Learning-306 要從這個 reward function 裡面得到分數而已
DRL_Lecture_8_-_Imitation_Learning-307 有時候很簡單的 reward function
DRL_Lecture_8_-_Imitation_Learning-308 也許可以推導出非常複雜的行為
DRL_Lecture_8_-_Imitation_Learning-309 那 Inverse Reinforcement Learning 實際上是怎麼做的呢？
DRL_Lecture_8_-_Imitation_Learning-310 首先，我們有一個 expert ，我們叫做 π\head
DRL_Lecture_8_-_Imitation_Learning-311 這個 expert 去跟環境互動
DRL_Lecture_8_-_Imitation_Learning-312 給我們很多 τ1\head 到 τn\head
DRL_Lecture_8_-_Imitation_Learning-313 如果是玩遊戲的話
DRL_Lecture_8_-_Imitation_Learning-314 就讓某一個電玩高手，去玩 n 場遊戲
DRL_Lecture_8_-_Imitation_Learning-315 把 n 場遊戲的 state 跟 action 的 sequence
DRL_Lecture_8_-_Imitation_Learning-316 通通都記錄下來
DRL_Lecture_8_-_Imitation_Learning-317 接下來，你有一個 actor
DRL_Lecture_8_-_Imitation_Learning-318 一開始 actor 很爛，他叫做 π
DRL_Lecture_8_-_Imitation_Learning-319 這個 actor 他也去跟環境互動
DRL_Lecture_8_-_Imitation_Learning-320 他也去玩了n 場遊戲
DRL_Lecture_8_-_Imitation_Learning-321 他也有 n 場遊戲的紀錄
DRL_Lecture_8_-_Imitation_Learning-322 接下來，我們要反推出 reward function
DRL_Lecture_8_-_Imitation_Learning-323 怎麼推出 reward function 呢？
DRL_Lecture_8_-_Imitation_Learning-324 這一邊的原則就是
DRL_Lecture_8_-_Imitation_Learning-325 expert 永遠是最棒的
DRL_Lecture_8_-_Imitation_Learning-326 今天是先射箭，再畫靶的概念
DRL_Lecture_8_-_Imitation_Learning-327 expert 他去玩一玩遊戲，得到這一些遊戲的紀錄
DRL_Lecture_8_-_Imitation_Learning-328 你的 actor 也去玩一玩遊戲，得到這些遊戲的紀錄
DRL_Lecture_8_-_Imitation_Learning-329 接下來，你要定一個 reward function
DRL_Lecture_8_-_Imitation_Learning-330 這個 reward function 的原則就是
DRL_Lecture_8_-_Imitation_Learning-331 expert 得到的分數，要比 actor 得到的分數高
DRL_Lecture_8_-_Imitation_Learning-332 先射箭，再畫靶
DRL_Lecture_8_-_Imitation_Learning-333 所以，我們今天就 learn 出一個 reward  function
DRL_Lecture_8_-_Imitation_Learning-334 你要用甚麼樣的方法都可以
DRL_Lecture_8_-_Imitation_Learning-335 你就找出一個 reward function
DRL_Lecture_8_-_Imitation_Learning-336 這個 reward function 會使 expert 所得到的 reward
DRL_Lecture_8_-_Imitation_Learning-337 大過於 actor 所得到的 reward
DRL_Lecture_8_-_Imitation_Learning-338 你有了新的 reward function 以後
DRL_Lecture_8_-_Imitation_Learning-339 你就可以去 learn 一個 actor
DRL_Lecture_8_-_Imitation_Learning-340 你有 reward function 就可以套用一般
DRL_Lecture_8_-_Imitation_Learning-341 Reinforcement Learning  的方法
DRL_Lecture_8_-_Imitation_Learning-342 去 learn 一個 actor
DRL_Lecture_8_-_Imitation_Learning-343 這個 actor 會對這一個 reward function
DRL_Lecture_8_-_Imitation_Learning-344 去 maximize 他的 reward
DRL_Lecture_8_-_Imitation_Learning-345 他也會採取一大堆的 action
DRL_Lecture_8_-_Imitation_Learning-346 但是，今天這個 actor
DRL_Lecture_8_-_Imitation_Learning-347 他雖然可以 maximize 這個 reward function
DRL_Lecture_8_-_Imitation_Learning-348 採取一大堆的行為，得到一大堆遊戲的紀錄
DRL_Lecture_8_-_Imitation_Learning-349 但接下來，我們就改 reward function
DRL_Lecture_8_-_Imitation_Learning-350 先射箭，再畫靶的概念
DRL_Lecture_8_-_Imitation_Learning-351 這個 actor 就會很生氣
DRL_Lecture_8_-_Imitation_Learning-352 它已經可以在這個 reward function 得到高分
DRL_Lecture_8_-_Imitation_Learning-353 但是他得到高分以後
DRL_Lecture_8_-_Imitation_Learning-354 我們就改 reward function
DRL_Lecture_8_-_Imitation_Learning-355 仍然讓 expert 比我們的 actor，可以得到更高的分數
DRL_Lecture_8_-_Imitation_Learning-356 這個就是 Inverse Reinforcement learning
DRL_Lecture_8_-_Imitation_Learning-357 你有新的 reward function 以後
DRL_Lecture_8_-_Imitation_Learning-358 根據這個新的 reward function
DRL_Lecture_8_-_Imitation_Learning-359 你就可以得到新的 actor
DRL_Lecture_8_-_Imitation_Learning-360 新的 actor 再去跟環境做一下互動
DRL_Lecture_8_-_Imitation_Learning-361 他跟環境做互動以後，你又會重新定義你的 reward function
DRL_Lecture_8_-_Imitation_Learning-362 讓 expert 得到 reward 大過，你請問
DRL_Lecture_8_-_Imitation_Learning-363 這邊其實就沒有講演算法的細節
DRL_Lecture_8_-_Imitation_Learning-364 那你至於說要，怎麼讓他大於他
DRL_Lecture_8_-_Imitation_Learning-365 其實你在 learning 的時候，你可以很簡單地做一件事
DRL_Lecture_8_-_Imitation_Learning-366 就是，我們的 reward function 也許就是 neural network
DRL_Lecture_8_-_Imitation_Learning-367 這個 neural network 它就是吃一個 τ
DRL_Lecture_8_-_Imitation_Learning-368 然後，output 就是這個 τ 應該要給他多少的分數
DRL_Lecture_8_-_Imitation_Learning-369 或者是說，你假設覺得 input 整個 τ 太難了
DRL_Lecture_8_-_Imitation_Learning-370 因為 τ 是 s  跟 a 一個很強的 sequence
DRL_Lecture_8_-_Imitation_Learning-371 也許就說 ，他就是 input s 跟 a
DRL_Lecture_8_-_Imitation_Learning-372 他是一個 s 跟 a 的 pair，然後 output 一個 real number
DRL_Lecture_8_-_Imitation_Learning-373 把整個 sequence，整個 τ 會得到的 real number 都加起來
DRL_Lecture_8_-_Imitation_Learning-374 就得到 total R，在 training 的時候，你就說
DRL_Lecture_8_-_Imitation_Learning-375 今天這組數字，我們希望他 output 的 R 越大越好
DRL_Lecture_8_-_Imitation_Learning-376 今天這個 ，我們就希望他 R 的值，越小越好
DRL_Lecture_8_-_Imitation_Learning-377 這邊對 Inverse Reinforcement Learning 的 framework
DRL_Lecture_8_-_Imitation_Learning-378 大家有沒有甚麼問題要問的呢？
DRL_Lecture_8_-_Imitation_Learning-379 你說，你說
DRL_Lecture_8_-_Imitation_Learning-380 我們假設說 expert 就是最好的
DRL_Lecture_8_-_Imitation_Learning-381 所以，什麼叫做一個最好的 reward function
DRL_Lecture_8_-_Imitation_Learning-382 最後你 learn 出來的 reward function ，應該就是
DRL_Lecture_8_-_Imitation_Learning-383 這個 expert 跟這個 expert ，它們在這個 reward function
DRL_Lecture_8_-_Imitation_Learning-384 都會得到一樣高的分數，這樣
DRL_Lecture_8_-_Imitation_Learning-385 最終你的 reward function 沒有辦法
DRL_Lecture_8_-_Imitation_Learning-386 分辨出誰應該會得到比較高的分數
DRL_Lecture_8_-_Imitation_Learning-387 這樣大家還有問題要問的嗎？請說
DRL_Lecture_8_-_Imitation_Learning-388 通常在這個 train 的時候，你當然是會 iterative 的去做
DRL_Lecture_8_-_Imitation_Learning-389 那今天的狀況是這樣
DRL_Lecture_8_-_Imitation_Learning-390 最早的 Inverse Reinforcement Learning
DRL_Lecture_8_-_Imitation_Learning-391 最早的 Inverse Reinforcement Learning
DRL_Lecture_8_-_Imitation_Learning-392 他對 R 的 function 有些限制
DRL_Lecture_8_-_Imitation_Learning-393 他是假設他是 linear
DRL_Lecture_8_-_Imitation_Learning-394 如果在 linear 的話，你可以 prove 說這個 algorithm 會 converge
DRL_Lecture_8_-_Imitation_Learning-395 但是如果不是 linear 的
DRL_Lecture_8_-_Imitation_Learning-396 你就沒有辦法 prove 說他會 converge
DRL_Lecture_8_-_Imitation_Learning-397 大家還有問題要問的嗎？
DRL_Lecture_8_-_Imitation_Learning-398 你有沒有覺得這個東西，其實看起來還頗熟悉呢？
DRL_Lecture_8_-_Imitation_Learning-399 其實你只要把他換個名字說
DRL_Lecture_8_-_Imitation_Learning-400 actor 就是 generator
DRL_Lecture_8_-_Imitation_Learning-401 然後說 reward function 就是 discriminator
DRL_Lecture_8_-_Imitation_Learning-402 其實他就是 GAN，他就是 GAN
DRL_Lecture_8_-_Imitation_Learning-403 所以你說，他會不會收斂這個問題
DRL_Lecture_8_-_Imitation_Learning-404 就等於是問說 GAN 會不會收斂
DRL_Lecture_8_-_Imitation_Learning-405 那你已經有做過作業三，你應該知道說也是很麻煩
DRL_Lecture_8_-_Imitation_Learning-406 不見得會收斂
DRL_Lecture_8_-_Imitation_Learning-407 但是，除非你對 R 下一個非常嚴格的限制
DRL_Lecture_8_-_Imitation_Learning-408 如果你的 R 是一個 general 的 network 的話
DRL_Lecture_8_-_Imitation_Learning-409 你就會有很大的麻煩就是了
DRL_Lecture_8_-_Imitation_Learning-410 那怎麼說他像是一個 GAN，我們來跟 GAN 比較一下
DRL_Lecture_8_-_Imitation_Learning-411 GAN 裡面，你有一堆很好的圖
DRL_Lecture_8_-_Imitation_Learning-412 然後你有一個 generator
DRL_Lecture_8_-_Imitation_Learning-413 一開始他根本不知道要產生甚麼樣的圖，他就亂畫
DRL_Lecture_8_-_Imitation_Learning-414 然後你有一個 discriminator，discriminator 的工作就是
DRL_Lecture_8_-_Imitation_Learning-415 expert  畫的圖就是高分
DRL_Lecture_8_-_Imitation_Learning-416 generator 畫的圖就是低分
DRL_Lecture_8_-_Imitation_Learning-417 你有 discriminator 以後
DRL_Lecture_8_-_Imitation_Learning-418 generator 會想辦法去騙過 discriminator
DRL_Lecture_8_-_Imitation_Learning-419 generator 會希望他產生的圖，discriminator 也會給他高分
DRL_Lecture_8_-_Imitation_Learning-420 這整個 process 跟 Inverse Reinforcement Learning 是一模一樣的
DRL_Lecture_8_-_Imitation_Learning-421 我們只是把同樣的東西換個名子而已
DRL_Lecture_8_-_Imitation_Learning-422 今天這些人畫的圖
DRL_Lecture_8_-_Imitation_Learning-423 在這邊就是 expert 的 demonstration
DRL_Lecture_8_-_Imitation_Learning-424 你的 generator 就是 actor
DRL_Lecture_8_-_Imitation_Learning-425 今天 generator 畫很多圖
DRL_Lecture_8_-_Imitation_Learning-426 但是 actor 會去跟環境互動，產生很多 trajectory
DRL_Lecture_8_-_Imitation_Learning-427 這些 trajectory 跟環境互動的記錄
DRL_Lecture_8_-_Imitation_Learning-428 遊戲的紀錄其實就是等於
DRL_Lecture_8_-_Imitation_Learning-429 GAN 裡面的這些遊戲畫面
DRL_Lecture_8_-_Imitation_Learning-430 不是遊戲畫面，就等於是 GAN 裡面的這些圖
DRL_Lecture_8_-_Imitation_Learning-431 然後，你 learn 一個 reward function
DRL_Lecture_8_-_Imitation_Learning-432 這個 reward function 其實就是 discriminator
DRL_Lecture_8_-_Imitation_Learning-433 這個 rewards function 要給 expert 的 demonstration 高分
DRL_Lecture_8_-_Imitation_Learning-434 給  actor 互動的結果低分
DRL_Lecture_8_-_Imitation_Learning-435 然後接下來，actor 會想辦法
DRL_Lecture_8_-_Imitation_Learning-436 從這個已經 learn 出來的 reward function 裡面得到高分
DRL_Lecture_8_-_Imitation_Learning-437 然後接下來 iterative 的去循環
DRL_Lecture_8_-_Imitation_Learning-438 跟 GAN 其實是一模一樣的
DRL_Lecture_8_-_Imitation_Learning-439 我們只是換個說法來講同樣的事情而已
DRL_Lecture_8_-_Imitation_Learning-440 那這個 IRL 其實有很多的 application
DRL_Lecture_8_-_Imitation_Learning-441 舉例來說，當然可以用開來自駕車
DRL_Lecture_8_-_Imitation_Learning-442 然後，有人用這個技術來學開自駕車的不同風格
DRL_Lecture_8_-_Imitation_Learning-443 每個人在開車的時候，其實你會有不同風格
DRL_Lecture_8_-_Imitation_Learning-444 舉例來說，能不能夠壓到線
DRL_Lecture_8_-_Imitation_Learning-445 能不能夠倒退
DRL_Lecture_8_-_Imitation_Learning-446 要不要遵守交通規則等等
DRL_Lecture_8_-_Imitation_Learning-447 每個人的風格是不同的
DRL_Lecture_8_-_Imitation_Learning-448 然後用 Inverse Reinforcement Learning
DRL_Lecture_8_-_Imitation_Learning-449 又可以讓自駕車學會各種不同的開車風格
DRL_Lecture_8_-_Imitation_Learning-450 這個是文獻上真實的例子，在這個例子裡面
DRL_Lecture_8_-_Imitation_Learning-451 Inverse Reinforcement Learning 有一個有趣的地方
DRL_Lecture_8_-_Imitation_Learning-452 通常你不需要太多的 training data
DRL_Lecture_8_-_Imitation_Learning-453 因為 training data 往往都是個位數
DRL_Lecture_8_-_Imitation_Learning-454 因為 Inverse Reinforcement Learning 他只是一種 demonstration
DRL_Lecture_8_-_Imitation_Learning-455 他只是一種範例
DRL_Lecture_8_-_Imitation_Learning-456 今天機器他仍然實際上可以去跟環境互動，非常的多次
DRL_Lecture_8_-_Imitation_Learning-457 所以在Inverse Reinforcement Learning 的文獻，往往會看到說
DRL_Lecture_8_-_Imitation_Learning-458 只用幾筆 data 就訓練出一些有趣的結果
DRL_Lecture_8_-_Imitation_Learning-459 比如說，在這個例子裡面
DRL_Lecture_8_-_Imitation_Learning-460 在這個例子裡面，是要讓機器學會在這個
DRL_Lecture_8_-_Imitation_Learning-461 讓自駕車學會在停車場裡面停車
DRL_Lecture_8_-_Imitation_Learning-462 在停車場裡面停車
DRL_Lecture_8_-_Imitation_Learning-463 這邊的 demonstration 是這樣
DRL_Lecture_8_-_Imitation_Learning-464 這個藍色的圈圈，藍色的方形是一部車
DRL_Lecture_8_-_Imitation_Learning-465 不，藍色是終點，他的車是從這邊開始開的
DRL_Lecture_8_-_Imitation_Learning-466 開開開，開開開到這邊停車，然後從這邊開
DRL_Lecture_8_-_Imitation_Learning-467 從這邊開，開到這邊停車
DRL_Lecture_8_-_Imitation_Learning-468 從這邊開，開到這邊停車這樣
DRL_Lecture_8_-_Imitation_Learning-469 然後就是給機器只看一個 row
DRL_Lecture_8_-_Imitation_Learning-470 的四個 demonstration
DRL_Lecture_8_-_Imitation_Learning-471 然後讓他去學怎麼樣開車
DRL_Lecture_8_-_Imitation_Learning-472 怎麼樣開車
DRL_Lecture_8_-_Imitation_Learning-473 最後他就可以學出
DRL_Lecture_8_-_Imitation_Learning-474 在這個位置，如果他要停車的話，他會這樣開
DRL_Lecture_8_-_Imitation_Learning-475 今天給機器看不同的 demonstration
DRL_Lecture_8_-_Imitation_Learning-476 最後他學出來開車的風格，就會不太一樣，舉例來說
DRL_Lecture_8_-_Imitation_Learning-477 這個是不守規的矩開車方式
DRL_Lecture_8_-_Imitation_Learning-478 因為他會開到道路之外
DRL_Lecture_8_-_Imitation_Learning-479 這邊，他會穿過其他的車，然後從這邊開進去
DRL_Lecture_8_-_Imitation_Learning-480 所以機器就會學到說，不一定要走在道路上
DRL_Lecture_8_-_Imitation_Learning-481 他可以走非道路的地方
DRL_Lecture_8_-_Imitation_Learning-482 或是這個例子，機器是可以倒退的
DRL_Lecture_8_-_Imitation_Learning-483 他可以倒退嚕一下，他也會學會說，他可以倒退
DRL_Lecture_8_-_Imitation_Learning-484 那這種技術，也可以拿來訓練機器人
DRL_Lecture_8_-_Imitation_Learning-485 你可以讓機器人，做一些你想要他做的動作
DRL_Lecture_8_-_Imitation_Learning-486 過去如果你要訓練機器人，做你想要他做的動作
DRL_Lecture_8_-_Imitation_Learning-487 其實是比較麻煩的
DRL_Lecture_8_-_Imitation_Learning-488 怎麼麻煩，這邊有一個例子
DRL_Lecture_8_-_Imitation_Learning-489 這個沒有字幕阿
DRL_Lecture_8_-_Imitation_Learning-490 我開大聲一點好了，希望你聽得懂
DRL_Lecture_8_-_Imitation_Learning-491 所以，這個想要告訴我們的事情是說
DRL_Lecture_8_-_Imitation_Learning-492 過去如果你要操控機器的手臂
DRL_Lecture_8_-_Imitation_Learning-493 你要花很多力氣去寫那 program 才讓機器做一件很簡單的事
DRL_Lecture_8_-_Imitation_Learning-494 那今天假設你有 Imitation Learning 的技術
DRL_Lecture_8_-_Imitation_Learning-495 那也許你可以做的事情是
DRL_Lecture_8_-_Imitation_Learning-496 讓人做一下示範
DRL_Lecture_8_-_Imitation_Learning-497 然後機器就跟著人的示範來進行學習
DRL_Lecture_8_-_Imitation_Learning-498 這個是學會擺盤子
DRL_Lecture_8_-_Imitation_Learning-499 拉著機器人的手去擺盤子，機器自己動
DRL_Lecture_8_-_Imitation_Learning-500 這學會倒水
DRL_Lecture_8_-_Imitation_Learning-501 他只教他 20 次，杯子每次放的位置不太一樣
DRL_Lecture_8_-_Imitation_Learning-502 就是這樣子，所以用這種方法來教機械手臂
DRL_Lecture_8_-_Imitation_Learning-503 其實還有很多相關的研究，舉例來說
DRL_Lecture_8_-_Imitation_Learning-504 你在教機械手臂的時候，要注意就是
DRL_Lecture_8_-_Imitation_Learning-505 也許機器看到的視野，跟人看到的視野
DRL_Lecture_8_-_Imitation_Learning-506 其實是不太一樣的
DRL_Lecture_8_-_Imitation_Learning-507 在剛才那個例子裡面
DRL_Lecture_8_-_Imitation_Learning-508 我們人跟機器的動作是一樣的
DRL_Lecture_8_-_Imitation_Learning-509 但是在未來的世界裡面
DRL_Lecture_8_-_Imitation_Learning-510 也許機器是看著人的行為學的
DRL_Lecture_8_-_Imitation_Learning-511 剛才是人拉著，假設你要讓機器學會打高爾夫球
DRL_Lecture_8_-_Imitation_Learning-512 在剛才的例子裡面就是
DRL_Lecture_8_-_Imitation_Learning-513 人拉著機器人手臂去打高爾夫球
DRL_Lecture_8_-_Imitation_Learning-514 但是在未來有沒有可能
DRL_Lecture_8_-_Imitation_Learning-515 機器就是看著人打高爾夫球
DRL_Lecture_8_-_Imitation_Learning-516 他自己就學會打高爾夫球了呢？
DRL_Lecture_8_-_Imitation_Learning-517 但這個時候，要注意的事情是
DRL_Lecture_8_-_Imitation_Learning-518 機器的視野
DRL_Lecture_8_-_Imitation_Learning-519 跟他真正去採取這個行為的時候的視野，是不一樣的
DRL_Lecture_8_-_Imitation_Learning-520 機器必須了解到
DRL_Lecture_8_-_Imitation_Learning-521 當他是作為第三人稱的時候
DRL_Lecture_8_-_Imitation_Learning-522 當他是第三人的視角的時候
DRL_Lecture_8_-_Imitation_Learning-523 看到另外一個人在打高爾夫球
DRL_Lecture_8_-_Imitation_Learning-524 跟他實際上自己去打高爾夫球的時候
DRL_Lecture_8_-_Imitation_Learning-525 看到的視野顯然是不一樣的
DRL_Lecture_8_-_Imitation_Learning-526 但他怎麼把他是第三人的時候，所觀察到的經驗
DRL_Lecture_8_-_Imitation_Learning-527 把它 generalize 到他是第一人稱視角的時候
DRL_Lecture_8_-_Imitation_Learning-528 第一人稱視角的時候，所採取的行為
DRL_Lecture_8_-_Imitation_Learning-529 這就需要用到 Third Person Imitation Learning 的技術
DRL_Lecture_8_-_Imitation_Learning-530 那這個怎麼做呢？
DRL_Lecture_8_-_Imitation_Learning-531 細節其實我們就不細講，他的技術
DRL_Lecture_8_-_Imitation_Learning-532 其實也是不只是用到 Imitation Learning
DRL_Lecture_8_-_Imitation_Learning-533 他用到了 Domain-Adversarial Training
DRL_Lecture_8_-_Imitation_Learning-534 我們在講 Domain-Adversarial Training 的時候，我們有講說
DRL_Lecture_8_-_Imitation_Learning-535 這也是一個 GAN 的技術
DRL_Lecture_8_-_Imitation_Learning-536 那我們希望今天有一個 extractor
DRL_Lecture_8_-_Imitation_Learning-537 有兩個不同 domain 的 image
DRL_Lecture_8_-_Imitation_Learning-538 通過這個 extractor 以後
DRL_Lecture_8_-_Imitation_Learning-539 沒有辦法分辨出他來自哪一個 domain
DRL_Lecture_8_-_Imitation_Learning-540 其實第一人稱視角和第三人稱視角
DRL_Lecture_8_-_Imitation_Learning-541 Imitation Learning 用的技術其實也是一樣的
DRL_Lecture_8_-_Imitation_Learning-542 希望 learn 一個 Feature Extractor
DRL_Lecture_8_-_Imitation_Learning-543 當機器在第三人稱的時候
DRL_Lecture_8_-_Imitation_Learning-544 跟他在第一人稱的時候
DRL_Lecture_8_-_Imitation_Learning-545 看到的視野其實是一樣的
DRL_Lecture_8_-_Imitation_Learning-546 就是把最重要的東西抽出來就好了
DRL_Lecture_8_-_Imitation_Learning-547 其實我們在講 Sequence GAN 的時候
DRL_Lecture_8_-_Imitation_Learning-548 我們有講過 Sentence Generation 跟 Chat-bot
DRL_Lecture_8_-_Imitation_Learning-549 那其實 Sentence Generation 或 Chat-bot 這件事情
DRL_Lecture_8_-_Imitation_Learning-550 也可以想成是 Imitation Learning
DRL_Lecture_8_-_Imitation_Learning-551 機器在 imitate 人寫的句子
DRL_Lecture_8_-_Imitation_Learning-552 你可以把寫句子這件事情
DRL_Lecture_8_-_Imitation_Learning-553 你在寫句子的時候，你寫下去的每一個 word
DRL_Lecture_8_-_Imitation_Learning-554 你都想成是一個 action
DRL_Lecture_8_-_Imitation_Learning-555 所有的 word 合起來就是一個 episode
DRL_Lecture_8_-_Imitation_Learning-556 舉例來說， sentence generation 裡面
DRL_Lecture_8_-_Imitation_Learning-557 你會給機器看很多人類寫的文字
DRL_Lecture_8_-_Imitation_Learning-558 那這個人類寫的文字，你要讓機器學會寫詩
DRL_Lecture_8_-_Imitation_Learning-559 那你就要給他看唐詩 300 首
DRL_Lecture_8_-_Imitation_Learning-560 這個人類寫的文字
DRL_Lecture_8_-_Imitation_Learning-561 其實就是這個 expert 的 demonstration
DRL_Lecture_8_-_Imitation_Learning-562 每一個詞彙，其實就是一個 action
DRL_Lecture_8_-_Imitation_Learning-563 今天，你讓機器做 Sentence Generation 的時候
DRL_Lecture_8_-_Imitation_Learning-564 其實就是在 imitate expert 的 trajectory
DRL_Lecture_8_-_Imitation_Learning-565 或是如果 Chat-bot 也是一樣
DRL_Lecture_8_-_Imitation_Learning-566 在 Chat-bot 裡面你會收集到很多人互動對話的紀錄
DRL_Lecture_8_-_Imitation_Learning-567 那一些就是 expert 的 demonstration
DRL_Lecture_8_-_Imitation_Learning-568 如果我們今天單純用 Maximum likelihood 這個技術
DRL_Lecture_8_-_Imitation_Learning-569 來 maximize 會得到 likelihood
DRL_Lecture_8_-_Imitation_Learning-570 這個其實就是 behavior cloning，對不對？
DRL_Lecture_8_-_Imitation_Learning-571 用我們今天做 behavior cloning ，就是看到一個 state
DRL_Lecture_8_-_Imitation_Learning-572 接下來預測，我們會得到
DRL_Lecture_8_-_Imitation_Learning-573 甚麼樣的 action，看到一個 state
DRL_Lecture_8_-_Imitation_Learning-574 然後有一個 Ground truth 告訴機器說
DRL_Lecture_8_-_Imitation_Learning-575 甚麼樣的 action 是最好的
DRL_Lecture_8_-_Imitation_Learning-576 在做 likelihood 的時候也是一樣
DRL_Lecture_8_-_Imitation_Learning-577 Given sentence 已經產生的部分
DRL_Lecture_8_-_Imitation_Learning-578 接下來 machine 要 predict 說
DRL_Lecture_8_-_Imitation_Learning-579 接下來要寫哪一個 word 才是最好的
DRL_Lecture_8_-_Imitation_Learning-580 所以，其實 Maximum likelihood
DRL_Lecture_8_-_Imitation_Learning-581 在做這種 Sequence generation 的時候
DRL_Lecture_8_-_Imitation_Learning-582 Maximum likelihood 對應到 Imitation Learning 裡面
DRL_Lecture_8_-_Imitation_Learning-583 就是 behavior cloning
DRL_Lecture_8_-_Imitation_Learning-584 那我們說光 Maximum likelihood 是不夠的
DRL_Lecture_8_-_Imitation_Learning-585 我們想要用 Sequence GAN
DRL_Lecture_8_-_Imitation_Learning-586 其實 Sequence GAN 就是對應到 Inverse Reinforcement Learning
DRL_Lecture_8_-_Imitation_Learning-587 我們剛才已經有講過說
DRL_Lecture_8_-_Imitation_Learning-588 其實 Inverse Reinforcement Learning
DRL_Lecture_8_-_Imitation_Learning-589 就是一種 GAN 的技術
DRL_Lecture_8_-_Imitation_Learning-590 你把 Inverse Reinforcement Learning 的技術
DRL_Lecture_8_-_Imitation_Learning-591 放在 Sentence generation，放到 Chat-bot 裡面
DRL_Lecture_8_-_Imitation_Learning-592 其實就是 Sequence GAN 跟他的種種的變形
Deep_Learning_Theory_1-2_-_Potential_of_Deep-0 為甚麼我們要用 deep 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1 接下來就是要講我們用 deep 的理由
Deep_Learning_Theory_1-2_-_Potential_of_Deep-2 那要用 deep 的理由一點都不是新鮮的東西
Deep_Learning_Theory_1-2_-_Potential_of_Deep-3 非常早以前就有人知道了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-4 我記得我昨天跟助教講說，我明天要來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-5 證一下 deep 是不是比較好的，助教就說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-6 這個不是三年前就講過一模一樣的東西了嗎？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-7 三年前聽你上課的時候，第一堂就講了類似的東西
Deep_Learning_Theory_1-2_-_Potential_of_Deep-8 但這跟三年前講的東西是不一樣的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-9 三年前講的時候，我們只有直覺而已
Deep_Learning_Theory_1-2_-_Potential_of_Deep-10 但是，現在在三年後，deep learning 發展得很快
Deep_Learning_Theory_1-2_-_Potential_of_Deep-11 不只有了直覺，還有了更多的理論的基礎
Deep_Learning_Theory_1-2_-_Potential_of_Deep-12 所以，跟之前講的東西是不一樣的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-13 那其實 deep 比較好這一件事情
Deep_Learning_Theory_1-2_-_Potential_of_Deep-14 在很早以前就有這樣子的猜想
Deep_Learning_Theory_1-2_-_Potential_of_Deep-15 舉例來說，你看那個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-16 Yann Lecun 他們在 09 年的時候，寫的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-17 為甚麼我們要用 deep network 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-18 裡面就有各式各樣的猜想告訴我們說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-19 deep 是比較好的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-20 只是在過去，並沒有太多的理論的證明
Deep_Learning_Theory_1-2_-_Potential_of_Deep-21 但是，現在已經有了很多理論的證明了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-22 那我把相關理論的 paper 都附在這個投影片的後面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-23 你有興趣的話再參考
Deep_Learning_Theory_1-2_-_Potential_of_Deep-24 今天所講的內容並不是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-25 base on 某一篇 paper 的理論講的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-26 你也知道上課嘛，上課要講的東西是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-27 還是希望你們可以聽得懂這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-28 所以稍微做了一些簡化，希望大家是可以聽得懂的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-29 那為甚麼我們要用 deep，我們都知道說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-30 雖然 shallow 的 network 可以表示任何的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-31 你用 shallow network 去 fit
Deep_Learning_Theory_1-2_-_Potential_of_Deep-32 任何的 **** function 到任何的精準度
Deep_Learning_Theory_1-2_-_Potential_of_Deep-33 只要 neuron 夠多
Deep_Learning_Theory_1-2_-_Potential_of_Deep-34 但是，沒有告訴你的是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-35 今天所謂的 neuron 夠多
Deep_Learning_Theory_1-2_-_Potential_of_Deep-36 到底要多到甚麼樣的地步
Deep_Learning_Theory_1-2_-_Potential_of_Deep-37 那我們剛才已經講說，其實你 neuron 的數目是 L / ε
Deep_Learning_Theory_1-2_-_Potential_of_Deep-38 neuron 的數目是 L / ε
Deep_Learning_Theory_1-2_-_Potential_of_Deep-39 接下來要看的就是說，如果我們用 deep 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-40 加一個 *****，是兩倍
Deep_Learning_Theory_1-2_-_Potential_of_Deep-41 那為甚麼剛才沒有加兩倍呢？是因為
Deep_Learning_Theory_1-2_-_Potential_of_Deep-42 其實你永遠可以想到更好的方法來 fit 這個東西
Deep_Learning_Theory_1-2_-_Potential_of_Deep-43 也就是說，我剛才是用比較笨的方法
Deep_Learning_Theory_1-2_-_Potential_of_Deep-44 是用兩個 neuron 才知道一條直線
Deep_Learning_Theory_1-2_-_Potential_of_Deep-45 其實你仔細想一想，應該用
Deep_Learning_Theory_1-2_-_Potential_of_Deep-46 一個 neuron 就可以知道一條直線了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-47 所以，比較好的寫法就是放一個 big O 這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-48 O(L/ε)，意思就是說前面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-49 不知道 O 是什麼的人，也就是說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-50 這前面有一個常數，我們不太確定這個常數是什麼
Deep_Learning_Theory_1-2_-_Potential_of_Deep-51 那我們等一下就會講說，deep 可不可以做得比這個好
Deep_Learning_Theory_1-2_-_Potential_of_Deep-52 可不可以做得比這個好
Deep_Learning_Theory_1-2_-_Potential_of_Deep-53 但是在講理論的部分之前
Deep_Learning_Theory_1-2_-_Potential_of_Deep-54 我們先回顧一下，在過去十年來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-55 人們是怎麼講的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-56 過去的時候，通常不是理論的證明，就是舉個例子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-57 舉個例子，就是打個比方說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-58 為什麼我們需要 deep，舉例來說我們在寫程式的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-59 其實你知道，任何的演算法
Deep_Learning_Theory_1-2_-_Potential_of_Deep-60 都可以用兩行程式就 implement
Deep_Learning_Theory_1-2_-_Potential_of_Deep-61 就好像說，將大象塞進冰箱就只要三個步驟一樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-62 其實任何演算法都可以用兩行程式 implement
Deep_Learning_Theory_1-2_-_Potential_of_Deep-63 怎麼 implement 呢？舉例來說，假設今天演算法
Deep_Learning_Theory_1-2_-_Potential_of_Deep-64 是 sorting 的演算法
Deep_Learning_Theory_1-2_-_Potential_of_Deep-65 你就把所有 sort 前的字串通通找出來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-66 窮舉所有可能 input
Deep_Learning_Theory_1-2_-_Potential_of_Deep-67 把每一個 output 的可能通通是先算好
Deep_Learning_Theory_1-2_-_Potential_of_Deep-68 當作它的 value
Deep_Learning_Theory_1-2_-_Potential_of_Deep-69 所有可能 input 都是 key
Deep_Learning_Theory_1-2_-_Potential_of_Deep-70 所有可對應的 output 就是 value
Deep_Learning_Theory_1-2_-_Potential_of_Deep-71 A 是一個數字的 sequence
Deep_Learning_Theory_1-2_-_Potential_of_Deep-72 A' 是他 sorting 好的結果
Deep_Learning_Theory_1-2_-_Potential_of_Deep-73 假設你要 implement 一個 sorting 的演算法的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-74 存一個巨大的 table，把它存好
Deep_Learning_Theory_1-2_-_Potential_of_Deep-75 接下來程式就這樣寫
Deep_Learning_Theory_1-2_-_Potential_of_Deep-76 input 一個 sequence 叫做 K
Deep_Learning_Theory_1-2_-_Potential_of_Deep-77 然後，第一行就是查表，call 一個 function 叫做
Deep_Learning_Theory_1-2_-_Potential_of_Deep-78 match key，看看這邊有沒有那個 key 出現
Deep_Learning_Theory_1-2_-_Potential_of_Deep-79 然後，把那個 key 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-80 落在第幾個 row 的那個 row 的 number
Deep_Learning_Theory_1-2_-_Potential_of_Deep-81 row 的 id 回傳回來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-82 然後再看那個 row 的 id 對應到的 value 是什麼
Deep_Learning_Theory_1-2_-_Potential_of_Deep-83 把那個 value 吐出去
Deep_Learning_Theory_1-2_-_Potential_of_Deep-84 你就 implement 完任何演算法了，就這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-85 所以我們知道說，其實任何演算法
Deep_Learning_Theory_1-2_-_Potential_of_Deep-86 都可以用兩行程式來把它完成
Deep_Learning_Theory_1-2_-_Potential_of_Deep-87 但是，沒有人用這樣的方法
Deep_Learning_Theory_1-2_-_Potential_of_Deep-88 來 implement sorting 這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-89 其實，講這樣子的方法
Deep_Learning_Theory_1-2_-_Potential_of_Deep-90 你仔細想想看，他有點像是 SVM 加上 kernel
Deep_Learning_Theory_1-2_-_Potential_of_Deep-91 對不對？如果大家熟悉 SVM 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-92 SVM 加上 kernel 是怎麼 implement 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-93 你有一筆 data, x 進來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-94 然後這邊的 n 代表說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-95 所有的 training data
Deep_Learning_Theory_1-2_-_Potential_of_Deep-96 你的 training data 有 n 筆，從 x^1 到 x^n
Deep_Learning_Theory_1-2_-_Potential_of_Deep-97 你把每一筆 data
Deep_Learning_Theory_1-2_-_Potential_of_Deep-98 input 的 data, x 跟每一筆 training data, x^n
Deep_Learning_Theory_1-2_-_Potential_of_Deep-99 都去計算他們的相似度
Deep_Learning_Theory_1-2_-_Potential_of_Deep-100 K(x^n, x) 代表他們之間的相似度
Deep_Learning_Theory_1-2_-_Potential_of_Deep-101 只是不同的 kernel 代表你使用了不同的相似度
Deep_Learning_Theory_1-2_-_Potential_of_Deep-102 x 跟 x^n 計算了相似度之後
Deep_Learning_Theory_1-2_-_Potential_of_Deep-103 然後再乘上 αn，最後就得到他的 output
Deep_Learning_Theory_1-2_-_Potential_of_Deep-104 所以，計算相似度這件事啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-105 其實就是 key matrix
Deep_Learning_Theory_1-2_-_Potential_of_Deep-106 乘上 αn，再輸出
Deep_Learning_Theory_1-2_-_Potential_of_Deep-107 其實就是你得到 row id 以後，把你的結果輸出來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-108 只是如果 SVM with kernel 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-109 他不是只抽一個 key 而已
Deep_Learning_Theory_1-2_-_Potential_of_Deep-110 他會算跟不同的 key 有不同的 match 程度
Deep_Learning_Theory_1-2_-_Potential_of_Deep-111 然後把你的 value 做 weighted sum
Deep_Learning_Theory_1-2_-_Potential_of_Deep-112 但是，其實 SVM 加上 kernel 他在本質上
Deep_Learning_Theory_1-2_-_Potential_of_Deep-113 他在做這個，解這個 machine learning 的問題的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-114 他在描述一個 function 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-115 就很像是這種兩行的演算法
Deep_Learning_Theory_1-2_-_Potential_of_Deep-116 但是我們知道說，你不會用
Deep_Learning_Theory_1-2_-_Potential_of_Deep-117 兩行 code 來 implement algorithm
Deep_Learning_Theory_1-2_-_Potential_of_Deep-118 你會用好多個 step 來 implement algorithm
Deep_Learning_Theory_1-2_-_Potential_of_Deep-119 為什麼？因為這樣是比較有效率的方式
Deep_Learning_Theory_1-2_-_Potential_of_Deep-120 因為你並不需要存一個碩大無朋的 table
Deep_Learning_Theory_1-2_-_Potential_of_Deep-121 那這個是一個比喻
Deep_Learning_Theory_1-2_-_Potential_of_Deep-122 還有另一個比喻是用這個 circuit
Deep_Learning_Theory_1-2_-_Potential_of_Deep-123 來做比喻，假設你是電機系的同學的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-124 一定修過邏輯電路設計，這個是必修
Deep_Learning_Theory_1-2_-_Potential_of_Deep-125 那在邏輯電路裡面，我們學到說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-126 所有的邏輯電路都是由 gate 所構成的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-127 就好像 neuron，neural network 都是由 neuron 所構成的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-128 那我們知道只要兩層的邏輯閘
Deep_Learning_Theory_1-2_-_Potential_of_Deep-129 就可以組成任何的 boolean function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-130 我們都知道說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-131 只要一個 hidden layer 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-132 一個 hidden layer network 其實也是兩層
Deep_Learning_Theory_1-2_-_Potential_of_Deep-133 hidden layer 跟 output layer 也是兩層
Deep_Learning_Theory_1-2_-_Potential_of_Deep-134 可以表示任何的 continuous function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-135 但是你不會用兩層的邏輯閘來設計電路
Deep_Learning_Theory_1-2_-_Potential_of_Deep-136 因為設計出來的電路太過龐大
Deep_Learning_Theory_1-2_-_Potential_of_Deep-137 如果你有用 hierarchical 的結構的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-138 你有用 multi-layer 的話，設計出來的電路
Deep_Learning_Theory_1-2_-_Potential_of_Deep-139 會是比較精簡的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-140 所以，如果你用 deep 的結構來設計電路
Deep_Learning_Theory_1-2_-_Potential_of_Deep-141 你的電路裡面，邏輯閘是有好幾層的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-142 那你設計出來的電路會比較精簡
Deep_Learning_Theory_1-2_-_Potential_of_Deep-143 你要做同樣的事情，只需要比較少的 gate
Deep_Learning_Theory_1-2_-_Potential_of_Deep-144 那對 network 來說，在過去就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-145 也不怎麼證明，就打個比方這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-146 就說 A 是這個樣子，B 應該也是一樣吧
Deep_Learning_Theory_1-2_-_Potential_of_Deep-147 如果有很多個 layer 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-148 就有很多個很多個 layer 的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-149 那你要描述一個 function 的時候，應該比較容易
Deep_Learning_Theory_1-2_-_Potential_of_Deep-150 所以你只要用 deep 的架構，只需要比較少的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-151 就可以描述同樣的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-152 那這樣可能會被反駁說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-153 A 很像 B，並不代表他們的性質就是一樣的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-154 但他們確實是很像的，理論可能是相同的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-155 那下一頁這個例子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-156 我打算把它略過，你可以自己看一下
Deep_Learning_Theory_1-2_-_Potential_of_Deep-157 當我們今天講到 network 的架構
Deep_Learning_Theory_1-2_-_Potential_of_Deep-158 把 network 的架構跟 deep 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-159 勾在一起的時候，通常就舉這個例子，告訴你說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-160 在電路裡面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-161 如果你要 implement 一個叫 parity check 的電路
Deep_Learning_Theory_1-2_-_Potential_of_Deep-162 你可以用 shallow 的 network 來 implement
Deep_Learning_Theory_1-2_-_Potential_of_Deep-163 但是，如果你 implement 的長度是 d 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-164 你要 O(2^d) 的 gate
Deep_Learning_Theory_1-2_-_Potential_of_Deep-165 但是，假設你今天用一個 deep 的架構來 implement
Deep_Learning_Theory_1-2_-_Potential_of_Deep-166 實際上的細節就不管了，反正就弄成這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-167 deep 的架構，那其實你只需要
Deep_Learning_Theory_1-2_-_Potential_of_Deep-168 O(d) gates，就可以做到這件事了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-169 我想這個是大家都知道的事情
Deep_Learning_Theory_1-2_-_Potential_of_Deep-170 但這就只是打個比方，而不是一個證明
Deep_Learning_Theory_1-2_-_Potential_of_Deep-171 接下來，我們要講說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-172 deep 為什麼他有潛能
Deep_Learning_Theory_1-2_-_Potential_of_Deep-173 有機會可以比 shallow 的 network 更好呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-174 在早年，所謂早年是指大概
Deep_Learning_Theory_1-2_-_Potential_of_Deep-175 14年(2014)，15年(2015) 那個時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-176 這個東西是 Ian Goodfellow 教科書裡面都有寫的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-177 所以他是比較早的東西，我們就知道說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-178 假設一個 network，它是 Shallow 跟 wide
Deep_Learning_Theory_1-2_-_Potential_of_Deep-179 跟假設另外一個 network，他是 deep 而 narrow 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-180 在他們有差不多參數量的情況下
Deep_Learning_Theory_1-2_-_Potential_of_Deep-181 這個時候，shallow 而 wide 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-182 他可以產生出來的 piecewise linear function 的這個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-183 piece 是比較少的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-184 而 deep, narrow 的架構
Deep_Learning_Theory_1-2_-_Potential_of_Deep-185 他可以產生出來的 piece 是比較多的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-186 如果你今天有一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-187 shallow 跟 deep 的 network，在相同參數量的情況下
Deep_Learning_Theory_1-2_-_Potential_of_Deep-188 shallow 的 network 可以產生出來的 piece 是比較少的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-189 deep 的 network 可以產生出來的 piece 是比較多的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-190 怎麼說呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-191 我們先來看看，假設給你一個 network 的架構
Deep_Learning_Theory_1-2_-_Potential_of_Deep-192 就隨便拿一個 ReLU 的 network 給你
Deep_Learning_Theory_1-2_-_Potential_of_Deep-193 這個 network 會有多少的 piece 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-194 他是 piecewise linear 的 function 嘛
Deep_Learning_Theory_1-2_-_Potential_of_Deep-195 他會有多少個 linear 的片段呢？他會有多少的 piece 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-196 我們先來看一下他的 upper bound
Deep_Learning_Theory_1-2_-_Potential_of_Deep-197 就是最多會有多少
Deep_Learning_Theory_1-2_-_Potential_of_Deep-198 怎麼想他的 upper bound 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-199 我們大家都對於 ReLU network 都很熟嘛
Deep_Learning_Theory_1-2_-_Potential_of_Deep-200 你知道在 ReLU network 裡面呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-201 每一個 neuron 有兩個 operation 的 region，對不對？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-202 一個 operation 的 region 是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-203 這個 neuron 的 output 是 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-204 另外一個 operation 的 region 是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-205 input = output
Deep_Learning_Theory_1-2_-_Potential_of_Deep-206 那我們之前在 machine learning 的課裡面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-207 我們都講過 ReLU 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-208 那時候會告訴你說，0 的那些
Deep_Learning_Theory_1-2_-_Potential_of_Deep-209 就把它拿掉，他就好像不存在一樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-210 剩下的部分，就好像是一個 linear 的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-211 這個時候很多人就會問我說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-212 其實 ReLU 只是 linear 的 function，那不是很弱嗎
Deep_Learning_Theory_1-2_-_Potential_of_Deep-213 但他不完全是一個 linear 的 function，他是一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-214 piecewise linear 的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-215 當今天你的 neuron 都作用在同樣的 region 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-216 他是一個 linear function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-217 但是，他今天換了，有一些 neuron 的 region
Deep_Learning_Theory_1-2_-_Potential_of_Deep-218 他的 operation 的狀態換了的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-219 他就進入了另一個 linear 的 region
Deep_Learning_Theory_1-2_-_Potential_of_Deep-220 這樣大家應該知道我的意思吧，就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-221 這個是你的 input, x，這個是你的 output, y
Deep_Learning_Theory_1-2_-_Potential_of_Deep-222 在某種 activation function 的 mode 下面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-223 他是一個 linear 的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-224 但是你看今天 input 超過某個範圍
Deep_Learning_Theory_1-2_-_Potential_of_Deep-225 某一個 neuron 的 operation 的 mode
Deep_Learning_Theory_1-2_-_Potential_of_Deep-226 換掉了，就本來他可能是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-227 output 都是 0，現在變成 input = output
Deep_Learning_Theory_1-2_-_Potential_of_Deep-228 或他本來 input = output，現在變成 output 都是 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-229 那你就變成另外一個 linear 的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-230 所以，今天我們來分析一個 ReLU 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-231 他有幾個 piece，他的 upper bound 就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-232 看你有幾個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-233 每一個 neuron 他都可以是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-234 作用在兩個 mode 的其中一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-235 所以，所有的 neuron 的 mode 的可能性
Deep_Learning_Theory_1-2_-_Potential_of_Deep-236 我們就這邊叫他 activation 的 pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-237 所以 activation pattern 的意思就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-238 某一種 neuron 的 mode 的組合，比如說這邊是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-239 linear, linear, 0, 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-240 0, linear, 0, linear
Deep_Learning_Theory_1-2_-_Potential_of_Deep-241 這個叫做一種 pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-242 你也可能有別種 pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-243 總共有多少種 pattern 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-244 這個是國小數學，每一個 neuron 有兩種可能性所以
Deep_Learning_Theory_1-2_-_Potential_of_Deep-245 假設有 n 個 neuron，就是有 2^n 個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-246 可能的 activation pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-247 所以有 N 個 neuron，就有 2^N 個可能的 activation pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-248 8 個 neuron，就有 2^8 個可能的 activation pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-249 每個 activation pattern 都只造了一個 linear function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-250 所以，想起來好像是有 N 個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-251 有 2^N 個 activation pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-252 那我們用這一個 network 架構所訂出來的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-253 應該要有 2^n 個 linear 的 pieces
Deep_Learning_Theory_1-2_-_Potential_of_Deep-254 有 2^n 個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-255 這個其實是一個 upper bound
Deep_Learning_Theory_1-2_-_Potential_of_Deep-256 這個是一個 upper bound
Deep_Learning_Theory_1-2_-_Potential_of_Deep-257 這個是一個最佳的狀況，你不可能
Deep_Learning_Theory_1-2_-_Potential_of_Deep-258 事實上，為什麼這是一個最佳的狀況呢？因為
Deep_Learning_Theory_1-2_-_Potential_of_Deep-259 為什麼這個狀況不一定能夠達到呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-260 因為有些 pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-261 可能是永遠沒有辦法出現的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-262 有些 pattern 是不可能出現的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-263 舉例來說，我們舉一個這樣子的例子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-264 這是一個非常簡單的 ReLU 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-265 只有一層，只有兩個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-266 我們剛才說，每一個 neuron 有兩個 mode
Deep_Learning_Theory_1-2_-_Potential_of_Deep-267 所以今天，按照這個 network，他 activation 的 pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-268 其實應該要有 4 種，對不對，就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-269 他的 activation 的 pattern，其實應該要有 4 種
Deep_Learning_Theory_1-2_-_Potential_of_Deep-270 但是，你實際上去想一想
Deep_Learning_Theory_1-2_-_Potential_of_Deep-271 你會發現說，這一個 network 的架構
Deep_Learning_Theory_1-2_-_Potential_of_Deep-272 他只定義得出，這三個 pieces 而已
Deep_Learning_Theory_1-2_-_Potential_of_Deep-273 他定義不出 4 個 pieces，對不對？因為
Deep_Learning_Theory_1-2_-_Potential_of_Deep-274 有一些 operation 的狀態
Deep_Learning_Theory_1-2_-_Potential_of_Deep-275 是不合理的，是沒有辦法呈現的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-276 我不知道大家能不能夠接受這個想法
Deep_Learning_Theory_1-2_-_Potential_of_Deep-277 你可以回去想一下，你把這一個 network 的架構
Deep_Learning_Theory_1-2_-_Potential_of_Deep-278 你把他的參數試不同的參數
Deep_Learning_Theory_1-2_-_Potential_of_Deep-279 你挪來挪去，你可能就只造得出三個 piece 而已
Deep_Learning_Theory_1-2_-_Potential_of_Deep-280 因為有一些，你只能產生三種 activation pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-281 你沒有辦法產生四種 activation pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-282 其實，事實上呢，今天假設我們有一個 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-283 他是只有一個 hidden layer，有 n 個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-284 本來，按照 Upper Bound 來想
Deep_Learning_Theory_1-2_-_Potential_of_Deep-285 應該要有 2^n 個 activation pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-286 他應該可以產生 2^n 個 piece
Deep_Learning_Theory_1-2_-_Potential_of_Deep-287 但是，如果你仔細想一下的話，你會發現說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-288 今天只有一個 hidden layer 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-289 他假設那個 hidden layer 的 neuron 是 n
Deep_Learning_Theory_1-2_-_Potential_of_Deep-290 其實他弄得出來的 pice 的數目
Deep_Learning_Theory_1-2_-_Potential_of_Deep-291 其實只是 O(n) 而已，可能是  n+1 這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-292 我們沒有辦法弄出 2^n 個不同的 pieces
Deep_Learning_Theory_1-2_-_Potential_of_Deep-293 這邊我們剛才講的是 upper bound
Deep_Learning_Theory_1-2_-_Potential_of_Deep-294 所以從 Upper bound 看起來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-295 雖然我們知道說，那個 upper bound
Deep_Learning_Theory_1-2_-_Potential_of_Deep-296 實際上可能是達不到的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-297 就是我們說，今天給我們一個 ReLU 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-298 他最多可以製造出來的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-299 會有幾個 linear 的 piece 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-300 最多是 2^n 個 piece
Deep_Learning_Theory_1-2_-_Potential_of_Deep-301 但這是一個 upper bound，很有可能，你怎麼調
Deep_Learning_Theory_1-2_-_Potential_of_Deep-302 都達不到那一個 piece 的數目
Deep_Learning_Theory_1-2_-_Potential_of_Deep-303 接下來，我們要講的是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-304 那 lower bound 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-305 那你要講 lower bound 很簡單，就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-306 兜一個 network，看看我們可以製造出多少個 piece
Deep_Learning_Theory_1-2_-_Potential_of_Deep-307 那個 piece 就是我們可以製造出來的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-308 piece 的數目的 lower bound
Deep_Learning_Theory_1-2_-_Potential_of_Deep-309 所以，我們就是講一個 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-310 找一個 ReLU 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-311 然後，分析一下說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-312 我們根據這個 ReLU 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-313 我們可以製造出多少的 pieces
Deep_Learning_Theory_1-2_-_Potential_of_Deep-314 那在講這一段之前呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-315 我們先製造一個特別的 activation function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-316 這個特別的 activation function 叫做
Deep_Learning_Theory_1-2_-_Potential_of_Deep-317 取絕對值的 activation function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-318 比如說，你的 input 是 x
Deep_Learning_Theory_1-2_-_Potential_of_Deep-319 我們把 x 乘上 w，再加上 bias, b
Deep_Learning_Theory_1-2_-_Potential_of_Deep-320 通過這個 activation function 以後呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-321 會取絕對值
Deep_Learning_Theory_1-2_-_Potential_of_Deep-322 怎麼 implement 這種取絕對值的 activation function 呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-323 其實你可以把兩個 ReLU 的 activation function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-324 組合起來，就可以變成一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-325 取絕對值的 activation function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-326 怎麼做？我們今天有兩個 ReLU 的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-327 我們有兩個 ReLU 的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-328 今天 x 進來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-329 走上面這個 ReLU 的 neuron 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-330 他乘以 w，再加上 bias
Deep_Learning_Theory_1-2_-_Potential_of_Deep-331 走下面這個 ReLU 的 neuron 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-332 他乘上 -w，再減掉 bias, b
Deep_Learning_Theory_1-2_-_Potential_of_Deep-333 今天，這兩個都是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-334 這兩個都是 ReLU 的 activation function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-335 所以，如果今天，(w*x + b) &gt; 0 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-336 那就是拿這邊的輸出，這邊的輸出就是 0 嘛
Deep_Learning_Theory_1-2_-_Potential_of_Deep-337 如果 (w*x + b) &lt; 0 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-338 就是拿這邊的輸出，這邊就是 0 嘛
Deep_Learning_Theory_1-2_-_Potential_of_Deep-339 所以，仔細想一下就會知道說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-340 用這個方法，你用 ReLU 的 activation function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-341 你可以製造一個取絕對值的 activation function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-342 你可以去製造一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-343 像這樣子取絕對值的 activation function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-344 那接下來呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-345 我們要講的事情是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-346 如果我們現在有這樣子的 activation function 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-347 像這樣子的 activation function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-348 我們就放一個這樣子的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-349 然後，我們的 input 是 x
Deep_Learning_Theory_1-2_-_Potential_of_Deep-350 我們的 output 是 a1
Deep_Learning_Theory_1-2_-_Potential_of_Deep-351 我們 input 的 x 跟 output 的 a1 間
Deep_Learning_Theory_1-2_-_Potential_of_Deep-352 可能會有什麼樣的關係呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-353 他們的關係可能是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-354 這個樣子，可能是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-355 在 0 到 1/2 中間
Deep_Learning_Theory_1-2_-_Potential_of_Deep-356 他的值是從 1 逐漸下降到 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-357 從 1/2 到 1 中間
Deep_Learning_Theory_1-2_-_Potential_of_Deep-358 今天 output a1 的值是從 1/2 逐漸上升到 1
Deep_Learning_Theory_1-2_-_Potential_of_Deep-359 那我們假設說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-360 第一個 hidden layer 做的事情就是這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-361 那我現在假設我們加上第二個 hidden layer
Deep_Learning_Theory_1-2_-_Potential_of_Deep-362 第二個 hidden layer 就是把 a1 吃進去
Deep_Learning_Theory_1-2_-_Potential_of_Deep-363 然後，變成 a2 吐出來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-364 我們假設
Deep_Learning_Theory_1-2_-_Potential_of_Deep-365 第二個 hidden layer 跟第一個 hidden layer 做的事情
Deep_Learning_Theory_1-2_-_Potential_of_Deep-366 其實是一樣的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-367 a1 跟 a2 的關係
Deep_Learning_Theory_1-2_-_Potential_of_Deep-368 和 x 和 a1 的關係，其實是一樣的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-369 當 a1 的變化從 0 到 1/2 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-370 a2 從 1 下降到 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-371 當 a1 從 1/2 變到 1的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-372 a2 從 0 上升到 1
Deep_Learning_Theory_1-2_-_Potential_of_Deep-373 x 和 a1 的關係和 a1 和 a2 的關係，其實是一樣的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-374 那我們知道 x 和 a1 的關係
Deep_Learning_Theory_1-2_-_Potential_of_Deep-375 我們就知道 a1 和 a2 的關係
Deep_Learning_Theory_1-2_-_Potential_of_Deep-376 那這整個 function 長什麼樣子呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-377 也就是說，x 跟 a2 之間的關係
Deep_Learning_Theory_1-2_-_Potential_of_Deep-378 像是什麼樣子呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-379 我們來想一想
Deep_Learning_Theory_1-2_-_Potential_of_Deep-380 input 是從 0 到 1
Deep_Learning_Theory_1-2_-_Potential_of_Deep-381 我們現在就是要看說，x 從 0 到 1 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-382 a2 是怎麼樣變化
Deep_Learning_Theory_1-2_-_Potential_of_Deep-383 是怎麼樣變化
Deep_Learning_Theory_1-2_-_Potential_of_Deep-384 那今天這個 a2 呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-385 在 a1 是 1/2 的地方
Deep_Learning_Theory_1-2_-_Potential_of_Deep-386 會有一個轉折的點
Deep_Learning_Theory_1-2_-_Potential_of_Deep-387 所以我們在考慮的時候，畫一個 a1 = 1/2 的線
Deep_Learning_Theory_1-2_-_Potential_of_Deep-388 然後，從 0 到 1 之間
Deep_Learning_Theory_1-2_-_Potential_of_Deep-389 我們分成四個區段來考慮
Deep_Learning_Theory_1-2_-_Potential_of_Deep-390 我們考慮 0 到 1/4
Deep_Learning_Theory_1-2_-_Potential_of_Deep-391 考慮 0 到 1/4
Deep_Learning_Theory_1-2_-_Potential_of_Deep-392 1/4 到 1/2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-393 1/2 到 3/4
Deep_Learning_Theory_1-2_-_Potential_of_Deep-394 3/4 到 1，分成四個片段來考慮
Deep_Learning_Theory_1-2_-_Potential_of_Deep-395 如果考慮第一個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-396 考慮第一個片段，x 從 0 變到 1/2 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-397 x 從 0 變到 1/2 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-398 a1 從 1 變到 1/2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-399 a1 從 1 變到 1/2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-400 那 a1 從 1 變到 1/2 的時候，a2 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-401 他是從 1 變到 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-402 所以今天 x 從 0 到 1/4 這段距離
Deep_Learning_Theory_1-2_-_Potential_of_Deep-403 當從 0 到 1/4 這段距離有變化的時候呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-404 a2 是從 1 變到 0 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-405 這個是第一段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-406 那如果你分析第二段從 1/4 到 1/2 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-407 分析從 1/4 到 1/2 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-408 x 的變化分從 1/4 到 1/2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-409 a1 的變化是從 1/2 一直跑到 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-410 它從 1/2 一直跑到 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-411 這個其實非常難想像啦
Deep_Learning_Theory_1-2_-_Potential_of_Deep-412 我不知道大家聽不聽得懂這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-413 如果你聽不懂的話，你就自己回去
Deep_Learning_Theory_1-2_-_Potential_of_Deep-414 你就自己仔細想一想，這個畫圖也不知道要怎麼畫才好
Deep_Learning_Theory_1-2_-_Potential_of_Deep-415 這段 x 是從 1/4 到 1/2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-416 那 a1 是從 1/2 變到 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-417 所以 a1 是從 1/2 變到 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-418 a2 是從 0 變到 1
Deep_Learning_Theory_1-2_-_Potential_of_Deep-419 所以， x 從這邊到中間
Deep_Learning_Theory_1-2_-_Potential_of_Deep-420 從 1/4 到 1/2 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-421 a2 是從 0 變到 1 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-422 所以是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-423 再講下去你可能就會覺得有點無聊了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-424 所以，今天 x 從 1/2 變到 1/4
Deep_Learning_Theory_1-2_-_Potential_of_Deep-425 那 a2 會有什麼樣的變化呢？他的變化是怎麼樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-426 x 從 3/4 變到 1
Deep_Learning_Theory_1-2_-_Potential_of_Deep-427 a2 的變化是這個樣子，所以他畫了一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-428 w 的形狀，他畫了一個 w 的形狀
Deep_Learning_Theory_1-2_-_Potential_of_Deep-429 所以，今天我們知道說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-430 a2 的輸出是這個 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-431 有兩個 neuron 的 function，他是 w 的形狀
Deep_Learning_Theory_1-2_-_Potential_of_Deep-432 那我們再加上第三個 neuron 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-433 如果再加上第三個 neuron 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-434 會發生什麼樣的事情呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-435 我們假設第三個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-436 跟前面兩個 neuron 做的事情是一模一樣的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-437 只是 input 是 a2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-438 output 是 a3，那 a2 跟 a3 的關係
Deep_Learning_Theory_1-2_-_Potential_of_Deep-439 長得是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-440 當我們加上這個紅色的線的時候呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-441 你就會發現說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-442 你加這個紅色線以後
Deep_Learning_Theory_1-2_-_Potential_of_Deep-443 接下來，你就分析一下這個 x 從 0 變到 1
Deep_Learning_Theory_1-2_-_Potential_of_Deep-444 你要分成 8 個區間去考慮
Deep_Learning_Theory_1-2_-_Potential_of_Deep-445 8 個區間去考慮
Deep_Learning_Theory_1-2_-_Potential_of_Deep-446 你要從 0 分析到 1/8
Deep_Learning_Theory_1-2_-_Potential_of_Deep-447 就 x 從 0 變到 1/8 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-448 a2 是從 1 跑到 1/2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-449 是從 1 跑到 1/2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-450 所以 a3 會從 1 變到 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-451 所以 a3 會從 1 變到 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-452 然後你就把每一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-453 piece，每一個小段啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-454 這個 x 跟 a2，a2 到 a3 的關係，通通畫出來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-455 看起來就是這個樣子，所以
Deep_Learning_Theory_1-2_-_Potential_of_Deep-456 本來是一個 w 的形狀
Deep_Learning_Theory_1-2_-_Potential_of_Deep-457 現在變成兩個 ｗ 的形狀
Deep_Learning_Theory_1-2_-_Potential_of_Deep-458 變成兩個 ｗ 連在一起，變成很多鋸齒的形狀
Deep_Learning_Theory_1-2_-_Potential_of_Deep-459 所以，現在你就會發現說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-460 本來 a1 他是長這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-461 就 a2 跟 a3 的關係和 x 跟 a1 的關係長得是一樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-462 他們長這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-463 他總共有兩個線段，就 2^1 個線段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-464 到 a2 的時候，他總共有
Deep_Learning_Theory_1-2_-_Potential_of_Deep-465 4 個線段，2^2 個線段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-466 到 a3 的時候，它就變成有
Deep_Learning_Theory_1-2_-_Potential_of_Deep-467 2^3 個線段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-468 所以，你會發現說呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-469 你會發現說呢，今天當我們用 deep structure 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-470 每次我多加了一個 layer
Deep_Learning_Theory_1-2_-_Potential_of_Deep-471 其實我的每個 layer 只有兩個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-472 我每次多加了兩個 neuron 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-473 我每次多加了兩個 neuron 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-474 我們可以產生的 linear 的 region
Deep_Learning_Theory_1-2_-_Potential_of_Deep-475 可以產生出來的線段的數目
Deep_Learning_Theory_1-2_-_Potential_of_Deep-476 就會變成兩倍，就每多加兩個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-477 你的線段的數目就會 double
Deep_Learning_Theory_1-2_-_Potential_of_Deep-478 所以，如果我們今天比較
Deep_Learning_Theory_1-2_-_Potential_of_Deep-479 shallow 的 network 跟 deep 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-480 在講 shallow 的 network 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-481 我們每次兩個 neuron 組合起來才產生一個線段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-482 所以今天如果你要產生 100 個線段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-483 你就要 200 個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-484 但是，今天如果是 deep 的 structure
Deep_Learning_Theory_1-2_-_Potential_of_Deep-485 每次多加了兩個 neuron ，這邊這個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-486 取絕對值的這個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-487 就是他是兩個 ReLU 所組成的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-488 每次多加一個 layer
Deep_Learning_Theory_1-2_-_Potential_of_Deep-489 那個 layer 就只有兩個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-490 你都可以讓你的線段的數目多增加兩倍
Deep_Learning_Theory_1-2_-_Potential_of_Deep-491 所以，你今天
Deep_Learning_Theory_1-2_-_Potential_of_Deep-492 舉例來說，你要產生 100 個線段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-493 100 個線段是多少？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-494 我只要 2 的 7 次方
Deep_Learning_Theory_1-2_-_Potential_of_Deep-495 對不對，我只要 7 個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-496 我就可以做到那件事情了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-497 結果發現，你要產生有比較多片段的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-498 你要產生有比較多片段的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-499 function 的時候，用 deep 是比較有效率的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-500 如果你仔細回想一下
Deep_Learning_Theory_1-2_-_Potential_of_Deep-501 為什麼 deep 可以產生比較多的線段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-502 他做的事情比較像是摺紙一樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-503 對不對？或者是他其實是把同樣的 pattern
Deep_Learning_Theory_1-2_-_Potential_of_Deep-504 反覆的出現，就本來你只有一個 v
Deep_Learning_Theory_1-2_-_Potential_of_Deep-505 然後第一層這個 v
Deep_Learning_Theory_1-2_-_Potential_of_Deep-506 第二層再把兩個 v 接起來變成 w
Deep_Learning_Theory_1-2_-_Potential_of_Deep-507 第三層則是把兩個 w 接起來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-508 變成有兩個 ｗ 拼在一起
Deep_Learning_Theory_1-2_-_Potential_of_Deep-509 接下來，下一層就把兩個 w
Deep_Learning_Theory_1-2_-_Potential_of_Deep-510 再 double，變成有 4 個 w
Deep_Learning_Theory_1-2_-_Potential_of_Deep-511 他是這樣的，他是把原來你的 piece 不斷的反覆產生
Deep_Learning_Theory_1-2_-_Potential_of_Deep-512 有點像這個，不知道大家能不能體會，他就像是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-513 那個雪花結晶的結構這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-514 他不斷地把原來的 pattern，不斷地複製
Deep_Learning_Theory_1-2_-_Potential_of_Deep-515 他可以產生很多個 piece，但那些 piece 是有規則的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-516 那講了這麼多以後呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-517 其實你可以非常輕易地
Deep_Learning_Theory_1-2_-_Potential_of_Deep-518 就證明說，假設你現在的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-519 寬度是 k，深度是 h
Deep_Learning_Theory_1-2_-_Potential_of_Deep-520 那你可以製造 k^h 個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-521 我們剛才是寬度是 2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-522 我們可以製造出 2^h 個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-523 就寬度是 2，深度是 h
Deep_Learning_Theory_1-2_-_Potential_of_Deep-524 我們可以製造 2^h 個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-525 那現在你可以輕易地自己想出來說，假設現在
Deep_Learning_Theory_1-2_-_Potential_of_Deep-526 寬度不是 2 而是 k
Deep_Learning_Theory_1-2_-_Potential_of_Deep-527 那深度是 h，你可以製造出
Deep_Learning_Theory_1-2_-_Potential_of_Deep-528 k^h 個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-529 其實你可以非常輕易地想出
Deep_Learning_Theory_1-2_-_Potential_of_Deep-530 找到一個 network 可以做一件事情
Deep_Learning_Theory_1-2_-_Potential_of_Deep-531 那這個東西就是這個 network 的架構
Deep_Learning_Theory_1-2_-_Potential_of_Deep-532 可以產生的 piece 的數目的 lower bound
Deep_Learning_Theory_1-2_-_Potential_of_Deep-533 所以，今天這個結果告訴我們什麼
Deep_Learning_Theory_1-2_-_Potential_of_Deep-534 告訴我們說，你可以產生的片段的數目啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-535 是 k^h 次方
Deep_Learning_Theory_1-2_-_Potential_of_Deep-536 所以，h 也就是深度，是放在指數的地方
Deep_Learning_Theory_1-2_-_Potential_of_Deep-537 所以，當你增加你的深度的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-538 你可以非常快的增加 piece 的數目
Deep_Learning_Theory_1-2_-_Potential_of_Deep-539 如果你想要知道更多更細的證明的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-540 下面列了一大堆 paper 這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-541 舉例來說，第一篇這個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-542 前面兩篇是這個 Benjo 的 paper
Deep_Learning_Theory_1-2_-_Potential_of_Deep-543 其實是最早做這樣子分析的 paper
Deep_Learning_Theory_1-2_-_Potential_of_Deep-544 那在裡面呢，因為我們今天
Deep_Learning_Theory_1-2_-_Potential_of_Deep-545 在上課的時候，我們都假設 input 只有一個 dimension
Deep_Learning_Theory_1-2_-_Potential_of_Deep-546 那他們不是這樣，他們會假設比較複雜的 case
Deep_Learning_Theory_1-2_-_Potential_of_Deep-547 就 input 是第一個 dimension
Deep_Learning_Theory_1-2_-_Potential_of_Deep-548 那這個時候狀況就比較複雜了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-549 沒有今天得到的式子那麼單純
Deep_Learning_Theory_1-2_-_Potential_of_Deep-550 原來在 ICLR, 2014 的 paper 裡面，他們
Deep_Learning_Theory_1-2_-_Potential_of_Deep-551 得到了一個 lower bound
Deep_Learning_Theory_1-2_-_Potential_of_Deep-552 後來呢，他們又在
Deep_Learning_Theory_1-2_-_Potential_of_Deep-553 下一篇 NIPS, 2014 裡面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-554 就 improve 了那一個 lower bound
Deep_Learning_Theory_1-2_-_Potential_of_Deep-555 後來也有很多人做類似的嘗試去做
Deep_Learning_Theory_1-2_-_Potential_of_Deep-556 去繼續 improve 那個 lower bound
Deep_Learning_Theory_1-2_-_Potential_of_Deep-557 那就把這個文獻列在這邊給大家參考
Deep_Learning_Theory_1-2_-_Potential_of_Deep-558 那我想要講的是最後一篇
Deep_Learning_Theory_1-2_-_Potential_of_Deep-559 在最後一篇除了有理論的證明之外
Deep_Learning_Theory_1-2_-_Potential_of_Deep-560 他還做了一些實驗
Deep_Learning_Theory_1-2_-_Potential_of_Deep-561 因為我們剛才講的只是一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-562 理論上是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-563 就我們用了一個很奇怪的方法
Deep_Learning_Theory_1-2_-_Potential_of_Deep-564 兜出了一個 ReLU 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-565 然後分析說，嗯，這個 ReLU 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-566 他有很多很多的 piece
Deep_Learning_Theory_1-2_-_Potential_of_Deep-567 可是你可能就覺得說，欸
Deep_Learning_Theory_1-2_-_Potential_of_Deep-568 這個會不會是一個非常非常 specefic 的 case
Deep_Learning_Theory_1-2_-_Potential_of_Deep-569 也許在一個正常的狀況下，你 train 一個 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-570 你根本就不會產生那麼多的 piece
Deep_Learning_Theory_1-2_-_Potential_of_Deep-571 所以，他就做了一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-572 做了一些實驗來 verify 這些事
Deep_Learning_Theory_1-2_-_Potential_of_Deep-573 他有一個實驗是做在 MNIST 上
Deep_Learning_Theory_1-2_-_Potential_of_Deep-574 在 MNIST 上面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-575 你這個有不同的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-576 我們先看第一個圖
Deep_Learning_Theory_1-2_-_Potential_of_Deep-577 第一個圖的橫坐標代表的是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-578 network 的深度
Deep_Learning_Theory_1-2_-_Potential_of_Deep-579 2 層, 4 層, 6 層, 8 層, 10 層, 12 層等等
Deep_Learning_Theory_1-2_-_Potential_of_Deep-580 那不同的顏色代表不同的寬度
Deep_Learning_Theory_1-2_-_Potential_of_Deep-581 50 個 neuron, 100  個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-582 500 個 neuron, 700 個 neuron 等等
Deep_Learning_Theory_1-2_-_Potential_of_Deep-583 然後這個 scl 啊，大家不用太在意他
Deep_Learning_Theory_1-2_-_Potential_of_Deep-584 他是 train network 的時候不同的 initialization 的參數
Deep_Learning_Theory_1-2_-_Potential_of_Deep-585 接下來這邊他做的實驗就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-586 他把這個 network 先拿出來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-587 然後再看說， input 從
Deep_Learning_Theory_1-2_-_Potential_of_Deep-588 某一點到某一點的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-589 output 總共通過了多少個 piece
Deep_Learning_Theory_1-2_-_Potential_of_Deep-590 了解嗎？network 就是一個 function 啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-591 就是一個 function ，piecewise linear function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-592 然後他就算說，從某一點
Deep_Learning_Theory_1-2_-_Potential_of_Deep-593 到某一點總共經過了多少個 pieces
Deep_Learning_Theory_1-2_-_Potential_of_Deep-594 那今天這個圖啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-595 縱軸就是 pieces 的數目
Deep_Learning_Theory_1-2_-_Potential_of_Deep-596 那注意一下這個縱軸
Deep_Learning_Theory_1-2_-_Potential_of_Deep-597 他是 exponential 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-598 所以今天這個直線的上升
Deep_Learning_Theory_1-2_-_Potential_of_Deep-599 其實是 exponential 的上升
Deep_Learning_Theory_1-2_-_Potential_of_Deep-600 所以今天固定你的 network 的 size
Deep_Learning_Theory_1-2_-_Potential_of_Deep-601 固定你的 network 的寬度，不是 size
Deep_Learning_Theory_1-2_-_Potential_of_Deep-602 固定 network 的寬度
Deep_Learning_Theory_1-2_-_Potential_of_Deep-603 增加深度的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-604 你會發現，這個時候你產生 pieces 的量
Deep_Learning_Theory_1-2_-_Potential_of_Deep-605 就算在實際的 case
Deep_Learning_Theory_1-2_-_Potential_of_Deep-606 他也是 exponential 增加的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-607 剛才講說，那個 upper bound 就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-608 寬 k 的深度的 h 次方
Deep_Learning_Theory_1-2_-_Potential_of_Deep-609 那這是一個理論上的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-610 想像，但是在實際上
Deep_Learning_Theory_1-2_-_Potential_of_Deep-611 在實際的 application 上
Deep_Learning_Theory_1-2_-_Potential_of_Deep-612 你可以觀察到這樣的現象
Deep_Learning_Theory_1-2_-_Potential_of_Deep-613 當你的 depth 增加的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-614 你產生的 piece 的數目是 exponential 增加
Deep_Learning_Theory_1-2_-_Potential_of_Deep-615 另外，這個實驗是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-616 layer 的數目固定，但是橫軸是改變 layer 的寬度
Deep_Learning_Theory_1-2_-_Potential_of_Deep-617 有 200 個 neuron, 400 個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-618 600 個 neuron, 800 個 neuron 等等
Deep_Learning_Theory_1-2_-_Potential_of_Deep-619 縱軸，一樣是 exponential
Deep_Learning_Theory_1-2_-_Potential_of_Deep-620 不同的顏色代表不同的深度
Deep_Learning_Theory_1-2_-_Potential_of_Deep-621 兩層、四層、六層等等
Deep_Learning_Theory_1-2_-_Potential_of_Deep-622 你會發現說，今天如果是固定你的深度
Deep_Learning_Theory_1-2_-_Potential_of_Deep-623 但是，只改變你的寬度的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-624 對產生的 piece 的數目會影響有多大？這個線呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-625 基本上看起來像是直線一樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-626 這個是第一個實驗
Deep_Learning_Theory_1-2_-_Potential_of_Deep-627 他做了另外一個有趣的實驗是說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-628 他假如他這樣做
Deep_Learning_Theory_1-2_-_Potential_of_Deep-629 就拿出一個 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-630 他 paper 裡面其實沒有講得很清楚
Deep_Learning_Theory_1-2_-_Potential_of_Deep-631 這個 network 是哪來的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-632 拿出一個 network 有很多層
Deep_Learning_Theory_1-2_-_Potential_of_Deep-633 那他在 input 的這個 space 上面啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-634 畫一個圈圈，假設 input 是二維的，畫一個圈圈
Deep_Learning_Theory_1-2_-_Potential_of_Deep-635 那通過第一個 hidden layer 以後
Deep_Learning_Theory_1-2_-_Potential_of_Deep-636 那些 neuron 不是會有 output 嗎？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-637 不是會變成一個 100 維的 vector 嗎？input 二維
Deep_Learning_Theory_1-2_-_Potential_of_Deep-638 假設你的第一個 hidden layer 是 100 維
Deep_Learning_Theory_1-2_-_Potential_of_Deep-639 他會變成一個 100 維的點嘛
Deep_Learning_Theory_1-2_-_Potential_of_Deep-640 但是你在 input 的時候，你是畫一個圈圈
Deep_Learning_Theory_1-2_-_Potential_of_Deep-641 那在高維的空間中
Deep_Learning_Theory_1-2_-_Potential_of_Deep-642 你也是走了某一個軌跡，對不對？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-643 假設你的 hidden layer size 是 100 維
Deep_Learning_Theory_1-2_-_Potential_of_Deep-644 那在 100 維的空間中，你也是走了某一個軌跡
Deep_Learning_Theory_1-2_-_Potential_of_Deep-645 只是它不見得是圓的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-646 他就把這個 100 維的軌跡
Deep_Learning_Theory_1-2_-_Potential_of_Deep-647 100 維空間中的軌跡 project 到二維，他說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-648 看起來像是這個樣子，這是第一個 layer
Deep_Learning_Theory_1-2_-_Potential_of_Deep-649 第二個 layer 看起來像是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-650 第三個 layer 看起來像是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-651 第四個 layer 看起來就是這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-652 就越來越複雜，就本來你在 input 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-653 你只是畫了一個圈圈
Deep_Learning_Theory_1-2_-_Potential_of_Deep-654 但是，通過很多個 layer 以後
Deep_Learning_Theory_1-2_-_Potential_of_Deep-655 這個軌跡，在 nerwork 的 output 看起來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-656 越來越複雜
Deep_Learning_Theory_1-2_-_Potential_of_Deep-657 這個軌跡越來越複雜，直到最後變得非常的複雜
Deep_Learning_Theory_1-2_-_Potential_of_Deep-658 你會發現說，這個複雜的結構裡面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-659 其實，如果你仔細看一下，他是有 pattern 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-660 他並不是一個完全隨機的複雜的結構
Deep_Learning_Theory_1-2_-_Potential_of_Deep-661 而是有某一些的對稱性這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-662 這邊有一些，某一些有趣的對稱性
Deep_Learning_Theory_1-2_-_Potential_of_Deep-663 就好像是，雪花那個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-664 所以，就呼應我們剛才講的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-665 說 network 他可以產生很多的 piece
Deep_Learning_Theory_1-2_-_Potential_of_Deep-666 那產生的這些 piece 中間，他是有某一種 pattern 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-667 他是把 v 變成 w，再把兩個 w 接起來這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-668 他是有一個固定的 pattern，他不是隨機的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-669 產生那些 piece
Deep_Learning_Theory_1-2_-_Potential_of_Deep-670 那個 paper 還有另外一個實驗，這個實驗想要驗證的是說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-671 low layer 的參數
Deep_Learning_Theory_1-2_-_Potential_of_Deep-672 就比較靠近 input 的那個 layer 的參數
Deep_Learning_Theory_1-2_-_Potential_of_Deep-673 相較於比較靠近 output layer 的參數
Deep_Learning_Theory_1-2_-_Potential_of_Deep-674 他是比較重要的，所以比較靠近 input 的那些參數
Deep_Learning_Theory_1-2_-_Potential_of_Deep-675 是比較重要的，因為在直覺上
Deep_Learning_Theory_1-2_-_Potential_of_Deep-676 想起來顯然是有道理的，因為
Deep_Learning_Theory_1-2_-_Potential_of_Deep-677 我們今天在做 deep learning 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-678 就好像是在折紙一樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-679 前面的 layer 做的事情就是去摺紙
Deep_Learning_Theory_1-2_-_Potential_of_Deep-680 那在折紙的時候，第一次對折是最重要的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-681 對不對，你第一次對折就歪掉了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-682 下面你再折就通通是歪的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-683 所以，這一個實驗想要驗證的是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-684 low layer 的參數是比較重要的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-685 那怎麼驗證呢？他先做了一下 CIFAR-10
Deep_Learning_Theory_1-2_-_Potential_of_Deep-686 他在 CIFAR-10 上面拿出一個正確率非常高的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-687 然後，在 network 的參數上面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-688 加一些 node，那分別加在
Deep_Learning_Theory_1-2_-_Potential_of_Deep-689 第一層、第二層，一直加到第七層
Deep_Learning_Theory_1-2_-_Potential_of_Deep-690 那發現說，假設現在
Deep_Learning_Theory_1-2_-_Potential_of_Deep-691 那些 noise 是加在最後第七層的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-692 對結果幾乎沒有影響
Deep_Learning_Theory_1-2_-_Potential_of_Deep-693 下面這個是 noise 越加越大
Deep_Learning_Theory_1-2_-_Potential_of_Deep-694 noise 越加越大
Deep_Learning_Theory_1-2_-_Potential_of_Deep-695 對縱軸這個正確率，本來正確率是 100%
Deep_Learning_Theory_1-2_-_Potential_of_Deep-696 但是，你加一些 noise 幾乎沒什麼影響
Deep_Learning_Theory_1-2_-_Potential_of_Deep-697 但是，如果你加在第一層
Deep_Learning_Theory_1-2_-_Potential_of_Deep-698 一樣的 noise，加在第一層，整個結果就壞掉了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-699 整個結果就突然爆炸這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-700 顯示說，第一層的 network 是非常 sensitive 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-701 你只要稍微加一點 noise，他就會壞掉了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-702 另外這個實驗，如果沒記錯，應該是做在 MNIST 上面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-703 縱軸是正確率，這個實驗是這樣子的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-704 這個實驗是說，拿一個 network 出來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-705 我們只 train 某一個 layer
Deep_Learning_Theory_1-2_-_Potential_of_Deep-706 其他 layer fix 住都是 random 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-707 只 train 一個 layer，其他都 fix 住
Deep_Learning_Theory_1-2_-_Potential_of_Deep-708 這邊的顏色跟這邊是一樣的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-709 所以，這個紫色就是這邊的紫色
Deep_Learning_Theory_1-2_-_Potential_of_Deep-710 假設我們只 train 第一層
Deep_Learning_Theory_1-2_-_Potential_of_Deep-711 第二層到最後一層通通是 error，通通是 random 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-712 其實你也可以得到大概 90% 的正確率
Deep_Learning_Theory_1-2_-_Potential_of_Deep-713 其實這沒有很高，因為 MNIST
Deep_Learning_Theory_1-2_-_Potential_of_Deep-714 MNIST 胡亂做都是 98% 這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-715 所以 90% 是很差這樣子，但是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-716 神奇的就是說，我只 learn 了第一層
Deep_Learning_Theory_1-2_-_Potential_of_Deep-717 後面都是 random
Deep_Learning_Theory_1-2_-_Potential_of_Deep-718 還是有 90%，顯示第一層非常的重要
Deep_Learning_Theory_1-2_-_Potential_of_Deep-719 但是，假設我們是說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-720 前面都是 random
Deep_Learning_Theory_1-2_-_Potential_of_Deep-721 只 learn 最後一層
Deep_Learning_Theory_1-2_-_Potential_of_Deep-722 結果就很爛這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-723 所以這告訴我們說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-724 deep network 裡面，前面的 layer 是比較 sensitive，比較重要的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-725 我們在這邊休息 10 分鐘，等一下再回來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-726 剛才講的是用 shallow 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-727 來 fit 某一個 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-728 剛才又講了說假設比較 deep 和 shallow network 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-729 deep 的 network 可以製造出比較多的片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-730 但是，這並沒有完全的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-731 deep 的 network 可以製造出比較多片段這件事情
Deep_Learning_Theory_1-2_-_Potential_of_Deep-732 跟我們要 fit 某一個 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-733 並沒有直接的相關
Deep_Learning_Theory_1-2_-_Potential_of_Deep-734 我們現在真正關心的問題是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-735 如果用 deep 的 structure 來 fit 某一個 function 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-736 會是什麼樣子？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-737 那我們假設我們現在要 fit 的 function 呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-738 是一個 比較簡單的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-739 從這個比較簡單的 function 討論起
Deep_Learning_Theory_1-2_-_Potential_of_Deep-740 這個 function 是 f(x) = x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-741 他長得就是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-742 x^2 在 0 到 1 區間呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-743 長的是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-744 現在就算是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-745 我們不管是用 shallow 的 structure
Deep_Learning_Theory_1-2_-_Potential_of_Deep-746 還是 deep 的 structure
Deep_Learning_Theory_1-2_-_Potential_of_Deep-747 我們製造出來的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-748 都是 piecewise linear 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-749 所以在討論怎麼用一個 shallow 的 network 或者甚至是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-750 deep 的 network 來 fit 某一個 function 之前
Deep_Learning_Theory_1-2_-_Potential_of_Deep-751 我們要討論第一件事情
Deep_Learning_Theory_1-2_-_Potential_of_Deep-752 都是怎麼用一個 piecewise 的 linear 的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-753 去 fit 我們現在的 target function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-754 我們現在 target function 是 x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-755 怎麼用一個 piecewise linear function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-756 去 fit 這一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-757 f(x) = x^2 這個 function 呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-758 我們現在定義另外一個 function 呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-759 叫做 fm(x)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-760 那這個 fm(x) 他總共會有 2^m 個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-761 那 f1(x) 長的是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-762 長得是這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-763 這個 f1(x) 怎麼來呢？你就把這個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-764 你就取 0.5 的地方
Deep_Learning_Theory_1-2_-_Potential_of_Deep-765 你就取 0.5 的地方
Deep_Learning_Theory_1-2_-_Potential_of_Deep-766 然後跟頭接起來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-767 跟尾巴接起來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-768 就得到兩個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-769 所以今天 f1(x) 呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-770 總共有 2^1，也就是兩個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-771 接下來 f2(x) 呢？f2(x) 有 4 個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-772 這 4 個片段怎麼產生呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-773 你就把 x 軸，橫軸啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-774 橫軸等於 0 到 1/4 的地方接起來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-775 x 軸等於 1/4 到 1/2 的地方接起來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-776 1/2 到 3/4 的地方接起來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-777 3/4 到 1 的地方接起來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-778 我知道你在台下其實很難看得清楚這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-779 但是畫出來就是這個樣子，我也沒有辦法這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-780 所以 f2(x) 他有 4 個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-781 2^2，總共 4 個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-782 他是從 0 到 1/2（此處口誤，應為 1/4）
Deep_Learning_Theory_1-2_-_Potential_of_Deep-783 0 到 1/4，1/4 到 1/2，1/2 到 3/4，3/4 到 1
Deep_Learning_Theory_1-2_-_Potential_of_Deep-784 總共四個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-785 那你會發現說其實
Deep_Learning_Theory_1-2_-_Potential_of_Deep-786 f2(x) 其實跟 x^2 其實也滿接近的，所以
Deep_Learning_Theory_1-2_-_Potential_of_Deep-787 你有點看不清楚 f2(x) 在哪裡
Deep_Learning_Theory_1-2_-_Potential_of_Deep-788 哪如果你畫 f3(x) 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-789 他就有八個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-790 畫 f4(x) 的話，他就有 16 個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-791 以此類推
Deep_Learning_Theory_1-2_-_Potential_of_Deep-792 接下來，我們問的問題是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-793 這個 m 的值到底要到多少的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-794 就是我們先給定這個 ε
Deep_Learning_Theory_1-2_-_Potential_of_Deep-795 m 的值要到多少的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-796 我們才能夠讓 f(x)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-797 跟 fm(x) 他們之間的最大的差
Deep_Learning_Theory_1-2_-_Potential_of_Deep-798 小於等於 ε 這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-799 當然我們知道說，m 越多
Deep_Learning_Theory_1-2_-_Potential_of_Deep-800 fm(x) 跟 f(x) 之間的差異就越小
Deep_Learning_Theory_1-2_-_Potential_of_Deep-801 你這邊畫越多的片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-802 那你畫圖做出來的 fm(x) 跟 f(x)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-803 也就是 x^2，他們就越接近
Deep_Learning_Theory_1-2_-_Potential_of_Deep-804 但是，假設今天他們的差距啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-805 不可以超過 ε，要小於等於 ε 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-806 這個 m 應該要有多少呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-807 你實際上算一下的話，這邊就省略掉計算過程
Deep_Learning_Theory_1-2_-_Potential_of_Deep-808 你就回去怒算一波這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-809 如果我算出來有錯你再告訴我這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-810 你就回去怒算一波，你就知道說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-811 今天要如何讓，m 要多少
Deep_Learning_Theory_1-2_-_Potential_of_Deep-812 才能夠小於等於 ε 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-813 才能夠讓他們差距小於等於 ε 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-814 這個 m 要大於等於 -1/2log2(ε)-1 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-815 這個 m 就會小於等於 ε
Deep_Learning_Theory_1-2_-_Potential_of_Deep-816 你說怎麼算哦
Deep_Learning_Theory_1-2_-_Potential_of_Deep-817 你就把 fm(x) 的式子列出來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-818 然後你就可以算他跟 x^2 的差距
Deep_Learning_Theory_1-2_-_Potential_of_Deep-819 這個計算並不困難，他只是有點繁瑣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-820 然後，你就可以算出說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-821 怎麼樣可以讓他們最大的差距小於等於 ε
Deep_Learning_Theory_1-2_-_Potential_of_Deep-822 總之，你算出來就是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-823 你說，欸這邊有一個負號
Deep_Learning_Theory_1-2_-_Potential_of_Deep-824 那是不是這一項是負的呢？不是啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-825 ε 是小於 1 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-826 ε 是一個很小的值
Deep_Learning_Theory_1-2_-_Potential_of_Deep-827 懂嗎？ε 是一個很小的值
Deep_Learning_Theory_1-2_-_Potential_of_Deep-828 所以 log2(ε) 是負的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-829 負的東西乘上負的東西是正的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-830 然後，再減掉 1 這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-831 ε 可能是一個很小的值
Deep_Learning_Theory_1-2_-_Potential_of_Deep-832 比如說，2^(-7)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-833 log 這邊就是變成 -7
Deep_Learning_Theory_1-2_-_Potential_of_Deep-834 (-7) * (-1/2) 變成正的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-835 所以 m 要大於等於這個數值
Deep_Learning_Theory_1-2_-_Potential_of_Deep-836 才能夠讓它小於等於 ε
Deep_Learning_Theory_1-2_-_Potential_of_Deep-837 當然今天 ε 的值
Deep_Learning_Theory_1-2_-_Potential_of_Deep-838 越小，m 就要越大，這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-839 那 m 是這個樣子，需要多少個片段呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-840 那你就這邊取 2^m
Deep_Learning_Theory_1-2_-_Potential_of_Deep-841 這邊取 2 的這一項次方
Deep_Learning_Theory_1-2_-_Potential_of_Deep-842 所以，你今天需要的片段的數目
Deep_Learning_Theory_1-2_-_Potential_of_Deep-843 至少要大於等於 1/2 * 1/sqrt(ε) 個片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-844 這個值怎麼來的呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-845 你就是取 2 的這個次方
Deep_Learning_Theory_1-2_-_Potential_of_Deep-846 你就把這個東西呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-847 放在指數項，2 的這個次方
Deep_Learning_Theory_1-2_-_Potential_of_Deep-848 就是 1/2 * 1/sqrt(ε)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-849 講到這邊，假設你沒有跟上的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-850 你只要知道說，現在有一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-851 x^2 的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-852 我們今天，如果要用
Deep_Learning_Theory_1-2_-_Potential_of_Deep-853 2 的 m 次方個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-854 一樣寬的片段
Deep_Learning_Theory_1-2_-_Potential_of_Deep-855 去 fit x^2 這個 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-856 那我們的 2^m 的數目
Deep_Learning_Theory_1-2_-_Potential_of_Deep-857 一定要大於 1/2 * 1/sqrt(ε)p[ieces
Deep_Learning_Theory_1-2_-_Potential_of_Deep-858 那你會發現說，這個東西算起來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-859 其實是比剛才，我們在第一堂課裡面得到的結果
Deep_Learning_Theory_1-2_-_Potential_of_Deep-860 l/ε 還要小的，對不對？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-861 你想想看，這邊他是除 ε
Deep_Learning_Theory_1-2_-_Potential_of_Deep-862 這邊是除根號 ε
Deep_Learning_Theory_1-2_-_Potential_of_Deep-863 所以，如果你算他的 big O 的話，這一項
Deep_Learning_Theory_1-2_-_Potential_of_Deep-864 是比較小的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-865 那這個結果其實也很合理的，因為
Deep_Learning_Theory_1-2_-_Potential_of_Deep-866 因為在第一堂課我們討論的是 general 的 case 嘛
Deep_Learning_Theory_1-2_-_Potential_of_Deep-867 任意的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-868 這邊我們只討論 x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-869 那你需要比較少的，根據 x^2 的特性
Deep_Learning_Theory_1-2_-_Potential_of_Deep-870 所以，你需要比較少的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-871 你需要比較少的片段就可以
Deep_Learning_Theory_1-2_-_Potential_of_Deep-872 fit 他，這個結果也是頗為合理的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-873 那現在，我們剛才講過說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-874 假設是一個 shallow network 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-875 你要兩個 ReLU
Deep_Learning_Theory_1-2_-_Potential_of_Deep-876 才能夠產生一個 piece
Deep_Learning_Theory_1-2_-_Potential_of_Deep-877 但其實更少一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-878 有時候一個 ReLU 就可以產生一個 piece
Deep_Learning_Theory_1-2_-_Potential_of_Deep-879 無論如何，你需要的 neuron 的數目
Deep_Learning_Theory_1-2_-_Potential_of_Deep-880 就是 O(1/sqrt(ε))
Deep_Learning_Theory_1-2_-_Potential_of_Deep-881 你需要至少這麼多的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-882 就假設你是 shallow 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-883 你需要至少這麼多的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-884 才可以 fit f(x) = x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-885 這個是 shallow 的狀況
Deep_Learning_Theory_1-2_-_Potential_of_Deep-886 那我們來看一下 deep 的狀況
Deep_Learning_Theory_1-2_-_Potential_of_Deep-887 我們來看一下 deep 的狀況
Deep_Learning_Theory_1-2_-_Potential_of_Deep-888 deep 他的厲害的地方就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-889 他要製造出這麼多 pieces
Deep_Learning_Theory_1-2_-_Potential_of_Deep-890 其實她並不需要這麼多的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-891 如果我們要用 deep 的 network 來產生 f(x)=x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-892 那要怎麼做呢？你看哦
Deep_Learning_Theory_1-2_-_Potential_of_Deep-893 你要產生 f1(x) 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-894 你要產生 f1(x)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-895 你其實只需要把一條
Deep_Learning_Theory_1-2_-_Potential_of_Deep-896 斜率是 1 的斜線
Deep_Learning_Theory_1-2_-_Potential_of_Deep-897 減掉這一個東西
Deep_Learning_Theory_1-2_-_Potential_of_Deep-898 他就是 f1(x)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-899 這樣大家可以接受嗎？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-900 想想看哦，這個是，我們先把
Deep_Learning_Theory_1-2_-_Potential_of_Deep-901 x^2 畫出來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-902 x^2 假設畫出來是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-903 把 f1(x) 畫出來這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-904 假設 f1(x) 畫出來是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-905 假設 f1(x) 畫出來是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-906 換藍色好了，是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-907 那 f1(x) 跟這一個斜率是 1 的直線
Deep_Learning_Theory_1-2_-_Potential_of_Deep-908 他們中間這邊差多少
Deep_Learning_Theory_1-2_-_Potential_of_Deep-909 假設這邊是 0.5 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-910 這邊的高是 (0.5)^2 是 0.25
Deep_Learning_Theory_1-2_-_Potential_of_Deep-911 這邊的高
Deep_Learning_Theory_1-2_-_Potential_of_Deep-912 這是一個斜率是 1 的直線
Deep_Learning_Theory_1-2_-_Potential_of_Deep-913 我知道我畫的很不標準，但是這邊的高
Deep_Learning_Theory_1-2_-_Potential_of_Deep-914 是0.5，所以這邊的差距是 0.25
Deep_Learning_Theory_1-2_-_Potential_of_Deep-915 所以，你把這一個斜線
Deep_Learning_Theory_1-2_-_Potential_of_Deep-916 減掉這中間的差
Deep_Learning_Theory_1-2_-_Potential_of_Deep-917 這邊最寬最高的地方是 0.25
Deep_Learning_Theory_1-2_-_Potential_of_Deep-918 後面會慢慢變小這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-919 這邊是 0，他頭尾的地方都是 0
Deep_Learning_Theory_1-2_-_Potential_of_Deep-920 中間差距最大是 0.5
Deep_Learning_Theory_1-2_-_Potential_of_Deep-921 就會變成這個 f1(x)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-922 這一個 piecewise linear 的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-923 這樣大家可以接受嗎？所以其實你把
Deep_Learning_Theory_1-2_-_Potential_of_Deep-924 這一個藍色的斜線
Deep_Learning_Theory_1-2_-_Potential_of_Deep-925 減掉這一個三角形
Deep_Learning_Theory_1-2_-_Potential_of_Deep-926 這個三角形就是這邊這一塊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-927 雖然看起來不像，但他就是啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-928 減掉這一塊現在塗顏色的地方
Deep_Learning_Theory_1-2_-_Potential_of_Deep-929 他就變成了 f1(x) 這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-930 接下來怎麼
Deep_Learning_Theory_1-2_-_Potential_of_Deep-931 所以我們現在可以用這兩個 function 相減
Deep_Learning_Theory_1-2_-_Potential_of_Deep-932 製造出 f1(x)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-933 怎麼製造出 f2(x) 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-934 你就再產生這樣子、這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-935 有兩個鋸齒的形狀
Deep_Learning_Theory_1-2_-_Potential_of_Deep-936 第一個鋸齒減在這邊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-937 第二個鋸齒減在這邊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-938 你就產生 f2(x) 了，這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-939 這樣 ok 嗎？就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-940 講到這邊大家還有問題要問的嗎？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-941 你說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-942 這兩個三角形嗎？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-943 他們高度是一樣的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-944 你可以自己 check 一下他們高度是一樣的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-945 那這個，我們就不要畫圖好了，這個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-946 這個圖其實是很難畫的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-947 你如果不相信就回去 check 看看這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-948 總之，你把這個斜線
Deep_Learning_Theory_1-2_-_Potential_of_Deep-949 減掉第一個三角形
Deep_Learning_Theory_1-2_-_Potential_of_Deep-950 你就得到藍色這條線
Deep_Learning_Theory_1-2_-_Potential_of_Deep-951 然後再減掉第二個這樣子的，兩個鋸齒的三角形
Deep_Learning_Theory_1-2_-_Potential_of_Deep-952 減掉兩個鋸齒的三角形
Deep_Learning_Theory_1-2_-_Potential_of_Deep-953 這滑鼠很難指
Deep_Learning_Theory_1-2_-_Potential_of_Deep-954 兩個鋸齒的三角形
Deep_Learning_Theory_1-2_-_Potential_of_Deep-955 你就得到 x2 這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-956 你就得到 f2(x)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-957 所以今天呢，如果我們要
Deep_Learning_Theory_1-2_-_Potential_of_Deep-958 產生，所以我們知道說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-959 這個一個斜線減掉這個就是 f1(x)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-960 一個斜線減掉這個，再減掉這個就是 f2(x)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-961 如果我們今天要產生 fm(x) 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-962 那我們知道 ｍ 的值要大於等於這一項
Deep_Learning_Theory_1-2_-_Potential_of_Deep-963 如果我們要產生 fm(x) 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-964 那我們做的事情就是把這條斜線
Deep_Learning_Theory_1-2_-_Potential_of_Deep-965 減掉一個三角形，再減兩個三角形
Deep_Learning_Theory_1-2_-_Potential_of_Deep-966 減四個三角形，減...
Deep_Learning_Theory_1-2_-_Potential_of_Deep-967 對，沒錯，減一個三角形
Deep_Learning_Theory_1-2_-_Potential_of_Deep-968 減掉一個三角形，兩個三角形
Deep_Learning_Theory_1-2_-_Potential_of_Deep-969 四個三角形，八個三角形
Deep_Learning_Theory_1-2_-_Potential_of_Deep-970 一直減下去，直到減到
Deep_Learning_Theory_1-2_-_Potential_of_Deep-971 2^m 個三角形這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-972 那他們的高度你要考慮一下
Deep_Learning_Theory_1-2_-_Potential_of_Deep-973 第一個三角形他的高度是 1/4
Deep_Learning_Theory_1-2_-_Potential_of_Deep-974 兩個三角形的時候，高度是 1/16
Deep_Learning_Theory_1-2_-_Potential_of_Deep-975 2^m 個三角形的時候，他的高度是 1/(4^m)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-976 接下來呢，你要做的事情就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-977 產生這一連串的三角形這樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-978 那怎麼產生這一連串的三角形呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-979 其實，就跟我們在上一堂課講的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-980 我們不是說，用這個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-981 V 字型的，用這個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-982 取絕對值的 neuron 其實就是兩個 ReLU
Deep_Learning_Theory_1-2_-_Potential_of_Deep-983 我們在第一層就可以製造出
Deep_Learning_Theory_1-2_-_Potential_of_Deep-984 一個 v 的形狀
Deep_Learning_Theory_1-2_-_Potential_of_Deep-985 再把 v 的形狀乘上一個負號，就是一個三角形了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-986 那第二層你可以製造出一個 w
Deep_Learning_Theory_1-2_-_Potential_of_Deep-987 把 w 的形狀乘上一個負號
Deep_Learning_Theory_1-2_-_Potential_of_Deep-988 就是變成一個 ｍ 的形狀，就變成兩個三角形了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-989 一直到你總共有 m 層
Deep_Learning_Theory_1-2_-_Potential_of_Deep-990 到第 ｍ 層的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-991 你就可以製造這個有 2^m 個三角形的鋸齒狀的圖
Deep_Learning_Theory_1-2_-_Potential_of_Deep-992 所以今天，假設你要製造這些 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-993 我們假設你要製造這些 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-994 其實你只需要一個
Deep_Learning_Theory_1-2_-_Potential_of_Deep-995 一個有 m 層的 ReLU 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-996 其實就可以辦到了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-997 所以，如果我今天要製造這個 fm(x)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-998 其實我只需要產生這個東西，那這個東西很簡單
Deep_Learning_Theory_1-2_-_Potential_of_Deep-999 even 不需要 activation function 就可以製造
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1000 就 input = output 嘛
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1001 就 input = output，就原來的 x
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1002 然後再減掉 a1
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1003 再減掉 a2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1004 再一直減到 am，只是你要稍微註明一下這中間的 scalar
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1005 你其實就製造出 fm(x) 了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1006 所以，假設你要製造 fm(x) 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1007 你只需要 m 個 layer
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1008 你只需要 O(m) 個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1009 然後，總之 O(m) 個 layer
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1010 就可以製造這種 activation function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1011 如果你把這個 ｍ 代 -1/2 log2(ε) - 1 的話
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1012 那這個 -1/2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1013 這個可以提到 log 裡面啦，變成
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1014 log2(1/sqrt(ε))
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1015 所以你只需要 log2(1/sqrt(ε)) 個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1016 然後 log2(1/sqrt(ε)) 個 layer
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1017 其實就可以產生、就可以去逼近
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1018 y = x^2 這樣的 target function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1019 那你可能會想說，好像只討論了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1020 就假設剛才的東西你都沒跟上的話，你就只要知道說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1021 如果我們今天用 deep 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1022 因為 deep 的 network 可以輕易地產生這種鋸齒的形狀
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1023 所以我們就可以輕易地 fit y = x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1024 比 general 的，用 shallow 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1025 還要的 neuron 少很多
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1026 那為什麼我們在意 y = x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1027 你可能覺得說只考慮 y = x^2 非常的 limited
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1028 但其實不會， y = x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1029 他有很多的妙用
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1030 怎麼用 y = x^2 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1031 我們現在知道說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1032 我們只要 O(log2(1/sqrt(ε))) 個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1033 我們就可以製造一個 Square Net
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1034 這個 Square Net 他做的事情
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1035 不像現在 R Net 都做一些很複雜的事情
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1036 他就是乘平方
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1037 然後他的誤差會小於等於 ε
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1038 那我們能夠製造 Square Net 以後
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1039 我們就可以製造一種特殊的 network 叫做
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1040 Multiply Net
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1041 他 Multiply Net 做的事情就是給他 x1, x2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1042 他給你 output 把 x1, x2 相乘
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1043 怎麼從 Square Net
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1044 製造 Multiply Net 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1045 因為 y = x1x2 = 1/2[(x1+x2)^2 - x1^2 - x2^2]
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1046 你只要把 x1, x2 的平方展開
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1047 減 x1 平方、減 x2 平方，再乘以 1/2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1048 那你就得到了 x1, x2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1049 所以，今天我們只要會做 Square Net
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1050 你接下來就可以用三個 Square Net
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1051 拼出一個 Multiply Net
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1052 怎麼拼呢？我們先把 x1, x2 加起來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1053 丟到 Square Net 裡面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1054 然後把 x1 獨自丟到 Square Net 裡面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1055 把 x2 獨自丟到 Square Net 裡面
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1056 然後這邊就是算出 (x1 + x2)^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1057 這邊算出 x2^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1058 這邊算出 x1^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1059 這三項就是這邊的這三項
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1060 把它算出來都乘上 1/2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1061 加起來就得到 x1*x2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1062 所以，今天我們能夠用
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1063 這麼多的 neuron 做出 Square Net
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1064 我們就是用三倍的 neuron 就可以做出 Multiply Net 了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1065 那 O 的這個 complexity 是不變的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1066 因為你只是在前面乘上一個常數而已
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1067 能夠做 Multiply Net 以後呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1068 接下來你就可以做 Polynomial
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1069 因為你就可以做，至少你可以先做 y = x^n
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1070 怎麼做 y = x^n 呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1071 其實很簡單，你就用一個 Square Net，把 x 變成 x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1072 接下來我們會做 Multiply Net
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1073 你就會把 x^2 跟 x 乘起來變成 x^3
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1074 你還可以把 x^3 跟 x 乘起來就變成 x^4
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1075 所以今天要產生 x^n
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1076 沒有問題，你可以用一堆的 Multiply Net
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1077 就可以做到這件事情
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1078 但這不是唯一的方法
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1079 你永遠可以想一下別的方法，舉例來說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1080 你也可以只用 Square Net 就算出 x^4 等等
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1081 總之你有 Square Net、Multiply Net，你就可以算 y = x^n
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1082 而這邊的每一個 block
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1083 需要的 neuron 的 complexity 啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1084 是這麼多，是 log2(1/sqrt(ε))
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1085 那接下來，你可以算 y = x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1086 你就製造一個東西叫做 Power(n) Net
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1087 這後面有一個 n
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1088 就是他可以把 input 的 x
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1089 乘一乘就變成 x^n，就叫他 Power(n) Net
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1090 你有 Power(n) Net 以後呢，你就可以
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1091 就可以產生 Polynomial 了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1092 你就可以產生 Polynomial，怎麼就產生 Polynomial
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1093 你用 Power(n) Net 產生 x^n
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1094 前面再乘 an
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1095 然後，你用 Power(n-1) Net，產生 x^(n-1)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1096 前面再乘 a(n-1)
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1097 把他們通通加起來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1098 你就可以產生任何你想要的 Polynomial 了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1099 那你說 Polynomial 不夠 general
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1100 其實  Polynomial 就夠 general 了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1101 你就可以用 Polynomial 去 fit 其他 continuous function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1102 就結束了
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1103 所以，我們現在可以用 deep structure
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1104 我們會做 x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1105 x^2 只要 O(log2(1/ε)) 個 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1106 接下來，最後就可以產生 Polynomial
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1107 我們就可以用 Polynomial 的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1108 去 fit 其他的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1109 我們就知道說，怎麼用 deep network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1110 的架構去 fit 其他的 function
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1111 當然，你可能會說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1112 在實作上，那個 network 不是這樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1113 這個 network 的參數不是像你這樣手設出來的啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1114 那個是 learn 出來的啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1115 我們現在還沒有討論那個問題
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1116 我們只是想一下，討論說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1117 假設你要 fit 某一個 function，有沒有辦法做到
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1118 實際上你找不找得到
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1119 那個 function，那是 optimization 問題
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1120 那是我們之後才要討論的問題
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1121 不是我們今天要討論的問題
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1122 所以，我們現在得到這樣的結果
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1123 我們要 fit y = x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1124 如果是 shallow 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1125 你的 neuron 的數目是 O(1/sqrt(ε))
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1126 那我們就把 1/sqrt(ε) 畫出來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1127 橫軸是 ε
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1128 然後把 1/sqrt(ε) 的線畫出來
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1129 那 deep network 是 log(1/sqrt(ε))
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1130 所以你發現說，他們中間的差距
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1131 是有一個 log 項
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1132 他們中間的差距有 exponential 那麼多
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1133 他們有 exponential 的差距
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1134 deep 和 shallow，你要達到同樣的 error 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1135 同樣的這個 error 的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1136 deep 他需要的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1137 是 shallow 需要的 neuron 再取 log
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1138 或者是說，deep 可以用某個數量的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1139 達成某個 accuracy
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1140 那 shallow 的 network 要 exponential 多的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1141 才可以達到那個 accuracy
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1142 但是，這樣子的討論呢
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1143 是不足夠的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1144 你覺得不足在哪裡呢？
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1145 你仔細想想看
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1146 你回憶一下列人第 20 集，比司吉告訴我們的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1147 比司吉說，假設有很多的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1148 每個人的能力範圍其實都是一個 range
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1149 今天我們拿 C 跟 A 來比較
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1150 然後 A，我們找他的一個 case
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1151 正好找到他，比如說，狀況特別差的一個 case
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1152 Ｃ我們找到一個 case
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1153 正好找到一個狀況特別好的 case
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1154 我們就會覺得說 C 可以贏過 A
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1155 但其實，那只是因為 A 正好選到狀況特別差的 case
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1156 也許 A 在最佳狀態的時候
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1157 他是可以打爆 C 的
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1158 所以今天的狀況也是一樣
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1159 我們剛才說 shallow
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1160 需要這麼多的 neuron 只是說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1161 我們想了一個方法需要這麼多的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1162 並不代表說那是最佳的 solution 啊
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1163 對不對？我們只是說
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1164 如果有這麼多 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1165 我可以 fit y = x^2
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1166 但是並不代表說，我一定要那麼多
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1167 才能 fit y = x^2，也許需要
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1168 也許只要比較少的 neuron
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1169 就可以辦到這件事情
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1170 也說不定，也許
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1171 這是 shallow network 他在很糟的狀態
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1172 他其實是 A，他在這邊，在很糟的狀態
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1173 也許他竭盡全力的時候是這個樣子
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1174 他可以打爆 deep
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1175 只是我們不知道而已
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1176 所以，接下來要問的下一個問題就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1177 假設我們讓
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1178 所以下一段要講的就是
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1179 假設我們現在讓 shallow 的 network
Deep_Learning_Theory_1-2_-_Potential_of_Deep-1180 竭盡全力，他能不能夠打爆 deep 呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-0 所以，接下來就是要看的就是 shallow 的 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-1 竭盡全力的狀態
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-2 那我們剛才反覆講過很多遍了
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-3 一個 ReLU 的 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-4 就是一個 piecewise 的 linear function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-5 我們現在要討論的就是
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-6 因為我們知道說，如果是一個 shallow 的 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-7 每一個 piece 我們都需要至少一個 neuron
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-8 才能夠製造出一個 piece
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-9 所以今天 piece 的數目愈少，需要的 neuron 就愈少
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-10 如果我們要想辦法 fit
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-11 y = x^2 的時候
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-12 到底最少要多少的 piece
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-13 才能做到這件事情呢
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-14 那我們剛才在討論的時候
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-15 我們在製造這個 piecewise 的 linear function 的時候
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-16 我們都是把我們目標的 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-17 是紅色這一條
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-18 紅色這條拿出來，然後在上面取幾個點
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-19 然後，把這些點連起來
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-20 就說這個是我們要去 fit target function 的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-21 piecewise linear 的 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-22 那能不能做得更好呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-23 我們假設其實是可以做得更好的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-24 怎麼樣做得更好，我們不要讓這些線段呢
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-25 他的頭還有尾跟紅色的線相接
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-26 對不對，因為就是這邊這個黑色的線段
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-27 我們之前在做的時候
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-28 我們都強迫他的這個頭跟尾
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-29 一定要跟我們目標的線
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-30 假如我們現在目標的線是 y = x^2
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-31 我們就假設他的頭跟尾
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-32 一定要跟目標的線接再在一起
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-33 我們現在能不能夠把這一條線
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-34 把他往下挪一點
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-35 不要讓他們頭接在一起，不要讓他們的尾接在一起
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-36 如果你這樣做的話
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-37 你從這張圖上，你直覺一看就知道說
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-38 這個 error 比較大
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-39 這麼做其實 error 是比較小的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-40 當然你可以，你可能會問說，左邊這個狀態
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-41 是確實是可以用 ReLU 產生這樣子的 network 的結構
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-42 但右邊這個狀態
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-43 這邊有一個非連續的東西
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-44 ReLU 能夠產生非連續的 output 嗎？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-45 你仔細想想應該是沒有辦法的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-46 那沒有關係，這個就是他的夢幻狀態這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-47 就是他在夢中可以使出彗星的一擊這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-48 所以，雖然實際上他只能這樣搞
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-49 他一定要每一個線段都接在一起
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-50 因為他必須要是連續的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-51 但是，我們就假設夢幻的狀態是
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-52 夢幻的狀態是不需要連續的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-53 那反正就先看看如果在不需要連續的狀況下
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-54 到底需要多少個片段
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-55 在不連續的狀況下到底需要多少個片段
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-56 我們才能夠去 fit
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-57 x^2 這個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-58 所以，這是個夢幻的狀態
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-59 那在夢幻的狀態我們可以有比較小的 error
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-60 另外，我們之前有講過
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-61 今天滿足上面這個條件就自動滿足了
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-62 下面這件事情
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-63 滿足下面這件事情
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-64 不見得滿足上面這件事情，對不對
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-65 因為你這邊是
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-66 他們兩個最大差距 ≦ ε
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-67 所以，你這個積分可以看出平均嘛
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-68 平均會 ≦ ε
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-69 但是平均 ≦ ε 並不代表他們最大的差距 ≦ ε
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-70 那沒有關係，我們現在不要管這一項
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-71 不要管最大差距，我們只看平均能不能 ≦ ε
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-72 那你說跟剛才我們狀態不一樣不太能比
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-73 沒關係，這個就是再給那個 shallow 的 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-74 一些 benefit 這樣
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-75 就好像說他的裁判什麼，通通都是自己人這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-76 就給他各種不同的優勢
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-77 各種不同的，不可能達成的夢幻的狀態
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-78 看說，在給他這麼多的優勢的情況下
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-79 他到底可以做到多好
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-80 所以現在呢，我們不考慮 max
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-81 我們不考慮 max，我們只考慮這個積分的結果
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-82 我們只考慮他的這個 Euclidean 的 norm
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-83 就這個 Euclidean 的 norm
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-84 那接下來我們現在要問的問題是
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-85 假設這個紅色的線就是 y = x^2
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-86 然後，在上面取一個線，取一個片段
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-87 這個片段的長度是 l，他的起始是 x_0
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-88 他的終止的位置是 x_0 + l，他的長度是 l
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-89 然後，我要用某一條直線去 fit
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-90 紅色這個 y = x^2
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-91 我到底可以 fit 多好呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-92 這條直線就如同我剛才講的，是一個夢幻的狀態
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-93 不需要頭對頭，不需要尾對尾
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-94 這個直線可以是任何一條直線
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-95 到底在給定 l 的狀態下，可以 fit 到多好呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-96 那你可以想像說我們其實就是，解這樣一個問題嘛
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-97 我們就算說，積分從 x_0 到 x_0 + l
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-98 積分從 x_0 到 x_0 + l
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-99 然後，我們要算 x^2 就是紅色這一條線跟 ax + b
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-100 也就是某一條直線 a 跟 b 的值是多少，我們還不知道
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-101 x^2 跟 ax + b 這一條直線
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-102 他們的差距的平方和
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-103 就得到了他的 error 的平方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-104 你就想，我們就是要算這一項
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-105 然後找出一個 a 跟 b 可以讓 e^2 的值最小
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-106 然後接下來，你就是怒算一波
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-107 你可以回去 check 一下，我有沒有算錯
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-108 算出來是 l^5/180 這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-109 如果這個是不對的，你再告訴我
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-110 我就不要把計算過程寫出來了
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-111 不然你等一下聽了就生氣了
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-112 這個是 l^5/180
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-113 所以，也就是說今天在
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-114 這個結果其實還蠻神奇的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-115 就是給一個線段，他的長度是 l
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-116 如果我今天考慮的是
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-117 y = x^2 在任何區間裡面
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-118 他的誤差都只跟 l 有關
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-119 他的誤差最小就是 l^5/180
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-120 但他是怎麼做的呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-121 這邊給大家一些方向
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-122 如果這一段你沒有聽懂的話沒有關係
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-123 就是你沒有聽懂就算了的意思
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-124 大家記不記得我們在線性代數的時侯，我們這樣學
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-125 我們學說，有兩個 function 叫做 w 跟 u
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-126 然後，我們希望要 minimize
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-127 我們希望把 w 跟 u，做 linear combination
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-128 找一個 w 跟 u 的 linear combination 的結果
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-129 aw + bu，然後希望這個新的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-130 linear combination 得出來的 vector
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-131 跟 v 的距離越近越好
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-132 那怎麼找呢？我們就是找出
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-133 w 跟 u 他們展開的那個 space
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-134 你要找，就假設 u 跟 w 就是
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-135 他們是 orthogonal basis 的就可以直接展開這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-136 如果不是，你就另外想辦法
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-137 反正就是把 u 跟 w 展開成一個 basis
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-138 展開成一個 basis，然後
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-139 把 v project 到那一個 basis 上面
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-140 你就可以知道 aw + bu 是哪一個了
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-141 就這樣，我們在線性代數學到這個東西
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-142 那其實在線性代數課本的最後一章有說
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-143 每一個 function 都是一個 vector 對不對
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-144 我們都可以用線性代數學到的 operation
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-145 來處理這些 function，所以我們假設
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-146 某一個
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-147 function 就是 x^2 ，x^2 是一個 function 叫做 fv
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-148 然後 x 是一個 function 叫做 fw
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-149 然後，常數項是一個 function 叫做 fu
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-150 我們現在要做的事情是希望
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-151 找到一組 a 跟 b，把 fw 跟 fu 做 linear combination
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-152 讓他跟 fv 的距離越接近越好
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-153 那我們要定義什麼叫做兩個 function 間的距離
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-154 那我定義兩個 function 間的距離
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-155 就是這個樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-156 就是計算從積分 x0 到 x0 + l
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-157 這兩個 fumction 的差的平方和
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-158 就是他們之間的距離
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-159 然後，接下來，你就可以說我們找出
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-160 afw + bfu 的這個展開的那個 space
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-161 然後把 fv project 到那一個 space 上面
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-162 然後，你就可以找出 a 跟 b
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-163 然後，怒算一波，就結束了這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-164 就這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-165 總之，無論如何就變成
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-166 反正就跳過去
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-167 然後算出來結果，就是
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-168 總之 error 是 l^5/180
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-169 那接下來狀況是這個樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-170 我們假設，我們可以放 n 個線段
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-171 那 n 個線段放上去的時候，我們得到的 error 的最小值
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-172 就我們要如何分配這 n 個線段
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-173 使得他的 error 的值越小越好呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-174 如果可以的話，可以小到什麼樣的地步
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-175 我們如何分配這 n 個線段
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-176 使得 error 的值最小，所以
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-177 我們在 0 到 1 之間，就給他切 n 個線段
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-178 這 n 個線段不用是一樣長的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-179 反正就想辦法讓
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-180 想辦法分配這個 n 個線段的位置
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-181 想辦法分配 l1 到 ln 的長度
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-182 我們希望最後算出來的 error 是最小的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-183 那 l1 加到 ln 的和阿
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-184 會等於 1，這是唯一的 constraint
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-185 因為我們要考慮的就是 0 到 1 的區間嘛
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-186 我們把他分成 n 個區間，把 0 到 1 之間切 n 份
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-187 那每一份的長度就是 l1、l2 一直到 ln
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-188 l1 + l2 + ln 合起來
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-189 他的值應該要是 1
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-190 那假設這邊的長度是 l1 的話
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-191 那他的 error, e1^2 就是 l1^5/180
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-192 e2^2 就是 l2^5/180
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-193 e3^2 就是 l3^5/180，以此類推
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-194 所以 total 的 error
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-195 我們用 e^2 代表 total error 平方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-196 就是每一項 e1, e2 一直到 en
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-197 每一項 e 的平方和
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-198 那每一項 e, ei 是 li^5/180
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-199 所以 e^2 是 summation n = 1 到 n，li^5 /180
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-200 那接下來的問題就是，怎麼分配 l1 到 ln
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-201 使得這個值最小這樣
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-202 就假設我們現在只能夠用 n 個片段
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-203 來 fit 這個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-204 那我們要 fit function 是 y = x^2
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-205 我們怎麼分配 l1 到 ln 使得
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-206 total 的 error 的值最小
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-207 我們知道 total error 的值，寫成這樣
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-208 summation i = 1 到 n，li^5/180
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-209 li 唯一的限制是這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-210 我講到這邊，大家有問題想要問的嗎？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-211 其實就算沒有證明，你直覺知道這一題的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-212 答案是什麼呢？應該是平均分配吧，對不對
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-213 你問為什麼，沒有為什麼，平均分配就
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-214 其實等一下，我告訴你為什麼
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-215 但我猜你大概也不想知道這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-216 你直覺想就知道平均分配一定
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-217 可以讓他的 e^2 最小這樣
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-218 我直覺就覺得應該是平均分配會讓 e^2 最小
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-219 如果平均分配的話
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-220 那 li 就等於 1/n
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-221 那 li = 1/n，把 1/n 代進去
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-222 代進去，所以 e^2
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-223 就是 summation i = 1 到 n，1/n^5/180
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-224 然後這邊這個重複 n 次嘛，對不對
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-225 這項重複 n 次所以乘個 n
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-226 所以 1/180 * 1/(n^4)
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-227 講到這邊大家有問題要問的嗎？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-228 你可能問說為什麼要平均分配，這邊有個
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-229 warning of math 告訴你為什麼要平均分配
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-230 怎麼平均分配呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-231 這邊要用一個 Holder's inequality 這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-232 怎麼說呢？我們現在的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-233 就是說，前面這邊有一個 180 這樣
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-234 我們不要管那個 180
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-235 因為這個不重要，他不會影響你的結果
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-236 我們只要知道說 summation i = 1 到 n li^5
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-237 要怎麼讓他的值最小這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-238 怎麼做呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-239 有一個 inequality 是這樣子說的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-240 我們現在有 n 個值 a1 到 an
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-241 有 n 個值 b1 到 bn
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-242 有兩個數值 t 跟 q 然後
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-243 1/t + 1/q 的值會等於 1
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-244 這個時候有一個 inequality 告訴我們說
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-245 ai * bi 的絕對值
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-246 從 i = 1 加到 n 會小於等於
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-247 ai^p 從 i = 1 加到 n
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-248 再開 p 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-249 然後 bi^q summation i = 1 到 n，再開 q 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-250 然後，接下來，我們只需要把
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-251 a1 跟 an 還有 b1 跟 bn 換成我們想要的東西
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-252 把 ai 跟 an 換成 l1 到 ln
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-253 b1 到 bn 通通換成 1
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-254 我們就可以簡化上面這個式子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-255 ai * bi，li * 1 就是 li
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-256 那 l 一定是正的，所以絕對值也不用了
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-257 因為 l 一定是正的嘛
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-258 他是線段的長度，所以他一定是正的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-259 所以，絕對值也不用
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-260 那 ai 就是 l 把他放到這邊
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-261 bi 就是 1 把他放到這邊
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-262 所以，我們得到了這一個不等式
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-263 那這個不等式可以輕易的簡化他
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-264 因為根據我們對 l 的限制
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-265 summation i = 1 到 n li 他的值
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-266 就是 1，所有線段 total 的和就是 1
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-267 1^q summation i = 1 到 n 就是 n
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-268 所以，這邊這一項是 1
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-269 這一項是 n 的 1/q 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-270 把 n 的 1/q 次方拿到左邊去
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-271 變成 n 的 -1/q 次方，這邊是 n 的 1/q 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-272 拿到左邊去，變成 n 的 -1/q 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-273 小於等於 summation i = 1 到 n li 的 p 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-274 再取 1/p 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-275 然後，這邊是 1/p 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-276 然後，兩邊都取 p 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-277 所以這邊 1/p 次方就拿到左邊
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-278 變成 n 的 -1/q 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-279 小於等於 summation i = 1 到 n li 的 p 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-280 然後這個 p/q 是多少呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-281 你就知道說 1/p + 1/q = 1
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-282 我們要把 p/q 製造出來
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-283 所以左右同乘以 p
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-284 所以 p/p 是 1，p/q 是 p/q
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-285 然後把 p 乘過來
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-286 p 在右邊，左右都乘 p
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-287 所以 1 + p/q 就等於 p
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-288 所以，負的 p/q 就是這一項，等於多少呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-289 把 p/q 拿到右邊去變成 1 - p
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-290 所以這一項是 1 - p
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-291 然後 p 代 5
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-292 這邊就有 summation li^5 了
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-293 這是我們要 minimize 的這一項
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-294 我們知道說，他的最小值
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-295 就是 n 的 1 - p 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-296 也就是 n 的 -4 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-297 跟我直覺想的是一樣的這樣
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-298 這沒什麼好笑的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-299 就是說，你直覺也覺得說應該是
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-300 每一個片段一樣應該會最小
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-301 果然怒導一波以後，是這個樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-302 這個不重要，假設你剛剛沒有聽懂的話真的就算了
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-303 那現在得到的結果阿
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-304 是這個樣子的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-305 E 的最小值
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-306 我的滑鼠還在嗎？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-307 E 的 E^2 的最小值是 (1/180) * (1/n^4)
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-308 所以 E 的最小值阿
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-309 他的最小值就是 sqrt(1/180) * (1/n^2)
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-310 這是他的 error
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-311 他的 error 的 lower bond 是這個樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-312 那我們希望這個 lower bond
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-313 他小於等於我們給定的誤差 ε
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-314 那我們希望這個 E 這一項
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-315 小於等於我們給定的誤差 ε 的話
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-316 那我們需要多大的 n 呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-317 我們 sqrt(1/180) * (1/n^2) 要 ≦ ε 的話
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-318 那我們會希望把 n^2 項拿上來
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-319 ε 項拿下去
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-320 我們希望 n^2 ≧ sqrt(1/180)*(1/ε)
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-321 我們希望 n 大於等於，就再開一次根號
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-322 變成 1/180 開 4 方根號，乘上 sqrt(1/ε)
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-323 所以，我們需要這麼多的 n
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-324 這麼多的 linear 的 piece
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-325 才能夠讓
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-326 才能夠讓 error ≦ ε
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-327 那我們知道說每一個 piece 都需要一個
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-328 在 shallow 的狀況下，都需要一個 neuron 來製造
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-329 所以，我們這邊仍然需要
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-330 O(1/sqrt(ε)) 的 neurons 才能夠去 fit y=x^2
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-331 所以現在，我們剛才疑慮就消失了
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-332 我們本來不知道 shallow 的最佳的狀態是什麼
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-333 那我們現在說，就算是給他一些不公平的狀態
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-334 給他一個夢幻的狀態
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-335 其實他就只能夠做到這樣
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-336 所以，我們剛才看到那一個不知道是好還是壞的狀態
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-337 已經是 shallow 的最佳的狀態
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-338 而另外一方面 deep 這一個狀態
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-339 我們不知道他是最好還是
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-340 沒有很好這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-341 我們只是可以這樣做到
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-342 我們有一個方法可以用這麼多的 neuron
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-343 就去 fit y = x^2
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-344 接下來就可以 fit polynomial
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-345 我們只需要這麼多的 neuron 就可以 fit y = x^2
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-346 能不能更少呢？ 不知道
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-347 但我們至少知道，隨便找一個方法就這樣子了
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-348 那 shallow 的方法竭盡全力也就只能夠做到這樣了
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-349 所以，這邊就是告訴我們說
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-350 deep 是比 shallow 好的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-351 後面有一些現在有的理論
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-352 然後，今天講的是
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-353 現在有的理論的異常簡化版這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-354 那你可以自己去看看
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-355 那些 paper 裡面講的都還蠻複雜的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-356 那我這邊就是用流水帳的方式呢
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-357 帶過去就那些理論是長什麼樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-358 那這邊都是簡化的版本
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-359 那實際上的，更詳盡的說明
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-360 你再自己去看看那些文章
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-361 在最早的時候
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-362 我看找到最早的跟這些 deep 的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-363 deep fit function 的 power 有關的文件
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-364 是從 2016 年開始
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-365 那這邊是發表在 COLT
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-366 他是我找到的最早的一篇
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-367 那他說，他證出來的理論是這樣
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-368 這個是比較早的結果
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-369 他證出來的理論是這樣
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-370 他是說，有某一個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-371 他是由三個 layer 的 feedforward network 所組成的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-372 那他這個 function 他沒有辦法
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-373 被兩個 layer 的 network 所逼近
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-374 但是他只是說，有存在這樣的 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-375 並不是說所有的function 都是這樣子的，注意一下
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-376 你隨便找一個很簡單的 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-377 比如說 linear 的 function，比如說
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-378 就是一條平的水平線的 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-379 那這樣比起來這個 deep 跟 shallow 比起來
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-380 在 fit 那種 function 上是沒有任何優勢的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-381 所以這邊的理論只是告訴說，存在某一個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-382 他可以用三個 layer 的 network 來表示
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-383 但卻沒有辦法被兩個 layer 的 network 來表示
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-384 除非那個兩個 layer 的 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-385 非常的巨大
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-386 那他的證明是個 general 的證明
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-387 我們剛才討論的都是在 ReLU 的 case 上面
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-388 他不限於 ReLU
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-389 他可以 try 在其他的 activation function 上面
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-390 然後，他告訴我們說
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-391 假設那個三個 layer 的 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-392 他的寬度，每一層的寬度是 k
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-393 那兩個 layer 的 network 要怎麼樣才能夠
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-394 逼近那個三個 layer 的 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-395 所構築出來的那個 function 呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-396 你要把那個寬度
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-397 他的那個寬度，你要把那個 k
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-398 取 4/19 次方
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-399 然後放在指數
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-400 然後這邊有兩個常數項 A 跟 B
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-401 然後，你需要這麼多的 neuron
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-402 兩個 layer 的 network 需要這麼多的 neuron 才可以去
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-403 approximate 這個三個 layer 的network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-404 你會發現說，這個三個 layer 的寬度
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-405 居然在兩個 layer 這邊是放在指數項的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-406 雖然他這邊有取一個 4/19 次方會讓
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-407 這個 k 的值變小，但是他是被放在指數項
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-408 好那這邊是只有證三個 layer 跟兩個 layer
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-409 所以，他是可能是比較 limit
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-410 這邊有一個更 general 的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-411 他也是在 COLT 的 2016
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-412 他說存在著一個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-413 他可以被 deep network 所表達
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-414 但是沒有辦法被 shallow 的 network 所表達
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-415 跟剛才講的一樣除非那個 shallow 的 network 非常的大
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-416 他的證明一樣也不限於 ReLU 的 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-417 他的證明是這樣，他說
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-418 假設你有一個很 deep 的 network，他的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-419 layer 的數目是 θ(k^3)
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-420 然後，他的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-421 每一個 layer 的 node 的數目是 θ(1)
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-422 那他的參數是 θ(1)
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-423 你可能說，怎麼突然討論參數
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-424 而且這邊他還討論了參數的量
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-425 因為這邊指的是不同的參數
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-426 也就是說，你的不同的 layer 是可以 share 參數的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-427 你可以不需要那麼多的參數
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-428 就構築出一個非常複雜的 deep network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-429 那 shallow 的 network 呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-430 shallow 的 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-431 如果他的 layer 的數目是 θ(k)
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-432 那他至少要 2^k 的 node
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-433 才可以逼近這個 deep 的 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-434 但是，這些 paper 裡面他們討論的都只有一直說
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-435 存在這樣子的一個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-436 那接下來有人就會問說
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-437 那雖然說存在這樣的 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-438 那個 function 會不會非常的奇怪
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-439 然後，在真實的 case 上
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-440 完全派不上用場
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-441 所以接下來，有人就證了說
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-442 那存在的那一個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-443 它不見得是一個很奇怪的 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-444 至少我們舉一個例子，他是一個
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-445 球狀的 function，就你做一個球
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-446 然後這個球裡面的值，你就做一個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-447 他在這個球裡面都是 1，在球外面都是 0
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-448 他就是前述的那幾個理論
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-449 裡面講的 function 的其中一個
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-450 那這種球狀的 function，一看就很有用
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-451 因為你可以做一個，做分類的時候可以用上
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-452 就是某一個類別都落在球裡面
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-453 某一個類別都落在球外面
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-454 然後，這篇 paper 還做了實驗的證明
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-455 假設我們現在要去 fit 這一個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-456 我們用兩個 layer 的 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-457 跟用三個 layer 的 network 來比較看看
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-458 那你會發現說，兩個 layer 的 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-459 你不管怎麼開他的寬度 100、200、400 到 800
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-460 都沒有辦法去 fit 那個球
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-461 那一弄到三層，寬度只有 100
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-462 你得到的 error 馬上就有很大的下降
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-463 那他只是用這個實驗來驗證這個理論而已
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-464 那剛才講的都是比較 specific 的 case
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-465 其實現在還有很多的證明
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-466 我就沒有把他們的理論
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-467 寫上來 check 一下，這幾篇 paper
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-468 那其實在這些 paper 裡面
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-469 你知道要證明所有的 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-470 都是 deep 比較好
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-471 shallow 比較差是不可能也不合理的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-472 因為你只是隨便拿一個 function 出來
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-473 比如說，他是一個 linear function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-474 所以，deep 就不見得會佔到優勢
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-475 所以，deep 要比較好的前提通常是
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-476 你對那個 function 還是要做一些限制的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-477 就你不能夠證說
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-478 任何 function 都是 deep 好，shallow 差
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-479 你沒辦法證這件事，而且實際上也不是這個樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-480 你只能是說，假設你的 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-481 有某種程度的複雜度
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-482 比如說他的 curvature 是多少那
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-483 每一篇 paper 證的就不太一樣
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-484 這樣子就是每一個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-485 你要考慮的那一個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-486 他在有某種複雜程度的前提之下
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-487 我們通常可以得到，類似以下這樣的結論
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-488 假設你有一個 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-489 你希望他的 L2 norm 的 accuracy 小於 ε
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-490 那他是一個 ReLU 的 network
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-491 他的深度是固定的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-492 那你需要的 neuron 的數目
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-493 可能是 1/ε 的某一個 polynomial 項
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-494 1/ε 放到某一個 polynomial 的 function 裡面
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-495 但是，如果你今天的 network 可以很深
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-496 他的深度，是跟 ε 有關係的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-497 這個時候，你的寬度只需要 log (1/ε)
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-498 代到 polynomial 裡面去
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-499 本來需要 1/ε 代到 polynomial 裡面去
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-500 現在是 log (1/ε) 代到 polynomial 裡面去
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-501 所以，他們的差別是差了一個 exponential
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-502 跟我們剛才在前面講得比較簡單的推導
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-503 其實，得到的結果是一致的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-504 這一頁投影片，想要講的是
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-505 其實還有別的說法
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-506 這個是，他說什麼情況
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-507 這個細節，大家再 check 這個 paper
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-508 什麼情況下 deep 會比較好呢？
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-509 他說，如果那個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-510 有一個 compositional 的 structure
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-511 那 deep 的會比 shallow 還要好
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-512 那我今天要講的差不多就是這樣
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-513 然後，我們就請助教來講一下作業
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-514 那在請助教來講作業之前
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-515 我還是會今天的課程下一個結論
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-516 就假設剛才講了那麼多東西，你覺得很煩燥
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-517 都沒有聽下去的話
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-518 那今天得到的結論就是這樣
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-519 你想要 fit 某一個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-520 那 deep 會比 shallow 還要好
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-521 然後，他的好是 exponential 的那麼好
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-522 那要 fit 那個 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-523 就是說這樣子，結論不是就這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-524 那一個 function，我們今天證了 y = x^2
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-525 是那樣子的 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-526 那你可以很直覺的覺得說
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-527 也許比 y = x^2 還要簡單
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-528 比如說一次的，不符合剛才的說法
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-529 但是，比 y = x^2 複雜的就會符合
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-530 剛才那一個說法，而我們要考慮要 fit 的 function
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-531 一定都是比 y = x^2 複雜
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-532 所以 deep learning 是很有用的
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-533 就是這樣子
Deep_Learning_Theory_1-3_-_Is_Deep_better_than_Shallow-534 那接下來我們就請助教來講一下作業
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-0 有講到 deep learning 這樣子剛才講的是一個 general 的 case
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-1 現在我們來講 deep learning
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-2 講真正的 deep learning 之前
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-3 我們先來分析 deep 的 linear 的 network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-4 他長什麼樣子，他的 loss function 長什麼樣子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-5 我們先來假設一個非常非常簡單的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-6 deep 的 linear network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-7 這個 deep linear network，他只有兩個 neuron
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-8 他只有兩個 neuron，他的 input 就只有一維，就是 x
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-9 他的每一個 neuron
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-10 都沒有 nonlinear 的 activation function
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-11 每一個 neuron 的 activation function 都是 linear
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-12 每一個 neuron 就只有一個 weight，w1 跟 w2
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-13 今天把一個 x 代進去
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-14 他只會 output 一個 value y
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-15 y 顯然就是 x * w1 * w2，就得到 y，就結束了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-16 我們今天在做 deep learning 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-17 你會給這個 network training data
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-18 我們假設現在 training data 只有一筆
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-19 也就是說，input 是 1，x = 1 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-20 我們希望 network 的 output，跟 1 愈接近愈好
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-21 我們現在有這樣子的，一個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-22 有現在這樣一個 linear 的 network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-23 他只有兩個 weight，因為他只有兩個 weight
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-24 我們甚至可以把他在一個二維平面上畫出來
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-25 其中一維是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-26 其中一維是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-27 w1 的值的變化
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-28 另外一維是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-29 w2 的值的變化
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-30 然後
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-31 這個圖上可能你就畫一些等高線
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-32 畫一些等高線，代表他的 loss
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-33 代表他的 loss，我們寫作 L
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-34 你要不要猜猜看
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-35 這個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-36 現在這一個 loss function
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-37 他是不是 convex 的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-38 我們想想看
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-39 假設我們有一個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-40 先問一下大家，給你想個三秒鐘
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-41 這一個問題
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-42 這個問題
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-43 你覺得他的 loss function 是 convex 的同學，舉手一下
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-44 有一些同學覺得是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-45 你覺得他不是 convex 的同學舉手一下
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-46 有一些同學
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-47 有人覺得是 convex，有人覺得不是 convex
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-48 那我們再給你一個提示吧
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-49 我們再考慮一個更簡單的 network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-50 這個 network 只有一個參數
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-51 他只有一個 neuron，只有一個參數叫 w
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-52 input x，output y = wx
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-53 這是一個，這一個 network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-54 他是一個 linear 的 function
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-55 這一個 network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-56 他就跟 linear regression 是一模一樣的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-57 如果我們今天考我們機器學習那門課
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-58 第一堂課就講 linear regression
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-59 他跟 linear regression 是一樣的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-60 input 是 1，那希望他的 output
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-61 跟 1 愈接近愈好
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-62 這一個 network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-63 如果我們橫軸是 w
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-64 縱軸是他的 loss
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-65 他是 convex 的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-66 如果今天 w = 1
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-67 這個時候他的 loss 會是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-68 當 w 偏離 1愈來愈多
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-69 loss 就會逐漸上升
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-70 這是一個二次曲線
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-71 他長這個樣子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-72 他是 convex 的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-73 那你會說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-74 這一個 function，w1 跟 w2 的 function
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-75 跟這個只有 w 的 function
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-76 他們有什麼不同嗎？其實沒什麼不同
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-77 你只是把這個 w 拆成 w1 乘上 w2 而已
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-78 這樣講完以後，你覺得
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-79 他是一個 convex 的 function 嗎？
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-80 你覺得他是一個 convex 的 function 的同學舉手一下
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-81 沒有人舉手，你覺得他不是一個 convex function 的同學舉手一下
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-82 還是有一些同學覺得他不是 convex 的 function
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-83 我們實際上把它畫出來看看是什麼樣子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-84 實際上畫出來，這個圖不是我自己畫的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-85 我是截了那個 Ian J. Goodfellow 的 paper
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-86 是長這個樣子的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-87 所以他不是 convex 的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-88 今天這個顏色&lt;，代表 loss 的值
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-89 越偏藍，代表 loss 的值越小
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-90 他不是 convex 的，他很神奇就是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-91 同樣是 linear 的 function
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-92 只有一個 weight 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-93 他是 convex 的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-94 今天有兩個 weight 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-95 你的 loss 對 weight 來說，就不是 convex 了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-96 他是一個 linear 的 function，但他的 loss 不是 convex
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-97 很多人聽到 linear function， 就覺得說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-98 他是 convex，沒有，這只有在這個 case 才是 convex 的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-99 這是一個 linear 的 function
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-100 但他的 loss 就已經不是 convex 的了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-101 他的 loss 長什麼樣，他的 loss 長這個樣子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-102 在這個地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-103 跟這個地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-104 他的 loss 是最低的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-105 loss 最低的值，可以是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-106 可以是 0 對不對，因為你看
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-107 w1 代 1，w2 代 1，loss 就是 0 了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-108 所以，w1 代 1，w2 代 1
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-109 loss 就是 0 了，當然還有其他的組合就是了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-110 連成一條曲線
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-111 這個圖上，黑色的箭頭代表 gradient 的方向
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-112 所以，在這個地方你就順著 gradient
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-113 方向走走走走走，走到這個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-114 local minima 的地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-115 不過這個 local minima 也是 global minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-116 因為他整個 function 裡面是最小的地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-117 然後，這個綠色的線代表是 global minima 的 manifold
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-118 他只是把 global minima 連起來而已
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-119 紅色的線是一個 saddle point，其實在 (0,0) 這個地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-120 你就自己等一下算一下
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-121 他有一個 saddle point 在 (0,0) 這個地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-122 他 gradient 是 0，但他不是 local minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-123 也不是 local maxima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-124 因為從這個方向走會增加
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-125 從這個方向走也是會變小
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-126 然後，藍色的這一條線是說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-127 假設你這邊是起始點
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-128 gradient descent 會這樣子走
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-129 走走走走走，走進去
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-130 然後，今天這條粉紅色的線是要告訴我們說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-131 他不是一個 convex 的 function
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-132 對不對，他不是一個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-133 他不是一個 convex 的 function
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-134 因為把這兩邊連起來的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-135 我們會發現說這個曲面上的這個值
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-136 是比紅色這一條線連起來的值還要大的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-137 剛才是真的因為你只有兩個參數
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-138 你真的把他硬畫出來以後
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-139 看起來是這個樣子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-140 如果今天沒有辦法硬畫呢？
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-141 我們就來分析一下
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-142 我們知道說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-143 現在這個 loss = (y/hat - w1 * w2 * x)^2
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-144 然後今天 y/hat 跟 x 都代 1
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-145 都代 1，所以變 (1 - w1 * w2)^2
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-146 然後，我們計算一下他的 gradient
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-147 你把 L 對 w1 算 gradient，對 w2 算 gradient
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-148 你得到這樣的式子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-149 你得到這樣子的式子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-150 然後接下來你就可以分析看看說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-151 什麼情況下
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-152 你會得到 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-153 什麼情況下 gradient 是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-154 有兩個可能，一個可能是說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-155 我把圖都畫出來好了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-156 我右上角本來有一個小圖啦
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-157 但不知道為什麼，他在這個螢幕上
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-158 沒有辦法顯示出來
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-159 什麼情況下，他會有
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-160 這個 critical point 呢，一個可能是 w1
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-161 跟 w2 相乘等於 1 的時候，對不對？
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-162 所以假設這一項是 0，這一項是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-163 他是一個 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-164 這個 critical point，他是 local minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-165 我們不需要想這件事情
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-166 因為 w1 * w2 = 1
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-167 這就是一個 global minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-168 今天有一些 critical point 他可以讓 w1 * w2 = 1
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-169 他是global minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-170 另外一個可能是 w1 = 0，w2 = 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-171 有在原點的地方，他是另外一個 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-172 這個 critical point 長什麼樣子呢？
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-173 如果我們今天算一下他的 hashing 的話
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-174 我們現在把 hashing 的 matrix 算出來
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-175 把 hashing matrix 算出來長這樣
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-176 那我們把
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-177 w1 跟 w2 都代 0，也就是考慮
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-178 原點那個地方的 hashing
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-179 我們會發現那個 hashing 算出來呢
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-180 那個 hashing 算出來呢
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-181 是 0、-2 、-2
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-182 那接下來我們要考慮一下
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-183 這一個 hashing 他是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-184 saddle point 還是 local minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-185 還是 local maximum 呢
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-186 那你就要找一下他的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-187 我這邊有沒有算錯啊
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-188 應該
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-189 應該沒有
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-190 應該沒有
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-191 今天這個 h，我們今天要找他的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-192 我們要算一下他是 local minima 還是 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-193 那我們就要看一下他的 eigen vector
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-194 那他有什麼樣的 eigen vector 呢
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-195 你可以自己回去翻一下線代課本看這個怎麼算
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-196 我心算一下以後覺得應該是 (1,1) 跟 (1,-1)
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-197 直覺就是這個樣子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-198 那你再算一下他的 eigen value，λ1
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-199 是什麼？你把 (1,1) 乘進去，所以是 -2
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-200 λ2 是 2
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-201 所以現在有正的 eigen value，有負的 eigen value
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-202 我們剛才講說有正的 eigen value，有負的 eigen value
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-203 他是一個 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-204 當我們今天從原點的地方往 1 的方向走
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-205 會減少 -2，往 (1,-1) 的方向走&lt;，會增加 2
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-206 不知道為什麼那個圖沒有顯示出來，我們把他
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-207 如果有錄音的話，他就會彈出一個要不要按確認
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-208 所以，如果沒有彈出來就代表說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-209 沒有錄到音
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-210 不過沒有關係剛才講的那個也還蠻 trivial 的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-211 剛才講的還蠻簡單的，所以
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-212 我剛才想要表的事情是說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-213 我們在原點這個地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-214 他是一個 saddle point，往 (1,1) 走會變小
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-215 原點這個地方是往 (1,1) 走會變小呢？
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-216 是，往 (1, -1) 走會變大
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-217 x 是 1
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-218 y 是 -1，往 (1,-1) 走這個方向會變大，沒錯
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-219 跟這個圖上看起來的是一模一樣
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-220 我們今天知道說在做 gradient descent 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-221 你最後可能會停在一個 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-222 也可能會停在一個 local minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-223 停在 local minima 沒有問題
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-224 因為今天這個地方 local minima 就是 global minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-225 停在 saddle point 就有問題了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-226 因為 saddle point 不是我們要的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-227 他不是一個 global minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-228 他的 loss 是很大的，他的 loss 不是最小的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-229 但是，今天在這個問題裡面
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-230 你不太需要害怕 saddle point，為什麼？
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-231 因為這個 saddle point，他非常容易被逃出去
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-232 首先，假設
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-233 你從這個圖上的任何一個點開始
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-234 你要最後走到 saddle point 是還蠻困難的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-235 要怎麼樣才能走到 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-236 你要正好初始在這個對角線的地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-237 順著 gradient 走，你才能夠走到 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-238 如果你稍微偏一點，像這個 case 一樣
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-239 你就錯過 saddle point，你就走到 global minima 去了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-240 所以，現在這個 case 裡面
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-241 其實不太容易卡在 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-242 就算你不幸卡在 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-243 也沒有關係，因為你可以算 hashing
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-244 你算了 hashing 以後
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-245 hashing 可以告訴你說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-246 往哪個地方走，可以讓 loss 下降
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-247 你就可以逃出那個 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-248 往 loss 比較小的地方走
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-249 這個是只有兩個 neuron 的狀況
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-250 我們現在來考慮一個更複雜的狀況是有三個 neuron
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-251 也就是有兩個 hidden layer 的狀況
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-252 當我們今天有三個 neuron 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-253 我們畫出來的 loss
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-254 這個 loss， L = (1 - w1 w2*w3)^2
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-255 接下來，你知道有三個參數所以就不太好畫圖
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-256 所以，這個時候你就用 gradient 跟 hashing 來分析一下
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-257 今天這個 lossＬ應該長什麼樣子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-258 我們現在把這個 lossＬ
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-259 分別對 w1 w2 和 w3，做偏微分
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-260 做完偏微分以後，我們就可以知道說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-261 哪些地方是 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-262 光知道 critical point 還不夠
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-263 我們要至少算出 hashing
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-264 才能夠知道說這個 critical point，他的性質是什麼
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-265 那這個 hashing
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-266 如果今天這個 hashing 算出來啊
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-267 理論上應該要有 3*3，9 個值啦
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-268 有點麻煩，我這邊就隨便舉一個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-269 隨便舉一個例子，就 w1 偏微分兩次
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-270 跟先對 w1 做偏微分，再對 w2 做偏微分的結果
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-271 那你可以輕易的想像說，剩下的值長什麼樣子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-272 我們先看一下 critical point 吧
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-273 有哪些地方可能是 critical point 呢
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-274 一個可能是 w1w2w3 = 1
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-275 如果 w1w2w3 都 = 1 會發生什麼事呢？
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-276 這三個偏微分的值
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-277 也就是 gradient 的第一項都是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-278 所以 gradient = 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-279 那還有另外兩個地方，兩個可能性
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-280 可能會製造出 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-281 一個可能是 w1w2w3 都是 0，也就是在原點的地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-282 如果 w1w2w3 都是 0 的話
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-283 後面這一項會是 0，所以這邊也是一個 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-284 那其實不需要三項都是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-285 才能夠製造 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-286 其實只要三項裡面的兩項是 0，不一定要 w1 w2 是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-287 只要三項
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-288 我這個想要表達意思是說，只要三項裡面的兩項是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-289 舉例來說，w1 w2 是 0 就可以
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-290 讓他變成一個 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-291 接下來，我們來分析說這三個 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-292 他分別是什麼樣的狀況
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-293 第一個 case，w1 w2 w3 = 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-294 他是一個 global minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-295 你說沒分析還沒算 hashing
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-296 你怎麼知道他是 global minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-297 你要至少先算個 hashing
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-298 看看他的那個 eigen value 是不是都是正的才知道
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-299 他是不是一個 local minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-300 你又不知道他是不是 global minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-301 但是今天這個 case 是比較特殊的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-302 我知道說 w1 w2 w3
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-303 這個 loss 的最小值就是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-304 w1 w2 w3 = 1 的時候可以讓 loss 是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-305 最小值是 0，這邊正好讓 loss 是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-306 所以，他就已經是 global minima，沒有別的可能了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-307 所以，這個問題、這個 critical point 就不用分析了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-308 那分析 w1 = w2 = w3 = 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-309 如果 w1 w2 w3 都是 0 的話
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-310 你就算一下他的 hashing，發現他的 hashing
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-311 是一個 zero matrix
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-312 hashing 是 zero matrix 是一件很可怕的事情，因為他的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-313 eigen value 都是 0，意味著說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-314 如果我們今天用 hashing 來考慮這一個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-315 原點的地方的話
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-316 hashing 告訴你說，不管往什麼方向走
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-317 都是 0，就如果我們不考慮更高的次數
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-318 更高的項，只考慮 hashing 那一項的話
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-319 這個 hashing 告訴我們說，在原點的地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-320 不管往哪個方向走，都是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-321 所以你今天如果走走走，走到
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-322 hashing 那個地方，你就逃不出去了，因為
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-323 你就算是算了 hashing
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-324 你仍然不知道哪個地方可以讓你的 loss
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-325 變小，除非你真的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-326 就是挪動一下你的數值去試試看，不然你
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-327 光是算 gradient，光是算 hashing
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-328 你就逃不出這個地方了，因為這個 hashing 告訴我們說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-329 不管往哪個方向走，值都不會增加
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-330 也不會減少，他是不增不減
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-331 你永遠都逃不出去了，所以這是一個可怕的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-332 可怕的 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-333 我們其實，你如果憑著直覺想你知道說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-334 他是一個 saddle point，就有些
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-335 在原點的地方往旁邊走，有些時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-336 可以讓 loss 增加，有些時候可以讓 loss 減小，他是一個 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-337 但是從 hashing 我們看不出來，從 hashing 這邊我們只知道說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-338 這邊是一個黑洞，陷進去
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-339 以後就出不來了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-340 那如果是 w1 w2 = 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-341 w3 = k 呢
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-342 這個時候我們得到了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-343 這樣子的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-344 一個 hashing matrix
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-345 這個 hashing 的 matrix，你實際上去算一下他的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-346 eigen value 以後，你會發現說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-347 他有一個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-348 positive eigen value，一個 negetive eigen value，一個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-349 zero eigen value
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-350 雖然他有 zero eigen value，但是他的 eigen value 又
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-351 正好一個 positive 一個 negetive，所以他往
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-352 某個方向走會增加，往某個方向走會減少
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-353 那往 zero 的那個方向，zero eigen value 那個方向走
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-354 不知道增加還是減少不過沒關係反正有增有減
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-355 所以他一定就是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-356 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-357 從這個例子裡面，我們發現什麼
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-358 從這個例子裡面我想要告訴大家的事情是，我們最後發現說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-359 所有的 minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-360 至少在這個 case 裡面
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-361 所有的 minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-362 他都是 global minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-363 其他的 critical point，他都不是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-364 local minima，只要你是一個 local minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-365 那你就是一個 global minima，另外一件事情
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-366 是有一些 critical point，他是很差的 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-367 你陷進去以後，他就會跑不出來了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-368 那什麼叫差的 critical  point 就是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-369 你從那個 critical point 算 eigen value
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-370 沒有任何 eigen value 是負的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-371 這樣你就不知道說哪個方向可以讓你的 loss 的值減少
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-372 你就走不出去了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-373 這個是兩個 hidden layer 的情況
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-374 如果是十個 hidden layer 的話
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-375 你其實也可以自己分析一下，也不會太難
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-376 那我就懶得再分析了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-377 我這邊做的事情就是，在那個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-378 十個 hidden layer 就有十個參數
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-379 兩個 hidden layer 有三個參數
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-380 所以十個 hidden layer 有十一個參數
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-381 那十一個參數的 space 上面，我取兩個點
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-382 那兩個點他們正好在原點的兩邊
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-383 然後把他們拉起來
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-384 再算說在這個區間之內的 loss 的變化
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-385 如果取某兩個點
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-386 他們的 loss 的變化可能是這樣
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-387 然後再取某兩個點
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-388 他們 loss 的變化可能是這個樣子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-389 你會發現說在原點的附近，這邊兩邊高是一樣
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-390 他們都是 1
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-391 那在原點附近，有一個 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-392 他是 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-393 在原點附近呢
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-394 但是如果你光看 hashing 的話
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-395 你不知道他是不是 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-396 在原點附近有一個 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-397 這個 critical point 他算出來的 hashing 都是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-398 然後在原點附近非常的平
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-399 非常的平，如果你走到原點附近
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-400 你就再也逃不出來，就像被吸入黑洞裡面
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-401 這個就是我們在開學學期初的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-402 demo 的那一個狀況
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-403 我們現在真的來看一個非常簡單的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-404 只有兩個 neuron 的 linear network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-405 現在我們的 training data 就像我們剛才講的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-406 他的 input 是 1，然後 output 的 target 也是 1
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-407 這個 network 只有兩個 neuron
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-408 第一個 neuron input 是只有一維
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-409 output 只有一維，然後沒有 bias
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-410 所以這個 neuron 他只有一個參數
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-411 這個參數是 weight，他沒有 bias
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-412 那他的 activation function 是 linear 的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-413 接下來用 for 迴圈再加第二個 neuron
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-414 那這第二個 neuron
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-415 他的 input dimension 是 1， output dimension 也是 1
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-416 一樣沒有加 bias
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-417 一樣他的 activation function，是 linear 的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-418 這種 network 我們可以輕易的把他 train 起來
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-419 但是我現在故意在做參數的 initialization 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-420 把這個 neuron 的所有的 weight
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-421 通通 initialize 在這個 weight = 0 的狀況
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-422 那要怎麼做到這件事情呢
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-423 其實在 Keras 裡面非常的容易
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-424 因為我這邊是用 randon normal
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-425 來做參數的 initialization
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-426 我為什麼要把 standard deviation 設成 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-427 在做參數的 initialization 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-428 initialize 的參數就會自動的被設為 0 了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-429 我們剛才有講說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-430 今天一個只有兩個 neuron 的 linear network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-431 他的這個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-432 在原點的地方有一個 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-433 所以，今天如果 initialize 是在原點那個地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-434 你會發現說你 train 的 loss 就完全降不下去
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-435 完全降不下去了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-436 就算用 Adam 加 momentum 也沒有用
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-437 因為一開始，初始的地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-438 gradient 就是 0，那你一開始初始在一個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-439 saddle point 的地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-440 你就沒有辦法再 train 下去了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-441 但是初始在 saddle point 的地方這件事情
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-442 其實是並沒有那麼容易發生
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-443 你現在只要在做 initialization 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-444 給這個 standard deviation 非常非常小的值
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-445 就算他的值非常非常小，也就是 initialize 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-446 你 initialize 的點在原點的附近
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-447 在兩個參數都是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-448 在所有參數都是 0 在這個原點的附近，你會發現說其實
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-449 network 只要他初始化
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-450 離原點有一點點的距離，就不會被卡在
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-451 那個 saddle point，所以我們現在給這個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-452 standard deviation 一個非常小，非常小的值
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-453 這樣夠小了吧
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-454 我們實際來 train 一下
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-455 你會發現說，現在 loss 就可以輕易的降為 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-456 如果你的參數初始化的時候，exactly 在原點的地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-457 沒有辦法 train，但只要離原點稍微偏離一點
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-458 就可以 train 了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-459 接下來，我們稍微再加深一下這個 network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-460 現在變成有三個 neuron
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-461 同樣的 initialization
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-462 你發現說，三個 neuron，同樣的 initialization
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-463 結果就 train 不起來了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-464 因為我們剛才有分析說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-465 如果今天有三個 neuron
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-466 那在原點那個地方的 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-467 他會變成一個高高的 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-468 這個 saddle point 他的 hashing
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-469 是一個 zero matrix
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-470 也就是說在原點的地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-471 這個 error 的 surface 是非常平坦的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-472 他就是一個一望無際的大地一樣
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-473 平坦的就跟一個鏡面一樣
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-474 那邊完全沒有任何的 curvature
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-475 所以，如果你今天 initialize 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-476 initialize 在原點的附近
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-477 initialize 在非常平的那個 saddle point 附近
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-478 你就很難逃出那一個區域
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-479 接下來我們試一下
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-480 當把 network 加了非常深的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-481 我們現在總共有十個 neuron
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-482 假設你現在有十個 neuron
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-483 就算是你用正常的 initialization
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-484 你其實也逃不出去了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-485 因為我剛才有講過說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-486 如果你今天有
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-487 你今天的 network 是非常深的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-488 這個 linear network 是非常深的，在原點的地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-489 會產生一個這個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-490 非常非常平的區域
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-491 如果你今天 initialize 的時候，在這個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-492 非常非常平的區域的附近
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-493 你就會沒有辦法逃脫了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-494 我們現在給參數做 initialization
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-495 用一個比較正常的 standard deviation
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-496 然後 train train 看，現在 network 非常深
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-497 用一個正常的 standard deviation 看看
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-498 能不能跑出去，發現說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-499 現在 loss 降不下去了，這個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-500 一萬個 epoch，但是這個 loss 完全
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-501 都沒有辦法降下去
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-502 怎麼辦呢？
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-503 這個時候，因為我們知道在原點附近
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-504 有一個非常平的區域
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-505 所以，你就要避免在你做 initialization 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-506 initialize 在原點附近
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-507 盡量離原點足夠的遠
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-508 不要 initialize 在那個非常平的地方
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-509 initialize 離那個非常平的地方遠一點
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-510 不要在那個很平的區域
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-511 其實就可以順利的把他 train 起來了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-512 我們試試看把 standard deviation 故意設的大一點
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-513 你突然發現
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-514 這個時候雖然 network 非常深
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-515 但是 loss 也輕易的就下降到 0 了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-516 非常的遠，那我就告訴你說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-517 general 現在可以證明出來的東西是什麼樣子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-518 那這裡的細節
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-519 我就把 reference 放在後面，你再自己
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-520 你再自己看看
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-521 現在我們知道的事情是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-522 假設有一個 network 他是 linear 的，什麼意思？
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-523 剛才我們舉的是一個 linear 的 network 的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-524 一個非常 special 的 case
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-525 special 的 case
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-526 那現在是一個 general 的 case
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-527 input 是一個 vector, x
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-528 x 乘以 matrix w1 以後
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-529 再乘以 matrix w2 再乘以 matrix wk
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-530 得到 y 希望 y 跟 y/hat 他們的 tunel
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-531 越接近越好
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-532 接下來，可以證明說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-533 只要滿足一些非常寬鬆的條件
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-534 這一個 linear 的 network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-535 不管他疊幾層
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-536 他的所有 local minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-537 都是 global minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-538 就跟我剛才在
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-539 兩個 hidden layer 証的 case 是一樣的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-540 所有的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-541 你只要算一算他的 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-542 反正你找到一個 critical point，就發現說他是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-543 local minima，那他就一定是 global minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-544 為什麼我們的 paper
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-545 他們證明的時候需要的條件不太一樣
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-546 我現在找到的一個最寬鬆的條件是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-547 要求所有 hidden layer 的 size
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-548 要大於等於 input dimension 跟 output dimension
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-549 舉例來說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-550 你 input dimension 是五維，output dimension 是五維
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-551 中間每一個 hidden layer 的 output 都要至少五維
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-552 那為什麼需要這個限制呢
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-553 其實是在證明裡面會用到說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-554 因為在證明裡面，實際上會用到的概念是說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-555 現在 loss 最小是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-556 所以你需要讓你的這個 linear 的 network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-557 他的 global minima 的 loss 是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-558 也就是實際上證明的時候會用到說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-559 你找到一個 critical point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-560 發現他是 local minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-561 接下來你又說，如果他是 local minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-562 他的 loss 一定是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-563 因為 local minima 的 loss 是 0，所以他一定是 local minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-564 所以他是這樣子證的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-565 所以是這樣子證出來的所以需要加上這個條件
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-566 確保 global minima 的 loss 是 0
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-567 其實還有另外一個發現，這個發現是說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-568 當今天 network 的深度
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-569 大過兩個 hidden layer
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-570 那你就會產生我們剛才講的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-571 不好的 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-572 就會產生不好的 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-573 所謂不好的 saddle point 是說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-574 這個 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-575 他沒有任何 negative 的 eigen value
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-576 他是個 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-577 但是他沒有 negative 的 eigen value
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-578 所以，不知道往哪個方向走可以讓你的 loss 下降
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-579 這件事情在 network 的 hidden layer 一層的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-580 不會發生，兩層以上就會發生
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-581 我今天在講課的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-582 我並沒有直接證，我就只是舉個例子
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-583 我們就剛才舉的例子是說，只有兩個 neuron
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-584 也就是一個 hidden layer 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-585 沒有差的 saddle point
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-586 但多加一個 neuron，有兩個 hidden layer，三個 neuron 的時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-587 他有差的 saddle point，但是 in general
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-588 不是只有好幾個 neuron，而是一個
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-589 而是更 general 的 case，其實也是這樣
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-590 那詳情，大家自己再去看一下 paper
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-591 最早的完整的證明
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-592 應該是來自 NIPS , 2016 這一篇
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-593 Deep Learning without Poor Local Minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-594 那其實，這個 2016 離現在其實也沒有太久
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-595 但是在 deep learning 這個地方的時空的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-596 這個時間的流動是比較快的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-597 所以 2016 就覺得說非常久以前
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-598 那個時候
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-599 第一次有這樣證明的時候大家其實都還蠻驚訝
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-600 想說，哇！
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-601 原來 deep linear network 他雖然是 Non-convex 的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-602 但是他是沒有 local minima 的，好神奇
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-603 而且這個就是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-604 deep linear network 沒有 local minima 這件事
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-605 好像在不知道是 80 年還是 90 年的時候，就有人提出
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-606 這樣子的假說，但是沒有證
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-607 然後這篇 paper 就說過去有人提這樣子的假說沒證
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-608 然後，幫他證了一下
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-609 但是因為時間過得非常快，那後面就有很多其他的 paper
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-610 比如說這個 2018 的 paper
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-611 他 submit 到 ICL 就被 reject
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-612 然後看到 reject 那個 cover 就說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-613 linear 的 network 只證 linear network 實在是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-614 太 linear了，其實沒有什麼
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-615 過去可以上 NIPS , 2016 的 oral
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-616 但同樣差不多的東西，現在已經會被 reject 了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-617 大家已經覺得說 linear 的東西
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-618 分析的已經差不多了，所以接下來就是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-619 更 general 的 case
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-620 因為 linear 的 network 簡單來說就沒有什麼用
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-621 我們真正處理問題的時候我們需要的是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-622 nonlinear 的 network，但是
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-623 問題就是 nonlinear 的 network 到底長什麼樣子呢？
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-624 事實上在這一篇 paper 裡面
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-625 Deep Learning without Poor Local Minima 這篇 paper 裡面
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-626 他試著證明了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-627 nonlinear 的 deep network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-628 但他用的方式非常的怪異
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-629 如果你看他的 extra introduction，你會以為說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-630 原來所有 deep 的 network，不管 linear nonlinear 通通
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-631 沒有 local minima，太強了
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-632 但是其實你之後仔細看一下，發現說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-633 他真的證了 linear 的 network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-634 如果是 nonlinear 的 network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-635 他用一個非常奇怪的假設告訴你說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-636 根據這兩個 assumption
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-637 linear network = nonlinear network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-638 然後，那兩個 assumption 是完全不合理的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-639 我就不打算講那兩個 assumption
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-640 因為如果講，等一下一定被大家問到頭炸裂
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-641 因為那個就是不合理的
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-642 所以，接下來要怎麼證呢？
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-643 就很多人會試著想要去往說
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-644 deep learninig
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-645 一般的 general 的 deep network
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-646 是沒有 poor local minima
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-647 就只有 global minima 這個方向去證明
Deep_Learning_Theory_2-2_-_Deep_Linear_Network-648 但是發現非常困難，那你何不反過來
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-0 一般的、nonlinear 的 deep network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-1 他到底有沒有 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-2 你要證沒有 local minima 很難
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-3 但你要證有 local minima 太容易了
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-4 找個 local minima 來
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-5 就證他有 local minima 了，對不對
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-6 所以這個就好像說，你要證明一個東西不存在很難
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-7 但是，要證明事情存在是相對比較容易的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-8 怎麼做，這邊的說法是
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-9 是這個樣子的，這邊我就收集了一些資料
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-10 首先，這個都是文獻上的結果
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-11 首先，就算是非常簡單的 task
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-12 比如說 XOR 的 task
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-13 我們今天在，我們在講 machine learning 的時候
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-14 從 nondeep 的 network 跨到 deep network 的時候
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-15 我們就是舉了 XOR 的例子說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-16 如果今天有一個 data point
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-17 他是這樣的分布
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-18 這兩個 point 是一類，這兩個 point 是一類
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-19 linear network 沒有辦法分，所以
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-20 一定要 nonlinear 的 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-21 如果你實際上去 learn 一下這種 XOR 的 problem
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-22 你會發現說，如果你用一個 network 他只有一個 hidden layer
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-23 他的 neuron 的數目是 2 3 4 5 6 到 7
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-24 就算是增加到 7 個 neuron 那麼多
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-25 你使用 Adam 這個 optimization
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-26 都沒有辦法保證你
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-27 一定可以得到 100 % 的正確率
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-28 都沒有辦法保證你一定可以得到一個
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-29 找到一個 global 的optimal
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-30 如果是這個 sigmoid
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-31 在 network 比較大的時候倒是可以
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-32 接下來這篇 paper 裡面，他想了一個更複雜的問題
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-33 這個複雜的問題叫做 jellyfish 的問題
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-34 他說，他現在有 4 個點
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-35 這 4 個點的分布，就分布在這 4 個位置
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-36 這兩個黑點是一類，這兩個白點是一類
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-37 如果是 sigmoid 他還可以解 XOR 的 problem
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-38 如果是 jellyfish，不管是 ReLU 還是 sigmoid
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-39 你都是沒有辦法解的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-40 你都找不到，用一般的，用已經是
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-41 我們現在覺得最 state-of-the-art 的 optimization 的方法
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-42 用 Adam，你都找不到 global 的 optimal
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-43 這個是第一個
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-44 比較直接的證據告訴你說就算是
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-45 很簡單的 network 有時候你也 learn 不起來
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-46 再來，我們今天假設要證明說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-47 ReLU 的 network 有 local minima，其實非常簡單
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-48 找一個來就行了
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-49 找一個來就證明 ReLU 的 network 有 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-50 所以怎麼辦呢？
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-51 在文獻上有人就找了一個
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-52 他就隨便找一個，這個其實你要找十個八個也是有的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-53 他就說，我們現在有一個 case
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-54 我們有 5 筆 data
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-55 這 5 筆 data 分別都，這邊要畫一個點
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-56 我這邊要畫一個逗點，不知道為什麼畫成一個小數點
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-57 我畫錯
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-58 有 5 個點 (-1,3) (1,-3) (3,0) (4,1) (5,2)，有 5 個點
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-59 接下來，我們有一個全世界最簡單的 ReLU network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-60 他只有一個 neuron
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-61 那把 x 放進去，他會乘上一個 weight
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-62 加上一個 bias，再通過 ReLU 這個 activation function
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-63 再乘上一個 weight，加上一個 bias，得到最終的 y
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-64 現在在這一排這 5 個 pair 裡面
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-65 前一個數字都代表 x
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-66 output 就是，第二個數字就是 target
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-67 今天假設你有一個 ReLU 的 network 是 (1,-3)，(1,0)
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-68 他的曲線畫出來
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-69 他 x 跟 y 的關係畫起來是這個樣子
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-70 他正好可以 fit 後面三個點
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-71 但他 fit 不了前面這兩個點
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-72 他的 loss 算出來是 18
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-73 那你可以輕易的證明
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-74 他是一個 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-75 怎麼輕易地證明呢？你可以證說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-76 這個 (1,-3) (1,0)，如果你都幫他們加一個小小的 △
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-77 不管你加的 △ 是什麼
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-78 他的 loss 一定都會增加，所以他是一個 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-79 那接下來你只要找到另外一個 minima 他的值
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-80 你只要能夠找到另外一組參數，他的 loss
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-81 比 18 還要低
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-82 那你就知道說這一組參數是一個 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-83 就結束了，對不對
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-84 那找不找的到呢，就胡亂找就一組
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-85 (-7,-4)、(1,0)，這個 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-86 他畫的 input 跟 output 關係長這個樣子
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-87 他算出來 loss 是 14，比他還要低
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-88 他是一個 minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-89 然後，又發現說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-90 有其他的參數 loss 更低
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-91 他就不是 global minima 了
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-92 所以就結束
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-93 所以 ReLU 的 network 是有 local minima 的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-94 有不是 global minima 的 local minima 的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-95 剛才舉的是一個非常 specific 的 case
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-96 也許你覺得這個 case 太過 specific
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-97 其實以 general 而言
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-98 ReLU 的 network 都可以找到 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-99 怎麼說呢？因為 ReLU 這種 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-100 有一個狀況叫做他的盲點
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-101 你可以為 ReLU 這個  network 製造盲點
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-102 什麼叫做 ReLU 的盲點呢？
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-103 我們知道 ReLU 這個 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-104 他的每一個 neuron 有兩種 operation 的 region
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-105 一種 region 是 input = output
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-106 一種是 output = 0
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-107 今天我們假設一個 case 是
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-108 所有的 neuron 他通通是 output = 0
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-109 這個就是代表
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-110 這種狀況叫做 ReLU 的 network 陷入盲點
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-111 就是今天，input 一個 x 進來
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-112 所有的 neuron
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-113 他的 output 通通都是 0
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-114 如果所有的 neuron 都
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-115 作用在 output 是 0 的 region 上
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-116 整個 network 最終的 output 當然也是 0
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-117 你會發現說在這個 case，你的 gradient 算出來
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-118 就是 0，對不對
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-119 因為你想想看，你今天假設在這個
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-120 盲點的這個區域裡面
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-121 你不管怎麼調整參數一點點
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-122 假設你不改變這些 neuron 的 operation 的 region
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-123 你的 output 永遠不會變
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-124 所以，今天所有的參數
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-125 他的 gradient 通通都是 0
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-126 所以你今天，你就找到一個 critical point
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-127 你就找到一個 critical point
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-128 這個 critical point，他其實是一個 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-129 怎麼說他是 local minima 的呢？
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-130 因為這個 critical point
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-131 他在他的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-132 他其實是這樣子的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-133 假設橫軸是你的參數的變化
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-134 假設橫軸是你的參數，我們寫作 θ 好了
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-135 這個盲點的意思就是說，在某一個區間內
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-136 在某一個區間內
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-137 你的 network 的 output
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-138 他的值都是 0
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-139 他 network 那個值的 output 值都是 0
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-140 今天在這個 output 是 0 的這個區間裡面選一個點
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-141 他其實都是 local minma
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-142 因為他跟旁邊的 region 比起來
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-143 都是大於等於旁邊的 region
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-144 所以他是一個 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-145 那他會不會是 global minima？
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-146 他一定不是 global minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-147 你只要你最後的那個 target
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-148 他不是 0 就好了
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-149 你只要你最好的 target 他不是 0
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-150 你只要有別的case
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-151 他可以找到的 loss 小於 output 是 0 的 case
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-152 他就不是 global minima 了
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-153 所以，對 ReLU 這個 network 來說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-154 你可以非常容易的製造出
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-155 只要他進入那種盲點的狀態
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-156 所有的 neuron 通通 output 是 0 的狀態
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-157 他很有可能就是 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-158 你說怎麼製造他都是盲點的狀態呢？
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-159 其實很簡單，你給他
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-160 比如說每一個 neuron 的 bias 設很大
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-161 設負的很大，那他的 output 就很容易是 0
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-162 他就很容易陷入盲點的狀態
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-163 在實驗裡面，你可以輕易的做出這件事
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-164 這是文獻上的實驗，他這實驗是這樣做的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-165 他 train 了很多 ReLU 的 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-166 然後做在 MNIST 上面，然後用 Adam
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-167 然後參數 update 一百萬次
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-168 就 update 到覺得已經不可能
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-169 再有任何變化，update 一百萬次
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-170 然後，今天他做了不同的 initialization 的 case
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-171 首先，上面這個 row 跟下面這個 row
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-172 上面這個 row 是用一般的 MNIST 的 training set train 的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-173 下面這個 row
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-174 是用奇怪的 MNIST train，什麼叫奇怪的 MNIST
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-175 就是把每一張圖片的 label，隨機置換成別的 label
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-176 就比如說他是一張 1
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-177 我故意說他是 5，他這個 2 也故意說他是 7
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-178 今天假設你用一個正常的 intialization 的方式
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-179 你用一個 normal distribution
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-180 mean 是 0，variance 是 0.01，來初始化你的參數
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-181 或者是 mean 是 0，variance 是 1
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-182 來初始化你的參數
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-183 那今天這個圖上的每一個點，都代表了一個 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-184 然後這個 network 橫軸代表說他們有不同的 hidden unit
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-185 不同的顏色代表兩個 hidden layer
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-186 或者是 5 個 hidden layer
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-187 那你發現說不管是多少 hidden layer
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-188 不管是多少的 hidden unit
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-189 只要用這種正常的 initialization 方式
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-190 用這種正常的 initialization
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-191 你 accuracy 都可以 train 到 1
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-192 除了這個 case 有點例外，當你 neuron 有點少的時候
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-193 在這特別的 data set 上，在這個怪怪的 data set 上
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-194 有時候會 train 不起來
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-195 但是，今天假如另外一個 case
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-196 試圖讓 network 進入盲點的區域
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-197 也就是在 intial 的時候
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-198 就給他非常奇怪的 initial 的值
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-199 舉例來說，在 initial w 的時候
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-200 故意讓他是從 mean 是 -10 的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-201 random variable sample 起
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-202 故意讓他是從 mean 是 -10 的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-203 random variable sample 起
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-204 或者是故意讓 bias 的 mean 是 -10
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-205 故意讓 bias 的 mean 是 -10
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-206 或是 w 跟 b weight 跟 bias 的 mean 都是 -1
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-207 都是 -1，你就故意讓 network 在一開始的時候
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-208 在初始化的時候就掉入那個盲點的區域
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-209 就掉入那個 local minima 的區域
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-210 他就再也爬不出來了
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-211 他的 accuracy 就一直在
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-212 趨近於 0 的地方，他就再也爬不出來
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-213 所以，這個告訴我們什麼，首先告訴我們說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-214 ReLU 的 network 是有 local minima 的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-215 會不會撞到那個 local minima，跟你怎麼
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-216 做 initialization 是有關係的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-217 這邊還有其他的實驗
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-218 這個實驗是說，不只是 initialization
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-219 會影響你有沒有 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-220 會不會碰到 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-221 你的 data 本身長什麼樣子
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-222 可能也會影響你的 network，你在 train 的時候
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-223 會不會容易遇到 local minima 這件事
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-224 怎麼說呢？
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-225 假設我們有一個 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-226 這個 network 只有一個 hidden layer
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-227 他有 k 個
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-228 他有 n 個 neuron
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-229 input 是一個 vector x
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-230 假設每一個 neuron 他的參數分別就是 w1, w2 到 wn
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-231 那每一個 neuron 的 input 就是 x 跟 w1 做 inner product
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-232 就是你把 w1 的 transpose 乘上 x
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-233 當作 activation function 的 input
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-234 w2 的 transpose 乘 x 當作 activation function 的 input
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-235 wn 的 transpose 乘 x 當作 activation function 的 input
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-236 通過 ReLU 以後再乘上 1
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-237 得到最終的 y，這是你的 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-238 而 w 是要被 train 出來的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-239 我們剛才在討論的時候
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-240 我們都沒有假設我們的 data 應該長什麼樣子
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-241 現在假設 data 是從一個 generator 生出來的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-242 這個 generator 怎麼生 data 呢？
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-243 這個 generator 是一個 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-244 但這個 network 的參數是給定的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-245 假設我們知道說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-246 這是一個 有 k 個 neuron 的 一個 hidden layer 的 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-247 他的參數，也就是每個 neuron 的 weight
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-248 分別是 v1 v2 到 vn
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-249 我寫錯一個地方，大家有發現嗎？
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-250 對，是 k 啊
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-251 其實是 k ，這個 k 跟 n 是不一樣的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-252 這個是這個實驗的重點
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-253 就是 k 跟 n 是不一樣的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-254 所以這個應該是 k，抱歉抱歉
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-255 應該是 k
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-256 v1 的 transpose 乘x，v2 的 transpose 乘 x 到 vn
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-257 這邊怎麼會是寫對的呢
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-258 vk 的 transpose 乘上 x
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-259 最後把他通通加起來，得到你的 label
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-260 所以，你今天在產生 data 的時候
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-261 怎麼產生 data
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-262 從一個 normal distribution
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-263 做 sample，sample 出 x
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-264 丟到這個 label generator，產生你的 label y\hat
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-265 今天這一個 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-266 他要做的事情是想盡辦法去 fit
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-267 這個 x 跟 y\hat 之間的關係
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-268 那你可以想見說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-269 假設今天這兩個 network，他們的 neuron 的數目
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-270 上面這個 network 的 neuron 的數目比較多
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-271 或上面這個 network 他的 neuron 的數目
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-272 大於等於下面這個 network neuron 的數目
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-273 我們只要
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-274 這邊的第一個 neuron 等於這邊的第一個 neuron
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-275 這邊的第二個 neuron 等於這邊的第二個 neuron
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-276 這邊的第 k 個 neuron 等於這邊的第 k 個 neuron
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-277 也就是這邊的第 i 個 neuron 等於這邊的第 i 個 neuron
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-278 你就可以找到 global minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-279 你的 loss 就會是 0
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-280 但是我們發現說，今天雖然只要 n = k
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-281 只要 n 大於等於 k
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-282 就可以找到
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-283 就一定保證
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-284 我們就知道 global minima 長什麼樣子
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-285 但是
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-286 n = k 跟 n &gt; k 得到的結果，其實是很不一樣
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-287 那這個是
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-288 實際實驗的結果
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-289 文獻上的結果，這個文獻告訴我們什麼呢？
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-290 這個文獻告訴我們
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-291 這個文獻告訴我們，他試了不同的 k
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-292 然後假設現在 n 的數目跟 k 的數目是一樣的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-293 也就是你的，要 train 的那個 network neuron 的數目
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-294 跟你的 label generator neuron 的數目是一樣的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-295 這個時候你發現說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-296 除了在 n = 6 的情況下
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-297 local minima 很少
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-298 這邊就是說你真的去 train 那個 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-299 那看有多少的百分比
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-300 就你 train, train, train，train 到停下來
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-301 然後在檢查說，他是不是一個 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-302 然後，看說有多少的百分比是停在一個 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-303 發現說，今天 有很大的機率會停在 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-304 那另外呢
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-305 他還對他停下來的時候，就假設他找到一個 critical point
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-306 他會去算那個 hasion，他也真的算了一下 hasion
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-307 把他的 eigen value 都找出來
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-308 他發現說，eigen value 的平均的最小值都是正的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-309 代表說他多數的時候找到的那個 critical point
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-310 他的 eigen value 都是正的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-311 代表說他是一個
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-312 eigen value 正的代表說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-313 你從這個 critical point 往四周走，都是增加的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-314 代表他是一個 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-315 這個是 n 跟 k
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-316 也就是 label generator 跟你要 train 那個 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-317 他們是一樣的 neuron 的數目的情況下
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-318 但假設你現在增加了
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-319 你的 network 的參數
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-320 假設現在 network 的 neuron
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-321 比 label generator 硬是多一個 neuron 呢
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-322 這個時候你就會發現
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-323 你的 local minima，你就很少遇到 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-324 當然這是一個實驗過程，他並不是理論上的證明
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-325 而是實驗上做出來發現說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-326 當 network 的參數量比較多的時候
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-327 有點那種 overparameterized 的情況
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-328 他參數比他需要的還要多的時候
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-329 這個時候，遇到 local minima 的機會
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-330 就變得很少
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-331 他甚至發現說，假設 n 是設
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-332 大於
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-333 大於等於 k + 2
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-334 k 是 8，你的 label generator
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-335 8 個 neuron 的時候，你的 network 有 9 個 neuron
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-336 如果今天，你的 label generator 有 8 個 neuron
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-337 你的 network 給他 10 個 neuron
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-338 這個時候你會發現說，你完全找不到 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-339 當然並不代表 local minima 真的不存在
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-340 但他在實驗的過程中
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-341 就一直試、一直試、一直 train、一直 train
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-342 然後，都不會卡在 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-343 都可以找到 global minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-344 這個實驗要告訴我們的事情是
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-345 data 對結果也是很重要
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-346 講這麼多到底想告訴大家是什麼呢？
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-347 因為剛才講法好像都是說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-348 ReLU 的 network 或是 deep nonlinear 的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-349 deep 的 network，他就是有 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-350 但是這些實驗的結果並不是要告訴大家說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-351 deep learning 是不 work
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-352 deep learning 是有沒辦法 train 的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-353 而是想要藉由找出什麼樣的狀況有 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-354 而反過來推說什麼樣的狀況沒有 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-355 就假設說我的狀況是這麼多
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-356 那你把有 local minima 的狀況都踢掉
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-357 剩下就知道說，什麼情況下會沒有 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-358 那這樣的理論還沒有出現
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-359 那可以想像說未來
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-360 假設有這種有關 ReLU 的 network
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-361 或 deep 的 network 有沒有 local minima 理論出現的話
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-362 那個理論的 statement 會是這樣
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-363 你不太可能證說 ReLU 的 network 就沒有 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-364 你不太可能做這種 statement，因為
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-365 他就是有 local minima
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-366 你不可能睜眼說，瞎話說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-367 他就是沒有 local minima，剛才很多例子
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-368 他就是有的，所以到時候
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-369 如果真的有理論出來的話，那個理論的說明應該是
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-370 在某種 condition 下，雖然我們還不知道是什麼
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-371 那種 condition 應該要考慮 initialization
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-372 我們剛才講說，你故意 initialize 在
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-373 ReLU network 的那個盲點，你就可以擊敗他
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-374 那你當然不能故意 initialize 在盲點的地方
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-375 所以，在某種 initialize 的情況下
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-376 在某種 data 的 distribution 情況下
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-377 我們剛才有講過說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-378 如果你現在的 data 相對於 network 是比較簡單的
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-379 你 network 給他過多的參數，你就找不到 local
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-380 在實驗上你就找不到 local minima 了
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-381 這意味著說
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-382 今天你的 data 跟你的 network 之間的關係
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-383 也應該被考慮的，在這些情況下
Deep_Learning_Theory_2-3_-_Does_Deep_Network_have_Local_Minima-384 可能會有一個演算法告訴我們說，我們一定能夠找到 global optimal
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-0 剩下的部分，其實還是需要講的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-1 等一下助教會講作業
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-2 請容我再講十分鐘
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-3 因為等下這個是跟作業有關係的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-4 那這個是，其實今天
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-5 在 deep learning 的領域，有這麼一個推論
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-6 這個推論是這樣
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-7 幾乎所有的 local minimum
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-8 他的 loss 跟 global 的 optimum
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-9 都是差不多的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-10 也就是說，你今天在做 optimization 的時候
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-11 卡到、走到 local minimum 也不用驚慌
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-12 因為 local minimum 的 loss 跟 global minimum 的 loss
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-13 應該是差距不大的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-14 而我們今天又知道說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-15 learning 跟 optimization 不一樣
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-16 你應是找一個
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-17 在你的 training set 上，loss 最小的東西
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-18 搞不好你只是 over-fitting 而已啊
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-19 找一個 local minimum
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-20 他的 loss 跟 global minimum 差不多
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-21 其實搞不好也就夠了
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-22 所以，在 deep learning 有這麼樣的一個傳說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-23 這個傳說，是怎麼來的呢？
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-24 其實這個傳說，我第一次聽到的時候是
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-25 Yann Lecun 在 07 年的一個，還是在 06 年的一個演講
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-26 距今已經十年了
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-27 或者是超過十年了，所以
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-28 十年前 Yann Lucan 就已經這麼說了
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-29 他就告訴我們說，train deep learning 的時候不用害怕
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-30 當你卡在 local minima 的時候和 global minima 的值
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-31 應該是差不多的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-32 那這個說法是怎麼來的呢？
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-33 以下是一個簡化的版本
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-34 但其實這些東西現在都已經有證明了
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-35 當年 Yann Lacun 十年前講的時候
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-36 應該是沒有證明的，憑著直覺他居然知道這些事情
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-37 但是十年後，我看到 2017 年的時候有一篇 paper
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-38 已經有對這樣子的假說做了一些證明
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-39 那 Yann Lacun 是怎麼說的呢？
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-40 這邊是一個簡化的版本
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-41 這個簡化的版本是這樣
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-42 我們今天在 train 的時候
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-43 我們用 gradient descent，最後會停在一個 critical point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-44 這個 critical point，他可能是一個 saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-45 也可能是一個 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-46 那他到底是 saddle point，還是一個 local minima 呢？
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-47 我們剛才說，我們要分析 Hessian, H
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-48 我們現在假設 network 有 N 個參數
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-49 那 Hessian, H 就是一個 N*N 的 matrix
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-50 我們又剛才講過說，一個 N*N 的、對稱的矩陣
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-51 他會有 N 個，彼此之間是 orthogonal 的 eigenvector
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-52 所以我們現在把 eigenvector 列出來就是 v1, v2...vN
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-53 每一個 eigenvector 會對應一個 eigne value
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-54 這邊寫作 λ1, λ2...λN
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-55 如果所有 eigenvalue 都是正的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-56 這個 critical point 就是 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-57 所有 eigenvalue 都是負的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-58 他跟 local maxima 有正有負
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-59 他就是 saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-60 現在我們隨便走到一個 critical point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-61 他的 matrix, H，他到底應該是 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-62 還是 saddle point，還是 local maxima 呢？
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-63 這邊引入了一個有點奇怪的假設
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-64 假設今天這些 λ
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-65 是從某一個，比如說 Gaussian distribution sample 出來
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-66 他有一半的機率是正的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-67 有一半的機率是負的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-68 所以，你不要問我說這個是哪來的這樣
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-69 先假設你相信說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-70 這些 λ，就你拿一個 λ1 出來
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-71 他的 eigenvalue 應該是正的還是負的呢
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-72 機率各一半一半
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-73 假設這個前提你相信的話
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-74 如果今天 network 只有一個參數
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-75 今天他走到一個 critical point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-76 因為只有一個參數
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-77 所以只有一個 eigenvector
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-78 你只有一個 eigenvalue
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-79 他有 1/2 的機率是正的，有 1/2 的機率是負的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-80 那意味著說，他有 1/2 的機率是 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-81 有 1/2 的機率是 local maxima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-82 除非正好是 0，不然他不可能是 saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-83 所以，今天在只有一個參數的情況下
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-84 有一半的機率是 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-85 有一半的機率是 local maxima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-86 幾乎不可能是一個￼ saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-87 假設有兩個參數
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-88 你有兩個 λ，λ1 跟 λ2
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-89 今天如果兩個 λ 都是正的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-90 那你有 1/4 的機率發生這件事情
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-91 那他是一個 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-92 兩個 λ 都負的有 1/4 的機率發生這件事情
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-93 那他是一個 local maxima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-94 兩個 λ，一個正、一個負
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-95 那他是一個 saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-96 saddle point 發生的機率有 1/2
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-97 所以發現有兩個參數的時候
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-98 saddle point 發生的機率就已經大過 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-99 假設今天有十個參數呢
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-100 有十個參數你就會發現說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-101 有非常低的機率是 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-102 因為你要全部的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-103 你有 λ1 到 λ10
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-104 然後每次擲骰子，決定他是正的還是負的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-105 是1/2，你擲個銅板決定他是正的還是負的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-106 你要全部都是正的，他才是 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-107 全部都是正的機率是 1/1024
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-108 全部都是負的機率也是 1/1024
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-109 通常你只要有其中一個正的、有一個負的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-110 他就是 saddle point，所以通通、幾乎你每次
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-111 走到一個 critical point 的時候，他都是一個 saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-112 所以，這整套說法告訴我們說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-113 假設一個 network 他的參數很多
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-114 network 參數越多
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-115 當我們今天碰到一個 critical point 的時候
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-116 他就越不可能是一個 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-117 他就越有可能是一個 saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-118 當你今天的參數非常非常多的時候
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-119 你走到一個 critical point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-120 他是 local minima 的機率是幾乎不可能的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-121 幾乎一定是 saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-122 而且這個假設
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-123 還有下半部
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-124 這個下半部是說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-125 我們剛才說 λ 有 1/2 的機率他是正的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-126 有 1/2 的機率他是負的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-127 接下來，我們再進一步想像說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-128 這個機率並不是 1/2
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-129 這個機率跟你現在的 loss 是有關係的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-130 我們假設這個機率我們把它寫成 p
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-131 然後我們說，今天這個 p 是跟
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-132 你現在 loss function 的那個 loss
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-133 我這邊寫 error，但意思是一樣的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-134 就跟 loss function 的 loss 是有關係的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-135 假設現在 loss 越大
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-136 假設現在 loss 越大
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-137 那 p 的值就越大
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-138 p 是負的 eigenvalue 出現的機率
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-139 負的 eigenvalue 代表說有某一條路可以讓你往下走
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-140 這個假設也是滿合理的，你想想看
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-141 loss 大就是你剛開始的時候
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-142 剛開始的時候，你在 loss 比較高的地方
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-143 應該會有很多條路
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-144 你找到一個 critical point，應該會有很多條路讓你
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-145 再往 loss 更低的地方走
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-146 所以你走到一個 loss 很低的地方的時候
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-147 可能 loss 就沒有再更低的地方了
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-148 所以，所有的路可能都是會讓 loss 變高的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-149 所以這個假設也是滿合理的，就是
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-150 如果今天你所在的 error 的 surface
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-151 你是在 training 剛開始的時候，loss 很大的時候
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-152 這個時候呢
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-153 比較有可能出現 negative 的 eigen value
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-154 當你今天 train 到後來，loss 已經很低了
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-155 出現 negative 負的 eigenvalue 的機率就很小
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-156 所以今天 loss，這邊用 ε 代表 loss
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-157 今天隨著 ε，loss 的不同
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-158 你走到一個 critical point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-159 他的 eigenvalue 的分佈啊
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-160 是可能會有像下面的變化
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-161 如果今天，他的 loss 很大
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-162 那你的分佈就是這樣
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-163 他有幾乎一半的機率是正的，幾乎一半的機率是負的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-164 如果今天隨著 loss 越來越小
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-165 那你的這個，這個 λ 代表 eigenvalue
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-166 你的 eigenvalue 就會逐漸往正的那邊偏
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-167 當你的 loss 真的很小的時候
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-168 eigenvalue 就很容易是正的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-169 eigenvalue 就很容易是正的，代表他很容易是一個
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-170 critical point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-171 很容易是一個 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-172 所以這個理論告訴我們說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-173 saddle point 比較容易出現在 loss 大的地方
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-174 local minima 比較容易出現在 loss 低的地方
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-175 所以，對於 deeo learning
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-176 error surface 的想像是長這樣子的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-177 所有的 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-178 可能有一個 global minima 在這個地方
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-179 所有的 local minima，他的 loss 都這麼低
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-180 可能就跟 global minima 差不多低了
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-181 而所有的 saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-182 他的 loss 都很高
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-183 所有的 saddle point 都會出現在 loss 比較高的地方
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-184 所以，如果你今天走到一個 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-185 那這個 local minima 的 loss
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-186 可能就跟 global minima 的 loss 差不多
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-187 而這件事情可能就已經足夠好
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-188 你也不一定要去找出 global minimal
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-189 也許 local optimal 的 loss 已經夠低了
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-190 那這件事情，剛才就只是一個假設而已
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-191 但是，首先在實驗上
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-192 有一些實驗室可以佐證的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-193 而這個是 Banjo 的 paper
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-194 他 train 了三個 network
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-195 這三個 network 呢
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-196 紅色的這個他的 error 很低
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-197 綠色的這個，error 是 23%
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-198 藍色的這個，error 更高，是 28%
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-199 現在他找到了一個 critical point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-200 這三個 critical point，loss 分別是這個樣子
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-201 找到 critical point 以後，你就
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-202 把他的 eigenvalue 解出來
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-203 把那三個 Hessian 的 eigenvalue 解出來
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-204 藍色的 critical point，eigenvalue 的分佈是這樣子
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-205 中線黑色這條線代表 eigenvalue 是 0
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-206 藍色的 critical point，他的 eigenvalue 有一部分是負的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-207 有一部分是正的，代表他是一個 saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-208 綠色的，有一部分 eigenvalue 是負的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-209 有一部分 eigenvalue 是正的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-210 代表他是一個 saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-211 紅色的這個 critical point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-212 你會發現說，他的 loss 很低
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-213 然後，他的 eigenvalue 幾乎通通都是正的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-214 紅色這個，他的 eigenvalue 都是正的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-215 代表他是一個 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-216 而 local minima 是出現在 loss 很低的地方
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-217 這邊只分析了三個點，當然你可以分析更多的點
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-218 那這個其實是作業要大家做的一件事
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-219 看看說這個 Banjo 講的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-220 Banjo 有沒有在騙我們這樣
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-221 這個實驗室什麼？這個實驗的縱軸是 training error
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-222 你就 train 一個 network，train 很多次
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-223 你每次 initialize 的值都不同
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-224 最後收斂的地方就會不太一樣
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-225 你就 train 很多次，找到很多的參數
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-226 找到很多的 critical point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-227 那這些 critical point 各自有不同的 error
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-228 把他們記錄下來
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-229 接下來，每一個 critical point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-230 你都去算出他的 Hessian
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-231 然後去算出那個 Hessian 的 eigenvalue
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-232 那假設你找到的那個 Hessian
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-233 他的 positive eigenvalue 越多的話
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-234 代表他越像是一個 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-235 因為你很難正好最後的 eigenvalue 通通都是正的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-236 是有正有負，只是比例不同而已
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-237 假設今天某一個 Hessian 算出來
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-238 正的 eigenvalue 的比例越高
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-239 代表說他越像是一個 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-240 反之，如果負的越多
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-241 代表他越像是一個 saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-242 現在就把每一個 critical point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-243 他像是 local minima 的比例把他畫出來
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-244 你就會發現說，如果今天一個 critical point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-245 他越像是 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-246 他的 error 就越低
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-247 或者說，一個 critical point 他越像是 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-248 他的 error 就越低
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-249 剛才講的，其實比較像是猜測，那實際上呢
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-250 剛才講的那件事情
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-251 是有理論的證明的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-252 那實際上理論的證明就
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-253 把它放在文獻裡，留給大家做參考
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-254 那理論上的證明給我們的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-255 是圖上的實線
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-256 是圖上的實線
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-257 這個實線告訴我們什麼，這個實線告訴我們說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-258 1 - local minima 的 degree
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-259 也就是負的 eigenvalue 佔的比例
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-260 會正比於 (ε/c - 1)^2/3
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-261 這個 c 一定要小於 ε
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-262 你可能想說 2/3 這個次方哪來的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-263 你就看到 paper 他證出來就是這樣子
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-264 他證出了這樣子
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-265 那這個需要一些假設啦，這個假設到底
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-266 合不合理，就是你未來要繼續研究這樣子
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-267 那至少根據這些假設就證出來說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-268 一個 critical point 他像不像是 local minima 的程度
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-269 確實跟 ε 是成正比的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-270 畫出圖來，就是這個實線
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-271 那這個點點是實際上做實驗的結果
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-272 跟實線其實是滿接近的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-273 還有其他的假說，在早年，14 年的時候
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-274 Yann Lacun 他就把
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-275 spin-glass 的 model 跟 neural network，把它連結在一起
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-276 我不知道大家知不知道 spin-glass 的 model
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-277 他是物理上的一個研究得比較透徹的 model
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-278 然後，他假設說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-279 network 跟 spin-glass 的 model 是非常像的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-280 他用了七個 assumption
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-281 那七個 assumption 有很多是不合理的這樣
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-282 意思是，告訴你說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-283 network 跟 spin-glass model 是一樣的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-284 spin-glass model 有這些這些特質
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-285 所以，network 應該也有這些這些特質
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-286 但是要在假設成立的前提下這樣講就是了
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-287 我就不打算講這部分
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-288 那七個假設有很多是很奇怪的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-289 所以這個留給大家研究
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-290 但是，他自己做了一些實驗，他說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-291 我們左邊是 spin-glass 的 model，我們就不要管他
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-292 右邊是說，我們來 train 一個 network
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-293 那 network 有比較小的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-294 只有 25 個 neuron 的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-295 有比較大的，有 500 個 neuron 的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-296 然後，train, train, train，看最後 loss 卡在哪裡
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-297 假設今天會卡住，就是走到 local minima 就會卡住
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-298 你會發現說，今天如果是一個比較小的 network
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-299 他有可能卡在，他有可能 loss 降的很低
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-300 但它的分布很廣，他有可能卡在 loss 很低的地方
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-301 隨著 network 越來越大，他就變得越來越集中
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-302 如果 network 非常大的時候，他的 loss
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-303 都會幾乎集中在某一個區域
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-304 所以，好像顯示說 local minima
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-305 他的 loss 通通集中在某一個 value 一樣
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-306 這邊還有另外一個理論
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-307 這個也是最新的，這個是 CVPR 2017 的 paper
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-308 這篇 paper 告訴我們說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-309 只要一個 network 夠大，但是他還沒有告訴我們說多大
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-310 只要大到某一個程度
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-311 我們有一個 global optimization 的方法
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-312 我們可以用 gradient descent
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-313 找到 global optimal，無視 optimization
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-314 只要 network 夠大
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-315 但是他這邊其實也有一些比較不 pratical 的假設
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-316 他有一個假設是說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-317 他假設 network 的架構必須是長這個樣子的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-318 也就是說，他有很多條支路
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-319 有很多條支路
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-320 而支路和支路之間是不相通的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-321 我們知道一般的 network 是 fully-connected 的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-322 並不是長這個樣子的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-323 如果我們今天把中間的每一條路簡化成
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-324 只有一個 neuron 的話
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-325 他就變成只有一個 hidden-layer 的 network
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-326 他變成一個正常的 fully-connected network
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-327 但是，證明只有一個 hidden layer 的 network
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-328 可以找到 global optimal
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-329 其實沒有特別厲害，因為你知道說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-330 SVM 他也可以看作是
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-331 一個 hidden layer 的 neural network
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-332 他已經可以找到 global optimal
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-333 他整個 algorithm 看起來有點像是 boosting
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-334 他每次會加一條支路進去，加一條支路進去
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-335 直到那個 error 變成 0 為止
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-336 這邊就是想引用一些文獻
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-337 最早的，有關 Hessian
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-338 最早有關 Hessian 的一些猜測
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-339 出現在這三篇 paper 裡面
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-340 分別是 2014 年的 paper，還有一篇 2015 年的 paper
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-341 那第一篇這個是，Yann Lacun
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-342 下面這一篇，The Loss Surface of Multilayer Networks
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-343 是試圖把 spin-glass model 跟 network 硬是連在一起
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-344 上面這兩篇，就是
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-345 講了一下 saddle point
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-346 才是真正的問題的所在
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-347 然後，local minima 比較不是問題
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-348 你知道裡面其實沒有證明
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-349 他都只是引用一個物理學的文獻
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-350 告訴你說應該是這樣
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-351 看到物理學的文獻跟 network 也是沒什麼太多的關係
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-352 就是他裡面都講得比較模糊
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-353 但是，這篇 paper 就有直接證明告訴你說
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-354 過去的那些猜測是對的
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-355 然後剛才 CVPR 的 paper 我也列在這邊
Deep_Learning_Theory_2-4_-_Geometry_of_Loss_Surfaces_(Conjecture)-356 他就想了一個演算法可以找到 global 的 optimum
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-0 那這邊要講的是在實驗上真正的觀察
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-1 那這個也是作業 1-2 的其中一題
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-2 就是我們試著用一些你想的到的方法
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-3 來 visualize training 的時候
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-4 這個 loss surface 長甚麼樣子
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-5 還有 training process 長甚麼樣子
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-6 那今天這個圖當然不是
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-7 這邊這個圖當然不是一個 network 真正的 error surface
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-8 它只是一個示意圖
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-9 希望大家可以想辦法看能不能夠 visualize network 的參數
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-10 還有它的 loss 之間的關係
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-11 但是怎麼 visualize 呢
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-12 network 的參數有成千上萬個
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-13 它並不是二維的空間 並不是只有兩個參數
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-14 那要怎麼 visualize network 的參數
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-15 跟它的 loss 之間的關係呢
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-16 那以下是*** 用的一個方法
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-17 那這個其實是我看到比較早的一篇
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-18 跟 network error surface visualization 有關係的文章
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-19 應該是在14 還是 15 年的時候就發表了
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-20 這是 *** 做的
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-21 它這邊怎麼說呢 它說
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-22 我們沒有辦法 visualize network  的所有參數
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-23 跟 loss 之間的關係
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-24 但是我們可以 visualize 參數在某一個方向上的變化
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-25 跟 loss 之間的關係
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-26 我們把參數往某一個方向改變的時候
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-27 我們去觀察它對 loss 的影響
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-28 但是要選擇哪一個方向呢
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-29 今天在一個 network 的參數所組成的 space 裡面
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-30 你可以選擇的方向有無窮多個
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-31 好那今天在 visualize 的時候
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-32 **** 他選擇的方向是
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-33 從 training 的 initial 的參數
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-34 到最後你 train 到停止的時候
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-35 你找出來的那一組參數
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-36 中間的這個方向
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-37 也就是說我們一開始 training 的時候
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-38 有一組 initialize 的參數這個寫作 θ0
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-39 train 到最後
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-40 你的 network 停下來了 你 training process 停下來了
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-41 你得到θ*
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-42 接下來我們在 θ0 跟 θ* 之間連一條線
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-43 當然實際上在 training 的時候
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-44 network 並不是從 θ0 直接跑到 θ*
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-45 它從 θ0 跑到 θ* 中間呢
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-46 可能有一大堆曲曲折折的關係
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-47 當然這件事情是發生在一個非常高維的空間上
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-48 只是現在我們把它畫在二維平面上
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-49 這個 θ0 跟 θ* 都是高維空間中的一個 point
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-50 我們只是把它畫在二維的空間上而已
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-51 那有了 θ0 有了 θ*  有了高維空間中的兩個點
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-52 我們就把高維空間的這兩個點連在一起
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-53 接下來我們 visualize 這一條連線上面 loss 的變化
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-54 所以就得到右上角這個圖
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-55 0 這邊代表 θ0 這個參數它的 loss
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-56 那這個 1.0 這邊
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-57 代表 θ* 跟你最後 train 出來的 solution 它的 loss
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-58 那你還可以再順著這個方向繼續向右
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-59 把這邊這一條連線上面的每一個點
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-60 它的 loss 都算出來放在這邊
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-61 放在這邊就會得到這個曲線
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-62 然後你從這個方向還可以繼續
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-63 再順著這個方向繼續往右走
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-64 你走到 θ0+2(θ*-θ0)
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-65 那你就得到右邊的這個部分
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-66 那上面這邊有三條曲線 這三條曲線分別是
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-67 三個不同的 network maxout network relu 還有 sigmoid
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-68 這三種不同的 network 它們的 error surface
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-69 那這個圖告訴我們甚麼呢
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-70 這個圖告訴我們事情其實是
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-71 這個 從這個地方 從 training 的起始
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-72 一直到 training 的結束
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-73 其實這個 error surface 看起來是頗為平坦
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-74 只有非常少的例外舉例來說
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-75 我們最後 training 的時候停在 我們看藍色這條線
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-76 藍色這條線是一個 maxout network
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-77 training 的時候是停在這個地方
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-78 但實際上有一個更低的地方是在這裡
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-79 所以顯然這個地方在用 gradient descent
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-80 並沒有真的找到一個 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-81 因為這個點它的 loss 是更低的
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-82 它比較更像一個 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-83 那你光看這個圖你並不能確定說
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-84 這邊是不是一個 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-85 因為它只是某一個方向上 loss 的變化
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-86 在這個螢幕上這個方向上看起來它是 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-87 但是也許垂直這個螢幕輸入輸出的地方
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-88 它原來還可以 loss 還可以再下降
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-89 其實它是一個 settle point
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-90 也是有可能的 我們不知道
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-91 因為我們只看了某一個方向上 loss 的變化而已
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-92 但是這個圖告訴我們甚麼 這個圖告訴我們說
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-93 其實 local minima 好像沒有我們想像的那麼容易出現
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-94 從這個地方到這個地方
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-95 從這個地方到這個地方
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-96 我們本來預期中間有很多的坑坑洞洞
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-97 它會讓 network 走到一些坑洞以後就陷進去了
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-98 但事實並不是這樣
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-99 從 θ0 到 θ* 中間它是非常平坦的
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-100 network 從這邊走走走
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-101 如果你用 gradient descent 從這邊滑下來
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-102 你 even 可以從起始的地方就滑到終點
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-103 可能在 training 的時候你根本就不需要像我們想像的一樣
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-104 繞了遠路 順著 gradient 的方向 才從θ0 回到 θ*
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-105 這是 ***給我們的一個觀察
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-106 好那剛才看的是一個一般的 fully**** 的 network
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-107 那我沒記錯的話實驗是做在 MNIST 上面
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-108 那有另外一個實驗是看 CNN 它是做在這個 CIFAR10 上面
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-109 然後一樣我們看 CNN 的時候會發現說
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-110 左邊這個 這邊是 training 起始的地方
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-111 中間這個是 training 停止的地方
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-112 就是你最後 training 停止的時候你得到 network 的參數
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-113 注意你最後 training 停止的時候你得到 network 的參數
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-114 它不見得是 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-115 甚至不見得是 critical point
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-116 所以如果你讀 *** paper 的話 你會發現他寫的時候是很小心的
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-117 他都沒有告訴你說這個地方叫 local minima 還是 critical point
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-118 他都只說那是我的 algorithm 的 solution
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-119 因為首先你要確定它是不是 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-120 檢查 *** 才知道它是不是 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-121 不檢查你怎麼能夠說它是一個 local minima 呢
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-122 再來假設你要確認它是不是一個 critical point
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-123 你只要確定你的微分是 0
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-124 如果你不確定微分是 0
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-125 gradient 是 0 你其實也不能夠說它是 critical point
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-126 所以 *** 從來沒有說過這個點叫做 critical point
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-127 他只說它就是 algorithm 的 solution
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-128 好所以從 initial 的地方到最後 algorithm 的 solution
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-129 中間的變化也是頗為平坦的
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-130 然後再往更右邊去
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-131 再往更右邊去會發現說 loss 就急遽的上升
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-132 所以確實這個最後找到的 solution
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-133 看起來有點像是一個 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-134 雖然不能百分之百保證它是 但是看起來有點像
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-135 那有人會說會不會是因為我們的解析度不夠
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-136 如果解析度高一點也許就會看到很多的 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-137 這邊*** 提供的一個解析度比較高的圖 他把這個位置
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-138 放很大然後來觀察
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-139 然後發現說確實有一點點高低起伏
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-140 這個點好像是比旁邊還要低
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-141 就是如果你用 gradient descent 因為這個地方稍微高一點
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-142 所以你用 gradient descent 你不可能
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-143 因為 gradient descent 就是順著那個坡度走
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-144 所以它沒有辦法 除非你有 momentum
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-145 不然你沒有辦法逆勢而為走過這個地方
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-146 所以 machine 在走的時候它應該還是有繞路
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-147 就是它並不是完全順著這條路走
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-148 它還是有繞路避開這個山坡的地方
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-149 所以 network 才能夠走到 solution
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-150 助教上週也有講說如果他把解析度放得非常非常大
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-151 他其實是有看到一些高低起伏的
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-152 那個就留給大家作業的時候觀察
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-153 看看 *** 是不是也在騙我們這樣
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-154 也有可能就是比如說它的 network 太 shallow 了
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-155 所以才沒有觀察到那些高低起伏
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-156 也許當 network 非常深的時候
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-157 你就可以觀察到一些崎嶇的變化
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-158 好那這個是更多的實驗
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-159 因為我剛才發現說很難找到 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-160 非常難找到 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-161 他說如果我們
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-162 他說今天如果隨便找兩個點
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-163 隨便找兩個點然後把它們連在一起
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-164 把它們連在一起
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-165 你會發現他們曲線的變化看起來像是這個樣子
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-166 非常平坦你很難找到一些高低起伏的地方
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-167 那這邊是另外一個 case 它說
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-168 它的 0.0 跟 1.0 分別是兩個 solution
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-169 所以從兩個不同的 initialization train 下去
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-170 會得到兩個不同的最終的參數的 solution
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-171 然後在這兩個參數的 solution 中間連線的這個地形
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-172 這邊我們就可以看到一個高起來的地方
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-173 所以看起來像是說這個 solution 有點像是一個 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-174 另外一個 solution 也有點像是一個 local minima
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-175 然後他們之間有一個高起來的小山丘把它們分開在兩邊
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-176 這個就是一些觀察
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-177 好那剛剛觀察的是只有一個維度而已
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-178 只有一個維度而已
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-179 那如果觀察兩個維度呢
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-180 怎麼觀察兩個維度呢
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-181 這邊的做法是說一個維度是從 θ0 到 θ*
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-182 這個代表的是 projection 這一維
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-183 那另外一個維度是
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-184 現在你的參數跟這一條藍色的線中間的距離
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-185 這個叫做 residual
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-186 所以你今天實際上在 train 的時候
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-187 你是從 θ0 走到 θ*
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-188 這條藍色的線的座標是 projection 這一維的座標
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-189 是 projection 這一維的座標
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-190 然後這個 residual 這一維的座標代表的是現在走的這條線
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-191 現在走的這條線它跟藍線中間的距離
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-192 那要注意一下因為今天在走的時候
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-193 舉例來說在這個點
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-194 你的 residual 是這個點和藍線的距離
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-195 這邊是這個點和這條線的距離 他們的方向是不一樣
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-196 但是它們都是正的值 我不知道大家知不知道我的意思
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-197 所以這個 residual 這個方向並不是高維空間中的某一個固定的方向
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-198 其實不斷的在旋轉這樣
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-199 我不知道大家知不知道我的意思就假設這個是起始的地方
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-200 這個是終止的地方起始到終止的地方是這樣子
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-201 那你在 train network 的參數的時候
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-202 在高維空間從這點走到這一點
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-203 他並不是這樣走直線過去的
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-204 它是歪歪曲曲的走 它可能有的時候網上偏一點
Deep_Learning_Theory_2-5_-_Geometry_of_Loss_Surfaces_(Empirical)-205 再下來 有時候往右邊偏一點再進來再往左邊偏一點
GAN_Lecture_1_(2018)_-_Introduction-0 各位同學大家好，今天要講 Generative Adversarial Network，縮寫是 GAN，就是 GAN
GAN_Lecture_1_(2018)_-_Introduction-1 一開始要做的第一件事情是
GAN_Lecture_1_(2018)_-_Introduction-2 要先知道 GAN 要怎麼正確的發音
GAN_Lecture_1_(2018)_-_Introduction-3 所以就用 Google 小姐來唸一下 GAN 是怎麼發音的
GAN_Lecture_1_(2018)_-_Introduction-4 所以其實是有非常多人在研究這個技術的
GAN_Lecture_1_(2018)_-_Introduction-5 如果在路上聽到有人大聲的說ㄍㄢ四聲，他們其實是在討論這個技術
GAN_Lecture_1_(2018)_-_Introduction-6 今天就要來講這個技術
GAN_Lecture_1_(2018)_-_Introduction-7 首先先說一些抬轎的話，讓你知道這個技術有甚麼重要性
GAN_Lecture_1_(2018)_-_Introduction-8 有一個人在 Quora 這個論壇上問
GAN_Lecture_1_(2018)_-_Introduction-9 最近在 Unsupervised Learning 的領域有甚麼樣的 breakthrough
GAN_Lecture_1_(2018)_-_Introduction-10 然後 Yann LeCun 就親自來回答了
GAN_Lecture_1_(2018)_-_Introduction-11 他說 Adversarial Training，今天要講的 GAN 的技術
GAN_Lecture_1_(2018)_-_Introduction-12 是 the coolest thing since sliced bread
GAN_Lecture_1_(2018)_-_Introduction-13 since sliced bread 是甚麼意思這邊可以學到一個新的英文片語
GAN_Lecture_1_(2018)_-_Introduction-14 since sliced bread 翻成中文的話可以翻成 有史以來 的意思
GAN_Lecture_1_(2018)_-_Introduction-15 google 了一下，因為在過去
GAN_Lecture_1_(2018)_-_Introduction-16 在賣吐司麵包的時候
GAN_Lecture_1_(2018)_-_Introduction-17 是沒有切片的
GAN_Lecture_1_(2018)_-_Introduction-18 所以家庭主婦買這個吐司麵包回去的時候要自己切片覺得很麻煩
GAN_Lecture_1_(2018)_-_Introduction-19 後來有人就發明了切片的吐司，大家就覺得很高興
GAN_Lecture_1_(2018)_-_Introduction-20 有一個好東西問世的時候
GAN_Lecture_1_(2018)_-_Introduction-21 就會說 since sliced bread
GAN_Lecture_1_(2018)_-_Introduction-22 可以翻譯成有史以來的意思
GAN_Lecture_1_(2018)_-_Introduction-23 所以他說 GAN 這個技術是有史以來他覺得最酷的東西
GAN_Lecture_1_(2018)_-_Introduction-24 這是另外一個問題有人問
GAN_Lecture_1_(2018)_-_Introduction-25 在 Deep Learning 領域，最近有沒有甚麼 breakthrough
GAN_Lecture_1_(2018)_-_Introduction-26 Yann LeCun 他也親自來回答了
GAN_Lecture_1_(2018)_-_Introduction-27 GAN 跟他的種種變形是
GAN_Lecture_1_(2018)_-_Introduction-28 近十年來他覺得 ML 的領域最有趣的 idea
GAN_Lecture_1_(2018)_-_Introduction-29 所以 Yann LeCun 他給 GAN 這個技術非常高的評價
GAN_Lecture_1_(2018)_-_Introduction-30 在網路可以找到 GAN 的動物園
GAN_Lecture_1_(2018)_-_Introduction-31 就直接 google GAN 加上 zooGAN、動物園就可以找到各式各樣相關的技術
GAN_Lecture_1_(2018)_-_Introduction-32 因為現在如果發明了一個新的跟 GAN 有關的技術
GAN_Lecture_1_(2018)_-_Introduction-33 你就會在 GAN 前面加一些英文的字
GAN_Lecture_1_(2018)_-_Introduction-34 可是英文的字母只有 26 個
GAN_Lecture_1_(2018)_-_Introduction-35 所以很快的名字就通通被用盡了
GAN_Lecture_1_(2018)_-_Introduction-36 這個名字通通都撞在一起比如說有兩個 LSGAN
GAN_Lecture_1_(2018)_-_Introduction-37 一個是 Least Sqaure GAN一個是 Loss Sensitive GAN
GAN_Lecture_1_(2018)_-_Introduction-38 完全是不一樣的東西但因為英文字母是有限的
GAN_Lecture_1_(2018)_-_Introduction-39 很快的名字就都被用盡了
GAN_Lecture_1_(2018)_-_Introduction-40 這個圖是甚麼？這個圖是講說
GAN_Lecture_1_(2018)_-_Introduction-41 這個 GAN 的動物園裡面
GAN_Lecture_1_(2018)_-_Introduction-42 收錄了各種有名有姓的 GAN
GAN_Lecture_1_(2018)_-_Introduction-43 你可以自己去查查看，裡面光 SGAN 就有四個以上
GAN_Lecture_1_(2018)_-_Introduction-44 它就統計了一下，到目前為止
GAN_Lecture_1_(2018)_-_Introduction-45 已經有接近三百種不同的 GAN 了
GAN_Lecture_1_(2018)_-_Introduction-46 因為英文字母是有限的所以會發生這樣的情形
GAN_Lecture_1_(2018)_-_Introduction-47 這邊是 Variational Approaches for Auto-Encoding Generative Adversarial Networks
GAN_Lecture_1_(2018)_-_Introduction-48 按照他的名字他應該叫 AGAN 或者是 AEGAN
GAN_Lecture_1_(2018)_-_Introduction-49 作者在 paper 裡面寫說 AEGAN 已經被用掉了
GAN_Lecture_1_(2018)_-_Introduction-50 看起來其他的英文字母也通通被用掉了
GAN_Lecture_1_(2018)_-_Introduction-51 他只好把它叫做 alphaGAN
GAN_Lecture_1_(2018)_-_Introduction-52 在其他領域，不止是 Machine Learning 的領域，當然 GAN 是非常的重要的
GAN_Lecture_1_(2018)_-_Introduction-53 在其他領域，GAN 其實也有很多的應用
GAN_Lecture_1_(2018)_-_Introduction-54 我知道在 Image Processing 上會看到很多 GAN 應用
GAN_Lecture_1_(2018)_-_Introduction-55 但在其他和 Image Processing 沒有那麼直接相關的領域
GAN_Lecture_1_(2018)_-_Introduction-56 也可以看到很多 GAN 的應用
GAN_Lecture_1_(2018)_-_Introduction-57 舉例來說我最近才剛去了 ICASSP
GAN_Lecture_1_(2018)_-_Introduction-58 ICASSP 是 Signal Processing 的 conference
GAN_Lecture_1_(2018)_-_Introduction-59 在 ICASSP 裡面我給了一個 GAN 的 tutorial，在 tutorial 的開場
GAN_Lecture_1_(2018)_-_Introduction-60 我就統計了一下近年 ICASSP 跟 GAN 有關的 paper
GAN_Lecture_1_(2018)_-_Introduction-61 我用關鍵字看看那些 paper 有包含這些關鍵字
GAN_Lecture_1_(2018)_-_Introduction-62 看看那些 paper 是跟這些關鍵字是有關係的
GAN_Lecture_1_(2018)_-_Introduction-63 我用 generative 這個關鍵字會發現從 2012 年到 2017 年
GAN_Lecture_1_(2018)_-_Introduction-64 和 generative 有關的 paper 都很少，當然還是有一些啦
GAN_Lecture_1_(2018)_-_Introduction-65 因為 generative 的 model 不是只有 GAN 而已他們用其他的 generative model 但是沒有非常多
GAN_Lecture_1_(2018)_-_Introduction-66 如果用 adversarial 這個詞彙
GAN_Lecture_1_(2018)_-_Introduction-67 在 2013 年的時候有兩篇 paper，他的 title 有 adversarial 這個詞彙，不過他跟 GAN 是完全沒有關係的
GAN_Lecture_1_(2018)_-_Introduction-68 2016 也有一篇，他跟 GAN 也是沒有關係的
GAN_Lecture_1_(2018)_-_Introduction-69 2017 有兩篇，他們確實跟 GAN 有關係
GAN_Lecture_1_(2018)_-_Introduction-70 但到今年 2018 年就有四十篇了所以他成長的速度是 20 倍
GAN_Lecture_1_(2018)_-_Introduction-71 按照這個速度明年應該要有八百篇
GAN_Lecture_1_(2018)_-_Introduction-72 我其實用了 reinforcement 這個詞彙當作對照組
GAN_Lecture_1_(2018)_-_Introduction-73 就會發現 reinforcement 這個詞彙他的成長速度就沒有 adversarial 成長速度這麼快
GAN_Lecture_1_(2018)_-_Introduction-74 GAN 他變成一個非常重要的技術
GAN_Lecture_1_(2018)_-_Introduction-75 接下來的四周都要來講 GAN 這個技術
GAN_Lecture_1_(2018)_-_Introduction-76 今天要講的是最 basic 的 idea
GAN_Lecture_1_(2018)_-_Introduction-77 給你一個概念假設你不知道 GAN 是甚麼的話
GAN_Lecture_1_(2018)_-_Introduction-78 給你一個概念讓你知道這個技術運作起來大概是甚麼樣子
GAN_Lecture_1_(2018)_-_Introduction-79 我們就從 GAN 最基本的概念開始說起
GAN_Lecture_1_(2018)_-_Introduction-80 在 GAN 裡面想要讓機器做到的事情
GAN_Lecture_1_(2018)_-_Introduction-81 是要讓機器來生成東西
GAN_Lecture_1_(2018)_-_Introduction-82 舉例來說讓機器生成影像
GAN_Lecture_1_(2018)_-_Introduction-83 或者是假設在文字處理的領域，你會讓機器
GAN_Lecture_1_(2018)_-_Introduction-84 來寫詩、讓他產生句子、讓他產生文章
GAN_Lecture_1_(2018)_-_Introduction-85 在 generation 這樣的 process 裡面
GAN_Lecture_1_(2018)_-_Introduction-86 你需要做的事情就是訓練出一個 generator
GAN_Lecture_1_(2018)_-_Introduction-87 如果你要做的是要影像生成那這個 generator 要做的事情就是
GAN_Lecture_1_(2018)_-_Introduction-88 你隨便給他一個輸入，舉例來說你 random sample 一個 vector
GAN_Lecture_1_(2018)_-_Introduction-89 有一個 Gaussian Distribution從 Gaussian Distribution 裡面
GAN_Lecture_1_(2018)_-_Introduction-90 random sample 一個 vector，把這個 vector 丟到 generator 裡面
GAN_Lecture_1_(2018)_-_Introduction-91 generator 就要產生一張 image
GAN_Lecture_1_(2018)_-_Introduction-92 丟不同的 vector 就應該產生不同的 image
GAN_Lecture_1_(2018)_-_Introduction-93 這個是 image 的 generation
GAN_Lecture_1_(2018)_-_Introduction-94 在文字的 generation、sentence 的 generation 也是一樣
GAN_Lecture_1_(2018)_-_Introduction-95 丟出一個 vector，generator 就輸出 How are you?
GAN_Lecture_1_(2018)_-_Introduction-96 丟另外一個 vector 他就說 Good morning.
GAN_Lecture_1_(2018)_-_Introduction-97 丟另外一個 vector 他就輸出 Good afternoon. 等等
GAN_Lecture_1_(2018)_-_Introduction-98 我們要用 GAN 來達成的目標
GAN_Lecture_1_(2018)_-_Introduction-99 就是要訓練出這樣的 NN 的 generator
GAN_Lecture_1_(2018)_-_Introduction-100 你可能會有點困惑說，輸入一個 random 的 vector
GAN_Lecture_1_(2018)_-_Introduction-101 他讓 output 一張 image 或者是 output 一段詞彙
GAN_Lecture_1_(2018)_-_Introduction-102 有甚麼用，具體而言就是沒有甚麼用
GAN_Lecture_1_(2018)_-_Introduction-103 我認為比較有用的是 Conditional Generation
GAN_Lecture_1_(2018)_-_Introduction-104 也就是可以輸入一些條件
GAN_Lecture_1_(2018)_-_Introduction-105 比如說輸入文字讓機器產生對應的圖片
GAN_Lecture_1_(2018)_-_Introduction-106 輸入圖片讓機器產生另外一張對應的圖片這之後會講到
GAN_Lecture_1_(2018)_-_Introduction-107 如果不是輸入 random 的東西
GAN_Lecture_1_(2018)_-_Introduction-108 而是輸入一個你了解那是甚麼、可以 control 的東西比如說文字
GAN_Lecture_1_(2018)_-_Introduction-109 或者是影像，讓機器產生對應的東西
GAN_Lecture_1_(2018)_-_Introduction-110 這個技術就有非常多的應用
GAN_Lecture_1_(2018)_-_Introduction-111 這個之後再提
GAN_Lecture_1_(2018)_-_Introduction-112 今天就只 focus 在讓機器吃一個隨機的向量
GAN_Lecture_1_(2018)_-_Introduction-113 他就要 output 你想要的 object
GAN_Lecture_1_(2018)_-_Introduction-114 這樣子的 application 上
GAN_Lecture_1_(2018)_-_Introduction-115 剛才講過在 GAN 裡面，想要訓練的東西
GAN_Lecture_1_(2018)_-_Introduction-116 想要找出來的東西
GAN_Lecture_1_(2018)_-_Introduction-117 就是一個 generator
GAN_Lecture_1_(2018)_-_Introduction-118 這個 generator 是一個 Neural Network
GAN_Lecture_1_(2018)_-_Introduction-119 我們都知道所謂的 Neural Network 他就是一個 function
GAN_Lecture_1_(2018)_-_Introduction-120 input 一個東西，output 一個東西
GAN_Lecture_1_(2018)_-_Introduction-121 在 GAN 裡面他的 input 就是一個 vector
GAN_Lecture_1_(2018)_-_Introduction-122 如果是影像的生成他的 output 就是一張 image
GAN_Lecture_1_(2018)_-_Introduction-123 或是講得更具體一點
GAN_Lecture_1_(2018)_-_Introduction-124 在影像生成的話，generator output 就是一個 high-dimensional 的向量
GAN_Lecture_1_(2018)_-_Introduction-125 generator output 是一個向量
GAN_Lecture_1_(2018)_-_Introduction-126 這個向量非常非常的長
GAN_Lecture_1_(2018)_-_Introduction-127 這向量的每一個 dimension 就對應到影像中的一個 pixel 的顏色
GAN_Lecture_1_(2018)_-_Introduction-128 把這個 high-dimensional 向量排成一張影像的樣子
GAN_Lecture_1_(2018)_-_Introduction-129 就可以讓 generator 產生一張圖片
GAN_Lecture_1_(2018)_-_Introduction-130 或者是講得更具體而言
GAN_Lecture_1_(2018)_-_Introduction-131 假設想要讓機器做二次元頭像的生成
GAN_Lecture_1_(2018)_-_Introduction-132 那得到的結果可能就是這樣，有一個 generator
GAN_Lecture_1_(2018)_-_Introduction-133 隨便給他一個向量，他的輸出就是一個二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-134 這張圖是機器真正生成的是用右上角這個程式生成的
GAN_Lecture_1_(2018)_-_Introduction-135 而通常輸入的向量他的每一個 dimension
GAN_Lecture_1_(2018)_-_Introduction-136 會對應到圖片的某種特徵
GAN_Lecture_1_(2018)_-_Introduction-137 也就是說改變了其中一個 dimension 的數值
GAN_Lecture_1_(2018)_-_Introduction-138 就會發現產生出來的圖片的某種特徵有所改變
GAN_Lecture_1_(2018)_-_Introduction-139 假設第一個 dimension 對應的是頭髮的長度
GAN_Lecture_1_(2018)_-_Introduction-140 把這個 vector 他的第一個 dimension 的值從 0.1 調到 3
GAN_Lecture_1_(2018)_-_Introduction-141 generator 的 output 就會是一個長頭髮的角色
GAN_Lecture_1_(2018)_-_Introduction-142 或者是假設 input 的 vector
GAN_Lecture_1_(2018)_-_Introduction-143 他的倒數第二個 dimension 對應到頭髮是不是藍色的
GAN_Lecture_1_(2018)_-_Introduction-144 值越大代表頭髮越藍
GAN_Lecture_1_(2018)_-_Introduction-145 把這個值從 2.4 調到 5.4
GAN_Lecture_1_(2018)_-_Introduction-146 產生出來的角色就會變成藍頭髮
GAN_Lecture_1_(2018)_-_Introduction-147 這兩個角色看起來非常像
GAN_Lecture_1_(2018)_-_Introduction-148 因為只改了倒數第二維而已其他維度的值固定不變的
GAN_Lecture_1_(2018)_-_Introduction-149 只改了倒數第二維而已
GAN_Lecture_1_(2018)_-_Introduction-150 所以他只改變了頭髮的顏色，其他的特徵
GAN_Lecture_1_(2018)_-_Introduction-151 仍然會是很相似的
GAN_Lecture_1_(2018)_-_Introduction-152 或者是說假設最後一個維度代表是嘴巴的大小
GAN_Lecture_1_(2018)_-_Introduction-153 本來這個值很小，所以他是無口的狀態
GAN_Lecture_1_(2018)_-_Introduction-154 把這個值調大，然後他就笑起來了，笑口常開這樣
GAN_Lecture_1_(2018)_-_Introduction-155 這個是 generator，在 GAN 裡面比較神奇的地方
GAN_Lecture_1_(2018)_-_Introduction-156 就是同時會訓練一個 discriminator
GAN_Lecture_1_(2018)_-_Introduction-157 等一下會講這個 discriminator 和 generator 之間
GAN_Lecture_1_(2018)_-_Introduction-158 有甚麼樣的關係
GAN_Lecture_1_(2018)_-_Introduction-159 先來看這個 discriminator 做的事情是甚麼
GAN_Lecture_1_(2018)_-_Introduction-160 這個 discriminator 也是一個 Neural Network
GAN_Lecture_1_(2018)_-_Introduction-161 剛才講過 Neural Network 就是一個 function
GAN_Lecture_1_(2018)_-_Introduction-162 吃一個東西當 input，就輸出一個東西
GAN_Lecture_1_(2018)_-_Introduction-163 discriminator 他是吃一張圖片
GAN_Lecture_1_(2018)_-_Introduction-164 當作 input 假設產生的是圖片的話
GAN_Lecture_1_(2018)_-_Introduction-165 他吃圖片當 input，假設要產生的不是圖片
GAN_Lecture_1_(2018)_-_Introduction-166 是句子的話，他就吃句子當作 input
GAN_Lecture_1_(2018)_-_Introduction-167 discriminator 吃一張圖片當 input
GAN_Lecture_1_(2018)_-_Introduction-168 他的輸出是一個 scalar 是一個數值
GAN_Lecture_1_(2018)_-_Introduction-169 這個數值代表甚麼意思這個數值代表產生出來的這張圖片的 quality
GAN_Lecture_1_(2018)_-_Introduction-170 這個數值越大，就代表產生出來的這張圖片的 quality 越高
GAN_Lecture_1_(2018)_-_Introduction-171 他看起來越像是真實的圖片
GAN_Lecture_1_(2018)_-_Introduction-172 他看起來越 realistic
GAN_Lecture_1_(2018)_-_Introduction-173 這一句要講的是產生出來的數值越大
GAN_Lecture_1_(2018)_-_Introduction-174 discriminator output 數值越大
GAN_Lecture_1_(2018)_-_Introduction-175 就代表輸入的圖片越真實
GAN_Lecture_1_(2018)_-_Introduction-176 假設要做二次元人物頭像的生成
GAN_Lecture_1_(2018)_-_Introduction-177 讓機器吃這張圖片，因為畫得很好所以就是 1.0
GAN_Lecture_1_(2018)_-_Introduction-178 這個也畫得很好所以 output 1.0假設 1.0 就是他可以輸出最大的值
GAN_Lecture_1_(2018)_-_Introduction-179 假設這個畫得很差，機器就給他 0.1 分
GAN_Lecture_1_(2018)_-_Introduction-180 這個也畫得很差，機器就給他 0.1 分
GAN_Lecture_1_(2018)_-_Introduction-181 這個就是 discriminator 做的事情
GAN_Lecture_1_(2018)_-_Introduction-182 等一下第三堂課助教會來講一下作業
GAN_Lecture_1_(2018)_-_Introduction-183 作業三做的是二次元人物頭像的生成
GAN_Lecture_1_(2018)_-_Introduction-184 一樣分成 3-1、3-2、3-3 這樣子
GAN_Lecture_1_(2018)_-_Introduction-185 3-1 就是做二次元人物頭像的生成等一下細節就交給助教來講
GAN_Lecture_1_(2018)_-_Introduction-186 在 GAN 裡面有一個 generator
GAN_Lecture_1_(2018)_-_Introduction-187 有一個 discriminator
GAN_Lecture_1_(2018)_-_Introduction-188 他們之間的關係就好像是獵食者跟他的獵物之間的關係
GAN_Lecture_1_(2018)_-_Introduction-189 怎麼說呢
GAN_Lecture_1_(2018)_-_Introduction-190 右上角這個是一隻枯葉蝶，這不是一個枯葉，是一個枯葉蝶
GAN_Lecture_1_(2018)_-_Introduction-191 枯葉蝶跟枯葉長得非常的相似
GAN_Lecture_1_(2018)_-_Introduction-192 為甚麼枯葉蝶會長的跟枯葉非常的相似呢
GAN_Lecture_1_(2018)_-_Introduction-193 那是因為有天擇的壓力
GAN_Lecture_1_(2018)_-_Introduction-194 枯葉蝶的祖先也是彩色的
GAN_Lecture_1_(2018)_-_Introduction-195 因為麻雀會吃枯葉蝶
GAN_Lecture_1_(2018)_-_Introduction-196 所以枯葉蝶在天擇的壓力之下就變成棕色的
GAN_Lecture_1_(2018)_-_Introduction-197 因為麻雀判斷蝴蝶能不能吃的標準就是他是甚麼顏色
GAN_Lecture_1_(2018)_-_Introduction-198 如果是彩色就會被吃掉如果是棕色就不會被吃掉
GAN_Lecture_1_(2018)_-_Introduction-199 天擇的壓力下，枯葉蝶的祖先就變成是棕色
GAN_Lecture_1_(2018)_-_Introduction-200 但是枯葉蝶天敵也是會演化的
GAN_Lecture_1_(2018)_-_Introduction-201 這個東西他是波波
GAN_Lecture_1_(2018)_-_Introduction-202 波波進化就會變成比比鳥
GAN_Lecture_1_(2018)_-_Introduction-203 如果說錯了等下記得糾正我
GAN_Lecture_1_(2018)_-_Introduction-204 還是比比鳥會進化變波波
GAN_Lecture_1_(2018)_-_Introduction-205 是比比鳥對不對，我記得的是對的這個是比比鳥
GAN_Lecture_1_(2018)_-_Introduction-206 波波後來就進化成比比鳥
GAN_Lecture_1_(2018)_-_Introduction-207 比比鳥判斷一個東西能不能吃的標準並不是看顏色而是看有沒有葉脈的紋路
GAN_Lecture_1_(2018)_-_Introduction-208 如果沒有葉脈的紋路還是會被吃掉的
GAN_Lecture_1_(2018)_-_Introduction-209 所以枯葉蝶的祖先在天擇的壓力之下就產生了看起來像是葉脈的條紋
GAN_Lecture_1_(2018)_-_Introduction-210 他可以騙過比比鳥
GAN_Lecture_1_(2018)_-_Introduction-211 其實比比鳥也會再進化
GAN_Lecture_1_(2018)_-_Introduction-212 他再進化，他可能有別的標準來判斷這個東西是不是可以吃的
GAN_Lecture_1_(2018)_-_Introduction-213 枯葉蝶也會再不斷地進化
GAN_Lecture_1_(2018)_-_Introduction-214 所以獵食者和天敵就會在互相拮抗之中變得越來越強
GAN_Lecture_1_(2018)_-_Introduction-215 而這個枯葉蝶就像是 generator
GAN_Lecture_1_(2018)_-_Introduction-216 而他的天敵就像是 discriminator
GAN_Lecture_1_(2018)_-_Introduction-217 所以假設要讓機器做二次元人物頭像的生成
GAN_Lecture_1_(2018)_-_Introduction-218 首先要準備一個 database
GAN_Lecture_1_(2018)_-_Introduction-219 這個 database 裡面有很多真實的二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-220 generator 就是一個 network
GAN_Lecture_1_(2018)_-_Introduction-221 但一開始他的參數是隨機的
GAN_Lecture_1_(2018)_-_Introduction-222 所以一開始 generator 也不知道怎麼產生二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-223 他只能產生看起來像是雜訊的東西
GAN_Lecture_1_(2018)_-_Introduction-224 discriminator 做的事情就是給他一張圖片
GAN_Lecture_1_(2018)_-_Introduction-225 判斷這張圖片像是 generator 生成的還是像是真實的圖片
GAN_Lecture_1_(2018)_-_Introduction-226 接下來 generator 就是要想辦法騙過第一代的 discriminator
GAN_Lecture_1_(2018)_-_Introduction-227 第一代的 discriminator 可以分辨第一代 generator 的 output 和真實圖片之間的差異
GAN_Lecture_1_(2018)_-_Introduction-228 他可能就用有沒有顏色來判斷他是真實的還是被生成的圖片
GAN_Lecture_1_(2018)_-_Introduction-229 所以第二代的 generator 進化了他想要騙過第一代的 discriminator
GAN_Lecture_1_(2018)_-_Introduction-230 所以他就會產生有色彩的圖片
GAN_Lecture_1_(2018)_-_Introduction-231 但是 discriminator 跟著也會再進化
GAN_Lecture_1_(2018)_-_Introduction-232 第一代的 discriminator 會被第二代 generator 產生的圖片騙過
GAN_Lecture_1_(2018)_-_Introduction-233 但是第二代的 discriminator 他會學著去分辨這兩種圖片之間的差異
GAN_Lecture_1_(2018)_-_Introduction-234 他可能會發現真實的圖片是有嘴巴的
GAN_Lecture_1_(2018)_-_Introduction-235 如果是第二代 generator 產生的圖片是沒有嘴巴的
GAN_Lecture_1_(2018)_-_Introduction-236 他就用這個標準來判斷一張圖片是不是真正的圖片
GAN_Lecture_1_(2018)_-_Introduction-237 是不是真正人畫的二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-238 generator 就會跟著再進化
GAN_Lecture_1_(2018)_-_Introduction-239 變成第三代的 generator
GAN_Lecture_1_(2018)_-_Introduction-240 第三代的 generator 產生出來的圖片
GAN_Lecture_1_(2018)_-_Introduction-241 會變過第二代的 discriminator
GAN_Lecture_1_(2018)_-_Introduction-242 一旦是第二代的 discriminator 會進化成第三代的 discriminator
GAN_Lecture_1_(2018)_-_Introduction-243 所以 generator 和 discriminator 他們就會不斷的進化
GAN_Lecture_1_(2018)_-_Introduction-244 所以 generator 產生出來的圖片就會越來越真實
GAN_Lecture_1_(2018)_-_Introduction-245 因為 generator 和 discriminator 有一個對抗的關係
GAN_Lecture_1_(2018)_-_Introduction-246 他們像是天敵與被獵食者之間的關係
GAN_Lecture_1_(2018)_-_Introduction-247 所以用 adversarial 這個詞彙來命名這個技術adversarial 就是對抗的意思
GAN_Lecture_1_(2018)_-_Introduction-248 叫做 Generative Adversarial Network
GAN_Lecture_1_(2018)_-_Introduction-249 但有人就會問為甚麼是讓兩個 network 互相對抗
GAN_Lecture_1_(2018)_-_Introduction-250 為甚麼不能彼此合作為甚麼世界不是充滿愛與和平
GAN_Lecture_1_(2018)_-_Introduction-251 這個東西只是一種擬人化的說法而已
GAN_Lecture_1_(2018)_-_Introduction-252 現在這個講法讓你覺得他們是在對抗的
GAN_Lecture_1_(2018)_-_Introduction-253 GAN 這個技術是 Ian Goodfellow 在 14 年提出來的
GAN_Lecture_1_(2018)_-_Introduction-254 原始的 paper 他是用作假鈔跟警察的例子
GAN_Lecture_1_(2018)_-_Introduction-255 generator 不斷的在做假鈔，他是壞人
GAN_Lecture_1_(2018)_-_Introduction-256 discriminator 是警察，他要去判斷這張鈔票是真鈔還是假鈔
GAN_Lecture_1_(2018)_-_Introduction-257 最後 generator 做的鈔票會越來越像真鈔
GAN_Lecture_1_(2018)_-_Introduction-258 直到 discriminator 完全沒有辦法分辨為止
GAN_Lecture_1_(2018)_-_Introduction-259 這個例子我沒有很喜歡，因為是充滿暴力犯罪的例子
GAN_Lecture_1_(2018)_-_Introduction-260 從那個例子看起來 generator 和 discriminator 是對抗的關係
GAN_Lecture_1_(2018)_-_Introduction-261 但是只要換一個例子，generator 跟 discriminator 其實可以看作是合作的關係
GAN_Lecture_1_(2018)_-_Introduction-262 可以想成 generator 是學生discriminator 是老師、是指導教授
GAN_Lecture_1_(2018)_-_Introduction-263 generator 要學習怎麼畫二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-264 discriminator 看過很多畫二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-265 他知道二次元人物的頭像應該長甚麼樣子
GAN_Lecture_1_(2018)_-_Introduction-266 所以一開始第一代的 generator 就是一年級的學生，他不知道怎麼畫二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-267 所以他畫出來的東西就非常的模糊、非常的奇怪
GAN_Lecture_1_(2018)_-_Introduction-268 他就把這些圖片拿給老師看
GAN_Lecture_1_(2018)_-_Introduction-269 一年級的老師會給他 feedback 告訴他
GAN_Lecture_1_(2018)_-_Introduction-270 你這些圖片
GAN_Lecture_1_(2018)_-_Introduction-271 跟真實圖片差異是真實圖片有兩個圈圈當然這只是一個擬人化的講法
GAN_Lecture_1_(2018)_-_Introduction-272 等一下會具體的告訴你實際上在 train network 是甚麼樣子
GAN_Lecture_1_(2018)_-_Introduction-273 這個 network 不要以為他會講話他不會講話
GAN_Lecture_1_(2018)_-_Introduction-274 這個只是個比喻的講法等一下會講實際上 training 是甚麼樣子
GAN_Lecture_1_(2018)_-_Introduction-275 discriminator 發現這兩種圖片的差別是這個有兩個圈圈他都沒有圈圈
GAN_Lecture_1_(2018)_-_Introduction-276 他就會把這個資訊 feedback 給 generator 告訴 generator 你畫的這些圖片跟真實圖片的差異
GAN_Lecture_1_(2018)_-_Introduction-277 是你沒有畫兩個圈圈
GAN_Lecture_1_(2018)_-_Introduction-278 這個 generator 接下來就升上二年級
GAN_Lecture_1_(2018)_-_Introduction-279 他會記得老師的教誨，他知道在畫二次元人物頭像的時候
GAN_Lecture_1_(2018)_-_Introduction-280 應該要把兩個圈圈點上去
GAN_Lecture_1_(2018)_-_Introduction-281 然後他就把畫出來的頭像再給二年級的老師看
GAN_Lecture_1_(2018)_-_Introduction-282 他本來以為老師會說他好棒
GAN_Lecture_1_(2018)_-_Introduction-283 但其實老師只會說他好棒棒而已
GAN_Lecture_1_(2018)_-_Introduction-284 因為二年級的老師就變得更嚴格了
GAN_Lecture_1_(2018)_-_Introduction-285 二年級的老師會告訴他
GAN_Lecture_1_(2018)_-_Introduction-286 你畫的跟真實的圖片仍然有很大的差異
GAN_Lecture_1_(2018)_-_Introduction-287 因為真實的圖片是彩色的你畫的是沒有彩色的
GAN_Lecture_1_(2018)_-_Introduction-288 所以 generator 接下來三年級以後他產生的圖片就會變成彩色的
GAN_Lecture_1_(2018)_-_Introduction-289 generator 就會不斷的進步
GAN_Lecture_1_(2018)_-_Introduction-290 discriminator 就會變得越來越嚴格
GAN_Lecture_1_(2018)_-_Introduction-291 學生畫的越來越好，老師越來越嚴格
GAN_Lecture_1_(2018)_-_Introduction-292 最後學生就可以畫出非常像是真的人所畫的二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-293 在互動的過程中
GAN_Lecture_1_(2018)_-_Introduction-294 其實會發現會有兩個問題
GAN_Lecture_1_(2018)_-_Introduction-295 第一個問題是為甚麼 generator 沒有辦法自己學呢
GAN_Lecture_1_(2018)_-_Introduction-296 為甚麼一定要有一個 discriminator 介入
GAN_Lecture_1_(2018)_-_Introduction-297 為甚麼 generator 沒有辦法直接從這些範例裡面
GAN_Lecture_1_(2018)_-_Introduction-298 學習怎麼產生二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-299 為甚麼一定要透過 discriminator 才能學習呢
GAN_Lecture_1_(2018)_-_Introduction-300 等一下會試著來回答這個問題
GAN_Lecture_1_(2018)_-_Introduction-301 第二個問題是 discriminator 這麼會批評
GAN_Lecture_1_(2018)_-_Introduction-302 為甚麼他不自己做呢
GAN_Lecture_1_(2018)_-_Introduction-303 我想大家都有這個困惑對不對
GAN_Lecture_1_(2018)_-_Introduction-304 老師為甚麼都不自己 coding 呢
GAN_Lecture_1_(2018)_-_Introduction-305 老師為甚麼都只用嘴巴 coding 呢
GAN_Lecture_1_(2018)_-_Introduction-306 等一下要來回答這個問題
GAN_Lecture_1_(2018)_-_Introduction-307 所以 generator 跟 discriminator 之間的關係
GAN_Lecture_1_(2018)_-_Introduction-308 他們就是寫作敵人，唸作朋友
GAN_Lecture_1_(2018)_-_Introduction-309 寫作敵人，唸做朋友的意思知道嗎
GAN_Lecture_1_(2018)_-_Introduction-310 就是像塔矢亮和進藤光的關係
GAN_Lecture_1_(2018)_-_Introduction-311 或者是像是佐助與鳴人的關係
GAN_Lecture_1_(2018)_-_Introduction-312 所以他們是朋友也是敵人
GAN_Lecture_1_(2018)_-_Introduction-313 接下來就來正式講一下
GAN_Lecture_1_(2018)_-_Introduction-314 Generative Adversarial Network 他的演算法是怎麼運作的
GAN_Lecture_1_(2018)_-_Introduction-315 我們之後會講到他的原理
GAN_Lecture_1_(2018)_-_Introduction-316 還有他背後的理論，今天先不講理論，就只講他操作起來看起來像是甚麼樣子
GAN_Lecture_1_(2018)_-_Introduction-317 就算你對他的理論還沒有任何的了解
GAN_Lecture_1_(2018)_-_Introduction-318 其實從這個操作的過程、演算法其實也可以體會為甚麼他是個有用的方法
GAN_Lecture_1_(2018)_-_Introduction-319 有一個 generator，有一個 discriminator 他們都是 network
GAN_Lecture_1_(2018)_-_Introduction-320 我們知道 train network 一開始參數 random initialized 的
GAN_Lecture_1_(2018)_-_Introduction-321 random initialized generator 跟 discriminator 的 network 的參數
GAN_Lecture_1_(2018)_-_Introduction-322 接下來要 iterative 的去 train 這個 generator 和 discriminator
GAN_Lecture_1_(2018)_-_Introduction-323 要跑很多個 iteration
GAN_Lecture_1_(2018)_-_Introduction-324 在每一個 iteration 裡面要做兩件事有兩個步驟
GAN_Lecture_1_(2018)_-_Introduction-325 第一個步驟，把 generator fix 住
GAN_Lecture_1_(2018)_-_Introduction-326 把 generator 參數固定住
GAN_Lecture_1_(2018)_-_Introduction-327 只去調 discriminator 參數
GAN_Lecture_1_(2018)_-_Introduction-328 只 train discriminator，把 generator 固定住
GAN_Lecture_1_(2018)_-_Introduction-329 怎麼做呢有一個固定住的 generator
GAN_Lecture_1_(2018)_-_Introduction-330 然後把一大堆 random vector 丟到 generator 裡面
GAN_Lecture_1_(2018)_-_Introduction-331 這個 generator 就會很產生很多圖片
GAN_Lecture_1_(2018)_-_Introduction-332 因為一開始 generator 參數是隨機的
GAN_Lecture_1_(2018)_-_Introduction-333 所以產生出來的圖片並不會特別好，可能是非常糟的
GAN_Lecture_1_(2018)_-_Introduction-334 接下來你有一個 database，剛才講過如果要讓機器產生某種東西
GAN_Lecture_1_(2018)_-_Introduction-335 你要蒐集那種東西的範例你要讓他產生二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-336 你要蒐集很多二次元人物的頭像當作範例
GAN_Lecture_1_(2018)_-_Introduction-337 你要讓機器寫詩，你要蒐集很多的詩詞作為範例
GAN_Lecture_1_(2018)_-_Introduction-338 有一個 database 裡面都是二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-339 從這個二次元頭像裡面 sample 出一些 example
GAN_Lecture_1_(2018)_-_Introduction-340 現在有兩組圖片，一組是從 database 裡面 sample 出來的
GAN_Lecture_1_(2018)_-_Introduction-341 另外一組圖片是 generator 所生成的
GAN_Lecture_1_(2018)_-_Introduction-342 接下來就要去訓練 discriminator，去調整 discriminator 的參數
GAN_Lecture_1_(2018)_-_Introduction-343 怎麼調 discriminator 參數
GAN_Lecture_1_(2018)_-_Introduction-344 目標就是，如果這個 image 是 realistic 的
GAN_Lecture_1_(2018)_-_Introduction-345 就給他比較高的分數
GAN_Lecture_1_(2018)_-_Introduction-346 如果這個 image 是 generator 所產生的
GAN_Lecture_1_(2018)_-_Introduction-347 就給他比較低的分數
GAN_Lecture_1_(2018)_-_Introduction-348 如果是從 database sample 出來的 image 就是高分
GAN_Lecture_1_(2018)_-_Introduction-349 從 generator 產生出來的 image 就是低分
GAN_Lecture_1_(2018)_-_Introduction-350 可以把它當作是 regression 的 problem
GAN_Lecture_1_(2018)_-_Introduction-351 可以把它當作是 classification 的 problem
GAN_Lecture_1_(2018)_-_Introduction-352 反正不管怎麼做，就是要訓練這個 network
GAN_Lecture_1_(2018)_-_Introduction-353 把這組圖片丟進去
GAN_Lecture_1_(2018)_-_Introduction-354 output 的分數就是大的
GAN_Lecture_1_(2018)_-_Introduction-355 把這組圖片丟進去，output 的分數就是小的
GAN_Lecture_1_(2018)_-_Introduction-356 講得更具體一點，訓練的目標就是
GAN_Lecture_1_(2018)_-_Introduction-357 把這四張圖片丟進去 discriminator，output 的值要離 1 越接近越好
GAN_Lecture_1_(2018)_-_Introduction-358 把這四張圖片丟進去 discriminator，output 的值要離 0 越接近越好
GAN_Lecture_1_(2018)_-_Introduction-359 就用這 criteria 去訓練 discriminator
GAN_Lecture_1_(2018)_-_Introduction-360 我相信這個在實作上對大家來說
GAN_Lecture_1_(2018)_-_Introduction-361 都不是問題，就跟一般訓練 network 作 regression，或是訓練一個 network 作 classification 意思是一樣的
GAN_Lecture_1_(2018)_-_Introduction-362 下一步要固定住 discriminator
GAN_Lecture_1_(2018)_-_Introduction-363 我們只去調 generator 的參數
GAN_Lecture_1_(2018)_-_Introduction-364 前一步是固定 generator 只調 discriminator
GAN_Lecture_1_(2018)_-_Introduction-365 discriminator 訓練好了固定住 discriminator
GAN_Lecture_1_(2018)_-_Introduction-366 只調 generator
GAN_Lecture_1_(2018)_-_Introduction-367 怎麼調 generator 的參數
GAN_Lecture_1_(2018)_-_Introduction-368 先把一個 vector 丟到 generator 裡面
GAN_Lecture_1_(2018)_-_Introduction-369 generator 會產生一張圖片
GAN_Lecture_1_(2018)_-_Introduction-370 接下來把這張圖片丟到 discriminator 裡面
GAN_Lecture_1_(2018)_-_Introduction-371 discriminator 會給他一個分數
GAN_Lecture_1_(2018)_-_Introduction-372 generator 訓練的目標就是要去騙過 discriminator
GAN_Lecture_1_(2018)_-_Introduction-373 但所謂的騙是一個擬人化的講法
GAN_Lecture_1_(2018)_-_Introduction-374 實際上做的事情是
GAN_Lecture_1_(2018)_-_Introduction-375 希望 generator 產生出來的圖片
GAN_Lecture_1_(2018)_-_Introduction-376 discriminator 可以給他比較高的分數
GAN_Lecture_1_(2018)_-_Introduction-377 固定住 discriminator 的參數
GAN_Lecture_1_(2018)_-_Introduction-378 只去調 generator 的參數
GAN_Lecture_1_(2018)_-_Introduction-379 希望 generator 的 output 丟到 discriminator 以後
GAN_Lecture_1_(2018)_-_Introduction-380 他 output 的值可以越大越好
GAN_Lecture_1_(2018)_-_Introduction-381 那你可以想見說
GAN_Lecture_1_(2018)_-_Introduction-382 剛才在前一步驟已經把 discriminator 訓練好了
GAN_Lecture_1_(2018)_-_Introduction-383 這個訓練好的 discriminator
GAN_Lecture_1_(2018)_-_Introduction-384 看到好的 image 就會給他比較大的分數
GAN_Lecture_1_(2018)_-_Introduction-385 既然 generator 可以調整他的參數
GAN_Lecture_1_(2018)_-_Introduction-386 使得他 output 的 image，discriminator 給他高分
GAN_Lecture_1_(2018)_-_Introduction-387 但顯然 generator 產生出來的 image
GAN_Lecture_1_(2018)_-_Introduction-388 會是比較真實的
GAN_Lecture_1_(2018)_-_Introduction-389 因為 discriminator 是看到真實的圖片才給他高分
GAN_Lecture_1_(2018)_-_Introduction-390 generator 既然可以調整參數產生出來的圖片是 discriminator 會給高分的
GAN_Lecture_1_(2018)_-_Introduction-391 但顯然 generator 產生出來的圖片會比較真實
GAN_Lecture_1_(2018)_-_Introduction-392 實際在 implement 這個 code 怎麼做呢
GAN_Lecture_1_(2018)_-_Introduction-393 實際上在 implement 的時候
GAN_Lecture_1_(2018)_-_Introduction-394 你會把 generator 跟 discriminator 合起來
GAN_Lecture_1_(2018)_-_Introduction-395 當作是一個巨大的 network
GAN_Lecture_1_(2018)_-_Introduction-396 大家知道意思嗎假設 generator 是五層
GAN_Lecture_1_(2018)_-_Introduction-397 discriminator 也是五層
GAN_Lecture_1_(2018)_-_Introduction-398 就是把前面的五層跟後面的五層接在一起
GAN_Lecture_1_(2018)_-_Introduction-399 變成一個有十層的 network
GAN_Lecture_1_(2018)_-_Introduction-400 這個十層的 network input 是一個 vector
GAN_Lecture_1_(2018)_-_Introduction-401 他的 output 就是一個數字
GAN_Lecture_1_(2018)_-_Introduction-402 但是在這個十層的 network 裡面其中的 Hidden Layer
GAN_Lecture_1_(2018)_-_Introduction-403 他很寬，他的 output 就是一張 image
GAN_Lecture_1_(2018)_-_Introduction-404 大家了解我意思嗎假設這個是 64 x 64 的 image
GAN_Lecture_1_(2018)_-_Introduction-405 那已就會有其中一個 Hidden Layer 他的 output 會 output 64 x 64 個數值
GAN_Lecture_1_(2018)_-_Introduction-406 他會 output 64 x 64 維
GAN_Lecture_1_(2018)_-_Introduction-407 把那個 Hidden Layer 拿出來
GAN_Lecture_1_(2018)_-_Introduction-408 就可以把他看作是一張 image
GAN_Lecture_1_(2018)_-_Introduction-409 在 train 這個 network 的時候
GAN_Lecture_1_(2018)_-_Introduction-410 就是固定最後幾個 Hidden Layer
GAN_Lecture_1_(2018)_-_Introduction-411 只調前面幾個 Hidden Layer
GAN_Lecture_1_(2018)_-_Introduction-412 他讓 output 的值越大越好
GAN_Lecture_1_(2018)_-_Introduction-413 因為我們現在是讓他 output 越大越好
GAN_Lecture_1_(2018)_-_Introduction-414 所以不是 Gradient Descent，Gradient Descent 是讓你的目標越小越好
GAN_Lecture_1_(2018)_-_Introduction-415 你做的事情其實是 Gradient Ascent
GAN_Lecture_1_(2018)_-_Introduction-416 不過 Gradient Descent 跟 Gradient Ascent 其實意思是一樣的
GAN_Lecture_1_(2018)_-_Introduction-417 對不對只是把 Objective Function 減一個負號而已
GAN_Lecture_1_(2018)_-_Introduction-418 所以實際上在做的時候
GAN_Lecture_1_(2018)_-_Introduction-419 就是把 generator 跟 discriminator 接起來變成一個巨大的 network
GAN_Lecture_1_(2018)_-_Introduction-420 然後把巨大的 network 最後幾個 layer fix 住
GAN_Lecture_1_(2018)_-_Introduction-421 只 train 前面幾個 layer
GAN_Lecture_1_(2018)_-_Introduction-422 目標是要讓整個 network output 的值越大越好
GAN_Lecture_1_(2018)_-_Introduction-423 你會發現因為你的目標是要讓整個 network 的值 output 越大越好
GAN_Lecture_1_(2018)_-_Introduction-424 所以固定住最後幾個 layer
GAN_Lecture_1_(2018)_-_Introduction-425 是非常有道理的
GAN_Lecture_1_(2018)_-_Introduction-426 假設不固定住最後幾個 layer 會發生甚麼事
GAN_Lecture_1_(2018)_-_Introduction-427 因為目標是要讓 output 越大越好
GAN_Lecture_1_(2018)_-_Introduction-428 其實他只要調最後這個 output 一個 scalar
GAN_Lecture_1_(2018)_-_Introduction-429 只要調最後一個 layer 的 weight
GAN_Lecture_1_(2018)_-_Introduction-430 讓他的值越大越好
GAN_Lecture_1_(2018)_-_Introduction-431 馬上就可以讓會後的 output 爆表
GAN_Lecture_1_(2018)_-_Introduction-432 但是因為最後這幾個 layer 是固定住的
GAN_Lecture_1_(2018)_-_Introduction-433 只能夠調前面幾個 layer
GAN_Lecture_1_(2018)_-_Introduction-434 想辦法讓最後的 output 越大越好
GAN_Lecture_1_(2018)_-_Introduction-435 等一下講完這一頁以後停下來問大家有沒有甚麼問題
GAN_Lecture_1_(2018)_-_Introduction-436 這邊正式的把這 algorithm 再講一次
GAN_Lecture_1_(2018)_-_Introduction-437 有一個 discriminator 參數是 θd
GAN_Lecture_1_(2018)_-_Introduction-438 有一個 generator 參數是 θg
GAN_Lecture_1_(2018)_-_Introduction-439 整個演算法就是這個樣子
GAN_Lecture_1_(2018)_-_Introduction-440 摳一下這個演算法就可以完成作業 3-1 了
GAN_Lecture_1_(2018)_-_Introduction-441 這個演算法是這樣
GAN_Lecture_1_(2018)_-_Introduction-442 有一個 database，這個 database 在作業 3-1 是我們提供的
GAN_Lecture_1_(2018)_-_Introduction-443 提供一個很大的 database，二次元人物頭像的 database裡面有好幾萬張的圖片
GAN_Lecture_1_(2018)_-_Introduction-444 接下來從這個 database 裡面
GAN_Lecture_1_(2018)_-_Introduction-445 sample 出 m 筆 examplesample 出 m 張 image
GAN_Lecture_1_(2018)_-_Introduction-446 這個 m 其實就是 batch size
GAN_Lecture_1_(2018)_-_Introduction-447 就像 train network 的 batch size
GAN_Lecture_1_(2018)_-_Introduction-448 m 要調一下，32、64，他是 batch size
GAN_Lecture_1_(2018)_-_Introduction-449 同時有一個 distribution，這個 distribution 我個人覺得沒有這麼重要
GAN_Lecture_1_(2018)_-_Introduction-450 所以可以在作業裡面 verify 一下這個 distribution 對你的結果有多大的影響
GAN_Lecture_1_(2018)_-_Introduction-451 這個 distribution 可以是一個 Uniform Distribution
GAN_Lecture_1_(2018)_-_Introduction-452 可以是 Gaussian Distribution 都可以，假設他是 Gaussian Distribution
GAN_Lecture_1_(2018)_-_Introduction-453 從 Gaussian Distribution 裡面 sample 出 m 個 vector
GAN_Lecture_1_(2018)_-_Introduction-454 至於這個 vector dimension 要多少，要五維、十維、還是一百維
GAN_Lecture_1_(2018)_-_Introduction-455 這個你自己決定，這個一個參數要調的
GAN_Lecture_1_(2018)_-_Introduction-456 你就 sample 出 m 個 vector 從 Gaussian Distribution 裡面
GAN_Lecture_1_(2018)_-_Introduction-457 接下來就根據這 m 個 vector 會去產生 m 張 image
GAN_Lecture_1_(2018)_-_Introduction-458 剛才有說 generator 就是吃一個 vector output 一個 image
GAN_Lecture_1_(2018)_-_Introduction-459 把 generator 產生出來的 image 用 x tilde 來表示
GAN_Lecture_1_(2018)_-_Introduction-460 把這 m 個 vector 丟到 generator 裡面讓他產生 m 張圖片
GAN_Lecture_1_(2018)_-_Introduction-461 接下來就要去調整 discriminator前半部是在訓練 discriminator
GAN_Lecture_1_(2018)_-_Introduction-462 怎麼調 discriminator
GAN_Lecture_1_(2018)_-_Introduction-463 底下這個式子只是一個範例
GAN_Lecture_1_(2018)_-_Introduction-464 你可以用這個式子也可以用別的之後會講也許用別的結果會更好
GAN_Lecture_1_(2018)_-_Introduction-465 這個只是最原始的 GAN 的 paper
GAN_Lecture_1_(2018)_-_Introduction-466 他的式子是這樣寫的
GAN_Lecture_1_(2018)_-_Introduction-467 他說要去 maximize 這個 Objective Function
GAN_Lecture_1_(2018)_-_Introduction-468 這個 Objective Function 是甚麼意思
GAN_Lecture_1_(2018)_-_Introduction-469 首先把 m 張真實的圖片拿出來
GAN_Lecture_1_(2018)_-_Introduction-470 把他們都通過 discriminator 得到分數
GAN_Lecture_1_(2018)_-_Introduction-471 然後取一個 log，再把 m 個圖片得到的分數通通平均起來
GAN_Lecture_1_(2018)_-_Introduction-472 我們要讓這個 Objective Function 的值越大越好
GAN_Lecture_1_(2018)_-_Introduction-473 意味著要讓 log D(x) 越大越好
GAN_Lecture_1_(2018)_-_Introduction-474 也就是要讓 D(x) 越大越好
GAN_Lecture_1_(2018)_-_Introduction-475 所以這個式子告訴我們要訓練 discriminator 讓他可以給這些真實的 image 的值越大越好
GAN_Lecture_1_(2018)_-_Introduction-476 這是第一項，第二項是甚麼
GAN_Lecture_1_(2018)_-_Introduction-477 第二項是把這些假的圖片，x tilde generator 產生出來的圖片 x tilde
GAN_Lecture_1_(2018)_-_Introduction-478 丟到 discriminator 裡面產生出一個 value
GAN_Lecture_1_(2018)_-_Introduction-479 這邊 discriminator 其實，output 會通過 sigmoid
GAN_Lecture_1_(2018)_-_Introduction-480 他的值會介於 0 到 1 之間
GAN_Lecture_1_(2018)_-_Introduction-481 不要 sigmoid 也可以，不過這邊如果沒有 sigmoid 的話
GAN_Lecture_1_(2018)_-_Introduction-482 1 在減掉大於 1 的值再取 log 會有問題
GAN_Lecture_1_(2018)_-_Introduction-483 所以這邊就給一個 sigmoid
GAN_Lecture_1_(2018)_-_Introduction-484 D 的值一定是落在 0 到 1 之間你可能會問為甚麼一定要這樣
GAN_Lecture_1_(2018)_-_Introduction-485 不一定要這樣，這樣做沒有比較好
GAN_Lecture_1_(2018)_-_Introduction-486 之後在接下來的課程會告訴你這樣做其實不是最好的方法
GAN_Lecture_1_(2018)_-_Introduction-487 這個只是最原始的方法
GAN_Lecture_1_(2018)_-_Introduction-488 用這個方法是做得出來作業 3-1 的
GAN_Lecture_1_(2018)_-_Introduction-489 今天把這個 generate 出來的 image 丟到 discriminator 裡面，他還會給他一個分數
GAN_Lecture_1_(2018)_-_Introduction-490 把 1 減掉這個分數再取 log 然後取平均值希望這一項越大越好
GAN_Lecture_1_(2018)_-_Introduction-491 希望這一項越大越好的意思就是希望 1 - D( x tilde ) 的值越大越好
GAN_Lecture_1_(2018)_-_Introduction-492 希望 1 - D(x) 的值越大越好就是希望D( x tilde ) 越小越好
GAN_Lecture_1_(2018)_-_Introduction-493 第二項的意思是希望訓練 discriminator
GAN_Lecture_1_(2018)_-_Introduction-494 這些 discriminator 如果把 generate 出來的 image 當作 imput 的話
GAN_Lecture_1_(2018)_-_Introduction-495 他 output 的值要越小越好
GAN_Lecture_1_(2018)_-_Introduction-496 你有加一個 Objective Function
GAN_Lecture_1_(2018)_-_Introduction-497 要去 maximize 這個 Objective Function
GAN_Lecture_1_(2018)_-_Introduction-498 怎麼 maximize 這個 Objective Function就套一下 Gradient Ascent
GAN_Lecture_1_(2018)_-_Introduction-499 如果是 Gradient Descent 這邊是減的
GAN_Lecture_1_(2018)_-_Introduction-500 如果要讓 Loss Function 越小越好的話
GAN_Lecture_1_(2018)_-_Introduction-501 會取 gradient 然後乘 Learning Rate 然後把他減掉
GAN_Lecture_1_(2018)_-_Introduction-502 但是現在要讓他越大越好所以這邊是加的，對你的 Objective Function
GAN_Lecture_1_(2018)_-_Introduction-503 算他的 gradient
GAN_Lecture_1_(2018)_-_Introduction-504 然後乘上 Learning Rate 然後再加上原來的參數
GAN_Lecture_1_(2018)_-_Introduction-505 就得到新的參數 θd
GAN_Lecture_1_(2018)_-_Introduction-506 至於 Learning Rate 要怎麼調，永遠可以用一些 Adam 等等進階的技術來設定 Learning Rate
GAN_Lecture_1_(2018)_-_Introduction-507 這邊在這個式子裡面
GAN_Lecture_1_(2018)_-_Introduction-508 我們只寫把 θd update 一次參數
GAN_Lecture_1_(2018)_-_Introduction-509 至於要 update 幾次參數這是另外一個 hyperparameter
GAN_Lecture_1_(2018)_-_Introduction-510 是你要調的，你可以說在這邊，我執行這一項
GAN_Lecture_1_(2018)_-_Introduction-511 執行這一項三次，執行這一項五次都可以這是個參數你要去調他的
GAN_Lecture_1_(2018)_-_Introduction-512 這個是訓練 discriminator，把 discriminator 訓練好以後
GAN_Lecture_1_(2018)_-_Introduction-513 接下來要訓練 generator
GAN_Lecture_1_(2018)_-_Introduction-514 怎麼訓練 generator
GAN_Lecture_1_(2018)_-_Introduction-515 一樣去 sample 出 m 筆 data
GAN_Lecture_1_(2018)_-_Introduction-516 m 個 random vector
GAN_Lecture_1_(2018)_-_Introduction-517 這 m 個 random vector 不需要跟這 m 個 random vector 一樣，他可以都是另外 sample m 個 random 的 vector
GAN_Lecture_1_(2018)_-_Introduction-518 剛才說 generator 要想辦法騙過 discriminator
GAN_Lecture_1_(2018)_-_Introduction-519 把它的式子寫出來就這樣
GAN_Lecture_1_(2018)_-_Introduction-520 把這個 random vector 丟到 generator 裡面
GAN_Lecture_1_(2018)_-_Introduction-521 generator 會產生一張 image
GAN_Lecture_1_(2018)_-_Introduction-522 這邊看起來有點複雜， log ( D ( G ( z) ) )
GAN_Lecture_1_(2018)_-_Introduction-523 但我們一項一項來看
GAN_Lecture_1_(2018)_-_Introduction-524 G(z) 就是一張圖片，把 z 丟到 generator 讓他產生一張圖片
GAN_Lecture_1_(2018)_-_Introduction-525 G(z) 是一張圖片
GAN_Lecture_1_(2018)_-_Introduction-526 把這張圖片丟到 discriminator 裡面
GAN_Lecture_1_(2018)_-_Introduction-527 D ( G ( z) ) ) 是一個數值
GAN_Lecture_1_(2018)_-_Introduction-528 然後再把它取 log，再對所有 sample 出來的 data、一個 batch 裡面的 data 取平均
GAN_Lecture_1_(2018)_-_Introduction-529 希望這個值越大越好
GAN_Lecture_1_(2018)_-_Introduction-530 也就是希望 generator output 出來的這個 image
GAN_Lecture_1_(2018)_-_Introduction-531 丟到 discriminator 以後
GAN_Lecture_1_(2018)_-_Introduction-532 輸出來的值越大越好
GAN_Lecture_1_(2018)_-_Introduction-533 這個是要訓練的目標
GAN_Lecture_1_(2018)_-_Introduction-534 接下來用 Gradient Ascent 去調 generator 參數，希望它可以讓 Objective Function 的 output 值越大越好
GAN_Lecture_1_(2018)_-_Introduction-535 紅色的部分就是訓練 generator
GAN_Lecture_1_(2018)_-_Introduction-536 這兩個步驟就會反覆的執行
GAN_Lecture_1_(2018)_-_Introduction-537 訓練好 discriminator，接下來訓練 generator
GAN_Lecture_1_(2018)_-_Introduction-538 再回頭去訓練 discriminator
GAN_Lecture_1_(2018)_-_Introduction-539 再訓練 generator
GAN_Lecture_1_(2018)_-_Introduction-540 這兩個步驟會反覆的執行
GAN_Lecture_1_(2018)_-_Introduction-541 講到這邊有沒有針對這個 algorithm 要問的呢
GAN_Lecture_1_(2018)_-_Introduction-542 沒有的話就實際看一下你應該要作出甚麼樣的結果
GAN_Lecture_1_(2018)_-_Introduction-543 這個是我實際上作一下 Anime Face Generation
GAN_Lecture_1_(2018)_-_Introduction-544 網路上可以找到 database 連結再下面
GAN_Lecture_1_(2018)_-_Introduction-545 在做的其實用的不是這個 database，等一下告訴你那個 database 是哪來的
GAN_Lecture_1_(2018)_-_Introduction-546 載了一個 database 下來
GAN_Lecture_1_(2018)_-_Introduction-547 就可以摳 GAN 開始去 train 它
GAN_Lecture_1_(2018)_-_Introduction-548 這個是 update 一百次參數以後的結果
GAN_Lecture_1_(2018)_-_Introduction-549 看起來像是這個樣子
GAN_Lecture_1_(2018)_-_Introduction-550 update 一千次，這個時候 generator 就知道二次元的人物
GAN_Lecture_1_(2018)_-_Introduction-551 應該是有眼睛的，所以它就把眼睛點出來
GAN_Lecture_1_(2018)_-_Introduction-552 update 兩千次，這個時候
GAN_Lecture_1_(2018)_-_Introduction-553 我發現它把嘴巴也點出來
GAN_Lecture_1_(2018)_-_Introduction-554 update 五千次以後
GAN_Lecture_1_(2018)_-_Introduction-555 這個時候 machine 發現動畫人物就是要有水汪汪的大眼睛
GAN_Lecture_1_(2018)_-_Introduction-556 這個眼睛都變得很大，都有一個反白
GAN_Lecture_1_(2018)_-_Introduction-557 update 一萬次得到的結果像是這樣
GAN_Lecture_1_(2018)_-_Introduction-558 看起來像是水彩畫的感覺他有很多地方都暈開了
GAN_Lecture_1_(2018)_-_Introduction-559 看起來比較有二次元人物頭像的樣子
GAN_Lecture_1_(2018)_-_Introduction-560 只是感覺沒有畫得很好是用水彩畫得所以有一點暈開的感覺
GAN_Lecture_1_(2018)_-_Introduction-561 這個是 update 兩萬次的結果
GAN_Lecture_1_(2018)_-_Introduction-562 這個是 update 五萬次的結果
GAN_Lecture_1_(2018)_-_Introduction-563 就停在 update 五萬次的地方
GAN_Lecture_1_(2018)_-_Introduction-564 會發現這邊有很多頭像已經看起來非常真實了
GAN_Lecture_1_(2018)_-_Introduction-565 這個並不是真的做得特別好的
GAN_Lecture_1_(2018)_-_Introduction-566 這個是上學期同學交給我的結果
GAN_Lecture_1_(2018)_-_Introduction-567 所以理論上你應該要可以做到這樣
GAN_Lecture_1_(2018)_-_Introduction-568 他們其實用了自己蒐集的 data
GAN_Lecture_1_(2018)_-_Introduction-569 這學期用的就是他們蒐集的 data
GAN_Lecture_1_(2018)_-_Introduction-570 感謝他們提供他們蒐集的 data 給我們
GAN_Lecture_1_(2018)_-_Introduction-571 理論上這學期做出來就應該要像這個樣子
GAN_Lecture_1_(2018)_-_Introduction-572 這個看起來非常的真實對不對
GAN_Lecture_1_(2018)_-_Introduction-573 比很多會崩壞的動畫公司畫的都還要好
GAN_Lecture_1_(2018)_-_Introduction-574 還是有一些崩壞但看可不可以做的更好一點
GAN_Lecture_1_(2018)_-_Introduction-575 這個是二次元人物的生成
GAN_Lecture_1_(2018)_-_Introduction-576 這個就是作業 3-1
GAN_Lecture_1_(2018)_-_Introduction-577 當然也可以做三次元人物的生成
GAN_Lecture_1_(2018)_-_Introduction-578 可以做真實人臉生成這些人臉都是機器生成的
GAN_Lecture_1_(2018)_-_Introduction-579 網路上有名人臉的 database 就載下來就可以讓機器學習產生真實的人臉
GAN_Lecture_1_(2018)_-_Introduction-580 你可能會問產生真實的人臉有甚麼用
GAN_Lecture_1_(2018)_-_Introduction-581 基本上沒有甚麼用
GAN_Lecture_1_(2018)_-_Introduction-582 隨便去路邊拍一個人都會比機器產生的更加真實
GAN_Lecture_1_(2018)_-_Introduction-583 有趣的是機器可以產生你沒有見過的頭像
GAN_Lecture_1_(2018)_-_Introduction-584 剛才有說 input 的 vector
GAN_Lecture_1_(2018)_-_Introduction-585 代表了輸出圖片的特性
GAN_Lecture_1_(2018)_-_Introduction-586 這個只是舉例，實際上 input 的 vector 不會是二維至少做個十維、五十維之類的或者是一百維
GAN_Lecture_1_(2018)_-_Introduction-587 假設 input 是 [ 0, 0 ] output 是這個人
GAN_Lecture_1_(2018)_-_Introduction-588 input [ 0.9, 0.9 ] output 是這個人
GAN_Lecture_1_(2018)_-_Introduction-589 接下來可以 input 這兩個 vector 之間的內差
GAN_Lecture_1_(2018)_-_Introduction-590 input 這兩個 vector 之間的 interpolation
GAN_Lecture_1_(2018)_-_Introduction-591 就會得到這樣的 output 結果
GAN_Lecture_1_(2018)_-_Introduction-592 就會看到這兩個人臉之間的連續的變化
GAN_Lecture_1_(2018)_-_Introduction-593 上面是更多的例子
GAN_Lecture_1_(2018)_-_Introduction-594 機器真的可以學到說這個人臉他是朝右看的
GAN_Lecture_1_(2018)_-_Introduction-595 這個人臉是朝左看的
GAN_Lecture_1_(2018)_-_Introduction-596 當機器要產生這個朝右看的人臉和朝左看的人臉之間的內差的時候
GAN_Lecture_1_(2018)_-_Introduction-597 他並不是把兩張臉疊起來變成一個雙面人
GAN_Lecture_1_(2018)_-_Introduction-598 他自己知道朝右看的臉跟朝左看的臉如果內差的話
GAN_Lecture_1_(2018)_-_Introduction-599 就是朝正面看所以機器自己知道說把這個臉轉正
GAN_Lecture_1_(2018)_-_Introduction-600 然後再轉過來
GAN_Lecture_1_(2018)_-_Introduction-601 這邊有更多的例子
GAN_Lecture_1_(2018)_-_Introduction-602 機器要學到這件事情完全不需要 handcrafting
GAN_Lecture_1_(2018)_-_Introduction-603 就並不需要規則、物理模型都不用
GAN_Lecture_1_(2018)_-_Introduction-604 看很多真實的照片以後
GAN_Lecture_1_(2018)_-_Introduction-605 他自己就可以學到這些事情了這個是滿神奇的
GAN_Lecture_1_(2018)_-_Introduction-606 剛才講的是 GAN 的操作如果你知道那些操作就可以做出作業 3-1 了
GAN_Lecture_1_(2018)_-_Introduction-607 接下來想要花一些時間從概念上講一下 GAN 的原理
GAN_Lecture_1_(2018)_-_Introduction-608 為甚麼他是這樣運作
GAN_Lecture_1_(2018)_-_Introduction-609 開學的時候就有講過 GAN 其實可以被視為一種 Structured Learning 的技術
GAN_Lecture_1_(2018)_-_Introduction-610 甚麼是 Structured Learning其實開學就有講過了，但我們非常快的複習一下
GAN_Lecture_1_(2018)_-_Introduction-611 所謂 Structured Learning 的意思是我們都知道 Machine Learning 就是
GAN_Lecture_1_(2018)_-_Introduction-612 找個 function，input 一個東西，output 另外一個東西
GAN_Lecture_1_(2018)_-_Introduction-613 如果是 regression task，machine 就是 output 一個數值
GAN_Lecture_1_(2018)_-_Introduction-614 如果是 classification task，machine 就 output 一個 class
GAN_Lecture_1_(2018)_-_Introduction-615 如果遇到問題是更複雜的
GAN_Lecture_1_(2018)_-_Introduction-616 要機器的 output 不是一個數值也不是一個 class
GAN_Lecture_1_(2018)_-_Introduction-617 他可能是一個 sequence、可能是一個 matrix
GAN_Lecture_1_(2018)_-_Introduction-618 可能是一個 graph、可能是一個 tree
GAN_Lecture_1_(2018)_-_Introduction-619 這個時候這個任務就叫做 Structured Learning
GAN_Lecture_1_(2018)_-_Introduction-620 舉例來說，translation 是 Structured Learning 的問題
GAN_Lecture_1_(2018)_-_Introduction-621 因為 machine 要輸出一個句子
GAN_Lecture_1_(2018)_-_Introduction-622 語音辨識是 Structured Learning 的問題machine 要輸出一個句子
GAN_Lecture_1_(2018)_-_Introduction-623 chatbot，在作業 2-2 裡面已經有做了 chatbot
GAN_Lecture_1_(2018)_-_Introduction-624 chatbot 其實也是一種 Structured Learning 的問題
GAN_Lecture_1_(2018)_-_Introduction-625 所以也是 input 一個 sequenceoutput 一個 sequence
GAN_Lecture_1_(2018)_-_Introduction-626 你可能會想說 GAN 是一個可以用在 Structured Learning  的技術
GAN_Lecture_1_(2018)_-_Introduction-627 可不可以把 GAN 用在 chatbot 上呢
GAN_Lecture_1_(2018)_-_Introduction-628 可以，如果 sequence to sequence 如果 train 得起來的話
GAN_Lecture_1_(2018)_-_Introduction-629 其實可以再加上 discriminator
GAN_Lecture_1_(2018)_-_Introduction-630 可以讓他的回答更好
GAN_Lecture_1_(2018)_-_Introduction-631 要先把 sequence to sequence train 到 OK 這樣子
GAN_Lecture_1_(2018)_-_Introduction-632 不知道大家做得怎麼樣了
GAN_Lecture_1_(2018)_-_Introduction-633 也可以輸出讓機器產生一個 matrix
GAN_Lecture_1_(2018)_-_Introduction-634 甚麼時候要機器產生 matrix 呢
GAN_Lecture_1_(2018)_-_Introduction-635 如果要讓機器畫一張圖
GAN_Lecture_1_(2018)_-_Introduction-636 那就是產生 matrix，之前有講過如果輸入這種圖案
GAN_Lecture_1_(2018)_-_Introduction-637 那他產生這樣真實的房子或是輸入黑白的圖片，讓他產生彩色的圖片
GAN_Lecture_1_(2018)_-_Introduction-638 甚至可以輸入文字讓機器產生 image
GAN_Lecture_1_(2018)_-_Introduction-639 所以在作業 3-2 要做的就是要大家做輸入一些二次元人物頭像的特徵
GAN_Lecture_1_(2018)_-_Introduction-640 比如說頭髮顏色、眼睛的顏色要輸出對應的二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-641 這個是作業 3-2 要大家做的事情細節就下周再公告
GAN_Lecture_1_(2018)_-_Introduction-642 為甚麼 Structured Learning 是一個特別具有挑戰性的問題呢
GAN_Lecture_1_(2018)_-_Introduction-643 首先我覺得 Structured Learning 它可以被視為是一個 One Shot Learning 或者是 Zero Shot Learning
GAN_Lecture_1_(2018)_-_Introduction-644 甚麼叫 One Shot Learning 或者是 Zero Shot Learning 呢
GAN_Lecture_1_(2018)_-_Introduction-645 假設有一個分類的問題
GAN_Lecture_1_(2018)_-_Introduction-646 在做分類問題的時候每一個類別都要給機器一些例子
GAN_Lecture_1_(2018)_-_Introduction-647 舉例來說要機器分別蘋果和橘子之間的差異你就要給他一百張蘋果的圖片、一百張橘子的圖片
GAN_Lecture_1_(2018)_-_Introduction-648 他才能夠分辨，給他一個水果他是蘋果還是橘子
GAN_Lecture_1_(2018)_-_Introduction-649 所謂 One Shot Learning 或是 Zero Shot Learning 的意思是假設有些類別根本沒有任何範例
GAN_Lecture_1_(2018)_-_Introduction-650 或者是只有非常非常少的範例
GAN_Lecture_1_(2018)_-_Introduction-651 那能不能夠做得起來
GAN_Lecture_1_(2018)_-_Introduction-652 我覺得 Structured Learning 這個問題他可想成是一個極端的 One Shot Learning 或 Zero Shot Learning 的 problem
GAN_Lecture_1_(2018)_-_Introduction-653 因為在 Structured Learning 裡面 output 的東西是一個 structure，比如說一個句子
GAN_Lecture_1_(2018)_-_Introduction-654 在整個 training data 裡面，沒有任何句子是重複的
GAN_Lecture_1_(2018)_-_Introduction-655 testing data 跟 training data 他們的句子可能也是完全沒有重疊的
GAN_Lecture_1_(2018)_-_Introduction-656 假設把每一個不同的句子假設今天做的是 Structured Learning 是 output 一個句子
GAN_Lecture_1_(2018)_-_Introduction-657 比如說翻譯
GAN_Lecture_1_(2018)_-_Introduction-658 假設把各種不同可能的每一種 output
GAN_Lecture_1_(2018)_-_Introduction-659 都視為一個 class 的話
GAN_Lecture_1_(2018)_-_Introduction-660 在 training data 裡面每一個 class 可能就只出現一次
GAN_Lecture_1_(2018)_-_Introduction-661 testing data 出現的那些 class 更慘他根本在 training data 裡面根本就沒有出現過
GAN_Lecture_1_(2018)_-_Introduction-662 假設把每一種機器可能的 output
GAN_Lecture_1_(2018)_-_Introduction-663 視為一個 class 的話，其實 Structured Learning 是一個極端的 One Shot Learning 或者是 Zero Shot Learning 的 problem
GAN_Lecture_1_(2018)_-_Introduction-664 所以因為這些 class 他都只會出現一次
GAN_Lecture_1_(2018)_-_Introduction-665 所以在這種 Structured Learning 的 task 裡面
GAN_Lecture_1_(2018)_-_Introduction-666 如何學到一般化、如何學著去輸出
GAN_Lecture_1_(2018)_-_Introduction-667 從來沒有看過的東西
GAN_Lecture_1_(2018)_-_Introduction-668 就變成一個很重要的問題
GAN_Lecture_1_(2018)_-_Introduction-669 你可以想像要讓機器做到這件事情
GAN_Lecture_1_(2018)_-_Introduction-670 要讓機器可以輸出他在 training 的時候從來沒有看過的東西
GAN_Lecture_1_(2018)_-_Introduction-671 那這個機器是需要有一定程度的智慧的
GAN_Lecture_1_(2018)_-_Introduction-672 或者講得更擬人化一點講得更像農場文一點的話
GAN_Lecture_1_(2018)_-_Introduction-673 機器必須要要學會創造他才能夠解 Structured Learning 的問題
GAN_Lecture_1_(2018)_-_Introduction-674 因為他在 testing 的時候，他需要輸出的正確答案
GAN_Lecture_1_(2018)_-_Introduction-675 很有可能是 training 的時候他一次也沒有看過的
GAN_Lecture_1_(2018)_-_Introduction-676 這個東西就可以把他視為是創造
GAN_Lecture_1_(2018)_-_Introduction-677 如果機器也解決 Structured Learning 的問題
GAN_Lecture_1_(2018)_-_Introduction-678 他必須要有規劃的概念
GAN_Lecture_1_(2018)_-_Introduction-679 他必須要有大局觀
GAN_Lecture_1_(2018)_-_Introduction-680 因為機器要產生很複雜的物件
GAN_Lecture_1_(2018)_-_Introduction-681 而這個複雜的物件，是由幾個比較簡單的 component 所組成
GAN_Lecture_1_(2018)_-_Introduction-682 如果是影像生成的話
GAN_Lecture_1_(2018)_-_Introduction-683 機器是產生一個一個 pixel
GAN_Lecture_1_(2018)_-_Introduction-684 但是所有的 pixel 合起來
GAN_Lecture_1_(2018)_-_Introduction-685 必須要能夠變成一張人臉
GAN_Lecture_1_(2018)_-_Introduction-686 那機器在產生這張圖片的時候
GAN_Lecture_1_(2018)_-_Introduction-687 他一定要在心裡分辨清楚說
GAN_Lecture_1_(2018)_-_Introduction-688 我點這個 pixel 上去代表是眼睛
GAN_Lecture_1_(2018)_-_Introduction-689 點這個 pixel 上去代表是嘴巴
GAN_Lecture_1_(2018)_-_Introduction-690 才不會變成畫三隻眼睛、畫兩張嘴巴等等
GAN_Lecture_1_(2018)_-_Introduction-691 所以在 Structured Learning 裡面
GAN_Lecture_1_(2018)_-_Introduction-692 真正重要的不是產生了甚麼 component
GAN_Lecture_1_(2018)_-_Introduction-693 而是 component 和 component 之間的關係
GAN_Lecture_1_(2018)_-_Introduction-694 假設要讓機器學會寫一個數字出來
GAN_Lecture_1_(2018)_-_Introduction-695 機器一開始在整張圖上面中間點一個點
GAN_Lecture_1_(2018)_-_Introduction-696 點一個點這件事情本身是中性的
GAN_Lecture_1_(2018)_-_Introduction-697 他不見得代表結果會是好的
GAN_Lecture_1_(2018)_-_Introduction-698 也不見得代表結果會是不好的
GAN_Lecture_1_(2018)_-_Introduction-699 因為假設機器最後畫出來的數字長這樣
GAN_Lecture_1_(2018)_-_Introduction-700 那結果是好的
GAN_Lecture_1_(2018)_-_Introduction-701 畫出來的數字長這樣，那整個就是壞掉的
GAN_Lecture_1_(2018)_-_Introduction-702 文字的生成也是一樣
GAN_Lecture_1_(2018)_-_Introduction-703 機器要產生的是一個完整的句子
GAN_Lecture_1_(2018)_-_Introduction-704 甚至是一篇完整的文章
GAN_Lecture_1_(2018)_-_Introduction-705 單看生成的一部份沒有辦法判斷他是好的還是不好的
GAN_Lecture_1_(2018)_-_Introduction-706 這讓我想到一個紀曉嵐的故事
GAN_Lecture_1_(2018)_-_Introduction-707 紀曉嵐故事是說，有一天紀曉嵐去幫翰林的老婆祝壽
GAN_Lecture_1_(2018)_-_Introduction-708 然後就寫了一個詩
GAN_Lecture_1_(2018)_-_Introduction-709 詩的開頭就是：這個婆娘不是人
GAN_Lecture_1_(2018)_-_Introduction-710 然後大家都生氣了
GAN_Lecture_1_(2018)_-_Introduction-711 還有下一句：九天玄女下凡塵
GAN_Lecture_1_(2018)_-_Introduction-712 然後大家就高興了
GAN_Lecture_1_(2018)_-_Introduction-713 這就告訴我們如果只看第一個句子
GAN_Lecture_1_(2018)_-_Introduction-714 他是一個負面的句子
GAN_Lecture_1_(2018)_-_Introduction-715 但是把第一個句子和第二個句子合起來
GAN_Lecture_1_(2018)_-_Introduction-716 如果有大局觀的話，如果可以看整個完整輸出的話
GAN_Lecture_1_(2018)_-_Introduction-717 他就變成一個正面的句子
GAN_Lecture_1_(2018)_-_Introduction-718 而這個東西是機器在做 Structured Learning 的時候
GAN_Lecture_1_(2018)_-_Introduction-719 他必須要學會的
GAN_Lecture_1_(2018)_-_Introduction-720 所以這邊想要表達的是 Structured Learning 有趣而富有挑戰性的問題
GAN_Lecture_1_(2018)_-_Introduction-721 我覺得 GAN 他其實是一個 Structured Learning 的 solution
GAN_Lecture_1_(2018)_-_Introduction-722 在傳統 Structured Learning 的文件裡面
GAN_Lecture_1_(2018)_-_Introduction-723 有兩套方法
GAN_Lecture_1_(2018)_-_Introduction-724 一套方法是 Bottom Up 的方法
GAN_Lecture_1_(2018)_-_Introduction-725 一套方法是 Top Down 的方法
GAN_Lecture_1_(2018)_-_Introduction-726 所謂 Bottom Up 的方法是我們要產生一個完整的物件
GAN_Lecture_1_(2018)_-_Introduction-727 機器是一個一個 component 分開去產生這個物件
GAN_Lecture_1_(2018)_-_Introduction-728 這樣的缺點就是很容易失去大局觀
GAN_Lecture_1_(2018)_-_Introduction-729 所謂 Top Down 的方法是從產生完一個完整的物件以後
GAN_Lecture_1_(2018)_-_Introduction-730 再去從整體來看看這個產生的物件好不好
GAN_Lecture_1_(2018)_-_Introduction-731 這樣講有點抽象等下會具體地告訴大家這件事是怎麼做的
GAN_Lecture_1_(2018)_-_Introduction-732 這邊的壞處是用這個方法很難做 generation
GAN_Lecture_1_(2018)_-_Introduction-733 Generator 可以視為是一個 Bottom Up 的方法
GAN_Lecture_1_(2018)_-_Introduction-734 Discriminator 可以視為是一個 Top Down 的方法
GAN_Lecture_1_(2018)_-_Introduction-735 把這兩個方法結合起來就是 Generative Adversarial Network，就是 GAN
GAN_Lecture_1_(2018)_-_Introduction-736 接下來想要分享的是
GAN_Lecture_1_(2018)_-_Introduction-737 剛才不是說兩個問題
GAN_Lecture_1_(2018)_-_Introduction-738 第一個問題是為甚麼 generator 不能夠自己學
GAN_Lecture_1_(2018)_-_Introduction-739 就來看看 generator 是不是可以自己學
GAN_Lecture_1_(2018)_-_Introduction-740 實事上 generator 是可以自己學的
GAN_Lecture_1_(2018)_-_Introduction-741 generator 的學習就是 input 一個 vector
GAN_Lecture_1_(2018)_-_Introduction-742 output 一個 image假設用手寫數字當作例子
GAN_Lecture_1_(2018)_-_Introduction-743 output 一個 vectoroutput 一個數字的圖片
GAN_Lecture_1_(2018)_-_Introduction-744 output ( 應該是 input ) 就要產生不同的圖片
GAN_Lecture_1_(2018)_-_Introduction-745 要訓練這樣一個 network 的 generator
GAN_Lecture_1_(2018)_-_Introduction-746 訓練這樣 NN 的 generator 怎麼做呢
GAN_Lecture_1_(2018)_-_Introduction-747 我們知道在傳統的 Supervised Learning 裡面
GAN_Lecture_1_(2018)_-_Introduction-748 就給 network input 跟 output 的 pair
GAN_Lecture_1_(2018)_-_Introduction-749 然後 train 下去就得到結果了對不對
GAN_Lecture_1_(2018)_-_Introduction-750 今天要 generator 輸入就是 vector 輸出就是圖片
GAN_Lecture_1_(2018)_-_Introduction-751 能不能夠蒐集這樣的 pair 呢
GAN_Lecture_1_(2018)_-_Introduction-752 當然可以蒐集這樣的 pair
GAN_Lecture_1_(2018)_-_Introduction-753 因為假設有一個 database 裡面都是一大堆數字的圖片
GAN_Lecture_1_(2018)_-_Introduction-754 接下來給一個數字 assign 一個 vector
GAN_Lecture_1_(2018)_-_Introduction-755 就結束了
GAN_Lecture_1_(2018)_-_Introduction-756 因為接下來你就可以說我 train 一個 network
GAN_Lecture_1_(2018)_-_Introduction-757 然後看這個 1 他的 vector 是 [ 0.1, 0.9 ]
GAN_Lecture_1_(2018)_-_Introduction-758 所以輸入 [ 0.1, 0.9 ] 丟到這個 generator 裡面
GAN_Lecture_1_(2018)_-_Introduction-759 就要 output 一張圖片，這個圖片跟他的目標也就是這個 1 越接近越好
GAN_Lecture_1_(2018)_-_Introduction-760 這完全跟一般 train Supervised Learning 是一模一樣的
GAN_Lecture_1_(2018)_-_Introduction-761 就用 Gradient Descent train 下去
GAN_Lecture_1_(2018)_-_Introduction-762 就結束了相信大家都可以秒 implement 這種 network
GAN_Lecture_1_(2018)_-_Introduction-763 這個跟 train 一個 classifier 是一樣的
GAN_Lecture_1_(2018)_-_Introduction-764 只是有點反過來因為這邊是輸入一個 vector
GAN_Lecture_1_(2018)_-_Introduction-765 輸出一張圖片，雖然說平常不會做輸出圖片這件事
GAN_Lecture_1_(2018)_-_Introduction-766 但實際上做的就只是輸出一個很長的向量而已
GAN_Lecture_1_(2018)_-_Introduction-767 一般的分類就相反了，一般的分類是輸入一個圖片，然後輸出一個向量
GAN_Lecture_1_(2018)_-_Introduction-768 這個向量每一個維度代表對應到的某一個數字
GAN_Lecture_1_(2018)_-_Introduction-769 這兩種 generator training 和 classifier training 其實根本可以用同樣的方式來 train 他
GAN_Lecture_1_(2018)_-_Introduction-770 但這邊的問題是怎麼產生這些數字呢
GAN_Lecture_1_(2018)_-_Introduction-771 你可以隨機產生，但是如果隨機產生的話
GAN_Lecture_1_(2018)_-_Introduction-772 到時候這個 generator training 可能非常困難，因為這兩個 1 是很像的
GAN_Lecture_1_(2018)_-_Introduction-773 但如果他們 input vector 非常不一樣的話
GAN_Lecture_1_(2018)_-_Introduction-774 可能很難 learn 出一個 network，input 非常不一樣的東西卻 output 非常像的東西
GAN_Lecture_1_(2018)_-_Introduction-775 你可能會希望這兩張圖片如果有共同的特徵
GAN_Lecture_1_(2018)_-_Introduction-776 他們對應到的這個 vector
GAN_Lecture_1_(2018)_-_Introduction-777 就應該有某些相似之處
GAN_Lecture_1_(2018)_-_Introduction-778 舉例來說在這邊在這個 vector 的時候
GAN_Lecture_1_(2018)_-_Introduction-779 1 第一維都給他是 0.1，2 的第一維給他 0.2，3 的第一維給他 0.3
GAN_Lecture_1_(2018)_-_Introduction-780 向左斜第二維就給他負的值，向右斜第二維就給他正的值等等
GAN_Lecture_1_(2018)_-_Introduction-781 你可能會希望你 input 的 vector 他跟 output 東西的特徵還是有關係的
GAN_Lecture_1_(2018)_-_Introduction-782 這件事情怎麼做到
GAN_Lecture_1_(2018)_-_Introduction-783 怎麼樣產生這樣的 vector 呢還是有辦法的
GAN_Lecture_1_(2018)_-_Introduction-784 可以 learn 一個 encoder，這個 encoder 是給他一張圖片，他把這個圖片的特徵用一個向量來表示
GAN_Lecture_1_(2018)_-_Introduction-785 這個向量就是這邊的 code
GAN_Lecture_1_(2018)_-_Introduction-786 給他一個圖片，他把這個圖片的特徵用向量來表示
GAN_Lecture_1_(2018)_-_Introduction-787 這個向量就是這邊的 code
GAN_Lecture_1_(2018)_-_Introduction-788 怎麼 train 這樣一個 encoder 呢
GAN_Lecture_1_(2018)_-_Introduction-789 記不記得在講 Machine Learning 的時候
GAN_Lecture_1_(2018)_-_Introduction-790 有講過一個技術叫做 Auto-encoder
GAN_Lecture_1_(2018)_-_Introduction-791 這個我想大家都記得我只是很快地複習一下
GAN_Lecture_1_(2018)_-_Introduction-792 Auto-encoder 做的就是給他一張圖片，他把它變成一個 code
GAN_Lecture_1_(2018)_-_Introduction-793 Auto-encoder 裡面有一個 encoder 有一個 decoder
GAN_Lecture_1_(2018)_-_Introduction-794 encoder 做的事情是給他一張圖片，他把這個圖片變成一個 code
GAN_Lecture_1_(2018)_-_Introduction-795 但是 encoder 本身不能夠自己 train
GAN_Lecture_1_(2018)_-_Introduction-796 一定要再加一個 decoder 才有辦法 train
GAN_Lecture_1_(2018)_-_Introduction-797 decoder 吃一個 code，他會把這個 code 變成一張圖片
GAN_Lecture_1_(2018)_-_Introduction-798 在 training 的時候給一張 image
GAN_Lecture_1_(2018)_-_Introduction-799 這張 image 被 encoder 變成一個 vector
GAN_Lecture_1_(2018)_-_Introduction-800 decoder 會把這個 vector 解回原來的 image
GAN_Lecture_1_(2018)_-_Introduction-801 希望 input 跟 output 越接近越好
GAN_Lecture_1_(2018)_-_Introduction-802 在 train Auto-encoder 的時候就隨便選一張圖片
GAN_Lecture_1_(2018)_-_Introduction-803 這張圖片變成 code
GAN_Lecture_1_(2018)_-_Introduction-804 這個 code 透過原來的 decoder 變回原來的 image
GAN_Lecture_1_(2018)_-_Introduction-805 會希望 input 跟 output 越接近越好
GAN_Lecture_1_(2018)_-_Introduction-806 你仔細想想這個 decoder 其實就是一個 generator
GAN_Lecture_1_(2018)_-_Introduction-807 剛才說那個 generator 做的就是給他一個 code
GAN_Lecture_1_(2018)_-_Introduction-808 要他產生某一張對應的圖片
GAN_Lecture_1_(2018)_-_Introduction-809 現在已經告訴你有一個 encoder 他的工作就是產生一個圖片的 code
GAN_Lecture_1_(2018)_-_Introduction-810 這張圖片他對的 code 就是這個
GAN_Lecture_1_(2018)_-_Introduction-811 decoder 的學習就是看到這個 code要產生那個 code 對應的圖片
GAN_Lecture_1_(2018)_-_Introduction-812 這個 decoder 根本就是我們要學的 generator
GAN_Lecture_1_(2018)_-_Introduction-813 事實上 learn 好一個 Auto-encoder 以後
GAN_Lecture_1_(2018)_-_Introduction-814 把 decoder 拿出來就是我們的 generator
GAN_Lecture_1_(2018)_-_Introduction-815 他就可以拿來產生 image
GAN_Lecture_1_(2018)_-_Introduction-816 你就可以隨便丟一些東西他就會 output 你想要的 object
GAN_Lecture_1_(2018)_-_Introduction-817 舉例來說，你完全可以 learn 一個數字的產生器
GAN_Lecture_1_(2018)_-_Introduction-818 這個非常簡單，你回去一秒鐘就可以做出來
GAN_Lecture_1_(2018)_-_Introduction-819 就 learn 一個 Auto-encoder 他的中間的 code 是兩維
GAN_Lecture_1_(2018)_-_Introduction-820 然後把 decoder 拿出來隨便給他一個兩維的 vector
GAN_Lecture_1_(2018)_-_Introduction-821 他 output 就是一個數字
GAN_Lecture_1_(2018)_-_Introduction-822 假設給他 [ -1.5, 0 ]這是真實的例子
GAN_Lecture_1_(2018)_-_Introduction-823 他就輸出這樣子的 image
GAN_Lecture_1_(2018)_-_Introduction-824 給他 [ 1.5, 0 ]，他就輸出這樣子的 image
GAN_Lecture_1_(2018)_-_Introduction-825 如果在四方形的範圍內，等距的 sample
GAN_Lecture_1_(2018)_-_Introduction-826 就可以看到一堆數字的連續的變化
GAN_Lecture_1_(2018)_-_Introduction-827 舉例來說第一維他可能跟有沒有圈圈有關
GAN_Lecture_1_(2018)_-_Introduction-828 越往左就有圈圈往右就是棍子
GAN_Lecture_1_(2018)_-_Introduction-829 如果是看縱軸的變化，可能跟他的方向有關
GAN_Lecture_1_(2018)_-_Introduction-830 舉例來說本來朝左偏的，往上以後就會變成朝右偏的
GAN_Lecture_1_(2018)_-_Introduction-831 這個是 Auto-encoder 可以做的事情
GAN_Lecture_1_(2018)_-_Introduction-832 那用 Auto-encoder 會有甚麼樣的問題呢
GAN_Lecture_1_(2018)_-_Introduction-833 你可能會遇到這樣子的一個問題
GAN_Lecture_1_(2018)_-_Introduction-834 因為 training data 裡面的 data 是有限的
GAN_Lecture_1_(2018)_-_Introduction-835 generator 可能可以 learn 到看到 vector a 產生這張圖片
GAN_Lecture_1_(2018)_-_Introduction-836 看到 vector b 產生這張圖片
GAN_Lecture_1_(2018)_-_Introduction-837 但是看到 vector a 跟 vector b 的平均
GAN_Lecture_1_(2018)_-_Introduction-838 他到底會產生甚麼樣的圖片呢
GAN_Lecture_1_(2018)_-_Introduction-839 你可能覺得 vector a 對應到的 1 是向左的
GAN_Lecture_1_(2018)_-_Introduction-840 vector b 對應到的 1 是向右的
GAN_Lecture_1_(2018)_-_Introduction-841 假設兩個合起來應該是產生正的 1
GAN_Lecture_1_(2018)_-_Introduction-842 但是實際上並不是這個樣子
GAN_Lecture_1_(2018)_-_Introduction-843 因為 NN generator 是一個 network
GAN_Lecture_1_(2018)_-_Introduction-844 他是非線性的，a 可以產生這個圖片
GAN_Lecture_1_(2018)_-_Introduction-845 b 可以產生這個圖片，把他們平均以後丟進去
GAN_Lecture_1_(2018)_-_Introduction-846 未必產生出來的就是數字，可能 output 出來的就是 noise
GAN_Lecture_1_(2018)_-_Introduction-847 實際 train train 看，很有可能發生這樣子的狀況
GAN_Lecture_1_(2018)_-_Introduction-848 要怎麼解決這個問題呢
GAN_Lecture_1_(2018)_-_Introduction-849 這個也是在 Machine Learning 那門課講過的技術
GAN_Lecture_1_(2018)_-_Introduction-850 這個技術就是 Variational Auto-encoder也就是 VAE
GAN_Lecture_1_(2018)_-_Introduction-851 那在 Variational Auto-encoder 裡面，encoder 不只是產生一個 code
GAN_Lecture_1_(2018)_-_Introduction-852 這邊這個 m 代表他產生出來的 code
GAN_Lecture_1_(2018)_-_Introduction-853 他不只是產生一個 code他不只是產生一個 vector
GAN_Lecture_1_(2018)_-_Introduction-854 他還會產生每一個 dimension 的 variance
GAN_Lecture_1_(2018)_-_Introduction-855 接下來從 normal distribution 裡面去 sample 一堆 noise 出來
GAN_Lecture_1_(2018)_-_Introduction-856 把這個 noise 跟 variance 相乘，把這個 noise 加到 code 上面去
GAN_Lecture_1_(2018)_-_Introduction-857 再把有加 noise 的 code 丟到 decoder 裡面
GAN_Lecture_1_(2018)_-_Introduction-858 decoder 要跟具有 noise 的 code 還原出原來的圖片
GAN_Lecture_1_(2018)_-_Introduction-859 如果有這個技術的話，machine 就會知道它不只看到 vector a 要產生數字
GAN_Lecture_1_(2018)_-_Introduction-860 看到 vector b 也要產生數字看到 vector a 加一些 noise
GAN_Lecture_1_(2018)_-_Introduction-861 看到 vector b 加一些 noise 產生出來的東西也必須要是數字
GAN_Lecture_1_(2018)_-_Introduction-862 所以有了 Variational Auto-encoder，可以把你的 decoder train 的更加穩定
GAN_Lecture_1_(2018)_-_Introduction-863 可以讓你的 decoder 就算 input 這個 vector
GAN_Lecture_1_(2018)_-_Introduction-864 是在 training 的時候從來沒有看過的 vector
GAN_Lecture_1_(2018)_-_Introduction-865 他 output 的東西仍然可能是合理的 object
GAN_Lecture_1_(2018)_-_Introduction-866 這個都是複習在 Machine Learning 那門課曾經講過的東西
GAN_Lecture_1_(2018)_-_Introduction-867 接下來要講的是這整套技術他少了甚麼樣的東西
GAN_Lecture_1_(2018)_-_Introduction-868 講到這邊不知道大家有沒有問題想要問的呢
GAN_Lecture_1_(2018)_-_Introduction-869 那個是你自己決定的他就跟 network 的參數一樣是你要調的
GAN_Lecture_1_(2018)_-_Introduction-870 但是通常 code 他會是一個 low dimension 的東西
GAN_Lecture_1_(2018)_-_Introduction-871 因為我們的假設通常是這樣子假設你今天做的是 image 的 generation
GAN_Lecture_1_(2018)_-_Introduction-872 你的 decoder output 是 image
GAN_Lecture_1_(2018)_-_Introduction-873 雖然 image 是一個 high dimension 的 vector
GAN_Lecture_1_(2018)_-_Introduction-874 我們相信這個 image 他在 high dimension 當中他其實是一個 low-dimensional 的 variable
GAN_Lecture_1_(2018)_-_Introduction-875 本質上他的分布是 low dimension 的
GAN_Lecture_1_(2018)_-_Introduction-876 所以今天要產生這個 image 你 input 的 code
GAN_Lecture_1_(2018)_-_Introduction-877 不需要是 high dimension 的東西他只需要是一個 low dimension 的 vector 就好了
GAN_Lecture_1_(2018)_-_Introduction-878 但是 dimension 到底應該是多少五維、十維還是五十維，這個東西是需要調一下的
GAN_Lecture_1_(2018)_-_Introduction-879 train 不起來，增加 dimension 會有效
GAN_Lecture_1_(2018)_-_Introduction-880 但是增加 dimension 以後未必會得到你要的東西
GAN_Lecture_1_(2018)_-_Introduction-881 因為 train 的是一個 Auto-encoder
GAN_Lecture_1_(2018)_-_Introduction-882 訓練的目標是要讓 input 跟 output 越接近越好
GAN_Lecture_1_(2018)_-_Introduction-883 要達到這個目標其實非常簡單
GAN_Lecture_1_(2018)_-_Introduction-884 把中間那個 code 開得跟 input 一樣大
GAN_Lecture_1_(2018)_-_Introduction-885 那你就不會有任何 loss因為 machine 只要學著一直 copy 就好了
GAN_Lecture_1_(2018)_-_Introduction-886 但這個並不是我們要的結果
GAN_Lecture_1_(2018)_-_Introduction-887 雖然說 input 的 vector 開得越大
GAN_Lecture_1_(2018)_-_Introduction-888 loss 可以壓得越低，但 loss 壓得越低並不代表 performance 會越好
GAN_Lecture_1_(2018)_-_Introduction-889 所以這個東西是需要調一下的
GAN_Lecture_1_(2018)_-_Introduction-890 為甚麼 sigma 要是 train 的
GAN_Lecture_1_(2018)_-_Introduction-891 所以假設沒有任何額外的 constrain
GAN_Lecture_1_(2018)_-_Introduction-892 我們只想要 minimize Reconstruction Error
GAN_Lecture_1_(2018)_-_Introduction-893 最後 train 出來的結果 sigma 就會變成 0
GAN_Lecture_1_(2018)_-_Introduction-894 但是實際上在 training 的時候還有另外一個條件
GAN_Lecture_1_(2018)_-_Introduction-895 這個條件是這些 sigma 的值不能夠太小
GAN_Lecture_1_(2018)_-_Introduction-896 在 train 的時候 criterion 是這些 sigma 的值越接近 1 越好
GAN_Lecture_1_(2018)_-_Introduction-897 你不希望 sigma 太大，但是他也不可以太小
GAN_Lecture_1_(2018)_-_Introduction-898 所以在這個前提之下，最後 learn 出來 sigma 並不會變成 0
GAN_Lecture_1_(2018)_-_Introduction-899 這樣有回答到大家的問題嗎
GAN_Lecture_1_(2018)_-_Introduction-900 大家如果對這個東西有困惑的話
GAN_Lecture_1_(2018)_-_Introduction-901 你可以去看一下 Machine Learning 的錄影這個在 Machine Learning 那堂課有說過的
GAN_Lecture_1_(2018)_-_Introduction-902 大家還有沒有甚麼問題想要問的呢
GAN_Lecture_1_(2018)_-_Introduction-903 OK 的話要繼續喔
GAN_Lecture_1_(2018)_-_Introduction-904 接下來講的是說
GAN_Lecture_1_(2018)_-_Introduction-905 這個 Auto-encoder 的 training 他到底少了甚麼樣的東西
GAN_Lecture_1_(2018)_-_Introduction-906 你仔細想想看
GAN_Lecture_1_(2018)_-_Introduction-907 generator 在 train 的時候他的目標是希望他的 output 跟某一張圖片越像越好
GAN_Lecture_1_(2018)_-_Introduction-908 舉例來說這個 2 是你 input 的圖片
GAN_Lecture_1_(2018)_-_Introduction-909 希望 generator 的 output 跟這個 2 越像越好
GAN_Lecture_1_(2018)_-_Introduction-910 甚麼叫做越像越好呢
GAN_Lecture_1_(2018)_-_Introduction-911 通常計算的就是這兩張圖片他們 pixel by pixel 的差距
GAN_Lecture_1_(2018)_-_Introduction-912 通常每一個 pixel 他就是一個數字
GAN_Lecture_1_(2018)_-_Introduction-913 所以真的在算這兩張圖片的相似度做的事情就是
GAN_Lecture_1_(2018)_-_Introduction-914 把這兩張圖片表示成兩個 vector
GAN_Lecture_1_(2018)_-_Introduction-915 接下來就算這兩個 vector 的比如說 euclidean distance
GAN_Lecture_1_(2018)_-_Introduction-916 這個東西就是要去 minimize 的對象
GAN_Lecture_1_(2018)_-_Introduction-917 這兩個 vector euclidean distance
GAN_Lecture_1_(2018)_-_Introduction-918 就代表這兩張圖片的差異的程度
GAN_Lecture_1_(2018)_-_Introduction-919 就希望這兩張圖片他們的 L1 distance、L2 distance 越小越好
GAN_Lecture_1_(2018)_-_Introduction-920 假設 generator 他確實可以完全 copy 你的 target
GAN_Lecture_1_(2018)_-_Introduction-921 可能就沒有甚麼太大的問題
GAN_Lecture_1_(2018)_-_Introduction-922 真正在 training 的時候，generator 是會犯一些錯的
GAN_Lecture_1_(2018)_-_Introduction-923 generator 的 capacity 不會大到他 output 的 image 一定跟他的 target 一模一樣
GAN_Lecture_1_(2018)_-_Introduction-924 他勢必會有一些取捨
GAN_Lecture_1_(2018)_-_Introduction-925 他沒有辦法完全 copy 這個 output
GAN_Lecture_1_(2018)_-_Introduction-926 他會選擇在某些地方不得不做一些妥協
GAN_Lecture_1_(2018)_-_Introduction-927 沒有辦法跟他的目標一模一樣
GAN_Lecture_1_(2018)_-_Introduction-928 但是這個時候選擇在甚麼地方做妥協
GAN_Lecture_1_(2018)_-_Introduction-929 就會變的非常的重要
GAN_Lecture_1_(2018)_-_Introduction-930 舉例來說這個是你的目標
GAN_Lecture_1_(2018)_-_Introduction-931 有四個不同的 generator，他們產生四張這樣子的圖片
GAN_Lecture_1_(2018)_-_Introduction-932 如果是看這個圖片跟這個圖片在 pixel 上相似的程度
GAN_Lecture_1_(2018)_-_Introduction-933 你會發現這兩張圖片差了一個 pixel
GAN_Lecture_1_(2018)_-_Introduction-934 這兩張圖片差了一個 pixel
GAN_Lecture_1_(2018)_-_Introduction-935 而下面這個圖片跟他插了六個 pixel
GAN_Lecture_1_(2018)_-_Introduction-936 這個圖片跟他差了六個 pixel
GAN_Lecture_1_(2018)_-_Introduction-937 對一個 generator 來說，假設 learning 的時候是希望 output 的圖片跟目標越像越好
GAN_Lecture_1_(2018)_-_Introduction-938 如果他不得不犯一些錯誤
GAN_Lecture_1_(2018)_-_Introduction-939 他會傾向於產生這樣的圖片
GAN_Lecture_1_(2018)_-_Introduction-940 這樣子的圖片他的錯誤只有一個 pixel
GAN_Lecture_1_(2018)_-_Introduction-941 這樣的圖片他的錯誤還有六個 pixel
GAN_Lecture_1_(2018)_-_Introduction-942 但事實上假設從人的觀點來看你會知道
GAN_Lecture_1_(2018)_-_Introduction-943 這個是不行的這個錯一個 pixel 整個圖片看起來就不對了
GAN_Lecture_1_(2018)_-_Introduction-944 整個看起來就不像是人手寫的數字看起來就是錯的
GAN_Lecture_1_(2018)_-_Introduction-945 但是下面這個 case 他只是把筆畫弄得長一點而已
GAN_Lecture_1_(2018)_-_Introduction-946 所以其實你是可以接受的這個你是覺得可以的
GAN_Lecture_1_(2018)_-_Introduction-947 所以變成說你不能夠單純的去讓你的 output 跟目標越像越好
GAN_Lecture_1_(2018)_-_Introduction-948 單純的讓你的 output 跟目標越像越好，可能 generator 就會產生這種圖片，而不是這種圖片
GAN_Lecture_1_(2018)_-_Introduction-949 而在做 Structured Learning 的時候
GAN_Lecture_1_(2018)_-_Introduction-950 我們講過 Structured Learning output 就是個比較複雜的結構
GAN_Lecture_1_(2018)_-_Introduction-951 裡面有很多很多 component
GAN_Lecture_1_(2018)_-_Introduction-952 component 和 component 之間的關係是非常重要的
GAN_Lecture_1_(2018)_-_Introduction-953 我們覺得這張圖片是不行的
GAN_Lecture_1_(2018)_-_Introduction-954 並不是因為這邊放了一個 pixel 有甚麼不好
GAN_Lecture_1_(2018)_-_Introduction-955 在這邊放一個 pixel 這件事情本身是沒有錯的
GAN_Lecture_1_(2018)_-_Introduction-956 因為如果這邊放一個 pixel 但是可以把這邊補滿
GAN_Lecture_1_(2018)_-_Introduction-957 他仍然像是一個人手寫的數字
GAN_Lecture_1_(2018)_-_Introduction-958 並不是說在這邊塗黑是有錯的而是把這邊塗黑在相鄰的地方沒有把他跟著塗黑
GAN_Lecture_1_(2018)_-_Introduction-959 所以在一個 Structured Learning 裡面component 和 component 之間的關係是非常重要的
GAN_Lecture_1_(2018)_-_Introduction-960 但其實在 train 一個 generator 去生成圖片的時候會發現一個 network 的架構其實沒有那麼容易讓我們把
GAN_Lecture_1_(2018)_-_Introduction-961 component 和 component 之間的關係放進去
GAN_Lecture_1_(2018)_-_Introduction-962 假設你是做圖片的生成
GAN_Lecture_1_(2018)_-_Introduction-963 假設 layer L 就是整個 generator 的最後一個 layer
GAN_Lecture_1_(2018)_-_Introduction-964 這個 layer L 他生廚來的就是一張圖片
GAN_Lecture_1_(2018)_-_Introduction-965 這個 layer L 的每一個 neural
GAN_Lecture_1_(2018)_-_Introduction-966 他顯然就對應到圖片的每一個 pixel
GAN_Lecture_1_(2018)_-_Introduction-967 他 output 的數值就是那個 pixel 圖的顏色的深淺
GAN_Lecture_1_(2018)_-_Introduction-968 你會發現在這整個架構裡面
GAN_Lecture_1_(2018)_-_Introduction-969 假設 layer L - 1 的值是給定的
GAN_Lecture_1_(2018)_-_Introduction-970 那事實上每一個 layer 的 output假設 weight 已經給定了
GAN_Lecture_1_(2018)_-_Introduction-971 前面 layer L - 1 的輸出已經給定了
GAN_Lecture_1_(2018)_-_Introduction-972 事實上每一個 dimension 他的 output 其實是 independent
GAN_Lecture_1_(2018)_-_Introduction-973 假設這個 neural 他產生了顏色
GAN_Lecture_1_(2018)_-_Introduction-974 他希望它旁邊的人也跟著產生顏色
GAN_Lecture_1_(2018)_-_Introduction-975 產生像是 realistic 的 image
GAN_Lecture_1_(2018)_-_Introduction-976 實際上這件事情是沒有辦法做到的
GAN_Lecture_1_(2018)_-_Introduction-977 他們之間並沒有辦法互相影響他就自己產生自己的
GAN_Lecture_1_(2018)_-_Introduction-978 他們並沒有辦法互相配合去產生一個好的 image
GAN_Lecture_1_(2018)_-_Introduction-979 所以這個就是單純的 learn 一個 generator 困難的地方
GAN_Lecture_1_(2018)_-_Introduction-980 這個問題也不是沒有辦法解的
GAN_Lecture_1_(2018)_-_Introduction-981 雖然只看一個 dimension
GAN_Lecture_1_(2018)_-_Introduction-982 你沒有辦法考慮 pixel 和 pixel 之間的 correlation
GAN_Lecture_1_(2018)_-_Introduction-983 如果再多加幾個 hidden layer
GAN_Lecture_1_(2018)_-_Introduction-984 就可以把這種 correlation 考慮進來
GAN_Lecture_1_(2018)_-_Introduction-985 如果你不想要用 discriminator
GAN_Lecture_1_(2018)_-_Introduction-986 只單純的用 Auto-encoder 的技術想要做 generation 這件事情
GAN_Lecture_1_(2018)_-_Introduction-987 根據我們的經驗，如果有同樣的 network
GAN_Lecture_1_(2018)_-_Introduction-988 一個是用 GAN train一個是用 Auto-encoder train
GAN_Lecture_1_(2018)_-_Introduction-989 往往就是 GAN 的那個可以產生圖片，但是 Auto-encoder 那個
GAN_Lecture_1_(2018)_-_Introduction-990 需要更大的 network 才能夠產生跟 GAN 接近的結果
GAN_Lecture_1_(2018)_-_Introduction-991 所以如果你要把 correlation 考慮進去你會需要比較深的 network
GAN_Lecture_1_(2018)_-_Introduction-992 這個是一個 toy 的 example
GAN_Lecture_1_(2018)_-_Introduction-993 這個實驗要做的是 train 一個 generator
GAN_Lecture_1_(2018)_-_Introduction-994 generator 的 input 是一個二維的 Gaussian 的 noise
GAN_Lecture_1_(2018)_-_Introduction-995 他的 output 就是綠色的這些點 ( 講錯 )
GAN_Lecture_1_(2018)_-_Introduction-996 他在 output 的時候他的目標希望他能夠學會 output
GAN_Lecture_1_(2018)_-_Introduction-997 generator 的 output 是藍色這些點
GAN_Lecture_1_(2018)_-_Introduction-998 綠色的是他學習的目標
GAN_Lecture_1_(2018)_-_Introduction-999 我們希望 generator 產生出來的是像綠色這樣的 distribution
GAN_Lecture_1_(2018)_-_Introduction-1000 如果用 Variational Auto-encoder 的技術 learn 下去以後
GAN_Lecture_1_(2018)_-_Introduction-1001 你會好只能夠讓這個 generator 產生藍色的這些點
GAN_Lecture_1_(2018)_-_Introduction-1002 因為對 generator 來說要考慮 pixel 和 pixel
GAN_Lecture_1_(2018)_-_Introduction-1003 也就是 dimension 1 和 dimension 2 之間的 correlation 是有點困難的
GAN_Lecture_1_(2018)_-_Introduction-1004 他並不知道如果在 x1 值很大的時候
GAN_Lecture_1_(2018)_-_Introduction-1005 x2 如果值很大是好的如果值很小也是好的
GAN_Lecture_1_(2018)_-_Introduction-1006 如果值介於不大小中間是不好的他不太容易學到這件事
GAN_Lecture_1_(2018)_-_Introduction-1007 你就會發現在這個 distribution 跟這個 distribution 之間還有一大堆的點散布
GAN_Lecture_1_(2018)_-_Introduction-1008 而這兩個 distribution 他們的 mixture 靠得太近了
GAN_Lecture_1_(2018)_-_Introduction-1009 generator 根本就沒有辦法把他們區分開來
GAN_Lecture_1_(2018)_-_Introduction-1010 這個是用 Variational Auto-encoder 可能會遇到的問題
GAN_Lecture_1_(2018)_-_Introduction-1011 在休息之前想給大家看一個東西
GAN_Lecture_1_(2018)_-_Introduction-1012 一邊 train 還是可以一邊講的
GAN_Lecture_1_(2018)_-_Introduction-1013 接下來問的第二個問題就是為甚麼 discriminator 他沒有辦法自己產生 image
GAN_Lecture_1_(2018)_-_Introduction-1014 其實 discriminator 他可以自己產生 image
GAN_Lecture_1_(2018)_-_Introduction-1015 只是他非常的卡
GAN_Lecture_1_(2018)_-_Introduction-1016 複習一下 discriminator 是甚麼discriminator 就是輸入一個 object 輸出一個分數
GAN_Lecture_1_(2018)_-_Introduction-1017 這個分數代表輸入的東西有多好
GAN_Lecture_1_(2018)_-_Introduction-1018 事實上這個 discriminator 在不同的文獻上他可能有不同的名字
GAN_Lecture_1_(2018)_-_Introduction-1019 他可能叫做 Evaluation Function
GAN_Lecture_1_(2018)_-_Introduction-1020 他可能叫 Potential Function
GAN_Lecture_1_(2018)_-_Introduction-1021 他可能叫 Energy Function
GAN_Lecture_1_(2018)_-_Introduction-1022 如果在別其他的領域曾經看過這些名字、function，比如說 Potential Function，你想想看
GAN_Lecture_1_(2018)_-_Introduction-1023 他是不是也是吃一個 input 然後就 output 一個 scalar
GAN_Lecture_1_(2018)_-_Introduction-1024 這個 scalar 就決定這個 input 有多好或是多不好
GAN_Lecture_1_(2018)_-_Introduction-1025 所以 discriminator 在不同的領域其實是有不同的名字
GAN_Lecture_1_(2018)_-_Introduction-1026 能不能用 discriminator 來生成 object
GAN_Lecture_1_(2018)_-_Introduction-1027 其實是可以的
GAN_Lecture_1_(2018)_-_Introduction-1028 怎麼用 discriminator 來做 generation
GAN_Lecture_1_(2018)_-_Introduction-1029 就套下面這個式子
GAN_Lecture_1_(2018)_-_Introduction-1030 不過在講他怎麼做之前
GAN_Lecture_1_(2018)_-_Introduction-1031 先來講 discriminator 相較於 generator 有甚麼樣的優勢
GAN_Lecture_1_(2018)_-_Introduction-1032 剛才有說過 generator 在產生 object 的時候
GAN_Lecture_1_(2018)_-_Introduction-1033 他是每一個 component 一個一個獨立去生成
GAN_Lecture_1_(2018)_-_Introduction-1034 所以對他來說要考慮 component 和 component 之間的 correlation 是比較困難的
GAN_Lecture_1_(2018)_-_Introduction-1035 但對 discriminator 來說，要考慮 component 和 component 之間的 correlation 就比較容易了
GAN_Lecture_1_(2018)_-_Introduction-1036 對 discriminator 來說，因為是產生完一張完整的 image 以後
GAN_Lecture_1_(2018)_-_Introduction-1037 再把這張 image 丟給 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1038 讓 discriminator 去給他評價看他好還是不好
GAN_Lecture_1_(2018)_-_Introduction-1039 所以對 discriminator 來說他可以輕易地告訴你這張圖片就是不好的，應該得到低分
GAN_Lecture_1_(2018)_-_Introduction-1040 給他這張圖片，是好的，就應該拿到高分
GAN_Lecture_1_(2018)_-_Introduction-1041 實際上怎麼做到這件事情呢
GAN_Lecture_1_(2018)_-_Introduction-1042 可能 discriminator 也是一個 Convolutional Neural Network
GAN_Lecture_1_(2018)_-_Introduction-1043 這 Convolutional Neural Network 有一個 filter 是長這樣的 filter
GAN_Lecture_1_(2018)_-_Introduction-1044 這個 filter 會去 detect 有沒有 isolated 的 pixel
GAN_Lecture_1_(2018)_-_Introduction-1045 有沒有 pixel 他的周圍都沒有其他 pixel
GAN_Lecture_1_(2018)_-_Introduction-1046 如果有這種就給他低分
GAN_Lecture_1_(2018)_-_Introduction-1047 所以對 discriminator 來說當產生完一張完整的圖片以後
GAN_Lecture_1_(2018)_-_Introduction-1048 要檢查這張圖片裡面的 component 和 component 之間的 correlation 對不對
GAN_Lecture_1_(2018)_-_Introduction-1049 是比較容易的在生成的時候因為是一個一個 component 獨立生成
GAN_Lecture_1_(2018)_-_Introduction-1050 不容易考慮他們之間的關係但是等整張圖片已經生成完以後
GAN_Lecture_1_(2018)_-_Introduction-1051 要再檢查這關係對不對，是比較容易的
GAN_Lecture_1_(2018)_-_Introduction-1052 這個就是 discriminator 所佔到的優勢
GAN_Lecture_1_(2018)_-_Introduction-1053 接下來要講怎麼使用 discriminator 來產生東西
GAN_Lecture_1_(2018)_-_Introduction-1054 就套用下面這個式子
GAN_Lecture_1_(2018)_-_Introduction-1055 假設已經有一個 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1056 這個 discriminator 他確實可以鑑別 input 一個 x 他是好的還是不好的
GAN_Lecture_1_(2018)_-_Introduction-1057 怎麼拿這個 discriminator 來做生成
GAN_Lecture_1_(2018)_-_Introduction-1058 窮舉所有可能的 x
GAN_Lecture_1_(2018)_-_Introduction-1059 一個一個丟到 discriminator 裡面
GAN_Lecture_1_(2018)_-_Introduction-1060 看哪一個 x discriminator 會給他最高的分數或給他很高的分數
GAN_Lecture_1_(2018)_-_Introduction-1061 discriminator 會給他高分的 x就是生成的結果，結束
GAN_Lecture_1_(2018)_-_Introduction-1062 你可能會覺得非常的 surprise 有可能窮舉所有的 x
GAN_Lecture_1_(2018)_-_Introduction-1063 一個一個去檢查他丟到 discriminator 以後會得到高分還是低分，假設 x 是一張 image
GAN_Lecture_1_(2018)_-_Introduction-1064 image 就是由一堆 pixel 所組成的
GAN_Lecture_1_(2018)_-_Introduction-1065 有可能窮舉所有 pixel 顏色的組合
GAN_Lecture_1_(2018)_-_Introduction-1066 看怎麼樣 pixel 顏色的組合會得到高分嗎
GAN_Lecture_1_(2018)_-_Introduction-1067 仔細想想好像有一些難度但這邊要給你的答案就是
GAN_Lecture_1_(2018)_-_Introduction-1068 不是這個問題，先假設這一個窮舉所有的 x
GAN_Lecture_1_(2018)_-_Introduction-1069 在哪一個 x 可以得到高分這件事情
GAN_Lecture_1_(2018)_-_Introduction-1070 somehow 就是有某一個演算法
GAN_Lecture_1_(2018)_-_Introduction-1071 可以做到這件事情假設你很仔細的思考以後
GAN_Lecture_1_(2018)_-_Introduction-1072 也許你可以想出一個演算法來解這個 arg max 的 problem
GAN_Lecture_1_(2018)_-_Introduction-1073 如果你可以解這個 arg max 的 problem就可以用 discriminator 來做生成
GAN_Lecture_1_(2018)_-_Introduction-1074 但是這個生成是非常痛苦的
GAN_Lecture_1_(2018)_-_Introduction-1075 因為這個 discriminator 擅長的就是批評
GAN_Lecture_1_(2018)_-_Introduction-1076 他不擅長做有建設性的鑑別他只擅長批評就跟政論節目的名嘴一樣
GAN_Lecture_1_(2018)_-_Introduction-1077 你給他甚麼東西他都說這個是不好的
GAN_Lecture_1_(2018)_-_Introduction-1078 你叫他真的想一個好的東西他是很難想出來的
GAN_Lecture_1_(2018)_-_Introduction-1079 那這很痛苦
GAN_Lecture_1_(2018)_-_Introduction-1080 假設說他就是有辦法生成東西接下來的問題是
GAN_Lecture_1_(2018)_-_Introduction-1081 在他有辦法生成東西的前提之下
GAN_Lecture_1_(2018)_-_Introduction-1082 我們怎麼樣訓練這個 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1083 當我們在訓練 discriminator 的時後簡單來說就是要給他很多好的 example
GAN_Lecture_1_(2018)_-_Introduction-1084 告訴他這好的 example 是高分的
GAN_Lecture_1_(2018)_-_Introduction-1085 給他一大堆爛的 example 告訴他爛的 example 就是低分的
GAN_Lecture_1_(2018)_-_Introduction-1086 但實際上手上只有好的 example
GAN_Lecture_1_(2018)_-_Introduction-1087 手上真正有的假設做二次元人物的生成
GAN_Lecture_1_(2018)_-_Introduction-1088 手上的 data 是人生成的 data是人畫出來的二次元人物的頭像
GAN_Lecture_1_(2018)_-_Introduction-1089 這些東西通通都是好的這些東西 discriminator 都應該給他高分
GAN_Lecture_1_(2018)_-_Introduction-1090 但是這個時候 discriminator 只有 positive 的 example
GAN_Lecture_1_(2018)_-_Introduction-1091 只有正面的例子完全沒有反面的例子
GAN_Lecture_1_(2018)_-_Introduction-1092 如果只拿正面的例子去 train discriminator
GAN_Lecture_1_(2018)_-_Introduction-1093 會發生甚麼樣的問題
GAN_Lecture_1_(2018)_-_Introduction-1094 你會發現這個 discriminator 之後學到看到甚麼東西他就會覺得是正面的例子，都給他高分
GAN_Lecture_1_(2018)_-_Introduction-1095 他訓練的時候只有高分的東西從來沒有看過任何低分的東西
GAN_Lecture_1_(2018)_-_Introduction-1096 他就會學到看到甚麼東西，都給他高分就是對的
GAN_Lecture_1_(2018)_-_Introduction-1097 這個顯然不是我們要的
GAN_Lecture_1_(2018)_-_Introduction-1098 所以怎麼辦需要給 machine 一些 negative example
GAN_Lecture_1_(2018)_-_Introduction-1099 但是從哪裡找 negative example 就變得非常的關鍵了
GAN_Lecture_1_(2018)_-_Introduction-1100 如果找出來的 negative example 非常的差
GAN_Lecture_1_(2018)_-_Introduction-1101 你就跟機器說人畫的就是好的，就是要給高分
GAN_Lecture_1_(2018)_-_Introduction-1102 然後隨機產生一大堆的 noise這些 noise 就是要給他低分
GAN_Lecture_1_(2018)_-_Introduction-1103 對機器來說當然可以分辨這兩種圖片的差別
GAN_Lecture_1_(2018)_-_Introduction-1104 看到這種給他高分看到這種給他低分
GAN_Lecture_1_(2018)_-_Introduction-1105 但之後給他這種圖片也許畫得很差
GAN_Lecture_1_(2018)_-_Introduction-1106 但是他覺得這個還是比 noise 好很多
GAN_Lecture_1_(2018)_-_Introduction-1107 也會給他高分這個不是我們要的
GAN_Lecture_1_(2018)_-_Introduction-1108 所以怎麼產生好的 negative example 就變得很重要
GAN_Lecture_1_(2018)_-_Introduction-1109 假設可以產生非常真實的 negative example
GAN_Lecture_1_(2018)_-_Introduction-1110 他這個還是有些錯，比如說這兩隻眼睛的顏色不一樣
GAN_Lecture_1_(2018)_-_Introduction-1111 你可以產生非常好的 negative example
GAN_Lecture_1_(2018)_-_Introduction-1112 這樣 discriminator 才能夠真的學會鑑別好的 image 跟壞的 image
GAN_Lecture_1_(2018)_-_Introduction-1113 現在問題就是怎麼產生這些非常好的 negative example
GAN_Lecture_1_(2018)_-_Introduction-1114 要產生這些好的 negative example
GAN_Lecture_1_(2018)_-_Introduction-1115 也需要一個很好的 process 去產生這些 negative example
GAN_Lecture_1_(2018)_-_Introduction-1116 現在就是不知道怎麼產生 image 才要 train model這樣就變一個雞生蛋，蛋生雞的問題
GAN_Lecture_1_(2018)_-_Introduction-1117 要有好的 negative example 才能夠訓練 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1118 要有好的 discriminator 才能夠幫我們找出好的 negative example
GAN_Lecture_1_(2018)_-_Introduction-1119 這樣就陷入一個雞生蛋，蛋生雞的問題
GAN_Lecture_1_(2018)_-_Introduction-1120 實際上怎麼解這個問題
GAN_Lecture_1_(2018)_-_Introduction-1121 用 iterative 的方法來解這個問題
GAN_Lecture_1_(2018)_-_Introduction-1122 要怎麼訓練 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1123 假設有一堆 positive example
GAN_Lecture_1_(2018)_-_Introduction-1124 假設有一堆 negative example
GAN_Lecture_1_(2018)_-_Introduction-1125 一開始 positive example 是人畫的 image
GAN_Lecture_1_(2018)_-_Introduction-1126 negative example 是 random sample 出來的一堆 noise
GAN_Lecture_1_(2018)_-_Introduction-1127 在每一個 iteration 裡面
GAN_Lecture_1_(2018)_-_Introduction-1128 discriminator 學會做的事情就是給這些 positive example 高的分數
GAN_Lecture_1_(2018)_-_Introduction-1129 給這些 negative example 低的分數
GAN_Lecture_1_(2018)_-_Introduction-1130 接下來當你學出一個 discriminator 以後
GAN_Lecture_1_(2018)_-_Introduction-1131 可以用這個 discriminator 去做 generation
GAN_Lecture_1_(2018)_-_Introduction-1132 我們剛才說 discriminator 是可以做 generation 的
GAN_Lecture_1_(2018)_-_Introduction-1133 只要會解這個 arg max 的 problem
GAN_Lecture_1_(2018)_-_Introduction-1134 就可以用這個 discriminator 做 generation
GAN_Lecture_1_(2018)_-_Introduction-1135 就用這個 discriminator generate 出一堆他覺得是好的 image
GAN_Lecture_1_(2018)_-_Introduction-1136 這些 image 我們用 x tilde 來表示他
GAN_Lecture_1_(2018)_-_Introduction-1137 有了這些原來的 discriminator 覺得是好的 image 以後
GAN_Lecture_1_(2018)_-_Introduction-1138 在下一個 iteration 把原來 random 的 image 換成這些第一代的 discriminator 覺得是好的 image
GAN_Lecture_1_(2018)_-_Introduction-1139 接下來再 update discriminator 參數告訴他這些是好的
GAN_Lecture_1_(2018)_-_Introduction-1140 這些是不好的
GAN_Lecture_1_(2018)_-_Introduction-1141 用了新的 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1142 新的 discriminator 再解一次 arg max 的 problem因為 discriminator 變了
GAN_Lecture_1_(2018)_-_Introduction-1143 所以他可能會產生更好的 image
GAN_Lecture_1_(2018)_-_Introduction-1144 這個 process 就一直反覆的循環下去
GAN_Lecture_1_(2018)_-_Introduction-1145 只有一個 discriminator 他去分辨圖片的好壞
GAN_Lecture_1_(2018)_-_Introduction-1146 他參數 update 以後他可以去產生更好的 image
GAN_Lecture_1_(2018)_-_Introduction-1147 接下來就可以 learn 更好的 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1148 不斷的反覆這個 process
GAN_Lecture_1_(2018)_-_Introduction-1149 這邊是一個圖示化的方式來講這個運作看起來像是甚麼樣子
GAN_Lecture_1_(2018)_-_Introduction-1150 這個紅色的線代表的是 discriminator 的 valueinput 一個 object，這個 object 如果是 image 的話
GAN_Lecture_1_(2018)_-_Introduction-1151 他是分布在一個很高維的空間中
GAN_Lecture_1_(2018)_-_Introduction-1152 現在為了簡化起見，就假設 object 分布在一維中
GAN_Lecture_1_(2018)_-_Introduction-1153 object 隨著分布的位子不同
GAN_Lecture_1_(2018)_-_Introduction-1154 discriminator 會給他不同的分數
GAN_Lecture_1_(2018)_-_Introduction-1155 那你的 real example 假設是落在這個地方
GAN_Lecture_1_(2018)_-_Introduction-1156 discriminator 當然要給落在這個區域的東西高的分數
GAN_Lecture_1_(2018)_-_Introduction-1157 他應該要給其他的區域，不在這個高分的區域低的分數
GAN_Lecture_1_(2018)_-_Introduction-1158 實際上這整個 object 可以分布的 space 是非常巨大的
GAN_Lecture_1_(2018)_-_Introduction-1159 如果在一維空間裡面你可能會覺得反正這個地方是高分的
GAN_Lecture_1_(2018)_-_Introduction-1160 那兩邊就應該給他低分的
GAN_Lecture_1_(2018)_-_Introduction-1161 但是如果是一個高維的空間
GAN_Lecture_1_(2018)_-_Introduction-1162 高維空間裡面沒有出現 real example 的地方的分數
GAN_Lecture_1_(2018)_-_Introduction-1163 都把它壓低
GAN_Lecture_1_(2018)_-_Introduction-1164 所以實際上怎麼做
GAN_Lecture_1_(2018)_-_Introduction-1165 實際上的做法看起來像這個樣子
GAN_Lecture_1_(2018)_-_Introduction-1166 剛才講說 discriminator 在 train 的時候他是 iterative 去 train 的
GAN_Lecture_1_(2018)_-_Introduction-1167 假設一開始 real data 分布長這個樣子
GAN_Lecture_1_(2018)_-_Introduction-1168 有一些 generate 出來的 data
GAN_Lecture_1_(2018)_-_Introduction-1169 有一些 negative example 他的分布是藍色的這個
GAN_Lecture_1_(2018)_-_Introduction-1170 接下來 discriminator 會去學著給綠色的點高分給藍色的點低分
GAN_Lecture_1_(2018)_-_Introduction-1171 因為它只知道給這個區域高分
GAN_Lecture_1_(2018)_-_Introduction-1172 給這個區域低分
GAN_Lecture_1_(2018)_-_Introduction-1173 他可能沒有學到要給這個區域多少分數
GAN_Lecture_1_(2018)_-_Introduction-1174 所以搞不好 learn 完以後，這個區域的分數還比 real data 分數要高
GAN_Lecture_1_(2018)_-_Introduction-1175 這個是有可能的，因為並沒有給機器 constrain 這個地方的分數是怎樣，你只說這邊分數要高
GAN_Lecture_1_(2018)_-_Introduction-1176 這邊分數要低
GAN_Lecture_1_(2018)_-_Introduction-1177 這個地方要怎樣其實是不知道的
GAN_Lecture_1_(2018)_-_Introduction-1178 接下來 learn 出第一個 discriminator 以後
GAN_Lecture_1_(2018)_-_Introduction-1179 用這個 discriminator 去產生 example
GAN_Lecture_1_(2018)_-_Introduction-1180 也就是說去找出這個 discriminator 的弱點
GAN_Lecture_1_(2018)_-_Introduction-1181 剛才說在這個 iterative 的 process 裡面 learn discriminator
GAN_Lecture_1_(2018)_-_Introduction-1182 接下來用 discriminator 去產生 negative example
GAN_Lecture_1_(2018)_-_Introduction-1183 用 discriminator 去找出自己的弱點
GAN_Lecture_1_(2018)_-_Introduction-1184 用 discriminator 產生 negative example 的時候我們有說怎麼讓 discriminator 做生成
GAN_Lecture_1_(2018)_-_Introduction-1185 就是看那些圖片 discriminator 會給他高分，那些東西就是 negative example
GAN_Lecture_1_(2018)_-_Introduction-1186 如果是看這個例子的話
GAN_Lecture_1_(2018)_-_Introduction-1187 discriminator 是給這些圖片高分如果看這個 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1188 它其實在這邊分數是特別高的
GAN_Lecture_1_(2018)_-_Introduction-1189 所以他是給這些圖片高分
GAN_Lecture_1_(2018)_-_Introduction-1190 這些圖片就是 discriminator 自己產生出來的 negative example
GAN_Lecture_1_(2018)_-_Introduction-1191 在下一個步驟裡面，discriminator 產生出這些 negative example 以後
GAN_Lecture_1_(2018)_-_Introduction-1192 它又要學到它給這些 example 低分，這些 example 高分
GAN_Lecture_1_(2018)_-_Introduction-1193 discriminator 形狀就會改變，可能就變成這個樣子
GAN_Lecture_1_(2018)_-_Introduction-1194 這個 process 就反覆繼續下去
GAN_Lecture_1_(2018)_-_Introduction-1195 這就好像是 discriminator 不斷去尋找它的這個 function 裡面的弱點
GAN_Lecture_1_(2018)_-_Introduction-1196 看看哪邊分數特別高，就某個地方如果不是 real example
GAN_Lecture_1_(2018)_-_Introduction-1197 它又凸起來，在那邊 sample 一些 negative example
GAN_Lecture_1_(2018)_-_Introduction-1198 然後在下一個 iteration，discriminator 就會把那個區域的值壓下去
GAN_Lecture_1_(2018)_-_Introduction-1199 大家聽得懂我的意思嗎大家有問題要問嗎
GAN_Lecture_1_(2018)_-_Introduction-1200 這樣久不會 overfit 嗎
GAN_Lecture_1_(2018)_-_Introduction-1201 這個問題問得很好這個問題問的還滿難回答的
GAN_Lecture_1_(2018)_-_Introduction-1202 因為 machine 在做生成的時候
GAN_Lecture_1_(2018)_-_Introduction-1203 它就是看它的 real data 長甚麼樣子
GAN_Lecture_1_(2018)_-_Introduction-1204 它要產生像 real data 的東西
GAN_Lecture_1_(2018)_-_Introduction-1205 所以如果你的 data 很少，確實有可能 overfit
GAN_Lecture_1_(2018)_-_Introduction-1206 train 得起來
GAN_Lecture_1_(2018)_-_Introduction-1207 但是那些結果就是 database 裡面的那些圖片
GAN_Lecture_1_(2018)_-_Introduction-1208 你會希望你給他的圖片夠多
GAN_Lecture_1_(2018)_-_Introduction-1209 他可以學到 generalize 的東西產生 database 裡面沒有的圖片
GAN_Lecture_1_(2018)_-_Introduction-1210 這個又跟一般的 training 和 testing 不太一樣
GAN_Lecture_1_(2018)_-_Introduction-1211 所以在做 GAN 的生成的時候
GAN_Lecture_1_(2018)_-_Introduction-1212 其實很難知道到底有沒有 overfit
GAN_Lecture_1_(2018)_-_Introduction-1213 大家還有甚麼問題要問嗎
GAN_Lecture_1_(2018)_-_Introduction-1214 最後就希望訓練到
GAN_Lecture_1_(2018)_-_Introduction-1215 對 discriminator 來說在這個 input space 上面
GAN_Lecture_1_(2018)_-_Introduction-1216 只有出現 real data 的地方
GAN_Lecture_1_(2018)_-_Introduction-1217 分數才是高的
GAN_Lecture_1_(2018)_-_Introduction-1218 這個時候 negative example 跟 positive example 他們的 distribution 就會重合在一起
GAN_Lecture_1_(2018)_-_Introduction-1219 當 negative example 和 positive example 他們的 distribution 重合在一起的時候
GAN_Lecture_1_(2018)_-_Introduction-1220 training 的 process 就會停下來
GAN_Lecture_1_(2018)_-_Introduction-1221 這個是 discriminator 的 training
GAN_Lecture_1_(2018)_-_Introduction-1222 假設知道 discriminator 可以拿來做生成
GAN_Lecture_1_(2018)_-_Introduction-1223 其實根本就不需要 generator，光憑藉著 discriminator 也可以做生成這件事
GAN_Lecture_1_(2018)_-_Introduction-1224 你可能會想說有人真的拿 discriminator 做生成嗎
GAN_Lecture_1_(2018)_-_Introduction-1225 有的，其實有滿坑滿谷的 work
GAN_Lecture_1_(2018)_-_Introduction-1226 都是拿 discriminator 來做生成
GAN_Lecture_1_(2018)_-_Introduction-1227 假設你熟悉整個 graphical model 的 work 的話
GAN_Lecture_1_(2018)_-_Introduction-1228 你仔細回去想一下，剛才講的那個 train discriminator 的 process 其實就是 general 的 Graphical Model 的 training
GAN_Lecture_1_(2018)_-_Introduction-1229 只是在不同的 method 裡面講法會略有不同而已
GAN_Lecture_1_(2018)_-_Introduction-1230 因為我不知道大家對 Graphical Model 熟不熟
GAN_Lecture_1_(2018)_-_Introduction-1231 Graphical Model 其實就是 Structured Learning 的一種
GAN_Lecture_1_(2018)_-_Introduction-1232 然後 Graphical Model 裡面又分成很多類比如說有 Bayesian Network
GAN_Lecture_1_(2018)_-_Introduction-1233 比如說有 Markov Random Field 等等
GAN_Lecture_1_(2018)_-_Introduction-1234 然後有很多很多的基礎這樣子
GAN_Lecture_1_(2018)_-_Introduction-1235 這邊其實沒有列完所有和 Graphical Model 有關的技術
GAN_Lecture_1_(2018)_-_Introduction-1236 這邊列的是過去 MLDS 會講的東西
GAN_Lecture_1_(2018)_-_Introduction-1237 MLDS 的名字是 Machine Learning Having it Deep and Structured
GAN_Lecture_1_(2018)_-_Introduction-1238 本來這門課是有一半的時間會講 Structured Learning 這個技術
GAN_Lecture_1_(2018)_-_Introduction-1239 只是發現講到後來大家都不想聽了
GAN_Lecture_1_(2018)_-_Introduction-1240 大家都只想聽 Deep Learning
GAN_Lecture_1_(2018)_-_Introduction-1241 Structured Learning 大家都不知道是甚麼東西大家其實是比較不想聽的
GAN_Lecture_1_(2018)_-_Introduction-1242 所以後來就不太講 Structured Learning 的東西
GAN_Lecture_1_(2018)_-_Introduction-1243 但是 GAN 其實也可以被視為是 Structured Learning 的一種技術
GAN_Lecture_1_(2018)_-_Introduction-1244 而在 Structured Learning 的技術裡面，其中非常具有代表性的東西就是 Graphical Model
GAN_Lecture_1_(2018)_-_Introduction-1245 如果你熟悉 Graphical Model 比如說 Markov Random Field 或 Bayesian Network 的話
GAN_Lecture_1_(2018)_-_Introduction-1246 你想想看在 Markov Random Field、Bayesian Network 裡面
GAN_Lecture_1_(2018)_-_Introduction-1247 你是定一個 graph，這個 graph 上面有一個 Potential Function
GAN_Lecture_1_(2018)_-_Introduction-1248 這個東西就是你的 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1249 那個 graph 就是一個 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1250 然後你輸入你的 observation 那個 graph 會告訴你這組 data 產生出來的機率有多少
GAN_Lecture_1_(2018)_-_Introduction-1251 那個機率就是 discriminator assign 的分數
GAN_Lecture_1_(2018)_-_Introduction-1252 假設你不熟 Graphical Model 的話這邊你聽不懂沒有關係但如果你熟悉的話你回去比較一下，是不是
GAN_Lecture_1_(2018)_-_Introduction-1253 Graphical Model 裡面的那個 graph、你的 Potential Function、你的 Markov Random Field
GAN_Lecture_1_(2018)_-_Introduction-1254 你的 Bayesian Network 其實就是 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1255 再回想一下當你去 train Markov Random Field 或你 train Structured SVM、train 種種 Graphical Model 的時候
GAN_Lecture_1_(2018)_-_Introduction-1256 種種和和 Structured Learning 有關的技術的時候
GAN_Lecture_1_(2018)_-_Introduction-1257 是不是 iterative 的去 train 的
GAN_Lecture_1_(2018)_-_Introduction-1258 你做的事情是不是有 positive 有 negative example 訓練出你的 model
GAN_Lecture_1_(2018)_-_Introduction-1259 接下來用 model sample 出 negative example 再去 update model
GAN_Lecture_1_(2018)_-_Introduction-1260 就跟我剛才講的 training discriminator 的流程其實是一樣的
GAN_Lecture_1_(2018)_-_Introduction-1261 只是把同樣的事情換個名詞來講，讓你覺得不太一樣而已
GAN_Lecture_1_(2018)_-_Introduction-1262 但你仔細想想，他們就是同一回事
GAN_Lecture_1_(2018)_-_Introduction-1263 這邊就講了 generator 跟 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1264 來比較一下 generator 和 discriminator 各自所遇到的問題
GAN_Lecture_1_(2018)_-_Introduction-1265 generator 的優勢就是
GAN_Lecture_1_(2018)_-_Introduction-1266 它很容易做生成，generator 在生成很快
GAN_Lecture_1_(2018)_-_Introduction-1267 這是個 Feedforward Network，很快就可以生一個東西
GAN_Lecture_1_(2018)_-_Introduction-1268 它的缺點就是它不容易考慮 component 和 component 之間的 correlation
GAN_Lecture_1_(2018)_-_Introduction-1269 它在學習的時候他只是去模仿某一個目標所以他只學到了那個目標的表象
GAN_Lecture_1_(2018)_-_Introduction-1270 它沒有學到它的精神講擬人化一點就是它只學到表象
GAN_Lecture_1_(2018)_-_Introduction-1271 因為它只學 pixel 和 pixel 之間相似的程度
GAN_Lecture_1_(2018)_-_Introduction-1272 它學不到大局學不到它的精神
GAN_Lecture_1_(2018)_-_Introduction-1273 如果看 discriminator 的話
GAN_Lecture_1_(2018)_-_Introduction-1274 discriminator 的優勢就是可以考慮大局
GAN_Lecture_1_(2018)_-_Introduction-1275 它的劣勢就是你要叫 discriminator 生一個東西
GAN_Lecture_1_(2018)_-_Introduction-1276 這個是千難萬難我們要去解一個 arg max 的 problem
GAN_Lecture_1_(2018)_-_Introduction-1277 而要解那個 arg max 的 problem 勢必就必需要對你的 discriminator 的 model 做一些假設
GAN_Lecture_1_(2018)_-_Introduction-1278 在傳統的文獻裡面
GAN_Lecture_1_(2018)_-_Introduction-1279 都必須要假設 discriminator 的 model 是線性的
GAN_Lecture_1_(2018)_-_Introduction-1280 才有辦法解那個 arg max 的 problem
GAN_Lecture_1_(2018)_-_Introduction-1281 你限制它是線性的意味著你限制了它的能力
GAN_Lecture_1_(2018)_-_Introduction-1282 如果它是非線性的，你不知道要怎麼解
GAN_Lecture_1_(2018)_-_Introduction-1283 那個 arg max 的 problem所以這個就是一個大問題
GAN_Lecture_1_(2018)_-_Introduction-1284 過去在講 Structured Learning 的時候講到這段要解 arg max 的 problem
GAN_Lecture_1_(2018)_-_Introduction-1285 同學都會問我說怎麼解我通常只能說你就假設
GAN_Lecture_1_(2018)_-_Introduction-1286 你可以解，大家聽了生氣了，所以後來都沒有人要聽這一段
GAN_Lecture_1_(2018)_-_Introduction-1287 現在不一樣的就是我們有了 generator
GAN_Lecture_1_(2018)_-_Introduction-1288 generator 做甚麼事情generator 就是取代了這個 arg max 的 problem
GAN_Lecture_1_(2018)_-_Introduction-1289 想像成本來要想要一個 algorithm
GAN_Lecture_1_(2018)_-_Introduction-1290 來解這個 arg max 的 problem往往我們都不知道要怎麼解
GAN_Lecture_1_(2018)_-_Introduction-1291 那個 arg max 的 problem
GAN_Lecture_1_(2018)_-_Introduction-1292 使得這一套 framework假設可以解 arg max problem
GAN_Lecture_1_(2018)_-_Introduction-1293 聽起來沒有問題但就是因為你不能解，所以聽起來就有很多的問題
GAN_Lecture_1_(2018)_-_Introduction-1294 但現在用 generator 來產生 negative example
GAN_Lecture_1_(2018)_-_Introduction-1295 用 generator 來解 arg max 的 problem
GAN_Lecture_1_(2018)_-_Introduction-1296 我們說 generator 它就可以產生出 x tilde
GAN_Lecture_1_(2018)_-_Introduction-1297 它產生出來的 x tilde 就是那些可以讓 discriminator 給他高分的 image
GAN_Lecture_1_(2018)_-_Introduction-1298 記不記得 generator 是怎麼 train 的
GAN_Lecture_1_(2018)_-_Introduction-1299 generator 在 training 的時候它是不是就是去學著產生一些 image
GAN_Lecture_1_(2018)_-_Introduction-1300 這些 image 是 discriminator 會給他高分的
GAN_Lecture_1_(2018)_-_Introduction-1301 所以可以想成 generator 在學怎麼解 arg max 這個 problem
GAN_Lecture_1_(2018)_-_Introduction-1302 它在學習的過程中
GAN_Lecture_1_(2018)_-_Introduction-1303 它就是在學怎麼產生 discriminator 會給他高分的那些 image
GAN_Lecture_1_(2018)_-_Introduction-1304 過去是解這樣一個 optimization 的 problem
GAN_Lecture_1_(2018)_-_Introduction-1305 現在比較不一樣，是用一個 intelligent 的方法
GAN_Lecture_1_(2018)_-_Introduction-1306 說它比較 intelligent 是因為它是個 network
GAN_Lecture_1_(2018)_-_Introduction-1307 network 來解這個 arg max 的 problem
GAN_Lecture_1_(2018)_-_Introduction-1308 GAN 有甚麼好處從 discriminator 的角度來看
GAN_Lecture_1_(2018)_-_Introduction-1309 過去不知道怎麼解 arg max 的 problem現在用 generator 來解 arg max 的 problem
GAN_Lecture_1_(2018)_-_Introduction-1310 顯然是比這個方法更加有效，而且更容易一般化的
GAN_Lecture_1_(2018)_-_Introduction-1311 對 generator 來說它在產生 object 的時候
GAN_Lecture_1_(2018)_-_Introduction-1312 仍然是 component by component 的產生
GAN_Lecture_1_(2018)_-_Introduction-1313 但是他得到的 feedback 不再是 L1 L2 的 loss不再是 pixel by pixel 的去算兩張圖片的相似度
GAN_Lecture_1_(2018)_-_Introduction-1314 它的 loss 將是來自於一個有大局觀的 discriminator
GAN_Lecture_1_(2018)_-_Introduction-1315 希望透過 discriminator 帶領generator 可以學會產生有大局觀的結果
GAN_Lecture_1_(2018)_-_Introduction-1316 這是一個用 GAN 來產生的 toy example這個 example 跟剛才講 VAE 看到的 example
GAN_Lecture_1_(2018)_-_Introduction-1317 其實是一模一樣的
GAN_Lecture_1_(2018)_-_Introduction-1318 只是換了一下顏色而已
GAN_Lecture_1_(2018)_-_Introduction-1319 剛才是藍色跟綠色這邊換成藍色跟紅色
GAN_Lecture_1_(2018)_-_Introduction-1320 藍色的點是我們要 generator 去產生的 distribution
GAN_Lecture_1_(2018)_-_Introduction-1321 是 generator 學習的目標
GAN_Lecture_1_(2018)_-_Introduction-1322 紅色的點是 generator 最終可以得到的結果
GAN_Lecture_1_(2018)_-_Introduction-1323 這邊用的 generator 的架構跟前面一頁的 VAE 用的 generator 架構是一樣的
GAN_Lecture_1_(2018)_-_Introduction-1324 但你發現在學習之後有了 discriminator 這邊的 generator 學出來的結果是比 VAE 的 generator 還要更好的
GAN_Lecture_1_(2018)_-_Introduction-1325 你會發現這個 mixture 和 mixture 之間是幾乎沒有點的
GAN_Lecture_1_(2018)_-_Introduction-1326 而對應到真實的應用，假設要讓 machine 產生圖片
GAN_Lecture_1_(2018)_-_Introduction-1327 產生人臉，如果用 VAE 產生的人臉就會比較糊
GAN_Lecture_1_(2018)_-_Introduction-1328 假設真實的人臉、親戚的人臉是一個 mixture
GAN_Lecture_1_(2018)_-_Introduction-1329 因為 VAE 產生的會產生那種 mixture 和 mixture 之間的 sample
GAN_Lecture_1_(2018)_-_Introduction-1330 所以他就會產生這些比較糊的人臉
GAN_Lecture_1_(2018)_-_Introduction-1331 如果是用 GAN 就會產生比較清晰的人臉
GAN_Lecture_1_(2018)_-_Introduction-1332 這一頁是要比較一下 GAN 跟 VAE 之間的差別
GAN_Lecture_1_(2018)_-_Introduction-1333 這是來自於 Google 的一篇 paper
GAN_Lecture_1_(2018)_-_Introduction-1334 這篇 paper 主要的內容是想要比較各種不同 GAN 的技術
GAN_Lecture_1_(2018)_-_Introduction-1335 它比較了，這個我們之後會再提到這些
GAN_Lecture_1_(2018)_-_Introduction-1336 它比較了 MM GAN、NS GAN、LSGAN、WGAN、WGAN GP、DRAGON 還有 BEGAN，各式各樣的 GAN
GAN_Lecture_1_(2018)_-_Introduction-1337 它得到的結論是農場文最喜歡的那種
GAN_Lecture_1_(2018)_-_Introduction-1338 它得到的結論是所有不同的 GAN 其實 performance 都差不多
GAN_Lecture_1_(2018)_-_Introduction-1339 之前做的事情都是白忙一場
GAN_Lecture_1_(2018)_-_Introduction-1340 農場文最喜歡的結論這個圖怎麼看
GAN_Lecture_1_(2018)_-_Introduction-1341 這個縱軸是算一個 FID Score，這個之後再講
GAN_Lecture_1_(2018)_-_Introduction-1342 這邊做的都是 image generation
GAN_Lecture_1_(2018)_-_Introduction-1343 FID Score 越小代表產生出來的圖片越像真實的圖片
GAN_Lecture_1_(2018)_-_Introduction-1344 這邊的值是越小越好
GAN_Lecture_1_(2018)_-_Introduction-1345 對於不同的 GAN 他們都試了各種不同的參數
GAN_Lecture_1_(2018)_-_Introduction-1346 GAN 在 training 的時候是非常的 sensitive 的
GAN_Lecture_1_(2018)_-_Introduction-1347 往往可能只有特定某一組參數才 train 得起來
GAN_Lecture_1_(2018)_-_Introduction-1348 它會發現 GAN 用不同的參數它的 performance 有一個非常巨大的 range
GAN_Lecture_1_(2018)_-_Introduction-1349 看不出說他們做兩個不同的 corpus，一個是 MNIST，一個是 CIFAR10
GAN_Lecture_1_(2018)_-_Introduction-1350 生成數字跟生成十種不同類別的圖案他們做 MNIST 跟 CIFAR10
GAN_Lecture_1_(2018)_-_Introduction-1351 發現如果比較這兩個 corpus，不同方法之間的差距其實看不出不同的 GAN 有甚麼樣的不同
GAN_Lecture_1_(2018)_-_Introduction-1352 只知道他們的 range 都非常大，可以產生很好的圖也可以產生很壞的圖
GAN_Lecture_1_(2018)_-_Introduction-1353 有件事情他們 paper 沒有強調但我覺得是滿有意思的是
GAN_Lecture_1_(2018)_-_Introduction-1354 如果比較 VAE 跟這些 GAN 的話可以發現，VAE 倒是明顯的跟 GAN 有非常大的差別
GAN_Lecture_1_(2018)_-_Introduction-1355 甚麼樣的差別首先 VAE 比較穩
GAN_Lecture_1_(2018)_-_Introduction-1356 發現給他不同的參數，VAE 的分數是非常的集中
GAN_Lecture_1_(2018)_-_Introduction-1357 但是假設我們比較各種不同的 model
GAN_Lecture_1_(2018)_-_Introduction-1358 可以產生的最好的結果的話
GAN_Lecture_1_(2018)_-_Introduction-1359 剛才講 VAE 產生的圖片畢竟是比較糊模的雖然他比較穩，但它比較難做到最好
GAN_Lecture_1_(2018)_-_Introduction-1360 所以比較每一個 model 可以產生的最好的結果的話
GAN_Lecture_1_(2018)_-_Introduction-1361 會發現 VAE 相較於 GAN 還是輸了一截的
GAN_Lecture_1_(2018)_-_Introduction-1362 剩下還有一些投影片但我覺得下次再講好了今天講的是一些直觀的東西
GAN_Lecture_1_(2018)_-_Introduction-1363 下一堂課開始就是要講一些有數學的、比較理論的東西
GAN_Lecture_1_(2018)_-_Introduction-1364 如果可以的話你先去預習一下我把之前上課了錄影放在這邊
GAN_Lecture_1_(2018)_-_Introduction-1365 因為是比較困難的問題所以建議你先預習一下
GAN_Lecture_1_(2018)_-_Introduction-1366 等一下助教也會提到一些新的技術今天這個課沒有 cover 的技術但都是未來會講到的
GAN_Lecture_1_(2018)_-_Introduction-1367 你聽到這些新的技術的話也不用太驚慌
GAN_Lecture_1_(2018)_-_Introduction-1368 作業 3-1 不用到那些新的技術應該也是可以過得了 baseline
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-0 下周會請中研院曹昱博士講一下 GAN 在語音上的應用
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-1 除了 GAN 在語音上的應用以外
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-2 這個投影片就是 GAN 的最後要跟大家講的東西就是怎麼做 Evaluation
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-3 講完這個部分，GAN 的地方就講完今天就提早下課
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-4 Evaluation 是要做甚麼？我們要講的是怎麼 evaluate 用 GAN 產生的 object 的好壞
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-5 比如說在作業 1 2 3 裡面你用 GAN 產生了 image
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-6 怎麼知道你的 image 是好還是不好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-7 我覺得最準的方法就是人來看
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-8 但是在人來看往往不一定是很客觀如果你在看文獻上的話
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-9 很多 paper 只是秀幾張它產生的圖
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-10 然後加一個 comment 說你看到我今天產生的圖
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-11 我覺得這應該是我在文獻上看過最清楚的圖然後就結束了
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-12 你也不知道是真的還是假的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-13 今天要探討的就是有沒有哪一些比較客觀的方法
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-14 來衡量產生出來的 object 到底是好還是不好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-15 在傳統上怎麼衡量一個 generator？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-16 傳統衡量 generator 的方法是算 generator 產生 data 的 likelihood
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-17 也就是說 learn 了一個 generator 以後接下來給這 generator 一些 real data
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-18 假設做 image 生成已經有一個 image 的生成的generator
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-19 接下來拿一堆 image 出來，這些 image 是在 train generator 的時候 generator 沒有看過的 image
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-20 然後去計算 generator 產生這些 image 的機率
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-21 這個東西叫做 likelihood
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-22 這邊有一大堆的 image xi然後算 PG 這個 generator
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-23 產生 xi 這張 image 的機率
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-24 算出 PG( xi )，你可能會取 log
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-25 然後所有這些 real data，這些其實是你的 testing data 的 image 的 likelihood 通通算出來做平均
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-26 就得到一個 likelihood
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-27 這個 likelihood 就代表了 generator 的好壞
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-28 因為假設 generator 它有很高的機率產生這些 real data
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-29 就代表這個 generator 可能是一個比較好的 generator
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-30 但是如果是 GAN 的話，假設你的 generator 是一個 network 用 GAN train 出來的話
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-31 會遇到一個問題就是沒有辦法計算 PG( xi )為甚麼？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-32 因為 train 完一個 generator 以後，它是一個 network
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-33 這個 network 你可以丟一些 vector 進去讓它產生一些 data
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-34 但是你無法算出它產生某一筆特定 data 的機率
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-35 它可以產生東西
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-36 但你說指定你要產生這張圖片的時候
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-37 它根本不可能產生你指定出來的圖片所以根本算不出它產生某一張指定圖片的機率是多少
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-38 所以如果是一個 network 所構成的 generator
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-39 要算它的 likelihood 是有困難
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-40 假設這個 generator 不是 network 所構成的舉例來說這個 generator 就是一個 Gaussian Distribution
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-41 Gaussian Distribution 我想大家都應該知道吧
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-42 或是這個 generator 是一個 Gaussian Mixture ModelGaussian Mixture Model 大家應該知道吧
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-43 如果是一個 Gaussian Mixture Model，給它一個 x
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-44 Gaussian Mixture Model 可以推出它產生這個 x 的機率
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-45 但是因為那是 Gaussian Mixture Model它是個比較簡單的 model
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-46 如果 generator 不是一個簡單的 model是一個複雜的 network
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-47 你求不出它產生某一筆 data 的機率
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-48 但是我們又不希望 generator 就只是 Gaussian Mixture Model
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-49 我們希望我們的 generator 是一個比較複雜的模型
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-50 所以遇到的困難就是如果是一個複雜的模型
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-51 我們就不知道怎麼去計算 likelihood
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-52 不知道怎麼計算這個複雜的模型產生某一筆 data 的機率
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-53 怎麼辦？在文獻上一個 **** solution 叫做Kernel Density Estimation
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-54 也就是把你的 generator 拿出來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-55 讓你的 generator 產生很多很多的 data
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-56 接下來再用一個 Gaussian Distribution 去逼近你產生的 data，甚麼意思？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-57 假設有一個 generator 你讓它產生一大堆的 vector 出來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-58 假設做 Image Generation 的話
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-59 產生出來的 image 就是 high dimensional 的 vector
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-60 你用你的 generator 產生一堆 vector 出來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-61 接下來把這些 vector 當作 Gaussian Mixture Model 的 mean
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-62 然後每一個 mean 它有一個固定的 variance
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-63 然後再把這些 Gaussian 通通都疊在一起
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-64 就得到了一個 Gaussian Mixture Model
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-65 有了這個 Gaussian Mixture Model 以後，你就可以去計算這個 Gaussian Mixture Model
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-66 產生那些 real data 的機率
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-67 就可以估測出這個 generator 它產生出那些 real data 的 likelihood 是多少
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-68 要幾個 Gaussian 對不對
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-69 你的問題是，我們現在要做的事情是我們先讓 generator 先生一大堆的 data
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-70 然後再用 Gaussian 去 fit generator 的 output
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-71 到底要幾個 Gaussian？32 個嗎？64 個嗎？還是一個點一個？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-72 問題是你不知道，所以這就是一個難題
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-73 而且另外一個難題是你不知道 generator 應該要 sample 多少的點
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-74 才估的準它的 distribution
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-75 要 sample 600 個點還是 60,000 個點你不知道
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-76 所以這招在實作上也是有問題的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-77 在文獻上你會看到有人做這招
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-78 結果就會出現一些怪怪的結果怪怪的結果就是
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-79 舉例來說，你可能會發現你的 model 算出來的 likelihood 比 real data 還要大
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-80 你會算出很多奇奇怪怪的結果就對了
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-81 總之這個方法也是怪怪的，因為裡面問題太多了
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-82 你不知道要 sample 幾個點
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-83 然後你不知道要怎麼估測 Gaussian 的 Mixture有太多的問題在裡面了
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-84 還有接下來還有更糟的問題
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-85 我們就算退一步講說你真的想出了一個方法
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-86 可以計算 likelihood
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-87 likelihood 本身也未必代表 generator 的 quality
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-88 為甚麼這麼說？因為有可能第一個 likelihood 確有高的 quality
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-89 舉例來說有一個 generator，它很厲害它產生出來的圖都非常的清晰
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-90 所謂 likelihood 的意思是計算這個 generator產生某張圖片的機率
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-91 也許這個 generator 雖然它產生的圖很清晰
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-92 但它產生出來都是涼宮春日的頭像而已
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-93 如果是其他人物的頭像，它從來不會生成
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-94 但是 testing data 就是其他人物的頭像
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-95 所以如果是用 likelihood 的話
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-96 likelihood 很小，因為它從來不會產生這些圖所以 likelihood 很小
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-97 但是又不能說它做得不好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-98 它其實做得很好，它產生的圖是 high quality 的只是算出來 likelihood 很小
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-99 所以 likelihood 並不代表 quality他們倆者是不等價
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-100 反過來說，高的 likelihood 也並不代表你產生的圖就一定很好，你有一個 model 它 likelihood 很高
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-101 它仍然有可能產生的圖很糟，怎麼說？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-102 這邊舉一個例子
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-103 裡面有一個 generator 1，generator 1 很厲害
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-104 它的 likelihood 很大，假設我們不知道怎麼回事somehow 想了一個方法可以估測 likelihood
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-105 雖然之前我們在前期的投影片已經告訴你估測 likelihood 也是很麻煩，不知道怎麼做
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-106 現在 somehow 想了一個方法可以估測 likelihood
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-107 現在有個很強的 generator
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-108 它的 likelihood 是大 L
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-109 它產生這些圖片的機率很高
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-110 現在有另外一個 generatorgenerator 2 它有 99% 的機率產生 random noise
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-111 它有 1% 的機率，它做的事情跟 generator 1 一樣
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-112 如果我們今天計算 generator 2 的 likelihood
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-113 generator 2 它產生每一張圖片的機率是 generator 1的 1/100
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-114 假設 generator 1 產生某張圖片 xi 的機率是 PG( xi )
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-115 generator 2 產生那張圖片的機率就是 PG( xi ) * 100
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-116 因為 generator 有兩個 mode它有 99% 的機率會整個壞掉
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-117 但它有 1% 的機率會跟 generator 1 一樣
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-118 所以 generator 1 產生某張圖片的機率如果是 PG
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-119 那 generator 產生某張圖片的機率就是 PG / 100
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-120 現在問題來了，假設把這個 likelihood 每一項都除以 100
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-121 你會發現你算出來的值，也差不了多少因為除一百這項把它提出來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-122 就是 - log( 100 )，- log ( 100 ) 才減 4.65 而已
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-123 可以說那個 likelihood，如果看文獻 likelihood 算出來都幾百，差了 4 你可能會覺得沒什麼差別
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-124 但是如果看實際上的 generator 2 跟 generator 1 比的話
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-125 generator 1 你會覺得它應該是比 generator 2 好一百倍的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-126 只是你看不出來而已
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-127 數字上看不出來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-128 所以 likelihood 跟 generator 真正的能力其實也未必是有關係的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-129 今天這個文獻上你常常看到一種 Evaluation 的方法
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-130 常常看到一種客觀的 Evaluation 的方法
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-131 是拿一個已經 train 好的 classifier 來評價現在產生出來的 object
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-132 假設要產生出來的 object 是影像的話
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-133 你就拿一個影像的 classifier 來判斷這個 object 的好壞
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-134 就好像在作業裡面，我們是拿一個人臉的辨識系統
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-135 來看你產生的圖片，這個人臉辨識系統能不能夠辨識的出來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-136 如果可以就代表你產生出來的是還可以的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-137 如果不行就代表你產生出來的真的很弱
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-138 今天這個道理是一樣的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-139 假設你要分辨機器產生出來的一張影像好還是不好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-140 你就拿一個 Image Classifier 出來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-141 這 Image Classifier 通常是已經事先 train 好的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-142 舉例來說它是個 VGG，它是個 Inception Net
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-143 把這個 Image Classifier 丟一張機器產生出來的 image 給他
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-144 它會產生一個 class 的 distribution
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-145 它是個已經 train 好的 Image Classifier
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-146 給他一張圖，它會產生一個 class 的 distribution
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-147 它給每一個 class 一個機率
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-148 如果產生出來的機率越集中
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-149 代表產生出來的圖片的品質越高
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-150 因為這個 classifier 它可以輕易的判斷現在這個圖片它是什麼樣的東西
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-151 它是個狗，還是貓，還是人它可以輕易的判斷出這張圖片是什麼樣的東西
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-152 所以它給某一個 class 機率特別高
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-153 代表產生出來的圖片是這個 model 看得懂的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-154 但這個只是一個衡量的方向而已
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-155 你同時還要衡量另外一件事情
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-156 因為我們知道在 train GAN 會遇到一個問題就是
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-157 mode collapse 的問題
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-158 你的機器可能可以產生某張很清晰的圖
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-159 但他就只能夠產生那張圖而已
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-160 這個不是我們要的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-161 所以在 evaluate GAN 的時候還要從另外一個方向還要從 diverse 的方向去衡量它
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-162 甚麼叫從 diverse 的方向去衡量它呢？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-163 你讓你的機器產生一把
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-164 這邊舉例的時候就產生三張
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-165 把這三張圖通通丟到 CNN 裡面讓它產生三個 distribution
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-166 接下來把這三個 distribution 平均起來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-167 如果平均後的 distribution 很 uniform 的話
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-168 這個 distribution 平均完以後
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-169 它仍然很平均的話
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-170 那就意味著每一種不同的 class 都有被產生到
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-171 代表產生出來的 output 是比較 diverse
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-172 如果平均完發現某一個 class 分數特別高
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-173 就代表它的 output
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-174 你的 model 傾向於產生某個 class 的東西
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-175 就代表它產生出來的 output 不夠 diverse
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-176 所以我們可以從兩個不同的面向
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-177 用某一個 image train 好的，事先 train 好的 Image Classifier 來衡量 image
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-178 可以只給他一張圖
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-179 然後看產生出來的圖清不清楚
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-180 接下來給他一把圖，看看是不是各種不同的 class都有產生到
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-181 有了這些原則以後
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-182 就可以定出一個 Score，現在一個常用的 Score
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-183 叫做 Inception Score，那至於為甚麼叫做 Inception Score，當然是因為他用 Inception Net 去 evaluate
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-184 所以叫做 Inception Score
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-185 我們之前有講怎樣的 generator 叫做好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-186 好的 generator 它產生的單一的圖片
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-187 丟到 Inception Net 裡面
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-188 某一個 class 的分數越大越好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-189 它是非常的 sharp
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-190 把所有的 output 都掉到 classifier 裡面
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-191 產生一堆 distribution，把所有 distribution 做平均
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-192 它是越平滑越好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-193 根據這兩者就定一個 Inception Score把這兩件事考慮進去
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-194 在 Inception Score 裡面第一項要考慮的是summation over 所有產生出來的 x
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-195 每一個 x 丟到 classifier 去算它的 distribution
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-196 然後就計算 Negative entropy
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-197 Negative entropy 就是拿來衡量這個 distribution 夠不夠 sharp
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-198 如果越 sharp 的話，每一張 image 它 output 的 distribution 越 sharp 的話
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-199 就代表產生的圖越好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-200 同時要衡量另外一項
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-201 另外一項就是把所有的 distribution 平均起來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-202 如果平均的結果它的 entropy 越大也代表越好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-203 同時衡量這兩項，把這兩項加起來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-204 就是 Inception Score
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-205 其實還有其他衡量的方法，一個客觀的方法就是拿一個現成的 model 來衡量的你的 generator
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-206 這間介紹 Inception Score 給大家參考
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-207 還有另外一個 train GAN 要注意的問題
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-208 有時候就算 train 出來的結果非常的清晰
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-209 也並不代表你的結果是好的，為甚麼？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-210 因為有可能 generator 只是硬記了 training data 裡面的某幾張 image 而已
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-211 這不是我們要的，因為假設 generator 要硬記 image 的話，那直接從 database sample 一張圖不是更好嗎？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-212 幹嘛還要 train 一個 generator
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-213 所以我們希望 generator 它是有創造力的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-214 它產生出來的東西不要是 database 裡面本來就已經現成的東西
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-215 但是怎麼知道現在 GAN 產生出來的東西
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-216 是不是 database 已經現存的東西呢？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-217 這是另外一個 issue，因為沒有辦法把 database 裡面每張圖片都一個一個去看過
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-218 database 裡面圖片有上萬張根本沒辦法一張一張看過
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-219 所以根本不知道 generator 產生出來的東西是不是 database 裡面的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-220 就 GAN 產生一張圖片的時候
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-221 就把這張圖片拿去跟 database 裡面每張圖片都算L1 或 L2 的相似度
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-222 但光算 L1 或 L2 的相似度是不夠的，為甚麼？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-223 以下是文獻上舉的一個例子
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-224 這個例子是想要告訴大家
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-225 光算相似度，尤其是只算那種 pixel level 的相似度
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-226 是非常不夠的，為甚麼這麼說？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-227 這個例子是這樣
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-228 假設有一隻羊的圖
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-229 這個羊的圖跟誰最像，當然是跟自己最像，跟 database 裡面一模一樣的那張圖最像
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-230 這個圖上面有很多很多的線
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-231 這些線代表甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-232 就把 0 這邊每一個點就代表 database 裡面某一張圖片跟現在羊這張圖片的相似的程度
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-233 黑色這條線代表的是羊這張圖片
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-234 羊這張圖片跟自己的距離當然是 0
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-235 跟其他圖片的距離是比較大的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-236 這邊每一條橫線就代表一張圖片
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-237 把羊那張圖的 pixel 都往左邊移一格還是跟自己最像
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-238 但是如果往左邊移兩格，會發現最像的圖片就變成紅色這張
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-239 移三格就變綠色這張
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-240 移四格就變這個看起來也不知道甚麼，魚還是海豚
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-241 這個羊跟這個圖片最像
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-242 假設 generator 學到怪怪的東西就是把所有的 pixel 都往左移兩格
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-243 這個時候就算它 copy 了 database 你也看不出來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-244 因為檢測的方法檢測不出這個 case
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-245 這邊也是一樣，把卡車的圖片往左移一個 pixel 跟自己最像
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-246 往左移兩個就變跟他最像
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-247 移三個 pixel 就變跟飛機最像
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-248 移四個 pixel 就變跟船最像
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-249 很難算兩張圖片的相似度
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-250 所以 GAN 產生一個圖片的時候，你很難知道他是不是 copy 了 database 裡面的 specific 的某一張圖片
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-251 這個也都是尚待解決的問題
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-252 所以有時候 GAN 產生出來結果很好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-253 也不用太得意，因為它搞不好只是 copy 某一張圖片而已
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-254 這邊是另外一個 issue
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-255 在 train GAN 的時候會有一個問題叫做 Mode Dropping
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-256 假設 GAN 產生出來的是人臉的話
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-257 它產生人臉的多樣性不夠
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-258 怎麼檢測它產生出來的東西他的多樣性夠不夠
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-259 假設 train 了一個 DCGANDCGAN 是甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-260 DCGAN 是 Deep Convolutional GAN 的縮寫
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-261 他的 training 的方法跟 Ian Goodfellow 一開始提出來的方法是一樣的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-262 只是在 DCGAN 裡面那個作者爆搜了各種不同的參數
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-263 然後告訴你怎麼樣 train GAN 的時候結果會比較好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-264 有不同 network 的架構不同的 Activation Function
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-265 有沒有加 batch，各種方法都爆搜一遍然後告訴你怎麼樣做比較好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-266 DCGAN 就是某一種 GANDeep Convolutional GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-267 怎麼知道 DCGAN，train 一個產生人臉的 DCGAN它產生的人臉的多樣性是夠的呢？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-268 一個檢測方法是從 DCGAN 裡面 sample 一堆 image
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-269 叫 DCGAN 產生一堆 image
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-270 然後確認產生出來的 image 裡面有沒有非常像的有沒有人會覺得是同一個人
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-271 怎麼知道是不是同一個人
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-272 這個結果是來自於 ICLR 2018 的一篇 paper
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-273 他叫 "Do GANs learn the distribution?"它裡面的作法是讓機器產生一堆的圖片
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-274 接下來先用 classifier 決定有沒有兩張圖片看起來很像
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-275 再把長的很像的圖片拿給人看
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-276 問人說：你覺得這兩個是不是同一個人
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-277 如果是，就代表 DCGAN 產生重複的圖了
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-278 雖然產生圖片每張都略有不同
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-279 但人可以看出這個看起來算不算是同一個人
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-280 這邊列出一些被人判斷是感覺是同一個人的圖片
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-281 DCGAN 會產生很像的圖片
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-282 右邊這個虛線裡面是把這個圖片拿去 database裡面找一張最像的圖
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-283 會發像最像的圖跟這個圖沒有完全一樣
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-284 代表 DCGAN 沒有真的硬背了 training data 裡面的圖
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-285 但是不知道為甚麼他會產生很像的圖
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-286 他會產生很像的圖，但這個圖並不是從 database 裡面背出來的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-287 他要衡量 DCGAN 到底可以產生多少不一樣的 image
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-288 他發現如果 sample 四百張 image 的時候
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-289 有大於 50% 的機率，可以從四百張 image 裡面找到兩張人覺得是一樣的人臉
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-290 藉由這個機率就可以反推到底整個 database 裡面整個 DCGAN 可以產生的人臉裡面
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-291 有多少不同的人臉
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-292 詳細反推的細節，你再 check 一下 paper
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-293 這個推法回去自己想一下，這個國中數學而已
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-294 有一個 database 有面有 M 張 image
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-295 M 到底應該多大才會讓你 sample 四百張 image 的時候有大於 50% 的機率 sample 到重複的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-296 這個問題就這樣，就反推這個 database 裡面到底有幾張 image
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-297 反推出 DCGAN 他可以產生各種不同的 image其實只有 0.16 個 million 而已
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-298 只有十六萬張圖而已，覺得其實太少
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-299 有另外一個作法叫做 ALI，他就算出來 ALI 比較強
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-300 反推出來可以產生一百萬張各種不同的人臉
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-301 ALI 看起來可以產生的人臉多樣性是比較多的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-302 但是不論是哪些方法都覺得他們產生的人臉的多樣性
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-303 跟真實的人臉比起來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-304 還是有一定程度的差距
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-305 感覺 GAN 沒有辦法真的產生人臉的 distribution這些都是尚待研究的問題
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-306 我們知道 GAN 他的一個 issue 就是它產生出來的distribution 不夠大
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-307 它產生出來的 distribution 太 narrow
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-308 有一些 solution，比如說有一個方法，現在比較少人用
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-309 因為它 implement 起來很複雜，運算量很大，叫做 Unroll GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-310 我們沒有打算講他，沒有打算講他為甚麼要放在這邊等一下你就知道
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-311 有另外一個方法叫做 Mini-batch Discrimination
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-312 他的方法是這個樣子，我們簡單講一下就好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-313 一般在 train discriminator 的時候discriminator 只看一張 image
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-314 決定他是好的還是不好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-315 Mini-batch Discriminator 是讓 discriminator 看一把 image
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-316 決定他是好的還是不好
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-317 看一把 image 跟看一張 image 有甚麼不同
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-318 看一把 image 的時候不只要 check 每一張 image 是不是好的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-319 還要 check 這些 image 他們看起來像不像
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-320 discriminator 會從 database 裡面 sample 一把 image 出來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-321 會讓 generator sample 一把 image 出來
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-322 如果 generator 每次 sample 都是一樣的 image
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-323 發生 **** 的情形，discriminator 就會抓到這件事
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-324 因為在 training data 裡面每張圖都差很多
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-325 如果 generator 產生出來的圖都很像
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-326 discriminator 因為它不是只看一張圖
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-327 它是看一把圖
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-328 他就會抓到這把圖看起來不像是 realistic
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-329 大家可以了解嗎改了一下 discriminator 讓他看一把圖
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-330 還有另外一個也是看一把圖的方法
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-331 叫做 OTGANOptimal Transport GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-332 他的圖是彩色的畫得不錯就放在這裡
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-333 GAN 的東西就講到這裡
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-334 下課之前會 GAN 的部份下一個結論
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-335 這個是 GAN 的 from A to Z，from A to Z 是甚麼意思google 一下這個英文片語
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-336 A to Z 的意思就是從頭到尾
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-337 但是這邊的 A to Z 就是字面上的意思
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-338 就是 A to Z
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-339 世界上有各式各樣的 GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-340 當你發明一種新的 GAN 的時候
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-341 你就可以在前面加一個英文字母
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-342 你可以跟朋友玩一個遊戲就是誰可以說出最多的 GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-343 我今天早上查了一下 GAN 的 zoo
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-344 現在其實已經有超過在課堂開始的時候有三百種不同的 GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-345 剛才查了一下已經有 360 種不同的 GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-346 數目是不斷的在增加的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-347 今天就來複習一下在這門課裡面
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-348 提過的 GAN，然後從 A 開始講起
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-349 A 開頭的 GAN 有甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-350 你想的到 A 開頭的 GAN 嗎
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-351 這是一個你可以回去跟朋友玩的遊戲雖然我覺得沒有人要跟你這個遊戲就是了
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-352 A 的 GAN 有甚麼？有 ACGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-353 助教在講 3-2 的時候 Conditional Generation 有講到 ACGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-354 B 有甚麼？
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-355 BiGAN，其實也有 BGAN，BGAN 還有兩個
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-356 現在所有的名字都已經混在一起了
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-357 你去看 GAN 的 zoo，光 SGAN 就有四個
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-358 SGAN 已經搞不清楚他到底是甚麼東西了
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-359 現在非常的混亂
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-360 C 有甚麼？C 有 CycleGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-361 Conditional GAN，我忘了放 Conditional GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-362 D 有甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-363 DCGAN，還有 DuelGANDuelGAN 其實跟 CycleGAN 是一模一樣的東西
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-364 還有 dragon，你看一下他的名字還頗牽強可能是想要先湊梗才想方法
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-365 他的 title 是 How To Train Your Dragon他後來真的投到 ICLR 的時候，他又把他的 title 改掉了
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-366 E 有甚麼？E 有 EBGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-367 EBGAN 還有一個變形沒有講到，叫做 BEGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-368 就把 EB 反過來就變 BEGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-369 F 有甚麼？F 就是 fGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-370 G 有甚麼？就是 GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-371 H 有甚麼？HGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-372 我覺得 H 應該還滿好湊的不知道為甚麼沒有 H
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-373 等著你 propose
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-374 等著你湊梗
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-375 我覺得 H 應該滿好湊的，因為 **** 有 Hierarchical GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-376 他縮寫就是 HGAN，就等著你來 propose 而已
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-377 I 有甚麼？InfoGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-378 J 有甚麼？J 還真的沒有
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-379 K 有甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-380 K 還真的沒有
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-381 你查 GAN 的 zoo 有，有一些太 specific 所以就沒有講
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-382 有 KGAN、KEGAN，當然太 specific 就沒有講
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-383 L 有甚麼？有兩個 LSGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-384 M 有甚麼？你想得出來嗎
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-385 有 MMGAN，MMGAN 就是一開始有講在講 GAN 的 theory 我們說
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-386 有一個叫做 Min Max GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-387 還有一個 N 開頭的就是 Non Saturation GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-388 Ian Goodfellow 有說，大家在 paper 都會說 original GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-389 我們跟 original GAN 比
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-390 不好好講那個 original GAN 到底指的是 MMGAN 還是 NSGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-391 所以他特別幫他們取了名字告訴我們以後最早的 GAN 叫做 MMGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-392 實際上一開始 MMGAN Ian Goodfellow train 不起來所以他用的是 NSGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-393 O 有甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-394 O 就是 OTGAN，他為了硬湊梗才放了 OTGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-395 P 有甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-396 是不是沒講到 Progressive GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-397 沒講到 Progressive GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-398 所以現在講一下 Progressive GAN 是甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-399 我們不是講過 **** GAN，可以小張的 image 產生大張的 image
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-400 Progressive GAN 是 NVIDIA 做的，他的概念完全一樣
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-401 就從小張的 image 一直生、一直生，生到大張的他從 4x4 生到 8x8 到 16x16
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-402 最後生到 1024x1024
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-403 超大張的 image，連毛細孔都看的到的那種
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-404 非常的 realistic
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-405 Q 有甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-406 Q 還真的沒有，我覺得 Q 還滿好湊的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-407 你把用在 Reinforcement Learning 上就可以產生 Q-learning base GAN 之類的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-408 等著你 propose
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-409 R 有甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-410 R 有 Rank GAN 就是在講 Sequence Generation意思是提了一個 Rank GAN 就只是想要湊梗而已
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-411 S 有甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-412 S 有 SeqGAN，他有很多，有 StackGAN有 StarGAN 還有 SeqGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-413 StarGAN 就是在講 Style Transfer 的時候有講到 StarGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-414 T 有甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-415 在講 BiGAN 的時候，我順便提還有一個 Triple GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-416 這也是為了硬湊梗而已
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-417 U 有甚麼
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-418 Unroll GAN，為了硬湊梗，應放了一個 Unroll GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-419 V 有甚麼？V 有 VAEGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-420 W 大家都知道我們有 WGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-421 X 有出現在某個地方
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-422 有一個 XGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-423 在講 Style Transfer 的時候那兩個 encoder 兩個 decoder 那個東西，他就是 XGAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-424 Y 還真的沒有
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-425 Z 其實也沒有
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-426 我覺得 Y 應該還滿容易湊梗的吧
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-427 Y 的詞彙很少，所以很難 propose 一個跟 Y 有關的 GAN
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-428 但是可以從形狀來想
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-429 XGAN 都是從形狀來想的所以 Y 一定可以從形狀來想的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-430 這是一個 encoder 然後有兩個 decoder
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-431 就是一個 Y 甚麼之類的
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-432 怎麼會沒有我覺得這應該很好湊
GAN_Lecture_10_(2018)_-_Evaluation_&_Concluding_Remarks-433 今天要講的就是這個樣子，GAN 講得差不多了就下課
GAN_Lecture_2_(2018)_-_Conditional_Generation-0 我們要 train 一個
GAN_Lecture_2_(2018)_-_Conditional_Generation-1 輸入文字
GAN_Lecture_2_(2018)_-_Conditional_Generation-2 然後要產生對應的二次元人物的頭像
GAN_Lecture_2_(2018)_-_Conditional_Generation-3 那這個技術呢，你就要用到 conditional GAN
GAN_Lecture_2_(2018)_-_Conditional_Generation-4 所謂 conditional GAN 的意思就是說
GAN_Lecture_2_(2018)_-_Conditional_Generation-5 我們之前的 GAN，是隨機輸入一個
GAN_Lecture_2_(2018)_-_Conditional_Generation-6 vector，然後產生一張給你
GAN_Lecture_2_(2018)_-_Conditional_Generation-7 那你其實根本不能控制說你要 output 什麼樣的東西
GAN_Lecture_2_(2018)_-_Conditional_Generation-8 那我們這邊要講 conditional GAN 的意思是說
GAN_Lecture_2_(2018)_-_Conditional_Generation-9 你可以輸入一個，比如說是文字
GAN_Lecture_2_(2018)_-_Conditional_Generation-10 然後產生對應那個文字的圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-11 也就是你可以操控你要輸出的結果
GAN_Lecture_2_(2018)_-_Conditional_Generation-12 好那我們今天要舉的例子呢
GAN_Lecture_2_(2018)_-_Conditional_Generation-13 在作業2裡面要做的東西呢
GAN_Lecture_2_(2018)_-_Conditional_Generation-14 就是 Text-to-Image
GAN_Lecture_2_(2018)_-_Conditional_Generation-15 那這個東西就是輸入文字產生對應的圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-16 當然要做到這件事情
GAN_Lecture_2_(2018)_-_Conditional_Generation-17 要輸入文字產生對的圖片，它其實可以被當作是一個
GAN_Lecture_2_(2018)_-_Conditional_Generation-18 單純的 supervised learning problem 來看
GAN_Lecture_2_(2018)_-_Conditional_Generation-19 那怎麼用一個supervised learning 的方式來 learn 一個
GAN_Lecture_2_(2018)_-_Conditional_Generation-20 Text-to-Image 的 model 呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-21 你需要的就是一大堆的圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-22 每一張圖片都需要對應的文字描述
GAN_Lecture_2_(2018)_-_Conditional_Generation-23 然後套用一個傳統的supervised learning的方法
GAN_Lecture_2_(2018)_-_Conditional_Generation-24 你可以說我就 learn 一個network，它的輸入就是一段文字
GAN_Lecture_2_(2018)_-_Conditional_Generation-25 輸出就是對應的圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-26 那你希望輸出的圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-27 跟你的目標，越接近越好
GAN_Lecture_2_(2018)_-_Conditional_Generation-28 比如說你會 minimize 你的 network output 的
GAN_Lecture_2_(2018)_-_Conditional_Generation-29 你的 network output
GAN_Lecture_2_(2018)_-_Conditional_Generation-30 跟這個文字所對應的 image 的 L1 或 L2 的 loss
GAN_Lecture_2_(2018)_-_Conditional_Generation-31 但是光是這麼做，會有什麼樣的問題呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-32 你可以想像說，在你的 training data 裡面
GAN_Lecture_2_(2018)_-_Conditional_Generation-33 火車它的對應圖片其實是有很多張的
GAN_Lecture_2_(2018)_-_Conditional_Generation-34 這些是正面的火車，它們都是火車
GAN_Lecture_2_(2018)_-_Conditional_Generation-35 這些是側面的火車，它們也通通都是火車
GAN_Lecture_2_(2018)_-_Conditional_Generation-36 哪你現在如果要讓你 network 的 output
GAN_Lecture_2_(2018)_-_Conditional_Generation-37 如果你用傳統方法 learn 一個 network 的話
GAN_Lecture_2_(2018)_-_Conditional_Generation-38 它會覺得說輸入 train 的時候，輸入火車的時候
GAN_Lecture_2_(2018)_-_Conditional_Generation-39 它要長得像這 3 張，同時也要長得像這 3 張
GAN_Lecture_2_(2018)_-_Conditional_Generation-40 所以到時候你 network 的 output 就會變成是
GAN_Lecture_2_(2018)_-_Conditional_Generation-41 這些一大堆 image 的平均
GAN_Lecture_2_(2018)_-_Conditional_Generation-42 如果你產生這種正面的火車是好的結果
GAN_Lecture_2_(2018)_-_Conditional_Generation-43 你產生這個側面的火車是好的結果
GAN_Lecture_2_(2018)_-_Conditional_Generation-44 但是同時產生正面的火車跟側面的火車合起來
GAN_Lecture_2_(2018)_-_Conditional_Generation-45 這是錯誤的結果
GAN_Lecture_2_(2018)_-_Conditional_Generation-46 今天假設你用 traditional 的方法
GAN_Lecture_2_(2018)_-_Conditional_Generation-47 來 learn 一個 conditional 的 generation
GAN_Lecture_2_(2018)_-_Conditional_Generation-48 你用一般的方法來 learn 一個 text to image 的 generator
GAN_Lecture_2_(2018)_-_Conditional_Generation-49 你會發現說你產生出來的 image 都是特別模糊的
GAN_Lecture_2_(2018)_-_Conditional_Generation-50 為什麼？
GAN_Lecture_2_(2018)_-_Conditional_Generation-51 因為你產生出來的 image 會是多張 image 的平均
GAN_Lecture_2_(2018)_-_Conditional_Generation-52 你的 model 在學習的時候
GAN_Lecture_2_(2018)_-_Conditional_Generation-53 它想要產生的結果，是多張 image 的平均
GAN_Lecture_2_(2018)_-_Conditional_Generation-54 所以這邊就需要用到 GAN 的技術
GAN_Lecture_2_(2018)_-_Conditional_Generation-55 那我們說在原來的 GAN 裡面
GAN_Lecture_2_(2018)_-_Conditional_Generation-56 你的 generator 就是吃一個從 normal distribution sample 出來的 z
GAN_Lecture_2_(2018)_-_Conditional_Generation-57 根據這個 z ，產生一張 image x
GAN_Lecture_2_(2018)_-_Conditional_Generation-58 那在 conditional generation 裡面呢
GAN_Lecture_2_(2018)_-_Conditional_Generation-59 你的 generator 不是只有吃 z，同時也吃了另外一個東西
GAN_Lecture_2_(2018)_-_Conditional_Generation-60 吃了另外一個 conditional 的 text c
GAN_Lecture_2_(2018)_-_Conditional_Generation-61 所以你的 conditional GAN 呢
GAN_Lecture_2_(2018)_-_Conditional_Generation-62 它就是同時吃一個 normal distribution 的 sample 出來的 vector
GAN_Lecture_2_(2018)_-_Conditional_Generation-63 跟一段文字，然後根據這段文字
GAN_Lecture_2_(2018)_-_Conditional_Generation-64 還有這個 sample 出來的 vector
GAN_Lecture_2_(2018)_-_Conditional_Generation-65 它產生 generated 的結果
GAN_Lecture_2_(2018)_-_Conditional_Generation-66 那接下來呢，你要 train 一個 discriminator
GAN_Lecture_2_(2018)_-_Conditional_Generation-67 在我們原來的 GAN 裡面呢，discriminator 只吃一張 image x
GAN_Lecture_2_(2018)_-_Conditional_Generation-68 然後它告訴你說這個 x，它的 quality 好不好
GAN_Lecture_2_(2018)_-_Conditional_Generation-69 這個 discriminator 它吃一個 x，它 output 一個 scalar
GAN_Lecture_2_(2018)_-_Conditional_Generation-70 這個 scalar 代表說呢，input 的 x
GAN_Lecture_2_(2018)_-_Conditional_Generation-71 它到底有多好，或者是多不好
GAN_Lecture_2_(2018)_-_Conditional_Generation-72 那你在 train 這個 discriminator 的時候，你就會告訴這個 discriminator 說
GAN_Lecture_2_(2018)_-_Conditional_Generation-73 如果是 real image，就給它 1 分
GAN_Lecture_2_(2018)_-_Conditional_Generation-74 如果是 generated image，就給它 0 分
GAN_Lecture_2_(2018)_-_Conditional_Generation-75 但光是這麼做，是不夠的
GAN_Lecture_2_(2018)_-_Conditional_Generation-76 假設你是用我們之前看到的那種方式來 train discriminator 的話
GAN_Lecture_2_(2018)_-_Conditional_Generation-77 會發生什麼問題呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-78 你會發現說，今天 generator 在產生 image 的時候
GAN_Lecture_2_(2018)_-_Conditional_Generation-79 它會完全無視 input 的 condition
GAN_Lecture_2_(2018)_-_Conditional_Generation-80 因為 discriminator 它只檢查說你現在 input 的 image 是不是一張 high quality 的 image
GAN_Lecture_2_(2018)_-_Conditional_Generation-81 所以今天對 generator 來說，它要騙過 discriminator
GAN_Lecture_2_(2018)_-_Conditional_Generation-82 它只要產生high quality 的 image 就好了，它只要產生清晰的圖就好了
GAN_Lecture_2_(2018)_-_Conditional_Generation-83 它可完全無視你 input condition
GAN_Lecture_2_(2018)_-_Conditional_Generation-84 不管你輸入是貓，狗，火車，它可能都輸出貓
GAN_Lecture_2_(2018)_-_Conditional_Generation-85 反正只要這個 discriminator 覺得它產生出來的貓是一支高品質的貓
GAN_Lecture_2_(2018)_-_Conditional_Generation-86 看起來很像是真的貓，就結束了
GAN_Lecture_2_(2018)_-_Conditional_Generation-87 這個顯然不是我們要的
GAN_Lecture_2_(2018)_-_Conditional_Generation-88 我們希望機器是按照我們輸入的 condition
GAN_Lecture_2_(2018)_-_Conditional_Generation-89 產生不同的圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-90 所以今天你要注意，當你在做 conditional GAN 的時候
GAN_Lecture_2_(2018)_-_Conditional_Generation-91 你的 discriminator 不可以只看 generator 的輸出
GAN_Lecture_2_(2018)_-_Conditional_Generation-92 它要同時看 generator 的輸入跟輸出
GAN_Lecture_2_(2018)_-_Conditional_Generation-93 這時候你的 discriminator 有兩個任務
GAN_Lecture_2_(2018)_-_Conditional_Generation-94 我們說discriminator 它吃一個 condition 跟吃一個 object
GAN_Lecture_2_(2018)_-_Conditional_Generation-95 然後產生一個 scalar
GAN_Lecture_2_(2018)_-_Conditional_Generation-96 這個 scalar 它對應到兩件事情
GAN_Lecture_2_(2018)_-_Conditional_Generation-97 第一件事情是 x 是不是真實的？
GAN_Lecture_2_(2018)_-_Conditional_Generation-98 第二件事情是，你的 x 跟它的 condition c
GAN_Lecture_2_(2018)_-_Conditional_Generation-99 他們合起來，是不是應該湊成一對
GAN_Lecture_2_(2018)_-_Conditional_Generation-100 也就是 x 是真的還不是真的，x 跟 c 是不是應該湊成一對
GAN_Lecture_2_(2018)_-_Conditional_Generation-101 這兩個東西加起來就是 discriminator 的 output
GAN_Lecture_2_(2018)_-_Conditional_Generation-102 所以今天在 train 這種 discriminator 的時候
GAN_Lecture_2_(2018)_-_Conditional_Generation-103 這個就是我們在作業 3-2 要做的事情啦
GAN_Lecture_2_(2018)_-_Conditional_Generation-104 我們今天在 train 這種 discriminator 的時候
GAN_Lecture_2_(2018)_-_Conditional_Generation-105 對這種 discriminator 來說
GAN_Lecture_2_(2018)_-_Conditional_Generation-106 就是現在 discriminator 它不是只吃 image，它是吃一個 pair
GAN_Lecture_2_(2018)_-_Conditional_Generation-107 它是吃一個 image 跟一段文字
GAN_Lecture_2_(2018)_-_Conditional_Generation-108 什麼樣的文字跟 image 的組合要給它高分呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-109 當然是一段文字，它對應的 image，他們合起來
GAN_Lecture_2_(2018)_-_Conditional_Generation-110 要給它 1 分
GAN_Lecture_2_(2018)_-_Conditional_Generation-111 但是什麼樣的狀況，應該給它 0 分呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-112 如果今天是正確的文字，一段文字
GAN_Lecture_2_(2018)_-_Conditional_Generation-113 跟 generator 的輸出，正確的文字
GAN_Lecture_2_(2018)_-_Conditional_Generation-114 跟 generated 的 image，當然要給它低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-115 但是還有另外一個 case 應該要給它低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-116 因為這個 discriminator 它不是只要看你產生出來的 image 好不好
GAN_Lecture_2_(2018)_-_Conditional_Generation-117 它還要看你產生出來的 image 跟它 input 的 text
GAN_Lecture_2_(2018)_-_Conditional_Generation-118 他們有沒有被 match 在一起
GAN_Lecture_2_(2018)_-_Conditional_Generation-119 所以今天不是只有一個 case 給它第低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-120 還有另外一個 case 要給它低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-121 什麼樣的 case 應該要給它低分呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-122 你拿一張真實的 image
GAN_Lecture_2_(2018)_-_Conditional_Generation-123 但是隨便給它配個，隨便給它加上
GAN_Lecture_2_(2018)_-_Conditional_Generation-124 隨機的文字
GAN_Lecture_2_(2018)_-_Conditional_Generation-125 比如說這個是火車，你就說它一隻貓
GAN_Lecture_2_(2018)_-_Conditional_Generation-126 這個 case 要給 discriminator 低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-127 你要告訴 discriminator 說，就算是產生好的圖
GAN_Lecture_2_(2018)_-_Conditional_Generation-128 但是給它一個隨機的文字
GAN_Lecture_2_(2018)_-_Conditional_Generation-129 他們沒有辦法 match 在一起，他們是不匹配的
GAN_Lecture_2_(2018)_-_Conditional_Generation-130 這個時候，也應該要給它低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-131 所以跟一般的 generator 不一樣，跟一般的 GAN 不一樣
GAN_Lecture_2_(2018)_-_Conditional_Generation-132 當你做 conditional GAN 的時候，你的 discriminator 它是吃一個 pair 當作 input
GAN_Lecture_2_(2018)_-_Conditional_Generation-133 那你在 train 這個 discriminator 的時候
GAN_Lecture_2_(2018)_-_Conditional_Generation-134 它有兩種 negative 的 examples，有兩種 cases 是應該給它低分的
GAN_Lecture_2_(2018)_-_Conditional_Generation-135 一個 case 是，輸入一段文字給 generator
GAN_Lecture_2_(2018)_-_Conditional_Generation-136 generator 產生一張模糊的圖，所以就要給低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-137 給一張清晰的圖，但是隨便給它加上一個隨機的文字
GAN_Lecture_2_(2018)_-_Conditional_Generation-138 這個應該也要給它低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-139 如果你比較喜歡看演算法的話，這邊我就把演算法列一下
GAN_Lecture_2_(2018)_-_Conditional_Generation-140 你就照著這個 call 這樣，知不知道
GAN_Lecture_2_(2018)_-_Conditional_Generation-141 就是照著這個 call，怎麼 call 呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-142 我們就一行一行講給你聽，這怎麼 call
GAN_Lecture_2_(2018)_-_Conditional_Generation-143 我們從 database 裡面 sample 出 m 個 example
GAN_Lecture_2_(2018)_-_Conditional_Generation-144 那注意一下，因為今天是 conditional 的 generation
GAN_Lecture_2_(2018)_-_Conditional_Generation-145 所以你的每一個 example 都是
GAN_Lecture_2_(2018)_-_Conditional_Generation-146 假設是 text to image，都是 image 跟文字的 pair
GAN_Lecture_2_(2018)_-_Conditional_Generation-147 文字是 c，image 是 x
GAN_Lecture_2_(2018)_-_Conditional_Generation-148 所以你的每一個 sample data 都是文字跟 image 的 pair...
GAN_Lecture_2_(2018)_-_Conditional_Generation-149 這個是要給高分的
GAN_Lecture_2_(2018)_-_Conditional_Generation-150 是 positive examples
GAN_Lecture_2_(2018)_-_Conditional_Generation-151 對 discriminator 來說，要給高分的
GAN_Lecture_2_(2018)_-_Conditional_Generation-152 前半段是在訓練 discriminator
GAN_Lecture_2_(2018)_-_Conditional_Generation-153 那你接下來 sample 出 m 個 vector
GAN_Lecture_2_(2018)_-_Conditional_Generation-154 然後把這 m 個 vectors
GAN_Lecture_2_(2018)_-_Conditional_Generation-155 每一個都去加上一個 condition
GAN_Lecture_2_(2018)_-_Conditional_Generation-156 你這邊有 m 個 condition c1 到 cm 對不對？
GAN_Lecture_2_(2018)_-_Conditional_Generation-157 把這 m 個 vector，每一個都去加上 condition
GAN_Lecture_2_(2018)_-_Conditional_Generation-158 產生 x tilde，產生 m 張generated 的 image
GAN_Lecture_2_(2018)_-_Conditional_Generation-159 接下來，你再去你的 database 裡面
GAN_Lecture_2_(2018)_-_Conditional_Generation-160 sample 出 m 個 objects
GAN_Lecture_2_(2018)_-_Conditional_Generation-161 我們這邊寫作 x1 hat 到 xm hat
GAN_Lecture_2_(2018)_-_Conditional_Generation-162 這 m 張 image，這個 x hat 它也是好的 image
GAN_Lecture_2_(2018)_-_Conditional_Generation-163 可是 x tilde 是 generated 的 image
GAN_Lecture_2_(2018)_-_Conditional_Generation-164 這個 x hat
GAN_Lecture_2_(2018)_-_Conditional_Generation-165 它也是 real 的 image
GAN_Lecture_2_(2018)_-_Conditional_Generation-166 也是 database sample 出來的 image
GAN_Lecture_2_(2018)_-_Conditional_Generation-167 接下來在 train 你的 discriminator 的時候
GAN_Lecture_2_(2018)_-_Conditional_Generation-168 如果是正確的 c 跟 x 的 pair，就給它高分
GAN_Lecture_2_(2018)_-_Conditional_Generation-169 如果今天是 c 配上 x tilde
GAN_Lecture_2_(2018)_-_Conditional_Generation-170 一段文字敘述，配上 generated 出來的模糊的結果
GAN_Lecture_2_(2018)_-_Conditional_Generation-171 那應該要給它低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-172 如果今天是一段文字敘述
GAN_Lecture_2_(2018)_-_Conditional_Generation-173 配上 x hat
GAN_Lecture_2_(2018)_-_Conditional_Generation-174 從 training date 裡面 sample 出來的 real image
GAN_Lecture_2_(2018)_-_Conditional_Generation-175 從 training data 裡面 sample 出來的清楚的 image
GAN_Lecture_2_(2018)_-_Conditional_Generation-176 但是跟這個 c 它是沒有辦法配在一起的，這個也要給它低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-177 所以今天是這個 case 給它高分
GAN_Lecture_2_(2018)_-_Conditional_Generation-178 這兩個 case 也要給它低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-179 那這個實作跟之前講的 GAN，其實沒有太大差別
GAN_Lecture_2_(2018)_-_Conditional_Generation-180 唯一差別地方就是，你要多加這一項就是了
GAN_Lecture_2_(2018)_-_Conditional_Generation-181 之前只有一種 negative example
GAN_Lecture_2_(2018)_-_Conditional_Generation-182 現在變成有兩種 negative example
GAN_Lecture_2_(2018)_-_Conditional_Generation-183 那接下來 train generator
GAN_Lecture_2_(2018)_-_Conditional_Generation-184 train generator 怎麼做，sample 出 m 個 vectors
GAN_Lecture_2_(2018)_-_Conditional_Generation-185 然後再 sample 出 m 個 conditions
GAN_Lecture_2_(2018)_-_Conditional_Generation-186 然後把每一個 vector z，跟每一個 condition 加起來
GAN_Lecture_2_(2018)_-_Conditional_Generation-187 一起丟到 generator 裡面
GAN_Lecture_2_(2018)_-_Conditional_Generation-188 再通過 discriminator，那你希望這個分數，越大越好
GAN_Lecture_2_(2018)_-_Conditional_Generation-189 這個就是大家作業 3-2 要實做的東西
GAN_Lecture_2_(2018)_-_Conditional_Generation-190 那在 discriminator 的 network 的架構上
GAN_Lecture_2_(2018)_-_Conditional_Generation-191 應該要怎麼設計呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-192 我發現我最常看到的設計是這樣
GAN_Lecture_2_(2018)_-_Conditional_Generation-193 input 一個 object，就一張 image
GAN_Lecture_2_(2018)_-_Conditional_Generation-194 然後通過一個 network 把它變成一個 embedding
GAN_Lecture_2_(2018)_-_Conditional_Generation-195 然後你的 condition 現在是一串文字
GAN_Lecture_2_(2018)_-_Conditional_Generation-196 文字你可能也通過一個 network，把它也變成 embedding
GAN_Lecture_2_(2018)_-_Conditional_Generation-197 那你把這兩種 embedding 組合起來丟到 network 裡面
GAN_Lecture_2_(2018)_-_Conditional_Generation-198 然後最後 network output 一個 scalar
GAN_Lecture_2_(2018)_-_Conditional_Generation-199 這個 scalar 代表了兩件事，一件事是 input 的 x 有多好
GAN_Lecture_2_(2018)_-_Conditional_Generation-200 同時又代表了 x 跟 c 合起來，它們湊成一個 pair 有多合適
GAN_Lecture_2_(2018)_-_Conditional_Generation-201 但是我發現有另外一種 discriminator 的架構
GAN_Lecture_2_(2018)_-_Conditional_Generation-202 而這種 discriminator 的架構，在文獻上看起來它的 performance 是不錯的
GAN_Lecture_2_(2018)_-_Conditional_Generation-203 然後我發現有 3 篇 paper 都是做這樣的事情
GAN_Lecture_2_(2018)_-_Conditional_Generation-204 那我個人認為這個架構，其實好像是比較有道理的
GAN_Lecture_2_(2018)_-_Conditional_Generation-205 這個架構是這樣，這個架構是說
GAN_Lecture_2_(2018)_-_Conditional_Generation-206 我們有一個 object 進來
GAN_Lecture_2_(2018)_-_Conditional_Generation-207 object 先通過一個 network
GAN_Lecture_2_(2018)_-_Conditional_Generation-208 然後就 output 一個分數
GAN_Lecture_2_(2018)_-_Conditional_Generation-209 接下來這個綠色的 network
GAN_Lecture_2_(2018)_-_Conditional_Generation-210 它也吐出一個 embedding
GAN_Lecture_2_(2018)_-_Conditional_Generation-211 這個 invalid 也跟你的 condition 結合起來，丟到另外一個藍色 network 裡面
GAN_Lecture_2_(2018)_-_Conditional_Generation-212 藍色 network 也吐出一個分數
GAN_Lecture_2_(2018)_-_Conditional_Generation-213 所以現在你吐出兩個分數，一個是綠色的 network 吐出來的分數
GAN_Lecture_2_(2018)_-_Conditional_Generation-214 綠色的 network 只看你的 object，它不管這個 condition
GAN_Lecture_2_(2018)_-_Conditional_Generation-215 它只看你的 object
GAN_Lecture_2_(2018)_-_Conditional_Generation-216 然後它去看這個 object 決定說
GAN_Lecture_2_(2018)_-_Conditional_Generation-217 現在 output 的結果是 realistic 還是不是 realistic
GAN_Lecture_2_(2018)_-_Conditional_Generation-218 那這個藍色的 network 呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-219 藍色的 network 它同時看了 x，也看了 c
GAN_Lecture_2_(2018)_-_Conditional_Generation-220 兩個湊起來以後
GAN_Lecture_2_(2018)_-_Conditional_Generation-221 它會告訴你說，它應該是被 match 在一起的
GAN_Lecture_2_(2018)_-_Conditional_Generation-222 還是不應該被 match 在一起的
GAN_Lecture_2_(2018)_-_Conditional_Generation-223 所以今天這個藍色的 network 它同時看了 x，同時看了 c
GAN_Lecture_2_(2018)_-_Conditional_Generation-224 它決定說這兩個東西它到底應該是 match 的，還是不是 match 的
GAN_Lecture_2_(2018)_-_Conditional_Generation-225 但我覺得把這兩種 evaluation 的結果把它拆開
GAN_Lecture_2_(2018)_-_Conditional_Generation-226 其實可能是比較合理的
GAN_Lecture_2_(2018)_-_Conditional_Generation-227 因為今天在上面這個 case
GAN_Lecture_2_(2018)_-_Conditional_Generation-228 如果你給 network 一筆 data
GAN_Lecture_2_(2018)_-_Conditional_Generation-229 一組 data，告訴它說這是一個壞的 example，你應該給它低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-230 但是你沒有告訴它說為什麼是低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-231 我們都講說我們有兩種 native example
GAN_Lecture_2_(2018)_-_Conditional_Generation-232 一種 native example 是其實你的 image 跟文字還是 match 的
GAN_Lecture_2_(2018)_-_Conditional_Generation-233 只是 image 的 quality 不好，因為它是 generator 產生的
GAN_Lecture_2_(2018)_-_Conditional_Generation-234 另外一個 case 是，你的 image 的 quality 是好的，但是它是不 match 的
GAN_Lecture_2_(2018)_-_Conditional_Generation-235 這兩種 case 都要給它低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-236 但是對這個 network 來說，它就會 confused 這樣
GAN_Lecture_2_(2018)_-_Conditional_Generation-237 它就不知道為什麼這個東西是給它低分
GAN_Lecture_2_(2018)_-_Conditional_Generation-238 舉例來說你可能是說給它一個很清晰的圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-239 給它一個很清晰的火車，但是搭配狗
GAN_Lecture_2_(2018)_-_Conditional_Generation-240 告訴它說這個要給低分，也許它會覺得說
GAN_Lecture_2_(2018)_-_Conditional_Generation-241 會不會是因為這個火車不夠清晰
GAN_Lecture_2_(2018)_-_Conditional_Generation-242 應該要產生的更清晰一點
GAN_Lecture_2_(2018)_-_Conditional_Generation-243 所以今天對這種 network 來說
GAN_Lecture_2_(2018)_-_Conditional_Generation-244 兩種不同的錯誤就是你的 x 不夠 realistic
GAN_Lecture_2_(2018)_-_Conditional_Generation-245 還有 x 跟 c 不夠 match
GAN_Lecture_2_(2018)_-_Conditional_Generation-246 對它來說它不知道到底是哪一種錯誤
GAN_Lecture_2_(2018)_-_Conditional_Generation-247 你就把兩種 data 都倒給它，希望它自己可以分辨
GAN_Lecture_2_(2018)_-_Conditional_Generation-248 可是我覺得就下面這個 case 而言，你就可以
GAN_Lecture_2_(2018)_-_Conditional_Generation-249 把這兩個 case 分開
GAN_Lecture_2_(2018)_-_Conditional_Generation-250 你就會告訴它說
GAN_Lecture_2_(2018)_-_Conditional_Generation-251 假設你今天的 case 是你的 x 產生出來不夠清晰
GAN_Lecture_2_(2018)_-_Conditional_Generation-252 那只需要這個值變小就好
GAN_Lecture_2_(2018)_-_Conditional_Generation-253 如果你今天是 x 很清晰
GAN_Lecture_2_(2018)_-_Conditional_Generation-254 只是跟 c 不 match
GAN_Lecture_2_(2018)_-_Conditional_Generation-255 那你只需要讓這個值變小，那這個值就不用變小
GAN_Lecture_2_(2018)_-_Conditional_Generation-256 我覺得這可能是一個比較合理的設計
GAN_Lecture_2_(2018)_-_Conditional_Generation-257 不過就是給大家參考就是了
GAN_Lecture_2_(2018)_-_Conditional_Generation-258 不過我現在看到比較多的 network 是用上面這個方式設計的
GAN_Lecture_2_(2018)_-_Conditional_Generation-259 不過也有些 paper 用下面這個方式來設計它的 discriminator
GAN_Lecture_2_(2018)_-_Conditional_Generation-260 我覺得下面其實是比較合理的
GAN_Lecture_2_(2018)_-_Conditional_Generation-261 也許你可以在作業裡面試試看
GAN_Lecture_2_(2018)_-_Conditional_Generation-262 然後告訴我說你覺得哪一個結果 performance 比較好
GAN_Lecture_2_(2018)_-_Conditional_Generation-263 那還有一個技術你可以用在作業裡面的叫做，stack GAN
GAN_Lecture_2_(2018)_-_Conditional_Generation-264 你不用 stack GAN 你就可以過 base line，那如果你想要登峰造極
GAN_Lecture_2_(2018)_-_Conditional_Generation-265 你就可以用一下 stack GAN
GAN_Lecture_2_(2018)_-_Conditional_Generation-266 stack GAN 是怎麼樣呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-267 stack GAN 的概念是說
GAN_Lecture_2_(2018)_-_Conditional_Generation-268 先產生小張的圖，根據小張的圖，再產生大張的圖
GAN_Lecture_2_(2018)_-_Conditional_Generation-269 在原始的 stack GAN 的 paper 裡面，它想要產生的圖大小是 256x256
GAN_Lecture_2_(2018)_-_Conditional_Generation-270 不過太大張了
GAN_Lecture_2_(2018)_-_Conditional_Generation-271 你直接產生 256x256 的圖會壞掉
GAN_Lecture_2_(2018)_-_Conditional_Generation-272 所以 stack GAN 在 train 的時候
GAN_Lecture_2_(2018)_-_Conditional_Generation-273 它把整個 training process
GAN_Lecture_2_(2018)_-_Conditional_Generation-274 拆成兩階，先拆成兩階，有一個第一階的 generator
GAN_Lecture_2_(2018)_-_Conditional_Generation-275 第一階的 generator 它的工作是說，吃一段文字進來
GAN_Lecture_2_(2018)_-_Conditional_Generation-276 這 network 有點複雜，詳情你再去看一下 paper
GAN_Lecture_2_(2018)_-_Conditional_Generation-277 吃一個文字敘述進來
GAN_Lecture_2_(2018)_-_Conditional_Generation-278 再吃一個 noise 進來，把它們通通都 concatenate 在一起
GAN_Lecture_2_(2018)_-_Conditional_Generation-279 然後產生一張 image
GAN_Lecture_2_(2018)_-_Conditional_Generation-280 然後這個 image 比較小，只有64x64
GAN_Lecture_2_(2018)_-_Conditional_Generation-281 那另一個 discriminator check 說
GAN_Lecture_2_(2018)_-_Conditional_Generation-282 這個 image 搭配這段文字的敘述
GAN_Lecture_2_(2018)_-_Conditional_Generation-283 是不是 match 的
GAN_Lecture_2_(2018)_-_Conditional_Generation-284 那接下來你有第二個 generator
GAN_Lecture_2_(2018)_-_Conditional_Generation-285 第二個 generator 是什麼呢
GAN_Lecture_2_(2018)_-_Conditional_Generation-286 第二個 generator 就是吃一段文字的敘述
GAN_Lecture_2_(2018)_-_Conditional_Generation-287 配一張64x64 的 image
GAN_Lecture_2_(2018)_-_Conditional_Generation-288 然後產生一張 256x256 的圖
GAN_Lecture_2_(2018)_-_Conditional_Generation-289 然後第二個 discriminator 看說
GAN_Lecture_2_(2018)_-_Conditional_Generation-290 這個 256xx256 的圖是不是 realistic 的
GAN_Lecture_2_(2018)_-_Conditional_Generation-291 總之你在做的時候，你就是分成兩階
GAN_Lecture_2_(2018)_-_Conditional_Generation-292 先產生小的，再產生大的，那你直覺就會知道說
GAN_Lecture_2_(2018)_-_Conditional_Generation-293 這樣 performance 應該會比較好的
GAN_Lecture_2_(2018)_-_Conditional_Generation-294 像之前不是 Nvidia 有 report 說
GAN_Lecture_2_(2018)_-_Conditional_Generation-295 他們的選片可以產生 1024x1024 的超級大圖
GAN_Lecture_2_(2018)_-_Conditional_Generation-296 它產生出來的人臉是連毛細孔都看得到的那一種
GAN_Lecture_2_(2018)_-_Conditional_Generation-297 它們做的就是類似 stack GAN 的概念
GAN_Lecture_2_(2018)_-_Conditional_Generation-298 你就產生小張的圖，先產生4x4
GAN_Lecture_2_(2018)_-_Conditional_Generation-299 再根據 4x4 產生 8x8，再產生 16x16
GAN_Lecture_2_(2018)_-_Conditional_Generation-300 最後一直到產生1024x1024
GAN_Lecture_2_(2018)_-_Conditional_Generation-301 不過它們實際上在 train 的時候，這些所有疊在一起的 generator
GAN_Lecture_2_(2018)_-_Conditional_Generation-302 都是 jointly 合在一起 train 的
GAN_Lecture_2_(2018)_-_Conditional_Generation-303 它是先 train 第一個小的
GAN_Lecture_2_(2018)_-_Conditional_Generation-304 再疊第二個比較大的，然後大的跟小的一起 train
GAN_Lecture_2_(2018)_-_Conditional_Generation-305 再疊更大的，然後再一起 train 這樣子
GAN_Lecture_2_(2018)_-_Conditional_Generation-306 那剛才講的那個 conditional GAN 是產生文字
GAN_Lecture_2_(2018)_-_Conditional_Generation-307 再產生對應的圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-308 那我們現在也可以產生圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-309 然後產生另外一張對應的圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-310 那這個東西是怎麼做的呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-311 這邊你看文獻上可以看到很多的例子
GAN_Lecture_2_(2018)_-_Conditional_Generation-312 比如把黑白的圖轉成彩色的圖
GAN_Lecture_2_(2018)_-_Conditional_Generation-313 把白天轉成夜晚，這怎麼做的？
GAN_Lecture_2_(2018)_-_Conditional_Generation-314 你要 train 這種 network
GAN_Lecture_2_(2018)_-_Conditional_Generation-315 首先當然你要有 training data
GAN_Lecture_2_(2018)_-_Conditional_Generation-316 假設你今天是要把簡單的幾何圖形
GAN_Lecture_2_(2018)_-_Conditional_Generation-317 變成真實的房屋的樣子
GAN_Lecture_2_(2018)_-_Conditional_Generation-318 但你需要收集很多簡單的幾何圖形
GAN_Lecture_2_(2018)_-_Conditional_Generation-319 跟真實房屋的 pair
GAN_Lecture_2_(2018)_-_Conditional_Generation-320 這種 data 收集個好幾萬張
GAN_Lecture_2_(2018)_-_Conditional_Generation-321 然後去 train 一個 network
GAN_Lecture_2_(2018)_-_Conditional_Generation-322 當然你可以用 supervised 的方法來解這個問題
GAN_Lecture_2_(2018)_-_Conditional_Generation-323 但是用 supervised 的方法，問題就是
GAN_Lecture_2_(2018)_-_Conditional_Generation-324 你產生出來的圖片，會是比較模糊的
GAN_Lecture_2_(2018)_-_Conditional_Generation-325 如果用 supervised 的方法，你的做法是說，你有一個 network
GAN_Lecture_2_(2018)_-_Conditional_Generation-326 然後你 input 一張圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-327 它 output 一張對應的圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-328 然後你希望這張對應的圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-329 它的目標越接近越好
GAN_Lecture_2_(2018)_-_Conditional_Generation-330 你通常會算個比如說  L1 loss 或 L2 loss
GAN_Lecture_2_(2018)_-_Conditional_Generation-331 當你發現如果你只有這麼做的話，你會遇到的問題是
GAN_Lecture_2_(2018)_-_Conditional_Generation-332 你產生出來的 output 會是特別模糊的
GAN_Lecture_2_(2018)_-_Conditional_Generation-333 這跟我們之前講 text to image 一樣
GAN_Lecture_2_(2018)_-_Conditional_Generation-334 你同一張 image 它可能可以對應到很多不同的房子
GAN_Lecture_2_(2018)_-_Conditional_Generation-335 今天network 在學的時候
GAN_Lecture_2_(2018)_-_Conditional_Generation-336 它就是產生一個平均的結果
GAN_Lecture_2_(2018)_-_Conditional_Generation-337 所以它產生的圖片會是比較模糊的
GAN_Lecture_2_(2018)_-_Conditional_Generation-338 這個時候，你就可以引入 GAN 的概念
GAN_Lecture_2_(2018)_-_Conditional_Generation-339 在 GAN 裡面，你的 generator
GAN_Lecture_2_(2018)_-_Conditional_Generation-340 它吃一個簡單的圖，它吃這個 condition，跟一個 noise z
GAN_Lecture_2_(2018)_-_Conditional_Generation-341 產生一張對應的圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-342 那你的 discriminator 做的事情是什麼呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-343 你的 discriminator 會檢查這個 generator 的 input 跟output 湊起來
GAN_Lecture_2_(2018)_-_Conditional_Generation-344 是不是一個 pair
GAN_Lecture_2_(2018)_-_Conditional_Generation-345 那我們之前有講過說
GAN_Lecture_2_(2018)_-_Conditional_Generation-346 你今天在 conditional GAN 裡面，在 train discriminator 的時候
GAN_Lecture_2_(2018)_-_Conditional_Generation-347 你應該給它吃一個 pair
GAN_Lecture_2_(2018)_-_Conditional_Generation-348 而不是只是 generator 的 output
GAN_Lecture_2_(2018)_-_Conditional_Generation-349 你應該給它 generator 的input 跟output 的 pair
GAN_Lecture_2_(2018)_-_Conditional_Generation-350 然後它會給它一個分數
GAN_Lecture_2_(2018)_-_Conditional_Generation-351 如果你用 GAN 來做的話
GAN_Lecture_2_(2018)_-_Conditional_Generation-352 你會發現說，你產生的圖，就清晰很多
GAN_Lecture_2_(2018)_-_Conditional_Generation-353 但它的問題是
GAN_Lecture_2_(2018)_-_Conditional_Generation-354 GAN 它會產生一些奇奇怪怪的東西
GAN_Lecture_2_(2018)_-_Conditional_Generation-355 舉例來說這邊有一個又像是煙囪，又像是窗戶的東西
GAN_Lecture_2_(2018)_-_Conditional_Generation-356 這本來 input 裡面沒有的
GAN_Lecture_2_(2018)_-_Conditional_Generation-357 對這個 discriminator 來說
GAN_Lecture_2_(2018)_-_Conditional_Generation-358 產生這個東西，好像也沒有特別不對
GAN_Lecture_2_(2018)_-_Conditional_Generation-359 但如果你今天要給它下額外的 constrain 的話
GAN_Lecture_2_(2018)_-_Conditional_Generation-360 你可以下另外一個 constrain 說
GAN_Lecture_2_(2018)_-_Conditional_Generation-361 你希望 generator 的 output 跟 training data 裡面
GAN_Lecture_2_(2018)_-_Conditional_Generation-362 目標的那個 image，同時也要越靠越近越好
GAN_Lecture_2_(2018)_-_Conditional_Generation-363 也就是對你的 generator 來說
GAN_Lecture_2_(2018)_-_Conditional_Generation-364 它有兩個目標，一個目標是
GAN_Lecture_2_(2018)_-_Conditional_Generation-365 要產生夠清晰的圖去騙過 discriminator
GAN_Lecture_2_(2018)_-_Conditional_Generation-366 另外一方面，你又希望 generator 產生出來的 output
GAN_Lecture_2_(2018)_-_Conditional_Generation-367 不要跟原來的目標相差太多
GAN_Lecture_2_(2018)_-_Conditional_Generation-368 如果你把這兩個東西，同時一起考慮的話
GAN_Lecture_2_(2018)_-_Conditional_Generation-369 那你產生出來的結果，就會比較好
GAN_Lecture_2_(2018)_-_Conditional_Generation-370 不只是產生清晰的圖，這個圖上面，也不會產生奇怪的東西
GAN_Lecture_2_(2018)_-_Conditional_Generation-371 這個是 image to image
GAN_Lecture_2_(2018)_-_Conditional_Generation-372 那這 image to image 那篇paper 裡面呢
GAN_Lecture_2_(2018)_-_Conditional_Generation-373 它的 discriminator 有稍微經過設計
GAN_Lecture_2_(2018)_-_Conditional_Generation-374 因為如果你今天要產生出來的 image 非常大張
GAN_Lecture_2_(2018)_-_Conditional_Generation-375 那你的 discriminator 如果是吃整張image 當作 input 的話
GAN_Lecture_2_(2018)_-_Conditional_Generation-376 你結果很容易壞掉，為什麼？
GAN_Lecture_2_(2018)_-_Conditional_Generation-377 因為你的 discriminator，因為 image 很大嘛
GAN_Lecture_2_(2018)_-_Conditional_Generation-378 所以你 discriminator 參數也要很多
GAN_Lecture_2_(2018)_-_Conditional_Generation-379 那你很容易 train 一train 就很容易 over fitting
GAN_Lecture_2_(2018)_-_Conditional_Generation-380 或你 train 的時間就非常的長
GAN_Lecture_2_(2018)_-_Conditional_Generation-381 所以其實在前面那個 image to image 的那個 paper 裡面
GAN_Lecture_2_(2018)_-_Conditional_Generation-382 它做的事情是
GAN_Lecture_2_(2018)_-_Conditional_Generation-383 它的 discriminator 每次只檢查圖片裡面的一小塊而已
GAN_Lecture_2_(2018)_-_Conditional_Generation-384 它不是讓  discriminator 去檢查整張圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-385 因為這樣你的 discriminator 的參數量，會太多
GAN_Lecture_2_(2018)_-_Conditional_Generation-386 它只讓 discriminator 檢查一小塊圖片
GAN_Lecture_2_(2018)_-_Conditional_Generation-387 然後再說這一小塊圖片它到底是好的，還是不好的
GAN_Lecture_2_(2018)_-_Conditional_Generation-388 當然一個 discriminator 要檢查多大的區域
GAN_Lecture_2_(2018)_-_Conditional_Generation-389 就變成一個你需要去調整的參數
GAN_Lecture_2_(2018)_-_Conditional_Generation-390 在 paper 裡面它當然有調說，如果看整張 image 會怎麼樣
GAN_Lecture_2_(2018)_-_Conditional_Generation-391 如果小到只看一個 pixel 會怎麼樣
GAN_Lecture_2_(2018)_-_Conditional_Generation-392 這個如果只看一小塊，叫做 patch GAN
GAN_Lecture_2_(2018)_-_Conditional_Generation-393 如果只看一個 pixel 就叫做 pixel GAN
GAN_Lecture_2_(2018)_-_Conditional_Generation-394 那你可以想像說只看一個 pixel，當然是一點用都沒有
GAN_Lecture_2_(2018)_-_Conditional_Generation-395 如果只看一個 pixel，它不就只考慮那一個點的事情嗎？
GAN_Lecture_2_(2018)_-_Conditional_Generation-396 所以它產生出來的 image，就會整個都糊掉
GAN_Lecture_2_(2018)_-_Conditional_Generation-397 就會看不出來在產生什麼東西
GAN_Lecture_2_(2018)_-_Conditional_Generation-398 所以當然只用一個 pixel 是不行的
GAN_Lecture_2_(2018)_-_Conditional_Generation-399 但是只看整張 image，performance 也不是最好的
GAN_Lecture_2_(2018)_-_Conditional_Generation-400 你要調一下這個 patch 的 size
GAN_Lecture_2_(2018)_-_Conditional_Generation-401 看看說怎樣的 patch size 可以給你最好的結果
GAN_Lecture_2_(2018)_-_Conditional_Generation-402 這個東西叫做 patch GAN
GAN_Lecture_2_(2018)_-_Conditional_Generation-403 其實同樣的技術，不是只能用在影像上
GAN_Lecture_2_(2018)_-_Conditional_Generation-404 到目前為止，我們講 GAN 的時候，都是 apply 在影像上
GAN_Lecture_2_(2018)_-_Conditional_Generation-405 其實它的技術，也可以用在語音上
GAN_Lecture_2_(2018)_-_Conditional_Generation-406 舉例來說，你可以用 GAN 這個技術來做 speech enhancement
GAN_Lecture_2_(2018)_-_Conditional_Generation-407 什麼是 speech enhancement 呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-408 speech enhancement 的意思是說
GAN_Lecture_2_(2018)_-_Conditional_Generation-409 你有一段聲音訊號
GAN_Lecture_2_(2018)_-_Conditional_Generation-410 它被雜訊干擾，它加了很多背景的噪音
GAN_Lecture_2_(2018)_-_Conditional_Generation-411 你希望機器可以自動把背景噪音去掉
GAN_Lecture_2_(2018)_-_Conditional_Generation-412 那通常有兩個作用，一個是把背景噪音去掉以後
GAN_Lecture_2_(2018)_-_Conditional_Generation-413 再丟到語音辨識系統裡面，也許正確率會比較高
GAN_Lecture_2_(2018)_-_Conditional_Generation-414 另外一方面，把背景噪音去掉以後，再播給人聽
GAN_Lecture_2_(2018)_-_Conditional_Generation-415 也許人聽得比較清楚
GAN_Lecture_2_(2018)_-_Conditional_Generation-416 那如果今天是一般的 speech enhancement，怎麼 train 的呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-417 你要找很多聲音
GAN_Lecture_2_(2018)_-_Conditional_Generation-418 然後把這些聲音，也都加上一些雜訊
GAN_Lecture_2_(2018)_-_Conditional_Generation-419 接下來，你就 train 一個 generator
GAN_Lecture_2_(2018)_-_Conditional_Generation-420 input 一段有雜訊的聲音
GAN_Lecture_2_(2018)_-_Conditional_Generation-421 希望 output 就是沒有雜訊的聲音
GAN_Lecture_2_(2018)_-_Conditional_Generation-422 input 一段有雜訊的聲音
GAN_Lecture_2_(2018)_-_Conditional_Generation-423 把沒有雜訊的聲音，當作你的目標
GAN_Lecture_2_(2018)_-_Conditional_Generation-424 去訓練你的 generator
GAN_Lecture_2_(2018)_-_Conditional_Generation-425 這是一段聲音訊號，但它是用 spectrum 來表示它
GAN_Lecture_2_(2018)_-_Conditional_Generation-426 它看起來就像是一個 image 一樣
GAN_Lecture_2_(2018)_-_Conditional_Generation-427 所以這個 generator 常常也就會直接套用你在產生 image
GAN_Lecture_2_(2018)_-_Conditional_Generation-428 我們剛才說 conditional generation，它會做在 image 上嘛
GAN_Lecture_2_(2018)_-_Conditional_Generation-429 那其實那些 image 上常用的架構
GAN_Lecture_2_(2018)_-_Conditional_Generation-430 其實也可以直接套用到 speech enhancement 上面
GAN_Lecture_2_(2018)_-_Conditional_Generation-431 也是沒有什麼問題的
GAN_Lecture_2_(2018)_-_Conditional_Generation-432 但是我們剛才有講過說，直接 train generator
GAN_Lecture_2_(2018)_-_Conditional_Generation-433 你產生出來的結果就會比較模糊
GAN_Lecture_2_(2018)_-_Conditional_Generation-434 所以你要再加上 GAN 的概念
GAN_Lecture_2_(2018)_-_Conditional_Generation-435 不只要直接 train generator
GAN_Lecture_2_(2018)_-_Conditional_Generation-436 你還要 train 一個 discriminator
GAN_Lecture_2_(2018)_-_Conditional_Generation-437 discriminator 的工作呢
GAN_Lecture_2_(2018)_-_Conditional_Generation-438 就是看 generator 的 input 加output
GAN_Lecture_2_(2018)_-_Conditional_Generation-439 這個我們今天強調很多次了，在 conditional GAN 裡面
GAN_Lecture_2_(2018)_-_Conditional_Generation-440 你不可以只看 generator 的 output
GAN_Lecture_2_(2018)_-_Conditional_Generation-441 discriminator 要同時看generator 的 input 跟output
GAN_Lecture_2_(2018)_-_Conditional_Generation-442 然後給它一個分數
GAN_Lecture_2_(2018)_-_Conditional_Generation-443 這個分數決定說
GAN_Lecture_2_(2018)_-_Conditional_Generation-444 現在 output 的這一段聲音訊號是不是 clean 的
GAN_Lecture_2_(2018)_-_Conditional_Generation-445 同時 output 跟input 是不是 match 的
GAN_Lecture_2_(2018)_-_Conditional_Generation-446 你並不希望你本來說 I love you
GAN_Lecture_2_(2018)_-_Conditional_Generation-447 然後通過 generator 以後就變成 I hate you 這樣子
GAN_Lecture_2_(2018)_-_Conditional_Generation-448 那就不行，所以你希望 output 跟 input
GAN_Lecture_2_(2018)_-_Conditional_Generation-449 他們是 match 在一起的
GAN_Lecture_2_(2018)_-_Conditional_Generation-450 那同樣的技術，也可以做 video 的 generation
GAN_Lecture_2_(2018)_-_Conditional_Generation-451 那怎麼做 video 的 generation 呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-452 就是給 generator 看一段影片
GAN_Lecture_2_(2018)_-_Conditional_Generation-453 然後讓它預測，接下來會發生什麼樣的事情
GAN_Lecture_2_(2018)_-_Conditional_Generation-454 讓它產生影片接下來發生的事情
GAN_Lecture_2_(2018)_-_Conditional_Generation-455 那要怎麼做到這件事情呢？
GAN_Lecture_2_(2018)_-_Conditional_Generation-456 你就需要一個 discriminator
GAN_Lecture_2_(2018)_-_Conditional_Generation-457 那我們說 discriminator 不能夠只看 generator 的 output
GAN_Lecture_2_(2018)_-_Conditional_Generation-458 它要同時看，generator 的 input 跟 output
GAN_Lecture_2_(2018)_-_Conditional_Generation-459 你把 generator 的 input 跟output 接在一起
GAN_Lecture_2_(2018)_-_Conditional_Generation-460 變成一段完整的影片
GAN_Lecture_2_(2018)_-_Conditional_Generation-461 然後讓 discriminator 去檢查說，這一段影片
GAN_Lecture_2_(2018)_-_Conditional_Generation-462 到底是不是一個合理的影片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-0 作業 3-3，我們要講的是什麼呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-1 我們要講的是 Unsupervised conditional generation
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-2 conditional generation
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-3 我們上周已經講過了，就是 machine 輸入一個東西，然後輸出一個東西
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-4 這個就是 conditional generation
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-5 那我們在作業 3-2 的時候
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-6 大家也做過 conditional generation，但我們之前做的 conditional generation 是 supervised
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-7 舉例來說在作業 3-2 裡面，我們是收集了很多 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-8 這些 image 都有它對應的文字
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-9 你才能夠 train 一個 conditional generation 吃文字，output image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-10 這邊我們想要討論的是，conditional generation 這件事情
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-11 能不能夠做到 unsupervised
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-12 其實今天你可以看到很多的例子
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-13 conditional generation 它可以是 unsupervised
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-14 舉例來說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-15 它可以是 unsupervised
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-16 舉例來說，假設你有一個 domain x 的 image，它們是 real photo，是風景照
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-17 你有 domain y 的 image，它門是梵谷的畫作
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-18 那妳可以 learn 一個 generator，給它一張real 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-19 它 output 的 image 看起來就像是梵谷的畫作
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-20 而你在 training 的時候
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-21 你並不需要 labeled data
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-22 一般我們要 train 這種 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-23 如果是 supervised conditional generation，你需要 label 告訴它說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-24 看到這樣子的 input，應該有什麼樣的 output
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-25 input 紅髮這個句子
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-26 那 output 就是一個紅頭髮的角色到底長什麼樣子
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-27 但是今天假設你是要做這種比較類似風格轉換的 task
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-28 你是要把一張風景照
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-29 轉成梵谷的畫作
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-30 你可以收集到一堆風景照
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-31 你可以收集到一堆梵谷的畫作
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-32 但是你其實收集不到它們之間的 link
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-33 因為假設你收集一張風景照，是日月潭
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-34 梵谷沒有去過日月潭，所以他畫作裡面，也沒有日月潭
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-35 所以你也根本就沒有辦法 label 這種 link
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-36 所以今天我們要討論的問題就是，有沒有辦法做到 unsupervised conditional generation
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-37 只有兩堆 data，machine 自己學到說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-38 怎麼從其中一堆轉到另外一堆
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-39 那其實這樣的技術，有很多的應用
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-40 不是只能夠用在影像上
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-41 雖然你今天在文獻上看到的多數 application，都是影像
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-42 但是在語音或文字上
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-43 你當然也是可以做類似的事情的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-44 那之後我們會舉一些例子
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-45 那 conditional generation，到底要怎麼做呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-46 這個 unsupervised conditional generation
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-47 我 surveyed 一下文獻，我認為大致上可分為兩大類的作法
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-48 第一大類的做法是直接轉
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-49 什麼意思，直接 learn 一個 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-50 input x domain 的東西
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-51 想辦法轉成 y domain 的東西
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-52 那等一下我們會講說，這樣子的 generator 到底要怎麼 learn 出來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-53 那在經驗上啊
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-54 如果你今天要用這種 direct transformation 的方法
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-55 你的 input output 沒有辦法真的差太多
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-56 今天這個 generator 你給它一個 input
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-57 output 它通常只能夠小改而已
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-58 如果是影像的話，它通常能夠改的是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-59 顏色啊，質地啊，所以如果是那種畫風轉換
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-60 真實的圖片，轉成梵谷的畫作的風格
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-61 這個是比較有可能用第一個方法來實踐
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-62 那今天假設你要轉的 input 跟 output
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-63 差距很大，它們不是只有在顏色
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-64 紋理上面的轉換的話
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-65 那你就要用到第二個方法
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-66 第二個方法是這樣，假設你今天要做的是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-67 這個不是真正的例子
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-68 今天的技術做不到這個程度就是了
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-69 那這個怎麼做呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-70 如果今天你的 input 跟 output，差距很大
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-71 比如說你要把真人轉成動畫人物
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-72 那真人跟動畫人物就是不像
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-73 它不是你改改顏色，或改改紋理
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-74 就可以從真人轉成動畫人物的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-75 那怎麼辦，你先 learn 一個 encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-76 比如說第一個 encoder 做的事情
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-77 就是吃一張人臉的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-78 然後它把人臉的特徵抽出來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-79 它把這張臉的特徵抽出來，比如說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-80 這是男的，這是有戴眼鏡的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-81 接下來你生一個 decoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-82 這個 decoder 它畫出來的就是動畫的人物
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-83 它根據你 input 的人臉特徵
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-84 比如說是男的，有戴眼鏡的，去生出一個對應的角色
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-85 如果你 input output 真的差很多的時候
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-86 你就可以做這件事
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-87 那知道作業 3 就是一個 bonus 嘛
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-88 那根據過去的經驗會有非常多人做真人的臉轉動畫人物
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-89 很多人會做這個，但是過去通常比較多人是用 cycle GAN 做
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-90 結果都還還蠻失敗的，因為動畫人物的臉，就是跟真人臉不像啊
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-91 你如果要做動畫人物，轉真人的臉
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-92 你可以考慮一下第二個做法做起來看看會不會比較好一點
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-93 那先來講第一個做法
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-94 第一個做法是怎麼做的呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-95 第一個做法是說，我們要 learn 一個 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-96 這個 generator input x domain 的東西，要轉成 y domain 的東西
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-97 那我們現在 x domain 的東西有一堆
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-98 y domain 的東西有一堆，但是合起來的 pair 沒有
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-99 我們沒有它們中間的 link
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-100 那 generator 怎麼知道
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-101 給一個 x domain 的東西，要 output 什麼樣 y domain 的東西呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-102 用 supervised learning 當然沒有沒有問題，但現在是 unsupervised
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-103 generator 怎麼知道怎麼產生 y domain 的東西呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-104 這個時候你就需要一個 y domain 的 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-105 這個 discriminator 做的事情是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-106 他看過很多 y domain 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-107 所以給他一張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-108 它可以鑑別說這張 image 是 x domain 的 image，還是 y domain 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-109 接下來 discriminator 要做的事情
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-110 就是說給他一張image，它會判斷說是 x domain 還是 y domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-111 接下來 generator 要做的事情就是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-112 想辦法去騙過 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-113 如果 generator 可以產生一張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-114 去騙過 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-115 那 generator 產生出來的 image，就會像是一張 y domain 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-116 如果 y domain 現在是梵谷的畫作
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-117 generator 產生出來的 output 就會像是梵谷的畫作
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-118 因為 discriminator 知道梵谷的畫作，長得是什麼樣子
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-119 但是現在的問題是說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-120 generator 可以產生像是梵谷畫作的東西
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-121 但完全可以產生一個跟 input 無關的東西
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-122 舉例來說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-123 它可能就學到說，它畫這張自畫像
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-124 就可以騙過 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-125 因為這張自畫像，確實很像是梵谷畫的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-126 但是這張自畫像跟輸入的圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-127 完全沒有任何半毛錢的關係，這個就不是我們要的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-128 所以我們今天不是要讓 generator 騙過 discriminator 就好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-129 同時我們希望 generator 不只要騙過 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-130 generator 的輸入和輸出是必須有一定程度的關係的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-131 那這一件事情
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-132 怎麼做呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-133 那在文獻上，就有不同的做法
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-134 我們等一下會講道 cycle GAN，這是最知名的作法
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-135 那其實有一個最簡單的做法就是，無視這個問題
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-136 直接做下去，事實上有人試過
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-137 直接做下去，也做得起來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-138 舉例來說，你在做那個 cycle GAN 的時候
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-139 你會看到很多人做那個馬轉斑馬，斑馬轉馬
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-140 那 cycle GAN 有個 cycle consistency 的 loss 我們等一下會講到
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-141 那事實上，拔掉那個 cycle consistency 的 loss
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-142 你還是可以把馬轉成斑馬，還是 work
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-143 所以今天的一個可能性是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-144 無視這個問題
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-145 直接就 learn 一個 generator，一個 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-146 看看 work 不 work
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-147 為什麼這樣子有機會可以 work 呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-148 因為 generator 的 input 跟 output
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-149 其實不會差太多
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-150 就假設你的 generator 沒有很深
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-151 那 input 總不會你 input 這個圖片，然後 output一個梵谷的自畫像吧
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-152 這未免差太多了
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-153 所以今天其實 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-154 如果你沒有特別要求它的話
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-155 它其實喜歡 input 就跟 output 差不多
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-156 所以你給他這張圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-157 它其實不太想要改太多，它希望
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-158 改一點點就騙過 discriminator 就好，它不想要改太多
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-159 所以今天你直接 learn 一個這樣的 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-160 這樣的 discriminator，不加額外的 constrain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-161 其實也是會 work 的，你可以自己試試看
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-162 那在下面這個文獻裡面，他就嘗試說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-163 如果今天 generator 比較 shallow
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-164 比較淺，所以它 input 跟 output 會特別像
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-165 那這個時候，你就不需要做額外的 constrain，就可以把這個 generator learn 起來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-166 那如果你今天 generator 很深
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-167 如果你 generator 很 deep，有很多層，那他就真的可以讓 input output
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-168 非常不一樣，這個時候，你就需要做一些額外的處理
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-169 免得讓 input 跟 output 變成完全不一樣的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-170 這是第一個方法，第一個方法就是不要管它
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-171 第二個方法是這樣，你去拿一個 pre-trained 好的 network，比如說 VGG 之類的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-172 拿一個 pre trained 好的 network，接下來你把這個 generator 的 input 跟 output
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-173 通通都丟給這個 pre trained 好的 network
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-174 然後得到比如說 output 一個 embedded ***
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-175 接下來你在 train 的時候，generator 一方面會想要騙過 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-176 讓它 output 的 image 看起來像是梵谷的畫作
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-177 但是同時呢，這個 generator 它還有另外一個任務
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-178 它會希望這個 pre trained 的 model
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-179 他們 embedded**** 的 output 不要差太多
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-180 那這樣的好處就是，因為這兩個 vector 沒有差太多
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-181 所以代表說，這張圖跟這張圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-182 就不會差太多
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-183 generator 的 input 跟 output 就不會差太多
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-184 這個是第二個做法
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-185 第三個做法
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-186 就是大家所熟知的 cycle GAN，在 cycle GAN 裡面呢
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-187 你要 train 一個 x domain 和 y domain 的 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-188 你同時 train 一個 y domain 到 x domain 的 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-189 y domain 到 x domain 的 generator 它的目的是什麼呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-190 它的目的是說，給他一張 y domain 的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-191 接下來 input 一張風景畫
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-192 第一個 generator 把它轉成 y domain 的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-193 接下來第二個generator 要把 y domain 的圖，還原回來一模一樣的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-194 因為現在除了要騙過 discriminator 以外
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-195 generator 得到了一個新的任務，要讓 input 跟 output 越像越好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-196 為了要讓 input 跟 output 越像越好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-197 你就不可以在中間產生一個完全無關的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-198 如果你在這邊產生一個梵谷的自畫像
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-199 第二個 generator 就無法從梵谷的自畫像還原成原來的風景畫
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-200 因為它已經完全不知道原來的輸入是什麼了
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-201 所以這張圖片，必須要保留有原來輸入的資訊
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-202 那這樣第二個 generator 才可以根據這張圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-203 轉回原來的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-204 這個就是 cycle GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-205 那這樣 input 跟 output 越接近越好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-206 input 一張 image 轉換以後要能夠轉得回來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-207 這個兩次轉換要轉得回來這件事情
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-208 就叫做 cycle consistency
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-209 那 cycle GAN 你可以做雙向的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-210 所謂雙向的意思是說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-211 本來有 x domain 轉 y domain，y domain 轉 x domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-212 現在你再 train 另外一個 task
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-213 把 y domain 的圖丟進來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-214 然後把它轉成 x domain 的圖，
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-215 同時你要有一個 discriminator 確保說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-216 今天這個 generator 它 output 的圖像是 x domain 的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-217 接下來再把 x domain 的圖轉回原來 y domain 的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-218 一樣希望 input 跟 output 越接近越好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-219 這邊你就可以同時去 train，這邊有一個 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-220 這邊是第二個 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-221 有兩個 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-222 你就會把這兩個 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-223 這兩個 discriminator，一起去 train
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-224 這個就是 cycle GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-225 那其實 cycle GAN，現在還是有一些問題是沒有解決的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-226 cycle GAN 的一個問題就是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-227 今年的 NIPS 有一篇 paper 叫做 cycle GAN: a master of stenography
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-228 stenography 是什麼呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-229 stenography 是隱寫術
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-230 就是說 cycle GAN 會把 input 的東西藏起來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-231 然後在 output 的 時候，再把它呈現出
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-232 什麼意思呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-233 那個 paper 裡面就舉一個例子，它說這是 input
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-234 這個要做的事情是把這個真實的空拍照
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-235 轉成這個看起來像是衛星的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-236 那這個 input 是這張圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-237 然後第一個 generator 把它轉成這樣
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-238 然後第二個 generator 可以把它還原
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-239 那你發現說這個 input 跟 output 是蠻像的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-240 舉例來說，這個屋頂上有些黑點
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-241 在 reconstruct 回來之後，還是有些黑點
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-242 但神奇的地方是，中間的 image，它是沒有黑點的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-243 那 machine 怎麼知道說，這個屋頂應該是要有黑點的呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-244 就 input 是有黑點的，那沒有問題
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-245 output 也是有黑點的，但是中間的產物
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-246 居然是沒有黑點的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-247 如果你只看到這一張圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-248 對第二個 generator 來說，如果你只看到這一張圖，屋頂上是沒有黑點的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-249 你是怎麼知道上面應該要產生黑點的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-250 所以有一個可能是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-251 今天cycle GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-252 雖然有 cycle consistency 的 loss
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-253 強迫你 input 跟 output 要越像越好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-254 但是 generator 它有很強的能力把資訊藏在人看不出來的地方
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-255 也就是說這些你要怎麼如何 reconstruct 這張 image 的資訊
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-256 可能是藏在這張 image 裡面
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-257 它可能用非常非常小的數值
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-258 藏在 image 裡面，讓你看不出來這樣
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-259 也許這個屋頂上仍然是有黑點的，只是你看不出來而已
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-260 那如果是這樣子的情況，如果今天 cycle GAN 會藏資訊
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-261 那就失去 cycle consistency 的意義了，因為 cycle consistency 的意義就是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-262 第一個 generator output 的 image 跟 input 不要差太多
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-263 但如果今天 generator 很擅長藏資訊
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-264 然後再自己解回來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-265 那這個 output 的 image 就有可能跟這個 input 的 image 差距很大了
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-266 那這個就是一個尚待研究的問題，也就是 cycle consistency
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-267 不一定有用，machine 可能會自己學到一些方法去
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-268 避開 cycle consistency 帶給你的 constrain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-269 那在文獻上呢
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-270 除了 cycle GAN 以外，你可能還看到其他的 GAN 比如說， dual GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-271 Disco GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-272 這 3 個東西有什麼不同呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-273 就是沒有什麼不同這樣子，就不同的人
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-274 居然在幾乎同樣的時間
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-275 提出一樣的方法
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-276 然後 submit 到不同的 conference 這樣
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-277 這個類似的想法真的是很神奇，就大家想的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-278 方法都差不多，你自己去仔細看看，這些方法
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-279 其實就跟 cycle GAN 是一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-280 那現在還有一招
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-281 叫做 star GAN，star GAN 是什麼呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-282 star GAN 是說，我們在做 cycle GAN 的時候
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-283 你只能夠把 x domain 轉成 y domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-284 但有時候你會有一個需求是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-285 你有多個 domain，你要用多個 domain 互轉
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-286 比如說你有 4 個 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-287 你要在 4 個 domain 間互轉
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-288 那這樣理論上呢，你就要學出
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-289 D 4 取 2 個 transformation 的 network
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-290 才能在 4 個 domain 間互轉
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-291 star GAN 它做的事情是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-292 它只 learn 了一個 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-293 但就可以在多個 domain 間互轉
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-294 那我們這邊就很快地來，這個你可能
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-295 在作業上也用不上，不過還是跟大家很快地講一下 starGAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-296 那如果你有細節的問題的話，你再去 check 一下那個 paper
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-297 那在 star GAN 裡面是怎麼樣呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-298 在 starGAN 裡面你要 learn 一個 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-299 這個 discriminator 它會做兩件事
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-300 首先給他一張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-301 它要鑑別說這張 image 是 real 還是 fake 的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-302 再來呢
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-303 它要去鑑別這一張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-304 它來自於哪一個 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-305 剛剛說，你可能有 4 個 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-306 它就問說，它來自於這 4 個 domain 的哪一個
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-307 在 star GAN 裡面
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-308 你只需要 learn 一個 generator 就好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-309 這個 generator 它的 input 是一張圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-310 跟你目標的 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-311 就是你要把這張 input 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-312 轉成哪一個 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-313 它要有一個目標的 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-314 然後它根據這個 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-315 根據這個目標的 domain，就把新的 image 生成出來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-316 接下來再把這個同樣的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-317 丟給同一個 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-318 把這一張被 generated 出來的 image，丟給同一個 generator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-319 然後再告訴它說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-320 現在原來 input 的 image 是哪一個 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-321 然後再用這個 generator 合回另外一張圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-322 那你要希望這邊的 input 跟這邊的 output
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-323 越接近越好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-324 那這個東西就是 cycle consistency 的 loss 嘛
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-325 我們剛剛在講 cycle GAN 的時候說，input 一張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-326 你要還原回一模一樣的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-327 經過兩次轉換以後，還原回一模一樣的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-328 那對這個 generator 來說，做的事情是一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-329 告訴它說你把 input image 轉成 target domain，它就轉了
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-330 接下來再告訴 generator 說，給你這張轉出來的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-331 再給你原來的 domain，你要轉回一模一樣的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-332 那等一下我們會看一個比較具體的例子，你可能會比較了解
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-333 那這個 discriminator 做的事情就是要確認說這張轉出來的 image 到底
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-334 對不對
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-335 那要確認兩件事，第一件事是，這張轉出來的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-336 看起來有沒有真實
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-337 再來就是，這張轉出來的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-338 它是不是我們要的 target domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-339 然後 generator 就要去想辦法騙過 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-340 這邊是一個比較 realistic 的例子
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-341 這些圖都是從 paper 從裡面截出來的，就 paper 的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-342 它實在是畫做得太好了，所以我就沒有重做了這樣子
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-343 這個 star GAN 做的事情是什麼呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-344 就是你有一個 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-345 這個 discriminator 吃一張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-346 它首先要判斷它是 real 還是 fake 的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-347 同時它要判斷說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-348 這一個 image 來自於哪一個 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-349 事實上在原始的 paper 裡面
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-350 它的 domain 並不是說就是 5 個 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-351 而是每個 domain 都有一組編碼
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-352 這樣大家了解我的意思嗎？它並不是只有一個 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-353 而是說，你可以說我要一個黑頭髮的角色
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-354 要一個男性的角色
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-355 要一個年輕的角色，這樣子叫做一個 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-356 所以它 domain 可以有很多個，不是只有數個而已
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-357 所以今天是說給它這張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-358 這張 image 屬於哪個 domain 呢？這邊編碼是 00101
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-359 它就是一個褐色頭髮，然後年輕的角色
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-360 那這個 discriminator 要學到說，看到這張圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-361 必須要知道說，它是褐色頭髮，是年輕的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-362 那怎麼確認這個 generator 呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-363 你在確認這個 generator 的時候，就是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-364 你跟 generator 說，input 這張圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-365 我們想把它轉 domain，10011，10011 是什麼呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-366 黑色頭髮，男性，然後年輕，所以就把這張圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-367 轉成一個黑色頭髮
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-368 它本來是棕色頭髮嘛，轉成黑色頭髮
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-369 接下來，再把這個黑色頭髮圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-370 也丟回原來的 generator，然後跟他說現在要轉 00101
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-371 00101 是什麼呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-372 棕色頭髮，年輕的角色，所以它就把這個角色
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-373 轉回棕色的頭髮
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-374 那你希望 input 跟 output 的圖片，越接近越好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-375 這個是 cycle consistency 的 loss
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-376 接下來你要用 discriminator 去確保說這個 output 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-377 一方面它是 realistic
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-378 另外一方面，如果你今天 output 的 image，你希望 output 10011
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-379 也就是黑頭髮男性，年輕的角色，這個 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-380 看到這張圖片，必須要能夠判斷出
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-381 它是 10011
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-382 是一個黑頭髮，男性，年輕的角色
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-383 這個呢，就是 star GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-384 下一張圖舉的例子，也是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-385 一模一樣，就是這邊有一個人，它是生氣的 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-386 然後今天目標就是要把它轉成
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-387 笑口常開的 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-388 好 discriminator 要確認說不只這張圖片看起來要真實，而且要看起來像是笑口常開
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-389 然後今天這個 generator 說把這張圖片再轉回生氣的樣子\
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-390 那希望 input 跟 output 越接近越好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-391 這個是 star GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-392 那第二個做法呢，第二個做法是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-393 我們剛才有講過說，我們就是要 learn 一個 encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-394 然後把一張input 的 圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-395 轉到某一個 latent 的 space
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-396 然後再從 latent space 把它轉回來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-397 那你用這個技術，可以做到比較大的轉換
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-398 現在我們要講 unsupervised conditional generation 的第二個做法是這樣
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-399 第二個做法是說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-400 我們現在要把 image project 到一個
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-401 假設我們現在要 input 是一個 image 的話
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-402 我們把 input 的 object
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-403 投影到某一個 latent 的 space
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-404 再用 decoder 把它合回來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-405 那假設你今天有兩個 domain 一個是人的 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-406 真人人物頭像的 domain，一個是動畫人物的 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-407 那你今天想要在這兩個 domain 間做互相轉換的話，那怎麼辦呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-408 你今天呢，需要一個 x domain 的 encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-409 看到一張真人的頭像，就把它的特徵抽出來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-410 有一個 y domain 的 encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-411 看到真人的頭像
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-412 就把它特徵抽出來，那 x domain 的 encoder 跟 y domain 的 encoder，他們可能是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-413 不一樣的，參數可能是不一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-414 因為畢竟人臉和動畫人物的臉還是有一些差別
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-415 所以這兩個 network 不見得是一樣的 network
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-416 那你有一個 x domain 的 encoder 吃這個 image，它會抽出它的 attribute
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-417 那所謂的 attribute 就是一個 latent  的 vector，就你 input 一張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-418 encoder 的 output 就是一個 latent  的 vector
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-419 你 input 一張image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-420 這個 encoder y 的 output 就是一個 latent  的 vector
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-421 接下來把這個 latent 的 vector，丟到 decoder 裡面
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-422 如果丟到 x domain 的 decoder，它產生出來的就是真實人物的人臉
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-423 如果是丟到 y domain 的 decoder，它產生出來就是 2 次元人物的人臉
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-424 這是我們希望最後可以達到的結果是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-425 你給他一張真人的人臉
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-426 透過 x domain 的 encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-427 抽出 latent 的 representation
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-428 這個 latent 的 representation
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-429 它是一個 vector
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-430 但我們期待說這個 vector 的每一個 dimension
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-431 就代表了 input 的這張圖片的某種特徵
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-432 有沒有戴眼鏡，是什麼性別，等等
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-433 那接下來你用 y domain 的 decoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-434 吃這個 vector
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-435 根據這個 vector 裡面所表示的人臉的特徵
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-436 合出一張 y domain 的圖，我們希望做到這一件事
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-437 但是實際上如果我們今天有 x domain 跟 y domain 之間的對應關係
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-438 要做到這件事非常容易，因為就是一個 supervised learning 的問題
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-439 但是現在我們是一個 unsupervised learning 的問題
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-440 只有 x domain 的 image，跟 y domain 的 image，他們是分開的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-441 那怎麼 train 這些 encoder 跟這些 decoder 呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-442 那也可以這樣 train，這個 encoder x
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-443 跟decoder x 合起來，組成一個 auto encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-444 input 一張 x domain 的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-445 讓它 reconstruct 回原來 x domain 的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-446 y domain 的 encoder 跟 y domain 的 decoder，組成一個 auto encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-447 input 一個 y domain 的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-448 reconstruct 回原來 y domain 的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-449 那我們知道這兩個 auto encoder 在 train 的時候
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-450 它們都是要 minimize reconstruction error
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-451 用這樣的方法，你確實可以得到
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-452 2 個 encoder，2 個 decoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-453 但是這樣會造成的問題是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-454 這兩個 encoder，這兩個 decoder 之間
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-455 是沒有任何關聯的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-456 這邊你還可以多做一件事情是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-457 你可以把 discriminator 加進來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-458 你可以 train 一個 x domain 的 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-459 強迫 decoder 的 output 看起來像是 x domain 的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-460 因為我們知道說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-461 假設如果你只 learn auto encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-462 你只去 minimize reconstruction error
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-463 decoder output 的 image 會很模糊
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-464 那你如果不要讓你 decoder output 的 image 模糊
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-465 那你就會想要加一個 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-466 那這個 discriminator 呢
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-467 它就是吃這一張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-468 然後鑑別它是不是 x domain 的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-469 有一個 y domain 的 discriminator，它吃到一張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-470 鑑別它是不是 y domain 的圖
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-471 這樣你會強迫你的 x domain 的 decoder 跟 y domain 的 decoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-472 它們 output 的 image 都比較 realistic
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-473 你會發現說，這個 encoder 加這個 decoder，加這個 discriminator，它們 3 個合起來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-474 其實就是一個 VAE GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-475 對不對，我們上一堂可有講一個東西叫做 VAE GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-476 它可以看做是強化 VAE
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-477 或者可以看做是用 GAN 強化 VAE，也可以看做 VAE來強化 GAN，都可以
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-478 所以這個encoder 這個 decoder，加這個 discriminator 合起來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-479 是一個 VAE GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-480 這個 encoder 加這個 decoder，加這個 discriminator 合起來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-481 它是另外一個 VAE GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-482 但是因為這兩個 VAE GAN 他們的 training 是完全分開的，完全各自獨立的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-483 所以你實際上 train 完以後，你會發現
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-484 他們的 latent  space 可能意思是不一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-485 也就是說你今天丟這張人臉進去
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-486 變成一個 vector
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-487 你把這個 vector 丟到這張圖片裡面
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-488 搞不好它產生的就是一個截然不同的圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-489 因為今天這兩組 auto encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-490 它是分開 train 的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-491 也許上面這組 auto encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-492 是用這個 latent vector 的第一維代表性別
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-493 第二維代表有沒有戴眼鏡
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-494 下面這個是用第三維代表性別
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-495 第四維有沒有戴眼鏡
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-496 如果是這樣子的話，你就做不起來，因為你 input 這張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-497 它變成一個 vector，再解回來的時候
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-498 它會產生不一樣的圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-499 也就是說今天 x 這一群 encoder 跟 decoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-500 還有 y 這一群 encoder 跟 decoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-501 它們用的 language 是不一樣的，它們說的語言是不一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-502 所以 encoder 吐出一個東西
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-503 x domain 的 encoder 吐出一個東西，要叫 y domain 的 decoder 吃下去
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-504 它 output 並不會跟 x domain encoder 的 input 有任何的關聯性
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-505 接下來的問題就是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-506 怎麼解決這件事？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-507 所以在文獻上，就會有各式各樣的解法
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-508 一個常見的解法是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-509 你讓這個 encoder 跟 decoder，不同 domain 的 encoder 跟 decoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-510 他們的參數是被 tie 在一起的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-511 就我們知道說 encoder 有好幾個 hidden layer
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-512 x domain encoder 有好幾個 hidden layer，y domain encoder 也有好幾個 hidden layer
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-513 那你希望他們最後的幾個 hidden layer
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-514 參數是共用的，他們共用同一組參數
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-515 那可能前面幾個 layer 是不一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-516 但最後的幾個 layer，必須是共用的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-517 否則有兩個 decoder，不同 domain 的 decoder，它們前面幾個 layer 是共用的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-518 後面幾個 layer 是不一樣
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-519 那這樣的好處是什麼？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-520 這樣的好處是說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-521 因為他們最後幾個 hidden layer 是共用的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-522 也許因為透過最後幾個 hidden layer 是共用這件事
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-523 會讓這兩個 encoder 把 image 壓到同樣的 latent space 的時候
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-524 他們的 latent space 是同一個 latent space
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-525 他們的 latent space 會用同樣的 dimension
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-526 來表示同樣的人臉特徵
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-527 那這樣的技術，被用在 couple GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-528 跟 UNIT 裡面
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-529 那像這種 share 參數的 task，它最極端的狀況就是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-530 這兩個 encoder 共用同一組參數，就是同一個 encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-531 只是在使用的時候吃一個 flag，代表說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-532 現在要 encoder 的 image 是來自於 x domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-533 還是來自於 y domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-534 所以大家知道意思嗎？就是說，同一個 encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-535 然後但是 input 給它 x domain image 的時候，你要順便給它一個數值，比如說 1
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-536 然後 input y domain image 的時候，你給它另外一個數值，比如說 -1
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-537 讓他知道說現在 encode 的是 x domain 還是 y domain 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-538 如果你今天要 share 參數的話
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-539 最極端的狀況就是這樣
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-540 最極端的狀況是兩個 encoder 他們參數完全一樣
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-541 只是給它們不同的 input flag
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-542 讓它們知道現在以後的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-543 不是在同一個 domain 上，是不同的 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-544 這個是第一招，還有很多滿坑滿谷的招式
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-545 比如說有一個就是加一個 domain 的 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-546 那這個概念跟我們剛才在前一堂課講過的 domain adversarial training 是一樣的，其實是一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-547 它的概念是這樣子
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-548 原來 x domain 跟 y domain 都是自己搞自己的東西
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-549 但我們現在再加一個 domain discriminator，這個 domain discriminator 要做的事情是
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-550 給他這個 latent 的 vector
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-551 它去判斷說這個 vector 是來自於 x domain 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-552 還是來自於 y domain 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-553 這個 domain discriminator，要判斷說這個 x
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-554 這個 vector 呢
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-555 它是從 x domain 的 image 來的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-556 還是從 y domain 的 image 來的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-557 然後你的這兩個 encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-558 x domain encoder 跟 y domain encoder，他們的工作
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-559 就是想要去騙過這個 domain 的 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-560 讓 domain 的 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-561 沒辦法憑藉這個 vector 就判斷說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-562 它是來自於 x domain 還是來自於 y domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-563 如果今天domain 的 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-564 無法判斷說這個 vector 是來自於 x domain 和 y domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-565 這樣意味著什麼呢？意味著說，今天
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-566 這個 domain 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-567 跟這個 domain 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-568 它們都變成 code 的時候
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-569 他們的 distribution 都是一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-570 它們的 distribution 是一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-571 那我們今天可以假設說，假設比如說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-572 因為它們的 distribution 是一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-573 也許我們就可以期待同樣的維度
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-574 就代表了同樣的意思，舉例來說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-575 假設真人的照片男女比例是 1:1
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-576 動畫人物的照片，男女比例也是 1:1，但是實際上不是這樣子，我們就假設是 1:1
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-577 因為男女的比例都是 1:1
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-578 最後如果你要讓兩個 domain 的 feature，它的 distribution 一樣
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-579 那你就要用同一個維度
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-580 來存這個男女比例是 1:1 的 feature
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-581 如果是性別都用第一維來存
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-582 這樣他們的 distribution 才會變得一樣
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-583 所以假設你今天的這兩個 domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-584 他們的 attribute 的 distribution  是一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-585 比如說，男女的比例是一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-586 有戴眼鏡跟沒戴眼鏡的比例是一樣的，長髮短髮，比例是一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-587 那你也許期待說，透過 domain discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-588 強迫這兩個 domain 的 embedding latent feature 要是一樣的時候
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-589 那它們就會用同樣的 dimension
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-590 來表示同樣的事情，來表示同樣的 characteristic
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-591 那這個是一招，還有其他的招數
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-592 舉例來說，你也可以用 cycle 的 consistency
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-593 怎麼做呢？把這張 image，透過 x encoder 變成 code
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-594 再透過 y 的 decoder 把它解回來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-595 然後把這張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-596 再丟給 y domain 的 encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-597 再透過 x domain 的 decoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-598 把它解回來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-599 然後希望 input 跟 output 越接近越好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-600 那這個就是 cycle consistency
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-601 那如果把這個技術來跟 cycle GAN 來做比較的話
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-602 我們剛剛說 cycle GAN 就是有兩個 transformation 的 network
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-603 那你的 encoder， xx domain 的 encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-604 加 y domain 的 decoder，他們合起來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-605 就是從 x domain 轉到 y domain
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-606 然後你這邊有一個 discriminator
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-607 確定說這個 image 看起來像不像是 y domain 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-608 接下來，你再從這邊進來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-609 把這張 image，透過 y domain 的 encoder 跟 xx domain 的 decoder，轉回
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-610 原來的 image，那你希望 input 的 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-611 跟 output 的 image，越接近越好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-612 這個跟 cycle GAN 的 training 其實就是一模一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-613 只是原來在 cycle GAN 裡面，我們說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-614 從 x domain 到 y domain generator 就是一個 network
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-615 我們沒有把它特別切成 encoder  跟 decoder，只是在這邊，我們會把它切成
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-616 把 x domain 到 y domain 的 network 切成說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-617 它有一個 x domain 的 encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-618 它有一個 y domain 的 decoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-619 從 y domain 到 x domain 的 network，我們說它有一個 y domain 的 encoder
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-620 它有一個 x domain 的 decoder，network 的架構
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-621 不太一樣，然後中間的那個 latent space 是 shared
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-622 但是實際上它們是 training 的 criteria，其實就是一樣的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-623 那像這樣的技術，就用在 Combo GAN 裡面
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-624 就是有各式各樣的 GAN，這世界上有各式各樣的 GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-625 那還有一個叫做 semantic consistency
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-626 semantic consistency 是這樣，你把一張圖片
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-627 丟進來，然後把它變成 code
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-628 然後接下來，你在把這個 code 用 y domain 的 decoder 把它合回來
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-629 再把  y domain 的 image 丟到 y domain 的 encoder，再把它合回來，那你希望
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-630 透過 x domain encoder 的 encode
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-631 跟 y domain encoder 的 encode，他們的這個 code，要越接近越好
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-632 那這樣的好處是說，我們本來在做 cycle consistency 的時候
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-633 你算的是這張 image
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-634 跟這張 image，算是兩個 image 之間的 similarity
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-635 那如果是 image 和 image 之間的 similarity
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-636 你通常算的是 pixel wise 的 similarity
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-637 你不用考慮 semantic，你看它們表象上像不像
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-638 那如果是在這個 latent 的 space 上面考慮的話
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-639 那你就是算它們的 semantic 像不像
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-640 你算它們的 latent 的 code 像不像
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-641 意思就是說你在算它們的 semantic 像不像
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-642 這個技術可以用在 XGAN 裡面
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-643 就有一個 X GAN，這個東西看起來很像 X
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-644 所以你也可以叫它X GAN
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-645 那其實也可以做到 voice conversion
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-646 voice conversion 是什麼呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-647 voice conversion 就是把 A 的聲音，轉成 B 的 聲音
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-648 那這個技術一點都不稀奇，20 年前阿笠博士就已經做過了
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-649 所以這技術，並沒有什麼稀奇的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-650 但是過去在阿笠博士的時代是怎麼做的呢？
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-651 過去的 voice conversion
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-652 要做的話，你就是要收集兩個人的聲音，假如你要把 A 的聲音，轉成 B 的聲音
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-653 你就要把 A 找來念 50 句話，B 找來也念 50 句話
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-654 讓它們念一樣的句子， A 說 How are you，B 也說 How are you，A 說 Good morning，B 就說 Good morning
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-655 接下來怎麼做？你完全可以想像怎麼做
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-656 learn 一個 model，比如說sequence to sequence model 或是什麼其他的
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-657 吃一個 A 的聲音，然後轉成 B 的聲音
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-658 就結束了，這就是一個 supervised learning problem，對不對
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-659 但這樣的技術，有很大的缺陷
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-660 舉例來說，假如你想要把你的聲音轉成新垣結衣的聲音
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-661 那你就要把新垣結衣請來跟你念一樣的 50 句的句子
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-662 而且我們就算退一萬步說，你真的請到新垣結衣好了
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-663 她其實也不會說中文，所以她沒辦法跟你念同樣的句子
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-664 所以怎麼辦，我們需要用 GAN 的技術
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-665 就我們用今天學到的那些技術
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-666 你就可以在兩堆聲音間，作互轉
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-667 就你只需要收集 speaker A 的聲音
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-668 再收集 speaker B 的聲音
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-669 他們兩個甚至可以說的就是不同的語言
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-670 一個說中文，一個說英文
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-671 用我們剛才講的第二個方法
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-672 做下去，你就可以看看說
GAN_Lecture_3_(2018)_-_Unsupervised_Conditional_Generation-673 你能不能夠把 A 的聲音轉成 B 的聲音
GAN_Lecture_4_(2018)_-_Basic_Theory-0 各位同學大家好我們就來上課吧
GAN_Lecture_4_(2018)_-_Basic_Theory-1 上週我們講了 GAN 的直觀的想法
GAN_Lecture_4_(2018)_-_Basic_Theory-2 今天要來講 GAN 背後的理論
GAN_Lecture_4_(2018)_-_Basic_Theory-3 今天要講的是當初 2014 年 Ian Goodfellow 在 propose GAN 的時候它的講法
GAN_Lecture_4_(2018)_-_Basic_Theory-4 等一下可以仔細聽看看跟我們上週講的 GAN 的直觀的想法裡面有沒有矛盾的地方
GAN_Lecture_4_(2018)_-_Basic_Theory-5 其實是有一些地方還頗矛盾的
GAN_Lecture_4_(2018)_-_Basic_Theory-6 至今仍然沒有好的 solution、好的手法可以解決
GAN_Lecture_4_(2018)_-_Basic_Theory-7 如果大家沒有甚麼問題要問的話就來講一下
GAN_Lecture_4_(2018)_-_Basic_Theory-8 GAN 背後的理論，當初 Ian Goodfellow 是怎麼說的
GAN_Lecture_4_(2018)_-_Basic_Theory-9 這個是上周說的作業 3-1 裡面要讓 GAN 做的事情讓機器看了很多動畫的圖以後
GAN_Lecture_4_(2018)_-_Basic_Theory-10 自己產生二次元人物的頭像
GAN_Lecture_4_(2018)_-_Basic_Theory-11 GAN 要做的就是根據很多 example 自己去進行生成
GAN_Lecture_4_(2018)_-_Basic_Theory-12 所謂的生成到底是甚麼樣的問題
GAN_Lecture_4_(2018)_-_Basic_Theory-13 假設要生成的東西是 image用 x 來代表一張 image
GAN_Lecture_4_(2018)_-_Basic_Theory-14 每一個 image 都是 high dimensional 高維空間中的一個點
GAN_Lecture_4_(2018)_-_Basic_Theory-15 假設產生 64 x 64 的 image它是 64 x 64 維空間中的一個點
GAN_Lecture_4_(2018)_-_Basic_Theory-16 這邊為了畫圖方便假設每一個 x 就是二維空間中的一個點
GAN_Lecture_4_(2018)_-_Basic_Theory-17 雖然實際上它是高維空間中的一個點
GAN_Lecture_4_(2018)_-_Basic_Theory-18 現在要產生的東西比如說要產生 image
GAN_Lecture_4_(2018)_-_Basic_Theory-19 它其實有一個固定的 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-20 這邊寫成 Pdata ( x )
GAN_Lecture_4_(2018)_-_Basic_Theory-21 它有一個固定的 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-22 甚麼意思在這整個 image 的 space 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-23 在這整個 image 所構成的高維空間中
GAN_Lecture_4_(2018)_-_Basic_Theory-24 只有非常少的部分、一小部分
GAN_Lecture_4_(2018)_-_Basic_Theory-25 sample 出來的 image 看起來像是人臉
GAN_Lecture_4_(2018)_-_Basic_Theory-26 在多數的空間中 sample 出來的 image 都不像是人臉
GAN_Lecture_4_(2018)_-_Basic_Theory-27 在這個圖上的例子裡面可能只有藍色的這個區域
GAN_Lecture_4_(2018)_-_Basic_Theory-28 去 sample 出 x、sample 一個 image
GAN_Lecture_4_(2018)_-_Basic_Theory-29 它看起來像是人臉舉例來說在這個地方 sample
GAN_Lecture_4_(2018)_-_Basic_Theory-30 看起來的圖片長的是這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-31 在其他地方 sample，看起來的圖片看起來就不像是人臉
GAN_Lecture_4_(2018)_-_Basic_Theory-32 假設生成的 x 是人臉的話
GAN_Lecture_4_(2018)_-_Basic_Theory-33 它有一個固定的 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-34 這個 distribution 在藍色的這個區域，它的機率是高的
GAN_Lecture_4_(2018)_-_Basic_Theory-35 在藍色的區域以外，它的機率是低的
GAN_Lecture_4_(2018)_-_Basic_Theory-36 要機器做的是甚麼要機器找出這個 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-37 而這個 distribution 到底長甚麼樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-38 實際上是不知道的
GAN_Lecture_4_(2018)_-_Basic_Theory-39 可以蒐集很多的 x 知道 x 可能在某些地方分布比較高
GAN_Lecture_4_(2018)_-_Basic_Theory-40 但是要我們把這個式子找出來
GAN_Lecture_4_(2018)_-_Basic_Theory-41 我們是不知道要怎麼做的
GAN_Lecture_4_(2018)_-_Basic_Theory-42 所以現在 GAN 做的是一個 generative model 做的事情
GAN_Lecture_4_(2018)_-_Basic_Theory-43 就是要找出這個 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-44 在有 GAN 之前怎麼做 generative
GAN_Lecture_4_(2018)_-_Basic_Theory-45 是用 Maximum Likelihood Estimation
GAN_Lecture_4_(2018)_-_Basic_Theory-46 其實 Maximum Likelihood Estimation 在之前 Machine Learning 有講過，這邊只是一個很快地複習
GAN_Lecture_4_(2018)_-_Basic_Theory-47 現在有一個 data 的 distribution 它是 P data ( x )
GAN_Lecture_4_(2018)_-_Basic_Theory-48 這個 distribution 長甚麼樣子就是這個 distribution 它的 formulation 長甚麼樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-49 我們是不知道的
GAN_Lecture_4_(2018)_-_Basic_Theory-50 我們可以從裡面 sample 它
GAN_Lecture_4_(2018)_-_Basic_Theory-51 所謂從這個 distribution sample 它的意思就是
GAN_Lecture_4_(2018)_-_Basic_Theory-52 假設做二次元人物的生成
GAN_Lecture_4_(2018)_-_Basic_Theory-53 那就是從 database 裡面 sample 出 image
GAN_Lecture_4_(2018)_-_Basic_Theory-54 這個就是從這個 distribution 裡面 sample 一些 data 出來
GAN_Lecture_4_(2018)_-_Basic_Theory-55 我們可以 sample 它但我們不知道它長甚麼樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-56 接下來我們要自己去找一個 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-57 這個 distribution 寫成 P 下標 G ( x )
GAN_Lecture_4_(2018)_-_Basic_Theory-58 這個 distribution 是由一組參數 θ 所操控的
GAN_Lecture_4_(2018)_-_Basic_Theory-59 所謂由 θ 所操控的意思是這個 distribution 假設它是一個 Gaussian Mixture Model
GAN_Lecture_4_(2018)_-_Basic_Theory-60 這個 θ 指的就是 Gaussian 的 mean 跟 variance
GAN_Lecture_4_(2018)_-_Basic_Theory-61 我們要去調整 Gaussian 的 mean 跟 variance
GAN_Lecture_4_(2018)_-_Basic_Theory-62 使得我們得到的這個 distribution P 下標 G 跟真實的 distribution P data 越接近越好
GAN_Lecture_4_(2018)_-_Basic_Theory-63 雖然我們不知道 P data 長甚麼樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-64 我們只能夠從 P data 裡面去 sample
GAN_Lecture_4_(2018)_-_Basic_Theory-65 但是我們希望 P 下標 G 可以找一個 θ 讓 P 下標 G 跟 P data 越接近越好
GAN_Lecture_4_(2018)_-_Basic_Theory-66 怎麼做假設用 Maximum Likelihood
GAN_Lecture_4_(2018)_-_Basic_Theory-67 首先可以從 P data sample 一些東西出來
GAN_Lecture_4_(2018)_-_Basic_Theory-68 sample x1, x2 到 xm
GAN_Lecture_4_(2018)_-_Basic_Theory-69 可以 sample 一堆 x 出來
GAN_Lecture_4_(2018)_-_Basic_Theory-70 接下來對每一個 sample 出來的 x
GAN_Lecture_4_(2018)_-_Basic_Theory-71 我們都可以計算它的 likelihood所謂可以計算它的 likelihood 意思是
GAN_Lecture_4_(2018)_-_Basic_Theory-72 假設給定一組參數 θ
GAN_Lecture_4_(2018)_-_Basic_Theory-73 我們就知道 P 下標 G 這個 probability 的 distribution 長甚麼樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-74 我們就可以計算從這個 distribution 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-75 sample 出某一個 xi 的機率可以計算這個 likelihood
GAN_Lecture_4_(2018)_-_Basic_Theory-76 接下來要做的就是，我們要找出一個 θ
GAN_Lecture_4_(2018)_-_Basic_Theory-77 使得 P 下標 G 跟 P data 越接近越好
GAN_Lecture_4_(2018)_-_Basic_Theory-78 這件事情的意思就是說
GAN_Lecture_4_(2018)_-_Basic_Theory-79 我們希望這些從 P data 裡面 sample 出來的 example
GAN_Lecture_4_(2018)_-_Basic_Theory-80 如果是用 P 下標 G 這個 distribution 來產生的話
GAN_Lecture_4_(2018)_-_Basic_Theory-81 它的 likelihood 越大越好
GAN_Lecture_4_(2018)_-_Basic_Theory-82 每一筆 data 它從 P 下標 G 產生的 likelihood
GAN_Lecture_4_(2018)_-_Basic_Theory-83 我們都是可以算出來的
GAN_Lecture_4_(2018)_-_Basic_Theory-84 把所有的機率乘起來
GAN_Lecture_4_(2018)_-_Basic_Theory-85 就得到 total 的 likelihood
GAN_Lecture_4_(2018)_-_Basic_Theory-86 我們希望 total likelihood 越大越好
GAN_Lecture_4_(2018)_-_Basic_Theory-87 怎麼讓 total likelihood 越大越好
GAN_Lecture_4_(2018)_-_Basic_Theory-88 就是要去找一個 θ*，找一組最佳的參數
GAN_Lecture_4_(2018)_-_Basic_Theory-89 它可以去 maximize L 這個值
GAN_Lecture_4_(2018)_-_Basic_Theory-90 舉例來說在這個 case 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-91 假設你是一個 Gaussian Mixture Model
GAN_Lecture_4_(2018)_-_Basic_Theory-92 就可能希望它的 mean 落在這些地方
GAN_Lecture_4_(2018)_-_Basic_Theory-93 它的 variance 長這樣，可能這個 Gaussian Mixture Model 它產生這些 data 的機率
GAN_Lecture_4_(2018)_-_Basic_Theory-94 就是最大的
GAN_Lecture_4_(2018)_-_Basic_Theory-95 這個是 Maximum Likelihood
GAN_Lecture_4_(2018)_-_Basic_Theory-96 我記得我們在 Machine Learning 那門課講解了 Rating Model 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-97 就已經提過這些東西了
GAN_Lecture_4_(2018)_-_Basic_Theory-98 Maximum Likelihood 如果你覺得它沒有很直觀的話
GAN_Lecture_4_(2018)_-_Basic_Theory-99 這邊給你 Maximum Likelihood 的另外一個解釋
GAN_Lecture_4_(2018)_-_Basic_Theory-100 Maximum Likelihood 它等同於 Minimize KL Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-101 甚麼意思我們說在 Maximum Likelihood 裡面我們做的事情就是
GAN_Lecture_4_(2018)_-_Basic_Theory-102 我們從 P data 裡面 sample 出 m 筆 data
GAN_Lecture_4_(2018)_-_Basic_Theory-103 然後給我們一組參數 θ 就可以計算每一個 x 被 sample 出來的機率，我們把這些機率全部乘起來
GAN_Lecture_4_(2018)_-_Basic_Theory-104 這個是我們要去 maximize 的對象
GAN_Lecture_4_(2018)_-_Basic_Theory-105 這個式子可以稍微改變一下就前面取一個 log
GAN_Lecture_4_(2018)_-_Basic_Theory-106 取一個 log 不影響你找出來最好的 θ
GAN_Lecture_4_(2018)_-_Basic_Theory-107 這個大家都可以很快地知道
GAN_Lecture_4_(2018)_-_Basic_Theory-108 你可以把 log 放進去本來是很多項相乘取 log
GAN_Lecture_4_(2018)_-_Basic_Theory-109 等同於每一項取 log 以後相加
GAN_Lecture_4_(2018)_-_Basic_Theory-110 相信對大家來說沒有問題
GAN_Lecture_4_(2018)_-_Basic_Theory-111 這件事情是甚麼意思
GAN_Lecture_4_(2018)_-_Basic_Theory-112 summation over i = 1 到 m
GAN_Lecture_4_(2018)_-_Basic_Theory-113 summation over 第一筆 example 到第 m 筆 example
GAN_Lecture_4_(2018)_-_Basic_Theory-114 這件事情其實就是在 approximate 從 P data 這個 distribution 裡面 sample x 出來
GAN_Lecture_4_(2018)_-_Basic_Theory-115 從 P data 這個 distribution 裡面 sample x 出來
GAN_Lecture_4_(2018)_-_Basic_Theory-116 你就得到 x1, x2 到 xm
GAN_Lecture_4_(2018)_-_Basic_Theory-117 今天這個式子它要 maximize 的對象其實就是 maximize log P 下標 G ( x )
GAN_Lecture_4_(2018)_-_Basic_Theory-118 maximize 這一項的 expected value
GAN_Lecture_4_(2018)_-_Basic_Theory-119 那你這個 expectation distribution 是你要 sample 的 data
GAN_Lecture_4_(2018)_-_Basic_Theory-120 接下來可以把 expectation 這一項
GAN_Lecture_4_(2018)_-_Basic_Theory-121 把它展開，他其實就是一個積分
GAN_Lecture_4_(2018)_-_Basic_Theory-122 這個 x 是從 P data sample 出來的
GAN_Lecture_4_(2018)_-_Basic_Theory-123 從 P data 裡面 sample 出來的意思就是
GAN_Lecture_4_(2018)_-_Basic_Theory-124 對 x 做積分然後把它乘 P data ( x )
GAN_Lecture_4_(2018)_-_Basic_Theory-125 然後乘上 log P 下標 G ( x )
GAN_Lecture_4_(2018)_-_Basic_Theory-126 接下來加一項看起來沒有甚麼用的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-127 在後面加這麼一項
GAN_Lecture_4_(2018)_-_Basic_Theory-128 這一項是甚麼東西你看一下這一項
GAN_Lecture_4_(2018)_-_Basic_Theory-129 這一項裡面只有 P data
GAN_Lecture_4_(2018)_-_Basic_Theory-130 他跟 P 下標 G 是完全沒有任何關係的
GAN_Lecture_4_(2018)_-_Basic_Theory-131 所以加這一項根本不會影響你找出來的最大的 x
GAN_Lecture_4_(2018)_-_Basic_Theory-132 既然加這一項一點用都沒有那為甚麼要加這一項
GAN_Lecture_4_(2018)_-_Basic_Theory-133 加這一項的目的是為了告訴你 Maximum Likelihood 它就是 KL Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-134 因為加上這一項以後就可以把式子做一下整理
GAN_Lecture_4_(2018)_-_Basic_Theory-135 這邊是對 x 做積分乘上 P data(x) 再乘上 log P 下標 G (x)
GAN_Lecture_4_(2018)_-_Basic_Theory-136 這邊是減掉對 x 做積分，P data(x) 乘上 log P data(x)
GAN_Lecture_4_(2018)_-_Basic_Theory-137 所以這邊有積分 P data (x)，這邊有積分 P data (x)
GAN_Lecture_4_(2018)_-_Basic_Theory-138 所以可以把他提出來
GAN_Lecture_4_(2018)_-_Basic_Theory-139 這一項跟這一項可以把它提出來
GAN_Lecture_4_(2018)_-_Basic_Theory-140 這邊就剩下 log P 下標 G 減掉 log P data
GAN_Lecture_4_(2018)_-_Basic_Theory-141 大家一秒鐘就可以反應 log P 下標 G 減掉 log P data 就是 P 下標 G 除以 P data
GAN_Lecture_4_(2018)_-_Basic_Theory-142 總之這個式子他就是 P data 跟 P 下標 G 的 KL Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-143 所以找一個 θ 去 maximize likelihood
GAN_Lecture_4_(2018)_-_Basic_Theory-144 等同於找一個 θ 去 minimize P data  跟 P 下標 G 的 KL Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-145 如果看到這邊覺得數學式太多你沒有看懂的話
GAN_Lecture_4_(2018)_-_Basic_Theory-146 其實只要知道一件事就好
GAN_Lecture_4_(2018)_-_Basic_Theory-147 所謂的 Maximum Likelihood在機器學習裡面講
GAN_Lecture_4_(2018)_-_Basic_Theory-148 我們要找一個 Generative Model 去 Maximum Likelihood
GAN_Lecture_4_(2018)_-_Basic_Theory-149 就 Maximum Likelihood 這件事情就等同於 minimize
GAN_Lecture_4_(2018)_-_Basic_Theory-150 你的 Generative Model 所定義的 distribution P 下標 G 跟現在的 data P data 之間的 KL Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-151 接下來會遇到的問題是假設我們的 P 下標 G 只是一個  Gaussian Mixture Model
GAN_Lecture_4_(2018)_-_Basic_Theory-152 他顯然有非常多的限制
GAN_Lecture_4_(2018)_-_Basic_Theory-153 我們希望 P 下標 G 是一個 general 的 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-154 它可以不是 Gaussian它可以是比 Gaussian 更複雜的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-155 但假設把 P 下標 G 換成比 Gaussian 更複雜的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-156 會遇到甚麼問題呢
GAN_Lecture_4_(2018)_-_Basic_Theory-157 會遇到的問題就是算不出你的 likelihood
GAN_Lecture_4_(2018)_-_Basic_Theory-158 算不出 P 下標 G x given θ 這一項
GAN_Lecture_4_(2018)_-_Basic_Theory-159 假設 P 下標 G(x) 這個東西
GAN_Lecture_4_(2018)_-_Basic_Theory-160 他是一個 Gaussian Mixture Model
GAN_Lecture_4_(2018)_-_Basic_Theory-161 給你一個 x，你可以計算他被 sample 出來的機率
GAN_Lecture_4_(2018)_-_Basic_Theory-162 但是如果它是一個很複雜的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-163 等一下會講他可能是一個 Neural Network
GAN_Lecture_4_(2018)_-_Basic_Theory-164 你就沒有辦法計算它的 likelihood
GAN_Lecture_4_(2018)_-_Basic_Theory-165 所以怎麼辦
GAN_Lecture_4_(2018)_-_Basic_Theory-166 所以就有了一些新的想法
GAN_Lecture_4_(2018)_-_Basic_Theory-167 讓 machine 自動的生成東西比如說做 image generation
GAN_Lecture_4_(2018)_-_Basic_Theory-168 從來都不是新的題目，你可能看最近用 GAN 做了很多 image generation 的 task
GAN_Lecture_4_(2018)_-_Basic_Theory-169 好像 image generation 是這幾年才有的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-170 其實不是，image generation 在八零年代就有人做過了
GAN_Lecture_4_(2018)_-_Basic_Theory-171 那個時候的作法是用 Gaussian Mixture Model
GAN_Lecture_4_(2018)_-_Basic_Theory-172 蒐集很多很多的 image，每一個 image 就是高維中中間一個 data point
GAN_Lecture_4_(2018)_-_Basic_Theory-173 就可以 learn 一個 Gaussian Mixture Model 去 maximize 產生那些 image likelihood
GAN_Lecture_4_(2018)_-_Basic_Theory-174 但如果你看文獻的話
GAN_Lecture_4_(2018)_-_Basic_Theory-175 你看古聖先賢留下來的東西就會發現過去做的如果用 Gaussian Mixture Model 產生出來的 image
GAN_Lecture_4_(2018)_-_Basic_Theory-176 非常非常的糊
GAN_Lecture_4_(2018)_-_Basic_Theory-177 這個可能原因是因為 image 他是高維空間中一個 manifold
GAN_Lecture_4_(2018)_-_Basic_Theory-178 大家可能知道 image 在高維空間中它其實是高維空間中一個低維的 manifold
GAN_Lecture_4_(2018)_-_Basic_Theory-179 所以如果用 Gaussian Mixture Model他其實就不是一個 manifold
GAN_Lecture_4_(2018)_-_Basic_Theory-180 用 Gaussian Mixture Model 不管怎麼調 mean 跟 variance
GAN_Lecture_4_(2018)_-_Basic_Theory-181 他就不像是你的 copy distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-182 所以怎麼做都是做不好
GAN_Lecture_4_(2018)_-_Basic_Theory-183 所以需要用更 generalize 的方式來 learn generation 這件事情
GAN_Lecture_4_(2018)_-_Basic_Theory-184 在這個 GAN 裡面在 Generative Adversarial Network 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-185 generator 就是一個 network我們把他寫作 G
GAN_Lecture_4_(2018)_-_Basic_Theory-186 等一下就會告訴大家一個 network他怎麼被看作是一個 probability 的 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-187 我們知道 generator 就是一個 network
GAN_Lecture_4_(2018)_-_Basic_Theory-188 我們都知道 network 就是一個東西然後 output 一個東西
GAN_Lecture_4_(2018)_-_Basic_Theory-189 舉例來說，input 從某一個 distribution sample 出來的 noise z
GAN_Lecture_4_(2018)_-_Basic_Theory-190 input 一個隨機的 vector z
GAN_Lecture_4_(2018)_-_Basic_Theory-191 然後他就會 output 一個 x這個 x，如果 generator G 看作是一個 function 的話
GAN_Lecture_4_(2018)_-_Basic_Theory-192 這個 x 就是 G(z)
GAN_Lecture_4_(2018)_-_Basic_Theory-193 如果是做 image generation 的話
GAN_Lecture_4_(2018)_-_Basic_Theory-194 那你的 x 就是一個 image
GAN_Lecture_4_(2018)_-_Basic_Theory-195 我們說這個 z 是從某一個 prior distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-196 比如說是從一個 normal distribution sample 出來的
GAN_Lecture_4_(2018)_-_Basic_Theory-197 今天每次 sample 出一個 z
GAN_Lecture_4_(2018)_-_Basic_Theory-198 把他丟到這個 generator 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-199 就會得到一個 x
GAN_Lecture_4_(2018)_-_Basic_Theory-200 sample 不同的 z 得到的 x 就不一樣
GAN_Lecture_4_(2018)_-_Basic_Theory-201 今天這個 z 是從一個 Gaussian Distribution 裡面 sample 出來的
GAN_Lecture_4_(2018)_-_Basic_Theory-202 現在把這些從 Gaussian Distribution 裡面 sample 出來的 z 通通通過 G 得到另外一大堆 sample
GAN_Lecture_4_(2018)_-_Basic_Theory-203 把這些 sample 通通集合起來
GAN_Lecture_4_(2018)_-_Basic_Theory-204 得到的就會是另外一個 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-205 雖然 input 是一個 normal distribution 是一個單純的 Gaussian Distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-206 但是通過 generator 以後
GAN_Lecture_4_(2018)_-_Basic_Theory-207 因為這個 generator 是一個 network
GAN_Lecture_4_(2018)_-_Basic_Theory-208 它可以把這個 z 通過一個非常複雜的轉換把他變成 x
GAN_Lecture_4_(2018)_-_Basic_Theory-209 所以把通過 generator 產生出來的 x 通通集合起來
GAN_Lecture_4_(2018)_-_Basic_Theory-210 它可以是一個非常複雜的 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-211 而這個 distribution 就是我們所謂的 P 下標 G
GAN_Lecture_4_(2018)_-_Basic_Theory-212 有人可能會問這個 Prior Distribution 應該要設成甚麼樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-213 好像文獻上有人會用 Normal Distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-214 有人會用 Uniform Distribution你可以在作業裡面 verify 一下你覺得哪一種比較好
GAN_Lecture_4_(2018)_-_Basic_Theory-215 哪一種比較好我也忘記了
GAN_Lecture_4_(2018)_-_Basic_Theory-216 我覺得這邊其實 Prior Distribution 用哪種 distribution 也許影響並沒有那麼大
GAN_Lecture_4_(2018)_-_Basic_Theory-217 為甚麼？因為 generator 他是一個 network
GAN_Lecture_4_(2018)_-_Basic_Theory-218 我們在開學第一堂課就有講過
GAN_Lecture_4_(2018)_-_Basic_Theory-219 一個 hidden layer 的 network 他就可以 approximate 任何 function
GAN_Lecture_4_(2018)_-_Basic_Theory-220 更何況是有多個 hidden layer 的 network
GAN_Lecture_4_(2018)_-_Basic_Theory-221 它可以 approximate 非常複雜的 function
GAN_Lecture_4_(2018)_-_Basic_Theory-222 所以就算是 input distribution 是一個非常簡單的 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-223 通過了這個 network 以後，他也可以把這個簡單的 distribution 柪成各式各樣不同的形狀
GAN_Lecture_4_(2018)_-_Basic_Theory-224 所以不用擔心這個 input 是一個 normal distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-225 會不會對 output 來說有很大的限制其實不會
GAN_Lecture_4_(2018)_-_Basic_Theory-226 因為通過一個 generator 以後
GAN_Lecture_4_(2018)_-_Basic_Theory-227 可以把一個單純的 normal distribution 凹成各式各樣複雜的形狀
GAN_Lecture_4_(2018)_-_Basic_Theory-228 接下來目標是甚麼
GAN_Lecture_4_(2018)_-_Basic_Theory-229 接下來目標是希望根據這個 generator 所定義出來的 distribution P 下標 G
GAN_Lecture_4_(2018)_-_Basic_Theory-230 他跟我們的目標、跟我們的 data 的 distribution P data 越接近越好
GAN_Lecture_4_(2018)_-_Basic_Theory-231 如果要寫一個 Optimization Formulation 的話這個 formulation 看起來是這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-232 我們要找一個 generator G
GAN_Lecture_4_(2018)_-_Basic_Theory-233 這個 generator 可以讓它所定義出來的 distribution P 下標 G
GAN_Lecture_4_(2018)_-_Basic_Theory-234 跟我們的 data P data 之間的某種 divergence 越小越好
GAN_Lecture_4_(2018)_-_Basic_Theory-235 舉例來說如果是 Maximum Likelihood 的話它就是要 minimize KL Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-236 等一下我們會看到其實在 GAN 裡面minimize 的不是 KL Divergence 而是其他的 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-237 這邊寫一個 Div 就代表說反正他是某一種 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-238 現在要做的就是找一個 generator這個 generator 定義出一個 distribution P 下標 G
GAN_Lecture_4_(2018)_-_Basic_Theory-239 這個 P 下標 G 可以跟我們的 P data 的 Divergence 越接近越好
GAN_Lecture_4_(2018)_-_Basic_Theory-240 再來的問題是怎麼計算這個 Divergence 呢
GAN_Lecture_4_(2018)_-_Basic_Theory-241 假設能夠計算這個 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-242 要找一個 G 去 minimize 這個 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-243 那不就只是 Gradient Descent用 Gradient Descent 就可以做了對不對
GAN_Lecture_4_(2018)_-_Basic_Theory-244 但問題就是要怎麼計算出這個  Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-245 P data 它的 formulation
GAN_Lecture_4_(2018)_-_Basic_Theory-246 我們是不知道的它並不是甚麼 Gaussian Distribution，它是 formulation
GAN_Lecture_4_(2018)_-_Basic_Theory-247 P 下標 G 它的 formulation 我們也是不知道的
GAN_Lecture_4_(2018)_-_Basic_Theory-248 假設 P 下標 G 跟 P data 它的 formulation 我們是知道的
GAN_Lecture_4_(2018)_-_Basic_Theory-249 我們代進 Divergence 的 formulation 裡面我們就可以算出它的 Divergence 是多少
GAN_Lecture_4_(2018)_-_Basic_Theory-250 我們就可以用 Gradient Descent 去 minimize 它的 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-251 問題就是 P 下標 G 跟 P data 它的 formulation 我們是不知道的
GAN_Lecture_4_(2018)_-_Basic_Theory-252 我們根本就不知道要怎麼去計算它的 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-253 所以根本不知道要怎麼找一個 G 去 minimize 它的 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-254 所以怎麼辦這個就是 GAN 神奇的地方
GAN_Lecture_4_(2018)_-_Basic_Theory-255 在進入比較多的數學式之前我們先很直觀的講一下GAN 到底怎麼做到 minimize Divergence 這件事情
GAN_Lecture_4_(2018)_-_Basic_Theory-256 這邊的前提是我們不知道 P 下標 G 跟 P data 的 distribution 長甚麼樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-257 但是我們可以從這兩個 distribution 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-258 去 sample data 出來
GAN_Lecture_4_(2018)_-_Basic_Theory-259 甚麼意思呢
GAN_Lecture_4_(2018)_-_Basic_Theory-260 甚麼叫做從 P data 去 sample distribution 出來呢
GAN_Lecture_4_(2018)_-_Basic_Theory-261 你就是把你的 database 拿出來假設我們做二次元人物頭像生成的話
GAN_Lecture_4_(2018)_-_Basic_Theory-262 然後從裡面 sample 很多 image 出來這個就是從 P data 這個 distribution 裡面做 sample
GAN_Lecture_4_(2018)_-_Basic_Theory-263 怎麼對 P 下標 G 做 sample
GAN_Lecture_4_(2018)_-_Basic_Theory-264 這個 P 下標 G 是由你的 generator 所定義的
GAN_Lecture_4_(2018)_-_Basic_Theory-265 我們在使用這個 generator 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-266 我們是從某個 prior distribution 裡面去 sample 一大堆的 vector，每一個 vector 就會產生一張 image
GAN_Lecture_4_(2018)_-_Basic_Theory-267 所謂的從 P 下標 G 裡面做 sample
GAN_Lecture_4_(2018)_-_Basic_Theory-268 其實就是 random sample 一個 vector
GAN_Lecture_4_(2018)_-_Basic_Theory-269 把這個 vector 丟到 generator 裡面產生一張 image
GAN_Lecture_4_(2018)_-_Basic_Theory-270 這個就是從 P 下標 G 裡面做 sample
GAN_Lecture_4_(2018)_-_Basic_Theory-271 所以我們可以從 P data 裡面做 sample
GAN_Lecture_4_(2018)_-_Basic_Theory-272 我們也可以從 P 下標 G 裡面做 sample
GAN_Lecture_4_(2018)_-_Basic_Theory-273 接下來的問題是我們可以從 P 下標 G 和 P data 做 sample
GAN_Lecture_4_(2018)_-_Basic_Theory-274 根據這個 sample 我們要怎麼知道這兩個 distribution 的 Divergence 呢
GAN_Lecture_4_(2018)_-_Basic_Theory-275 GAN 神奇的地方就是透過 discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-276 我們可以量這兩個 distribution 間的 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-277 假設藍色的星星是從 P data 裡面 sample 出來的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-278 紅色的星星是從 P 下標 G sample 出來的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-279 根據這些 data 我們去訓練一個 discriminator上週我們已經講過訓練 discriminator 意思就是
GAN_Lecture_4_(2018)_-_Basic_Theory-280 給 P data 的分數越大越好
GAN_Lecture_4_(2018)_-_Basic_Theory-281 給 P 下標 G 的分數越小越好你去訓練一個 discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-282 這個訓練的結果就會告訴我們 P data 跟 P 下標 G 他們之間的 Divergence 有多大
GAN_Lecture_4_(2018)_-_Basic_Theory-283 上週講過訓練一個 discriminator我們怎麼訓練 discriminator 呢
GAN_Lecture_4_(2018)_-_Basic_Theory-284 我們會寫一個 Objective Function D
GAN_Lecture_4_(2018)_-_Basic_Theory-285 這個 Objective Function 它跟兩項有關一個是跟 generator 有關
GAN_Lecture_4_(2018)_-_Basic_Theory-286 一個是跟 discriminator 有關
GAN_Lecture_4_(2018)_-_Basic_Theory-287 在 train discriminator 的時候我們會 fix 住 generator
GAN_Lecture_4_(2018)_-_Basic_Theory-288 所以 G 這一項是 fix 住的
GAN_Lecture_4_(2018)_-_Basic_Theory-289 我們只是調這個 discriminator 參數想辦法去 maximize 後面這一項
GAN_Lecture_4_(2018)_-_Basic_Theory-290 後面這一像是甚麼意思呢
GAN_Lecture_4_(2018)_-_Basic_Theory-291 後面這項意思是 x 是從 P data 裡面 sample 出來的
GAN_Lecture_4_(2018)_-_Basic_Theory-292 我們希望 log D( x ) 越大越好
GAN_Lecture_4_(2018)_-_Basic_Theory-293 也就是我們希望 discriminator 的 output假設 x 是從 P data 裡面 sample 出來的
GAN_Lecture_4_(2018)_-_Basic_Theory-294 我們就希望 D ( x ) 越大越好
GAN_Lecture_4_(2018)_-_Basic_Theory-295 反之假設 x 是從 generator sample 出來的
GAN_Lecture_4_(2018)_-_Basic_Theory-296 因為要 maximize V 這一項所以要 maximize 第一項，也要 maximize 第二項
GAN_Lecture_4_(2018)_-_Basic_Theory-297 如果 x 是從 P 下標 G 裡面 sample 出來的
GAN_Lecture_4_(2018)_-_Basic_Theory-298 那我們要 maximize log ( 1 - D ( x ) )
GAN_Lecture_4_(2018)_-_Basic_Theory-299 就是要 maximize 1 - D ( x ) )也就是要 minimize D ( x )
GAN_Lecture_4_(2018)_-_Basic_Theory-300 如果 x 是從 P 下標 G 裡面 sample 出來的
GAN_Lecture_4_(2018)_-_Basic_Theory-301 我們就希望他的值越小越好然後在訓練的時候就是要找一個 D
GAN_Lecture_4_(2018)_-_Basic_Theory-302 它可以 maximize 這個 Objective Function
GAN_Lecture_4_(2018)_-_Basic_Theory-303 如果你之前 Machine Learning 有學通的話
GAN_Lecture_4_(2018)_-_Basic_Theory-304 下面這個 optimization 的式子跟 train 一個 Binary Classifier 的式子
GAN_Lecture_4_(2018)_-_Basic_Theory-305 其實是完全一模一樣的
GAN_Lecture_4_(2018)_-_Basic_Theory-306 假設今天要 train 一個 Logistic Regression 的 model
GAN_Lecture_4_(2018)_-_Basic_Theory-307 Logistic Regression Model 是一個 Binary Classifier
GAN_Lecture_4_(2018)_-_Basic_Theory-308 然後就把 P data 當作是 class 1
GAN_Lecture_4_(2018)_-_Basic_Theory-309 把 P 下標 G 當作是 class 2
GAN_Lecture_4_(2018)_-_Basic_Theory-310 然後 train 一個 Logistic Regression Model
GAN_Lecture_4_(2018)_-_Basic_Theory-311 你會發現你的 Objective Function 其實就是下面這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-312 所以這個 discriminator 在做的事情跟一個 Binary Classifier 在做的事情其實是一模一樣的
GAN_Lecture_4_(2018)_-_Basic_Theory-313 假設藍色的點是 class 1紅色的點是 class 2
GAN_Lecture_4_(2018)_-_Basic_Theory-314 discriminator 就是一個 Binary Classifier
GAN_Lecture_4_(2018)_-_Basic_Theory-315 train 下去，然後這個 Binary Classifier 它是在 minimize Cross Entropy
GAN_Lecture_4_(2018)_-_Basic_Theory-316 我們之前講過 Binary Classifier 是要 minimize Cross Entropy 不能 minimize Mean Square Error
GAN_Lecture_4_(2018)_-_Basic_Theory-317 要 minimize Cross Entropy如果你 minimize Cross Entropy 的話
GAN_Lecture_4_(2018)_-_Basic_Theory-318 你其實就是在解這個 optimization 的 problem
GAN_Lecture_4_(2018)_-_Basic_Theory-319 這邊神奇的地方是當我們解完這個 optimization 的 problem 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-320 你最後會得到一個最小的 loss
GAN_Lecture_4_(2018)_-_Basic_Theory-321 或者是得到最大的 objective value
GAN_Lecture_4_(2018)_-_Basic_Theory-322 我們今天這邊不是 minimize loss而是 maximize 一個 Objective Function
GAN_Lecture_4_(2018)_-_Basic_Theory-323 這個 V 是我們的 Objective Value
GAN_Lecture_4_(2018)_-_Basic_Theory-324 我們要調 D 去 maximize 這個 Objective Value
GAN_Lecture_4_(2018)_-_Basic_Theory-325 然後這邊神奇的地方是
GAN_Lecture_4_(2018)_-_Basic_Theory-326 這個 maximize Objective Value就是把這個 D train 到最好，給了這些 data
GAN_Lecture_4_(2018)_-_Basic_Theory-327 把這個 D train 到最好，找出最大的 D 可以達到的 Objective Value
GAN_Lecture_4_(2018)_-_Basic_Theory-328 這個 value 其實會跟 J-S Divergence 是有非常密切關係
GAN_Lecture_4_(2018)_-_Basic_Theory-329 等一下會有更詳細的證明
GAN_Lecture_4_(2018)_-_Basic_Theory-330 你可以說這個結果它其實就是 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-331 也許你聽了覺得很神奇但我們可以給你一個非常非常值觀的解釋
GAN_Lecture_4_(2018)_-_Basic_Theory-332 你一秒鐘就知道為甚麼這個東西它應該跟某一個 Divergence 是有關係的
GAN_Lecture_4_(2018)_-_Basic_Theory-333 你想想看，假設現在 sample 出來的 data 他們靠得很近
GAN_Lecture_4_(2018)_-_Basic_Theory-334 這個藍色的這些星星跟紅色的星星如果把他們視成兩個類別的話
GAN_Lecture_4_(2018)_-_Basic_Theory-335 他們靠得很近
GAN_Lecture_4_(2018)_-_Basic_Theory-336 對一個 Binary Classifier 來說
GAN_Lecture_4_(2018)_-_Basic_Theory-337 它很難區別紅色的星星跟藍色的星星的不同
GAN_Lecture_4_(2018)_-_Basic_Theory-338 因為對一個 Binary Classifier 也就是 discriminator 來說
GAN_Lecture_4_(2018)_-_Basic_Theory-339 他很難區別這兩個類別的不同所以直接 train 下去
GAN_Lecture_4_(2018)_-_Basic_Theory-340 loss 就沒有辦法壓低
GAN_Lecture_4_(2018)_-_Basic_Theory-341 假設這兩個 class 他們靠得很近他們很難分別
GAN_Lecture_4_(2018)_-_Basic_Theory-342 training data 上的 loss 就會壓不下去
GAN_Lecture_4_(2018)_-_Basic_Theory-343 或是反過來說在 training data 上的 loss 壓不下去
GAN_Lecture_4_(2018)_-_Basic_Theory-344 就是我們剛才看到的 Objective Value
GAN_Lecture_4_(2018)_-_Basic_Theory-345 沒有辦法把他拉得很高
GAN_Lecture_4_(2018)_-_Basic_Theory-346 沒有辦法把剛才看到的 V 這一項
GAN_Lecture_4_(2018)_-_Basic_Theory-347 沒有辦法找到一個 D 他讓 V 的值變得很大
GAN_Lecture_4_(2018)_-_Basic_Theory-348 這個時候意味著時麼
GAN_Lecture_4_(2018)_-_Basic_Theory-349 這個時候意味著這兩堆 data
GAN_Lecture_4_(2018)_-_Basic_Theory-350 他們是非常接近的
GAN_Lecture_4_(2018)_-_Basic_Theory-351 他們的 Divergence 是小的
GAN_Lecture_4_(2018)_-_Basic_Theory-352 所以如果對一個 discriminator 來說很難分別這兩種 data 之間的不同
GAN_Lecture_4_(2018)_-_Basic_Theory-353 他很難達到很大的 Objective Value
GAN_Lecture_4_(2018)_-_Basic_Theory-354 那意味著這兩堆 data 的 Divergence 是小的
GAN_Lecture_4_(2018)_-_Basic_Theory-355 所以最後你可以達到最好的 Objective Value
GAN_Lecture_4_(2018)_-_Basic_Theory-356 跟 Divergence 是會有非朝緊密的關係的
GAN_Lecture_4_(2018)_-_Basic_Theory-357 這是一樣的例子
GAN_Lecture_4_(2018)_-_Basic_Theory-358 假設藍色的星星跟紅色的星星他們距離很遠
GAN_Lecture_4_(2018)_-_Basic_Theory-359 他們有很大的 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-360 對 discriminator 來說他就可以輕易地分辨這兩堆 data 的不同
GAN_Lecture_4_(2018)_-_Basic_Theory-361 也就是說它可以輕易的讓你的 Objective Value，V 的這個 value 變得很大
GAN_Lecture_4_(2018)_-_Basic_Theory-362 當 V 的 value 可以變得很大的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-363 意味著這兩堆 data，從 P data 裡面 generate 出來的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-364 和從 P 下標 G generate 出來的東西他們的 Divergence 是大的
GAN_Lecture_4_(2018)_-_Basic_Theory-365 所以 discriminator 就可以輕易地分辨它的不同
GAN_Lecture_4_(2018)_-_Basic_Theory-366 discriminator 就可以輕易的 maximize Objective Value
GAN_Lecture_4_(2018)_-_Basic_Theory-367 在我們進入數學之前
GAN_Lecture_4_(2018)_-_Basic_Theory-368 在這邊先稍微停一下有沒有同學有問題要問的呢
GAN_Lecture_4_(2018)_-_Basic_Theory-369 如果沒有問題要問的話
GAN_Lecture_4_(2018)_-_Basic_Theory-370 接下來就是實際證明給你看為甚麼 Objective Value
GAN_Lecture_4_(2018)_-_Basic_Theory-371 跟 Divergence 是有關係的
GAN_Lecture_4_(2018)_-_Basic_Theory-372 再來就是一堆數學式
GAN_Lecture_4_(2018)_-_Basic_Theory-373 這個數學式是這樣
GAN_Lecture_4_(2018)_-_Basic_Theory-374 Objective Value 就寫在右上角他長這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-375 現在要做的事情是找一個 discriminator D
GAN_Lecture_4_(2018)_-_Basic_Theory-376 它可以 maximize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-377 G 是固定的要 maximize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-378 怎麼 maximize 這個式子我們把他展開
GAN_Lecture_4_(2018)_-_Basic_Theory-379 把本來 expectation 換成積分
GAN_Lecture_4_(2018)_-_Basic_Theory-380 從 P data sample x 出來
GAN_Lecture_4_(2018)_-_Basic_Theory-381 就變成對 x 做積分乘上 P data ( x )
GAN_Lecture_4_(2018)_-_Basic_Theory-382 乘上 log D ( x )
GAN_Lecture_4_(2018)_-_Basic_Theory-383 這邊是從 P 下標 G sample x 出來就是對 x 做積分
GAN_Lecture_4_(2018)_-_Basic_Theory-384 乘上 P 下標 G 乘上 log ( 1 - D ( x ) )我想這個對大家來說都不成問題
GAN_Lecture_4_(2018)_-_Basic_Theory-385 接下來就整理一下，因為這邊都對 x 做積分，所以把放在積分裡面的這一項根放在積分裡面的這一項加起來
GAN_Lecture_4_(2018)_-_Basic_Theory-386 接下來要想辦法找一個 D 它可以讓這個式子越大越好
GAN_Lecture_4_(2018)_-_Basic_Theory-387 這邊前提有一個假設是 D ( x ) 它可以是任何的 function
GAN_Lecture_4_(2018)_-_Basic_Theory-388 但實際上不見得是成立的因為假設 D ( x ) 是一個 network
GAN_Lecture_4_(2018)_-_Basic_Theory-389 network 除非他的 neural 無窮多不然他也沒有辦法變成任何的 function
GAN_Lecture_4_(2018)_-_Basic_Theory-390 除非 neural 是無窮多的
GAN_Lecture_4_(2018)_-_Basic_Theory-391 不然也沒有辦法變成任何的 function
GAN_Lecture_4_(2018)_-_Basic_Theory-392 這邊作一個非常強的假設
GAN_Lecture_4_(2018)_-_Basic_Theory-393 是假設 D ( x ) 它可以是任意的 function
GAN_Lecture_4_(2018)_-_Basic_Theory-394 假設 D ( x ) 可以是任意的 function 的話
GAN_Lecture_4_(2018)_-_Basic_Theory-395 就是 input 一個 x 他的值可以是任何的值
GAN_Lecture_4_(2018)_-_Basic_Theory-396 沒有限制你可以 assign 給 D ( x ) 任何的值
GAN_Lecture_4_(2018)_-_Basic_Theory-397 你可以 assign 給 D ( x1 ) 任何的值你可以 assign 給 D ( x2 ) 任何的值
GAN_Lecture_4_(2018)_-_Basic_Theory-398 這個時候假設你要 maximize 這一項的話
GAN_Lecture_4_(2018)_-_Basic_Theory-399 他實際上等同於甚麼等同於，這邊對 x 做積分
GAN_Lecture_4_(2018)_-_Basic_Theory-400 把中括號裡面的式子代各個不同的 x
GAN_Lecture_4_(2018)_-_Basic_Theory-401 再把它通通加起來這就是積分在做的事情
GAN_Lecture_4_(2018)_-_Basic_Theory-402 但你要 maximize 這一項然後 D ( x ) 又可以代任何的值
GAN_Lecture_4_(2018)_-_Basic_Theory-403 意思就是說可以把各個不同的 x 通通都分開算
GAN_Lecture_4_(2018)_-_Basic_Theory-404 所以今天要找一個 D 去 maximize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-405 意思就是說就把某一個 x 拿出來
GAN_Lecture_4_(2018)_-_Basic_Theory-406 這邊是 P data ( x ) * log D ( x )
GAN_Lecture_4_(2018)_-_Basic_Theory-407 加上 P 下標 G ( x ) * log ( 1 - D ( x ) )把某一個 x 拿出來
GAN_Lecture_4_(2018)_-_Basic_Theory-408 然後要找一個 D 它可以讓這個式子越大越好
GAN_Lecture_4_(2018)_-_Basic_Theory-409 所有不同的 x 通通都分開來算
GAN_Lecture_4_(2018)_-_Basic_Theory-410 因為所有的 x 都是沒有任何關係的因為不管是哪一個 x
GAN_Lecture_4_(2018)_-_Basic_Theory-411 你都可以 assign 給他一個不同的 D ( x )
GAN_Lecture_4_(2018)_-_Basic_Theory-412 所以積分裡面的每一項都分開來算
GAN_Lecture_4_(2018)_-_Basic_Theory-413 你就可以分開為他找一個最好的 D ( x )
GAN_Lecture_4_(2018)_-_Basic_Theory-414 講道這邊大家有沒有問題要問
GAN_Lecture_4_(2018)_-_Basic_Theory-415 如果你可以接受這個想法的話
GAN_Lecture_4_(2018)_-_Basic_Theory-416 再來的問題是怎麼找一個 D 讓這一項最大
GAN_Lecture_4_(2018)_-_Basic_Theory-417 這個數學其實是滿簡單的
GAN_Lecture_4_(2018)_-_Basic_Theory-418 現在的問題是找一個 D 讓這個式子最大
GAN_Lecture_4_(2018)_-_Basic_Theory-419 P data 是固定的
GAN_Lecture_4_(2018)_-_Basic_Theory-420 P 下標 G 也是固定的
GAN_Lecture_4_(2018)_-_Basic_Theory-421 唯一要做的事情就是找一個 D ( x ) 的值讓這個式子算起來最大
GAN_Lecture_4_(2018)_-_Basic_Theory-422 等一下為了計算方便把 P data 用 a 來表示
GAN_Lecture_4_(2018)_-_Basic_Theory-423 D ( x ) 用大 D 來表示，P 下標 G 用小 b 來表示D ( x ) 用大 D 來表示
GAN_Lecture_4_(2018)_-_Basic_Theory-424 整個式子寫起來就是這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-425 怎麼找一個 D 去 maximize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-426 你就求一下他的 gradient，求一下他的微分
GAN_Lecture_4_(2018)_-_Basic_Theory-427 然後先找出他的 Critical Point，就找出微分是 0 的地方
GAN_Lecture_4_(2018)_-_Basic_Theory-428 把這一項做微分以後得到什麼樣的結果
GAN_Lecture_4_(2018)_-_Basic_Theory-429 a * log ( D ) 對 D 做微分 = a * ( 1 / D )
GAN_Lecture_4_(2018)_-_Basic_Theory-430 b * log ( 1 - D ) 對 D 做微分得到的結果是b * 1 / ( 1 - D ) * ( -1 )
GAN_Lecture_4_(2018)_-_Basic_Theory-431 然後要找微分是 0 的地方
GAN_Lecture_4_(2018)_-_Basic_Theory-432 把式子整理一下，把 -1 這一項拿到右邊去所以 -1 就沒有了
GAN_Lecture_4_(2018)_-_Basic_Theory-433 a * 1 / D* = b * 1 / ( 1 - D* )
GAN_Lecture_4_(2018)_-_Basic_Theory-434 接下來就要求一下 D* 是多少
GAN_Lecture_4_(2018)_-_Basic_Theory-435 要求一下什麼樣的 D 可以讓這個微分的值是 0
GAN_Lecture_4_(2018)_-_Basic_Theory-436 什麼樣的 D 他是 Critical Point
GAN_Lecture_4_(2018)_-_Basic_Theory-437 就整理一下，這邊都很簡單的數學
GAN_Lecture_4_(2018)_-_Basic_Theory-438 這邊是 a * ( 1 - D* ) = b * D*
GAN_Lecture_4_(2018)_-_Basic_Theory-439 把 a 乘進去，得到 a - aD*  = bD*
GAN_Lecture_4_(2018)_-_Basic_Theory-440 整理一下，把有 D* 項挪到同一邊去
GAN_Lecture_4_(2018)_-_Basic_Theory-441 所以變成 a = ( a + b ) D*
GAN_Lecture_4_(2018)_-_Basic_Theory-442 D* = a / ( a + b )
GAN_Lecture_4_(2018)_-_Basic_Theory-443 所以知道假設 D* = a / ( a + b )a 就是這一項
GAN_Lecture_4_(2018)_-_Basic_Theory-444 b 就是這一項a 就是 P data，b 就是 P 下標 G
GAN_Lecture_4_(2018)_-_Basic_Theory-445 假設 D* ( x ) 的值是P data ( x ) / ( P data ( x ) + P 下標 G ( x ) )
GAN_Lecture_4_(2018)_-_Basic_Theory-446 他就可以讓這一項的對 D 的微分為 0
GAN_Lecture_4_(2018)_-_Basic_Theory-447 你可以輕易的檢查它現在它是一個 Local Maximum
GAN_Lecture_4_(2018)_-_Basic_Theory-448 而不是 Local Minimum 也不是 Saddle Point
GAN_Lecture_4_(2018)_-_Basic_Theory-449 你可以輕易的檢查他是一個 maximum 不是一個 minimum 也不是一個 Saddle Point
GAN_Lecture_4_(2018)_-_Basic_Theory-450 所以要怎麼讓這一項最大
GAN_Lecture_4_(2018)_-_Basic_Theory-451 你就把 D ( x ) 代這樣的值就可以讓這一項最大
GAN_Lecture_4_(2018)_-_Basic_Theory-452 所以接下來要做的事情是甚麼
GAN_Lecture_4_(2018)_-_Basic_Theory-453 接下來要做的事情就是把這一項代到這個式子裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-454 我們想知道當我們找出最好的 D
GAN_Lecture_4_(2018)_-_Basic_Theory-455 也就是 D* 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-456 Objective Function 的值到底長甚麼樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-457 展開來就會發現這一項其實就是 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-458 我們知道 D* 的 formulation 就寫成這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-459 把 D* 代到這裏面去
GAN_Lecture_4_(2018)_-_Basic_Theory-460 把 D* ( x ) 用 P data ( x ) / ( P data ( x ) + P 下標 G ( x ) )來表示
GAN_Lecture_4_(2018)_-_Basic_Theory-461 把這一項放進來
GAN_Lecture_4_(2018)_-_Basic_Theory-462 得到的結果就長這樣
GAN_Lecture_4_(2018)_-_Basic_Theory-463 這邊有 expectation，把 expectation 展開
GAN_Lecture_4_(2018)_-_Basic_Theory-464 這邊是對 P data sample x
GAN_Lecture_4_(2018)_-_Basic_Theory-465 就對 x 做積分這邊是對 P 下標 G sample x，就對 P 下標 G 做積分
GAN_Lecture_4_(2018)_-_Basic_Theory-466 得到的式子長這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-467 接下來為了要把整理成看起來像是 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-468 所以就做一些好像甚麼事都沒有做的變換
GAN_Lecture_4_(2018)_-_Basic_Theory-469 就把分子跟分母都同除 2
GAN_Lecture_4_(2018)_-_Basic_Theory-470 接下來可以把 1/2 這一項把它提出來
GAN_Lecture_4_(2018)_-_Basic_Theory-471 得到的結果是兩倍的 log * (1/2)
GAN_Lecture_4_(2018)_-_Basic_Theory-472 這個如果你沒有跟上的話，不太重要
GAN_Lecture_4_(2018)_-_Basic_Theory-473 反正這邊有 1/2 可以把它提出來變成 2 * log ( 1/2 )或等於 -2 * log2
GAN_Lecture_4_(2018)_-_Basic_Theory-474 接下來我們就知道假設已經有一個 generator G
GAN_Lecture_4_(2018)_-_Basic_Theory-475 找到了最佳的 discriminator 也就是 D*
GAN_Lecture_4_(2018)_-_Basic_Theory-476 D* 的 function 會長這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-477 當我們找到最佳的 D* 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-478 V 這個 Objective Function 他就寫成這個看起來很複雜的樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-479 這個是 -2 * log2 + 對 x 做積分 P data ( x ) * log( P data ( x ) 除上 P data ( x ) 跟 P 下標 G ( x ) 的平均
GAN_Lecture_4_(2018)_-_Basic_Theory-480 第二項這個是積分 x
GAN_Lecture_4_(2018)_-_Basic_Theory-481 P 下標 G ( x ) * log ( P 下標 G ( x ) 除掉 P 下標 G 和 P data 的平均 )
GAN_Lecture_4_(2018)_-_Basic_Theory-482 後面這一項是甚麼
GAN_Lecture_4_(2018)_-_Basic_Theory-483 後面這一項是 P data 跟二分之一 P data 跟 PG 的平均的 KL Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-484 如果你不知道它是甚麼的話沒有關係反正後面這兩項合起來
GAN_Lecture_4_(2018)_-_Basic_Theory-485 就叫做 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-486 或許你比較常聽過 KL Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-487 或者是 Inverse KL Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-488 不常聽過 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-489 但是反正後面這個式子它就是 P data 跟 PG 的某一種 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-490 如果 P data 跟 PG 他們距離的越遠
GAN_Lecture_4_(2018)_-_Basic_Theory-491 這兩項合起來就越大反之他們合起來就越小
GAN_Lecture_4_(2018)_-_Basic_Theory-492 總之現在得到的結論就是如果找到一個最佳的 D*
GAN_Lecture_4_(2018)_-_Basic_Theory-493 代進去以後得到的這個 value
GAN_Lecture_4_(2018)_-_Basic_Theory-494 它是 -2 * log2 + 2JSD ( P data || PG )
GAN_Lecture_4_(2018)_-_Basic_Theory-495 假設 learn 一個 discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-496 寫出了某一個 Objective Function
GAN_Lecture_4_(2018)_-_Basic_Theory-497 去 maximize 那個 Objective Function 以後
GAN_Lecture_4_(2018)_-_Basic_Theory-498 得到的結果，最後可以 maximize 的那個 Objective Function
GAN_Lecture_4_(2018)_-_Basic_Theory-499 最後 maximize 的那個 value
GAN_Lecture_4_(2018)_-_Basic_Theory-500 其實就是 P data 跟 PG 的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-501 假設這邊的數學推導沒有辦法跟上的話
GAN_Lecture_4_(2018)_-_Basic_Theory-502 也沒有關係反正這邊想要告訴你的事情是
GAN_Lecture_4_(2018)_-_Basic_Theory-503 當我們在 train 一個 discriminator 的時候我們想要做的事情是甚麼
GAN_Lecture_4_(2018)_-_Basic_Theory-504 當我們 train 一個 discriminator 的時候我們想做的事情就是去 evaluate
GAN_Lecture_4_(2018)_-_Basic_Theory-505 P data 跟 PG 這兩個 distribution sample 出來的 data
GAN_Lecture_4_(2018)_-_Basic_Theory-506 這兩個 distribution 他們之間的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-507 如果定的 Objective Function 是跟前面的式子一樣的話
GAN_Lecture_4_(2018)_-_Basic_Theory-508 你就是在量 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-509 等一下會看到如果把那個 Objective Function 寫的不一樣
GAN_Lecture_4_(2018)_-_Basic_Theory-510 你就可以量其他的各種不同的 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-511 現在整個問題變成這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-512 本來要找一個 G 他去 minimize PG 跟 P data 的 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-513 我們說後面這個式子你沒有辦法算
GAN_Lecture_4_(2018)_-_Basic_Theory-514 但現在是可以算的
GAN_Lecture_4_(2018)_-_Basic_Theory-515 後面這個式子到底是甚麼
GAN_Lecture_4_(2018)_-_Basic_Theory-516 後面這個式子就是我們寫出一個 Objective Function
GAN_Lecture_4_(2018)_-_Basic_Theory-517 V ( D, G )找一個 D 去 maximize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-518 它就是 PG 和 P data 之間的 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-519 所以我們可以把 Divergence 這一項用 max 這一項把它替換掉
GAN_Lecture_4_(2018)_-_Basic_Theory-520 變成這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-521 所以我們要找一個 generator
GAN_Lecture_4_(2018)_-_Basic_Theory-522 generate 出來的東西跟你的 data 越接近越好
GAN_Lecture_4_(2018)_-_Basic_Theory-523 實際上要解這樣一個 optimization problem
GAN_Lecture_4_(2018)_-_Basic_Theory-524 要解一個 min max 的 problem
GAN_Lecture_4_(2018)_-_Basic_Theory-525 這個 optimization problem 裡面它有一個 minimum
GAN_Lecture_4_(2018)_-_Basic_Theory-526 它有一個 maximum，看起來非常的複雜
GAN_Lecture_4_(2018)_-_Basic_Theory-527 這個非常的拗口，你要找一個 generator 他要去 minimize 這一個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-528 在這個式子它有一個 discriminator，discriminator 要去 maximize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-529 我知道這個看起來非常的複雜
GAN_Lecture_4_(2018)_-_Basic_Theory-530 它實際上做的事情像是以下這個例子所講的這樣
GAN_Lecture_4_(2018)_-_Basic_Theory-531 假設世界上只有三個 generator
GAN_Lecture_4_(2018)_-_Basic_Theory-532 假設要選一個 generator 去 minimize 這個 Objective Function
GAN_Lecture_4_(2018)_-_Basic_Theory-533 但是可以選的 generator 總共只有三個
GAN_Lecture_4_(2018)_-_Basic_Theory-534 一個是 G1 一個是 G2 一個是 G3
GAN_Lecture_4_(2018)_-_Basic_Theory-535 假設選了 G1 這個 generator 的話那 V ( G1, D )
GAN_Lecture_4_(2018)_-_Basic_Theory-536 它長這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-537 假設 generator 固定是 G1
GAN_Lecture_4_(2018)_-_Basic_Theory-538 但是你的 discriminator 它可以是不同的 discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-539 假設 discriminator 它可以用一個參數來操控
GAN_Lecture_4_(2018)_-_Basic_Theory-540 所以這個橫坐標在改變的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-541 代表選擇了不同的 discriminator 但實際上是更複雜的狀況
GAN_Lecture_4_(2018)_-_Basic_Theory-542 實際上 discriminator 是一個 Neural Network
GAN_Lecture_4_(2018)_-_Basic_Theory-543 它是由數百萬個參數所控制的
GAN_Lecture_4_(2018)_-_Basic_Theory-544 它應該是分布在一個高維空間裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-545 不過這邊為了簡化問題我們就說，假設當橫坐標變化的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-546 代表你選擇了不同的 discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-547 今天同樣的假設你的 generator 是 G2
GAN_Lecture_4_(2018)_-_Basic_Theory-548 把 G2 代進去，選擇不同 discriminator 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-549 就得到了不同的 V ( G2, D )
GAN_Lecture_4_(2018)_-_Basic_Theory-550 假設 generator 是 G3，選了不同的 discriminator 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-551 就得到不同的 V ( G3, D )
GAN_Lecture_4_(2018)_-_Basic_Theory-552 接下來的問題是我們在給定一個 generator 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-553 我們要找一個 discriminator 它可以讓 V ( G, D ) 最大
GAN_Lecture_4_(2018)_-_Basic_Theory-554 假設固定 generator 是 G1 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-555 哪一個 discriminator 可以讓 V ( G, D ) 最大
GAN_Lecture_4_(2018)_-_Basic_Theory-556 那個是落在這裡的這個 discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-557 可以讓 V ( G1, D ) 最大
GAN_Lecture_4_(2018)_-_Basic_Theory-558 落在這裡的 discriminator 可以讓 V ( G2, D ) 最大
GAN_Lecture_4_(2018)_-_Basic_Theory-559 落在這裡的 discriminator 可以讓 V ( G3, D ) 最大
GAN_Lecture_4_(2018)_-_Basic_Theory-560 接下來要找一個 G 去 minimize 最大的那個 discriminator 可以找到的 value
GAN_Lecture_4_(2018)_-_Basic_Theory-561 大家聽得懂嗎這有點拗口我們找一個 G
GAN_Lecture_4_(2018)_-_Basic_Theory-562 它可以最小化最大的那個 discriminator 得到的 value
GAN_Lecture_4_(2018)_-_Basic_Theory-563 找一個 G 它可以 minimize V ( G, D )
GAN_Lecture_4_(2018)_-_Basic_Theory-564 用最大的 D 可以達到的 value
GAN_Lecture_4_(2018)_-_Basic_Theory-565 這個時候假設 V ( G, D ) 長的就像是下面這個例子的話
GAN_Lecture_4_(2018)_-_Basic_Theory-566 那這個問題的 solution
GAN_Lecture_4_(2018)_-_Basic_Theory-567 最好的 G 到底應該是哪一個
GAN_Lecture_4_(2018)_-_Basic_Theory-568 給你十秒鐘的時間想一下
GAN_Lecture_4_(2018)_-_Basic_Theory-569 等一下按投影片已經不小心按到了所以你應該知道答案是甚麼
GAN_Lecture_4_(2018)_-_Basic_Theory-570 十秒鐘的時間過了
GAN_Lecture_4_(2018)_-_Basic_Theory-571 假設這是我們 optimization problemV ( G, D ) 就是長這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-572 現在要解這個 optimization problem哪一個 G 才是我們的 solution 呢
GAN_Lecture_4_(2018)_-_Basic_Theory-573 G 只有三個，你只有三個選擇
GAN_Lecture_4_(2018)_-_Basic_Theory-574 所以隨便猜也有 1/3 的正確率
GAN_Lecture_4_(2018)_-_Basic_Theory-575 你覺得 G1 是正確選擇的同學舉手一下
GAN_Lecture_4_(2018)_-_Basic_Theory-576 你覺得 G1 它可以 minimize 上面這個式子的同學舉手一下
GAN_Lecture_4_(2018)_-_Basic_Theory-577 你覺得 G2 它可以 minimize 上面這個式子的同學舉手一下
GAN_Lecture_4_(2018)_-_Basic_Theory-578 你覺得 G3 它可以 minimize 上面這個式子的同學舉手一下
GAN_Lecture_4_(2018)_-_Basic_Theory-579 手放下，多數人都選擇 G3，沒錯，正確答案就是 G3
GAN_Lecture_4_(2018)_-_Basic_Theory-580 所以如果你選擇 G3 的話代表你了解前面加 min 後面又加 max 到底是甚麼樣的意思
GAN_Lecture_4_(2018)_-_Basic_Theory-581 現在找出來的 G* 就是 G3
GAN_Lecture_4_(2018)_-_Basic_Theory-582 當我們給定一個 G1 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-583 這邊這個 D1* 的這個高其實就代表了 G1 的 generator 它所 generate 出來的 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-584 跟 P data 之間的距離
GAN_Lecture_4_(2018)_-_Basic_Theory-585 所以這個高就代表 G2 這個 generator 它所定義的 distribution 跟 data 間的距離
GAN_Lecture_4_(2018)_-_Basic_Theory-586 所以這個高就代表 G3 這個 generator 它的 distribution 跟 data 間的距離
GAN_Lecture_4_(2018)_-_Basic_Theory-587 所以 G1 這個 generator
GAN_Lecture_4_(2018)_-_Basic_Theory-588 它跟 data 的 Divergence 是這麼大
GAN_Lecture_4_(2018)_-_Basic_Theory-589 G2 它所定義的 distribution 跟 data 之間的 Divergence 是這麼大
GAN_Lecture_4_(2018)_-_Basic_Theory-590 G3 它所定義的 distribution 跟 data 之間的 Divergence 是這麼大
GAN_Lecture_4_(2018)_-_Basic_Theory-591 今天要 minimize Divergence 所以會選擇 G3 當作是最好的結果
GAN_Lecture_4_(2018)_-_Basic_Theory-592 接下來就是要想辦法解這個 min max 的 problem
GAN_Lecture_4_(2018)_-_Basic_Theory-593 這個 min max 的 problem 怎麼解
GAN_Lecture_4_(2018)_-_Basic_Theory-594 你記得我們在講 GAN 的時候我們說 GAN 是怎麼做的
GAN_Lecture_4_(2018)_-_Basic_Theory-595 GAN 是說有一個 generator 有一個 discriminator然後在 train 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-596 是 iterative 去 train generator 跟 discriminator固定住 generator
GAN_Lecture_4_(2018)_-_Basic_Theory-597 去 update discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-598 固定住 discriminator 接下來去 update generator
GAN_Lecture_4_(2018)_-_Basic_Theory-599 如果以下等一下要講的東西你聽不太懂的話你就知道一件事情
GAN_Lecture_4_(2018)_-_Basic_Theory-600 GAN 的這個演算法就是在解這個 min max problem
GAN_Lecture_4_(2018)_-_Basic_Theory-601 為甚麼要解 min max problem
GAN_Lecture_4_(2018)_-_Basic_Theory-602 解這個 min max problem 的目的就是要 minimize generator 跟你的 data 之間的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-603 如果你的 V 寫得像之前的投影片放的那個樣子就是 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-604 你可以寫別的樣子那你就是 minimize 其他的 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-605 為甚麼下面這個 algorithm 是在解這一個 optimization problem
GAN_Lecture_4_(2018)_-_Basic_Theory-606 以下投影片就要解釋為甚麼這一個 algorithm
GAN_Lecture_4_(2018)_-_Basic_Theory-607 是在解這個 optimization problem
GAN_Lecture_4_(2018)_-_Basic_Theory-608 假設要解這個 optimization problem 的話你要怎麼做
GAN_Lecture_4_(2018)_-_Basic_Theory-609 這個 max D V ( G, D )找一個 D 去 maximize V ( G, D ) 看起來有點複雜
GAN_Lecture_4_(2018)_-_Basic_Theory-610 把它用 L ( G ) 來取代
GAN_Lecture_4_(2018)_-_Basic_Theory-611 它其實跟 D 是沒有關係的
GAN_Lecture_4_(2018)_-_Basic_Theory-612 given 一個 G 就會找到最好的 D 讓 L ( G ) 的值越大越好
GAN_Lecture_4_(2018)_-_Basic_Theory-613 讓 V ( G, D ) 的值越大越好
GAN_Lecture_4_(2018)_-_Basic_Theory-614 假設最大的值就是 L ( G )
GAN_Lecture_4_(2018)_-_Basic_Theory-615 現在整個問題就變成要找一個最好的 generator G
GAN_Lecture_4_(2018)_-_Basic_Theory-616 它可以 minimize L(G) 這個問題任何人都會
GAN_Lecture_4_(2018)_-_Basic_Theory-617 他就跟 train 一般的 network 是一樣的
GAN_Lecture_4_(2018)_-_Basic_Theory-618 就是用 Gradient Descent 來解它
GAN_Lecture_4_(2018)_-_Basic_Theory-619 你對 L(G) 算一個 gradient
GAN_Lecture_4_(2018)_-_Basic_Theory-620 找一個 G 去 minimize L(G)
GAN_Lecture_4_(2018)_-_Basic_Theory-621 就把 θG 對 L(G) 的 gradient然後用 Gradient Descent 去 update θG
GAN_Lecture_4_(2018)_-_Basic_Theory-622 就可以 minimize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-623 就可以 optimize 這個 optimization problem
GAN_Lecture_4_(2018)_-_Basic_Theory-624 找出 G*
GAN_Lecture_4_(2018)_-_Basic_Theory-625 現在有一個麻煩的地方就是 L(G) 如果把式子寫出來的話它長的是這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-626 這個裡面是有 max 的
GAN_Lecture_4_(2018)_-_Basic_Theory-627 有人就會問這個式子裡面有 max 它可以微分嗎
GAN_Lecture_4_(2018)_-_Basic_Theory-628 會不會不能夠微分因為要算 L(G) 的 gradient
GAN_Lecture_4_(2018)_-_Basic_Theory-629 L(G) 這個式子裡面如果有 max
GAN_Lecture_4_(2018)_-_Basic_Theory-630 它會不會沒有辦法做微分
GAN_Lecture_4_(2018)_-_Basic_Theory-631 其實不會，因為想想看
GAN_Lecture_4_(2018)_-_Basic_Theory-632 我們之前有學到一個 Maxout Network
GAN_Lecture_4_(2018)_-_Basic_Theory-633 Maxout Network 裡面也有 max operation
GAN_Lecture_4_(2018)_-_Basic_Theory-634 但他顯然是有辦法用 Gradient Descent 解
GAN_Lecture_4_(2018)_-_Basic_Theory-635 到底實際上是怎麼做的呢
GAN_Lecture_4_(2018)_-_Basic_Theory-636 假設有一個 function f(x)
GAN_Lecture_4_(2018)_-_Basic_Theory-637 它裡面是有 max operation
GAN_Lecture_4_(2018)_-_Basic_Theory-638 來看一下有 max operation 的這種 function
GAN_Lecture_4_(2018)_-_Basic_Theory-639 要怎麼對它作微分
GAN_Lecture_4_(2018)_-_Basic_Theory-640 有一個 f(x) 它裡面有 max operation
GAN_Lecture_4_(2018)_-_Basic_Theory-641 它是對 f1(x), f2(x) 和 f3(x) 這三個 function 裡面找一個最大的出來，它就是 f(x)
GAN_Lecture_4_(2018)_-_Basic_Theory-642 假設 f1(x) 長這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-643 f2(x) 長這個樣子f3(x) 長這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-644 這個 f(x) 它是 f1, f2, f3 取大的那個
GAN_Lecture_4_(2018)_-_Basic_Theory-645 f(x) 長甚麼樣子呢
GAN_Lecture_4_(2018)_-_Basic_Theory-646 f(x) 就是長這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-647 如果 f1 比 f2, f3 大的話就取 f1
GAN_Lecture_4_(2018)_-_Basic_Theory-648 如果 f2 比 f1, f3 都大的話就取 f2
GAN_Lecture_4_(2018)_-_Basic_Theory-649 如果 f3 是最大的話就取 f3
GAN_Lecture_4_(2018)_-_Basic_Theory-650 這個 f(x) 它長的會是這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-651 我滑鼠移動的位子，它長的是這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-652 如果現在要把 f(x) 對 x 做微分的話
GAN_Lecture_4_(2018)_-_Basic_Theory-653 這件事情等同於甚麼
GAN_Lecture_4_(2018)_-_Basic_Theory-654 這件事情等同於看看現在的 x
GAN_Lecture_4_(2018)_-_Basic_Theory-655 可以讓哪一個 function f1, f2, f3 最大
GAN_Lecture_4_(2018)_-_Basic_Theory-656 就拿最大的那個出來算微分
GAN_Lecture_4_(2018)_-_Basic_Theory-657 就是 x 對 f(x) 的微分
GAN_Lecture_4_(2018)_-_Basic_Theory-658 這樣可能有點抽象就舉很具體的例子
GAN_Lecture_4_(2018)_-_Basic_Theory-659 假設要在這個地方
GAN_Lecture_4_(2018)_-_Basic_Theory-660 對 f(x) 做微分
GAN_Lecture_4_(2018)_-_Basic_Theory-661 算出來的微分值多少
GAN_Lecture_4_(2018)_-_Basic_Theory-662 算出來的微分就等同於是在對 f1 做微分
GAN_Lecture_4_(2018)_-_Basic_Theory-663 如果在這個區域
GAN_Lecture_4_(2018)_-_Basic_Theory-664 算出來的微分就等同於在對 f2 做微分
GAN_Lecture_4_(2018)_-_Basic_Theory-665 如果在這個區域算出來的微分就等同於是在對 f3 做微分
GAN_Lecture_4_(2018)_-_Basic_Theory-666 假如你的這個 function 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-667 有一個 max operation
GAN_Lecture_4_(2018)_-_Basic_Theory-668 實際上在算微分的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-669 你只是看現在在 f1, f2, f3 裡面哪一個人最大
GAN_Lecture_4_(2018)_-_Basic_Theory-670 就把最大的那個人拿出來算微分
GAN_Lecture_4_(2018)_-_Basic_Theory-671 你就可以用 Gradient Descent 去 optimize 這樣的式子
GAN_Lecture_4_(2018)_-_Basic_Theory-672 假設你要用 Gradient Descent 去 optimize 這個 f(x)
GAN_Lecture_4_(2018)_-_Basic_Theory-673 實際上作法就是假設 initialize 的時候在這個地方
GAN_Lecture_4_(2018)_-_Basic_Theory-674 算一下它的 gradient 就像右移
GAN_Lecture_4_(2018)_-_Basic_Theory-675 如果像右移的時候，已經移到另外一個 region
GAN_Lecture_4_(2018)_-_Basic_Theory-676 本來是在這個 region 裡面是 f1 最大
GAN_Lecture_4_(2018)_-_Basic_Theory-677 跑到這邊以後就變成 f2 最大
GAN_Lecture_4_(2018)_-_Basic_Theory-678 假設你移到另外一個 region
GAN_Lecture_4_(2018)_-_Basic_Theory-679 這個 region 是 f2 最大
GAN_Lecture_4_(2018)_-_Basic_Theory-680 這個時候算出來的微分
GAN_Lecture_4_(2018)_-_Basic_Theory-681 就會變成是 f2 的微分
GAN_Lecture_4_(2018)_-_Basic_Theory-682 以此類推
GAN_Lecture_4_(2018)_-_Basic_Theory-683 總之剛才講那麼多只想要告訴你
GAN_Lecture_4_(2018)_-_Basic_Theory-684 就算是 Objective Function 裡面有 max operation
GAN_Lecture_4_(2018)_-_Basic_Theory-685 你也不需要害怕
GAN_Lecture_4_(2018)_-_Basic_Theory-686 你一樣是可以對他做微分的
GAN_Lecture_4_(2018)_-_Basic_Theory-687 所以就回到現在要解的這個 optimization problem
GAN_Lecture_4_(2018)_-_Basic_Theory-688 實際上會怎麼解它呢
GAN_Lecture_4_(2018)_-_Basic_Theory-689 剛才有講說如果一個 optimization problem 裡面你的  Loss Function 是有 max 的
GAN_Lecture_4_(2018)_-_Basic_Theory-690 要怎麼解它呢
GAN_Lecture_4_(2018)_-_Basic_Theory-691 你會先看現在你落在哪一個 region 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-692 再算現在那個 region 裡面哪一個 function 是 max 的對 max 的 function 做微分
GAN_Lecture_4_(2018)_-_Basic_Theory-693 你現在初始的在這個地方你就看
GAN_Lecture_4_(2018)_-_Basic_Theory-694 這個 function 它是最大的就對它做微分
GAN_Lecture_4_(2018)_-_Basic_Theory-695 在 update 參數以後要重新檢查一下你落在哪個 region
GAN_Lecture_4_(2018)_-_Basic_Theory-696 發現你落在這個中間 f2 這個 function 是最大的 region，你就要對 f2 做微分
GAN_Lecture_4_(2018)_-_Basic_Theory-697 把它套用到這個要 optimize min max problem 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-698 其實也是一樣的
GAN_Lecture_4_(2018)_-_Basic_Theory-699 一開始有一個初始的 G0
GAN_Lecture_4_(2018)_-_Basic_Theory-700 接下來要算 G0 對 L(G) 的 gradient
GAN_Lecture_4_(2018)_-_Basic_Theory-701 但是在算 G0 對 L(G) 的 gradient 之前
GAN_Lecture_4_(2018)_-_Basic_Theory-702 因為 L(G) 裡面有 max
GAN_Lecture_4_(2018)_-_Basic_Theory-703 所以不知道 L(G) 長甚麼樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-704 所以要把 max D 找出來
GAN_Lecture_4_(2018)_-_Basic_Theory-705 所以假設在 given G0 前提之下
GAN_Lecture_4_(2018)_-_Basic_Theory-706 哪一個 D 可以讓 L(G) 最大
GAN_Lecture_4_(2018)_-_Basic_Theory-707 它是 D0*，D0* 可以讓 V( G0, D) 最大
GAN_Lecture_4_(2018)_-_Basic_Theory-708 如果這個 D 代 D0* 的話
GAN_Lecture_4_(2018)_-_Basic_Theory-709 就可以得到 L(G)
GAN_Lecture_4_(2018)_-_Basic_Theory-710 這件事情可以用 Gradient Ascent 就可以找出這個 D
GAN_Lecture_4_(2018)_-_Basic_Theory-711 它可以 maximize 這個 Objective Function
GAN_Lecture_4_(2018)_-_Basic_Theory-712 找到 D 可以 maximize 這個 Objective Function 以後
GAN_Lecture_4_(2018)_-_Basic_Theory-713 之後這個東西
GAN_Lecture_4_(2018)_-_Basic_Theory-714 就是 L(G)
GAN_Lecture_4_(2018)_-_Basic_Theory-715 接下來就可以把 θG 對這一項算 gradient
GAN_Lecture_4_(2018)_-_Basic_Theory-716 就可以 update 參數
GAN_Lecture_4_(2018)_-_Basic_Theory-717 就得到新的 generator G1
GAN_Lecture_4_(2018)_-_Basic_Theory-718 update G 的參數
GAN_Lecture_4_(2018)_-_Basic_Theory-719 就得到新的 generator G1
GAN_Lecture_4_(2018)_-_Basic_Theory-720 有新的 generator G1 以後
GAN_Lecture_4_(2018)_-_Basic_Theory-721 就要重新找一下最好的 D
GAN_Lecture_4_(2018)_-_Basic_Theory-722 你可能本來的 region 在這邊
GAN_Lecture_4_(2018)_-_Basic_Theory-723 最好的 D 是這個
GAN_Lecture_4_(2018)_-_Basic_Theory-724 但是現在移動 G1 以後
GAN_Lecture_4_(2018)_-_Basic_Theory-725 你可能已經進入下一個 region 所以你要重新找一個最好的 D
GAN_Lecture_4_(2018)_-_Basic_Theory-726 現在把這個 generator 從 G0 update 到 G1
GAN_Lecture_4_(2018)_-_Basic_Theory-727 可以讓這個 V(G1, D) 最大的那個 D 假設是 D1*
GAN_Lecture_4_(2018)_-_Basic_Theory-728 接下來就有一個新的 Objective Function
GAN_Lecture_4_(2018)_-_Basic_Theory-729 它是 V(G, D1*) 再把它對 generator 算 gradient 再 update generator
GAN_Lecture_4_(2018)_-_Basic_Theory-730 就得到 G2
GAN_Lecture_4_(2018)_-_Basic_Theory-731 這個 operation 就是有一個 G0
GAN_Lecture_4_(2018)_-_Basic_Theory-732 找一個可以讓 V(G0, D) 最大的 D0*
GAN_Lecture_4_(2018)_-_Basic_Theory-733 就得到 V 的 function 然後讓他對 G 做微分
GAN_Lecture_4_(2018)_-_Basic_Theory-734 再重新去找一個新的 D
GAN_Lecture_4_(2018)_-_Basic_Theory-735 再重新對 Objective Function 做微分
GAN_Lecture_4_(2018)_-_Basic_Theory-736 就會發現這整個 process 其實跟 GAN 是一模一樣的
GAN_Lecture_4_(2018)_-_Basic_Theory-737 你可以把它想成現在在找 D0*
GAN_Lecture_4_(2018)_-_Basic_Theory-738 去 maximize 這個 Objective Function 的 process
GAN_Lecture_4_(2018)_-_Basic_Theory-739 其實就是在量 P* 跟 PG0 的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-740 當我們找到一個 G1，找到一個 D1* 它可以讓這個 Objective Function 的值變 maximum
GAN_Lecture_4_(2018)_-_Basic_Theory-741 其實就是在計算 P data 跟 PG1 的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-742 而下面這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-743 這一項就是你的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-744 你要 update 你的 generator 去 minimize J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-745 這個時候你其實就是在減少你的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-746 就是在達成你的目標就是要去 minimize 你的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-747 但是這邊打了一個問號
GAN_Lecture_4_(2018)_-_Basic_Theory-748 這邊為甚麼打一個問號
GAN_Lecture_4_(2018)_-_Basic_Theory-749 因為這件事情未必等同於真的在 minimize J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-750 為甚麼這麼說，因為我們有說過
GAN_Lecture_4_(2018)_-_Basic_Theory-751 假設給你一個 generator
GAN_Lecture_4_(2018)_-_Basic_Theory-752 這裡的 generator 就是 G0
GAN_Lecture_4_(2018)_-_Basic_Theory-753 那你的 V( G0, D) 假設他長這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-754 找到一個 D0*，這個 D0* 的值
GAN_Lecture_4_(2018)_-_Basic_Theory-755 就是 G0 跟你的 data 之間的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-756 但是當你 update 你的 G0 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-757 變成 G1 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-758 這個時候 V( G1, D) 它的 function 可能就會變了
GAN_Lecture_4_(2018)_-_Basic_Theory-759 本來 V(G0, D) 是這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-760 V(G0, D0*) 就是 G0 跟你的 data 的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-761 今天你 update 你的 G0 變成 G1
GAN_Lecture_4_(2018)_-_Basic_Theory-762 這個時候整個 function 就變了
GAN_Lecture_4_(2018)_-_Basic_Theory-763 這個時候因為 G0* 仍然是固定的
GAN_Lecture_4_(2018)_-_Basic_Theory-764 所以 V( G1, D0* ) 他就不是在 evaluate J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-765 我們說 evaluate J-S Divergence 的 D 是今天這個V( G, D ) 這個值裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-766 最大的那一個
GAN_Lecture_4_(2018)_-_Basic_Theory-767 所以當你的 G 變了
GAN_Lecture_4_(2018)_-_Basic_Theory-768 你的這個 function 就變了
GAN_Lecture_4_(2018)_-_Basic_Theory-769 當你的 function 變的時候同樣的 D 就不是在 evaluate 你的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-770 如果在這個例子裡面J-S Divergence 會變成是這個值
GAN_Lecture_4_(2018)_-_Basic_Theory-771 而不是這個值
GAN_Lecture_4_(2018)_-_Basic_Theory-772 但是為甚麼我們又說這一項可以看作是在減少 J-S Divergence 呢
GAN_Lecture_4_(2018)_-_Basic_Theory-773 這邊作的前提假設就是這兩個式子可能是非常像的
GAN_Lecture_4_(2018)_-_Basic_Theory-774 假設只 update 一點點的 G 從 G0 變到 G1
GAN_Lecture_4_(2018)_-_Basic_Theory-775 G 的參數只動了一點點
GAN_Lecture_4_(2018)_-_Basic_Theory-776 那這兩個 function 他們的長相可能是比較像的
GAN_Lecture_4_(2018)_-_Basic_Theory-777 所以因為他們的長相仍是比較像的
GAN_Lecture_4_(2018)_-_Basic_Theory-778 所以一樣用 D0* 你仍然是在量 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-779 這樣的情形就這邊本來值很小
GAN_Lecture_4_(2018)_-_Basic_Theory-780 突然變很高的情形可能是不會發生的
GAN_Lecture_4_(2018)_-_Basic_Theory-781 因為 G0 跟 G1 是很像的所以這兩個 function 應該是比較接近
GAN_Lecture_4_(2018)_-_Basic_Theory-782 所以你可以只同樣用固定的 D0*
GAN_Lecture_4_(2018)_-_Basic_Theory-783 就可以 evaluate G0 跟 G1 的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-784 所以在 train GAN 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-785 它的 tip 就是因為你有這個假設
GAN_Lecture_4_(2018)_-_Basic_Theory-786 就是 G0 跟 G1 應該是比較像的
GAN_Lecture_4_(2018)_-_Basic_Theory-787 所以在 train generator 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-788 你就不能夠一次 update 太多
GAN_Lecture_4_(2018)_-_Basic_Theory-789 但是在 train discriminator 的時候，理論上應該把它 train 到底
GAN_Lecture_4_(2018)_-_Basic_Theory-790 應該把它 update 多次一點
GAN_Lecture_4_(2018)_-_Basic_Theory-791 因為在量 discriminator 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-792 你必須要找到 maximum 的值你才是在量 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-793 所以 train discriminator 的時候你其實會需要比較多的 iteration 把它 train 到底
GAN_Lecture_4_(2018)_-_Basic_Theory-794 但是 generator 的話
GAN_Lecture_4_(2018)_-_Basic_Theory-795 你應該只要跑比較少的 iteration
GAN_Lecture_4_(2018)_-_Basic_Theory-796 免得今天現在在這個投影片上講的假設是不成立的
GAN_Lecture_4_(2018)_-_Basic_Theory-797 接下來講一下實際上在做 GAN 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-798 其實是怎麼做的
GAN_Lecture_4_(2018)_-_Basic_Theory-799 我們的 Objective Function 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-800 要對 x 取 expectation
GAN_Lecture_4_(2018)_-_Basic_Theory-801 但是在實際上沒有辦法真的算 expectation
GAN_Lecture_4_(2018)_-_Basic_Theory-802 所以都是用 sample 來代替 expectation
GAN_Lecture_4_(2018)_-_Basic_Theory-803 本來是對 P data sample x 取它的 expectation
GAN_Lecture_4_(2018)_-_Basic_Theory-804 實際上在做的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-805 就是從 P data 這個 distribution 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-806 sample m 筆 data
GAN_Lecture_4_(2018)_-_Basic_Theory-807 把這 m 筆 data 的 D(x) 通通算出來然後再把它通通平均起來
GAN_Lecture_4_(2018)_-_Basic_Theory-808 就當作是 expectation
GAN_Lecture_4_(2018)_-_Basic_Theory-809 一樣，本來要從 PG 裡面 sample 一堆 x 出來算它的 expectation，但在實作上沒有辦法這麼做
GAN_Lecture_4_(2018)_-_Basic_Theory-810 所以實作上你的行為是從 PG 裡面 sample 出 m 筆 data
GAN_Lecture_4_(2018)_-_Basic_Theory-811 再把這 m 筆 data 都去計算它的 log ( 1 - D( x tilde ))然後把它全部平均起來，就得到這一項的 approximation
GAN_Lecture_4_(2018)_-_Basic_Theory-812 實際上在做的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-813 我們就是在 maximize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-814 而不是真的去 maximize 它的 expectation
GAN_Lecture_4_(2018)_-_Basic_Theory-815 這個式子如果你回去檢查看看的話
GAN_Lecture_4_(2018)_-_Basic_Theory-816 剛才講過這件事情就等同於是在 train 一個 Binary Classifier
GAN_Lecture_4_(2018)_-_Basic_Theory-817 所以在實作 GAN 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-818 你完全不需要用原來不知道的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-819 你在 train discriminator 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-820 你就是在 train 一個 Binary Classifier
GAN_Lecture_4_(2018)_-_Basic_Theory-821 實際在做的時候是怎樣
GAN_Lecture_4_(2018)_-_Basic_Theory-822 discriminator 是一個 Binary Classifier
GAN_Lecture_4_(2018)_-_Basic_Theory-823 這個 Binary Classifier 它是一個 Logistic Regression
GAN_Lecture_4_(2018)_-_Basic_Theory-824 它的 output 有接一個 sigmoid
GAN_Lecture_4_(2018)_-_Basic_Theory-825 所以它 output 的值是介於 0 到 1 之間的
GAN_Lecture_4_(2018)_-_Basic_Theory-826 然後從 P data 裡面 sample m 筆 data 出來
GAN_Lecture_4_(2018)_-_Basic_Theory-827 這 m 筆 data 就當作是 positive example 或是 class 1 的 example
GAN_Lecture_4_(2018)_-_Basic_Theory-828 然後從 PG 裡面再 sample 另外 m 筆 data 出來
GAN_Lecture_4_(2018)_-_Basic_Theory-829 這 m 筆 data 就當作是 negative example就當作是 class 2 的 example
GAN_Lecture_4_(2018)_-_Basic_Theory-830 接下來就 train 你的 Binary Classifier
GAN_Lecture_4_(2018)_-_Basic_Theory-831 train 一個 criterion 會 minimize Cross Entropy
GAN_Lecture_4_(2018)_-_Basic_Theory-832 會發現 minimize Cross Entropy，你把你的式子寫出來
GAN_Lecture_4_(2018)_-_Basic_Theory-833 它會等同於上面 maximize 這個 Objective Function
GAN_Lecture_4_(2018)_-_Basic_Theory-834 minimize Cross Entropy 等同於 maximize 上面這個 Objective Function
GAN_Lecture_4_(2018)_-_Basic_Theory-835 最後就再重新複習一次 GAN 的 algorithm
GAN_Lecture_4_(2018)_-_Basic_Theory-836 整個 GAN 的 algorithm 是這樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-837 有一個 θd，有一個 θg
GAN_Lecture_4_(2018)_-_Basic_Theory-838 這個是 initialize parameter
GAN_Lecture_4_(2018)_-_Basic_Theory-839 在每一個 iteration 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-840 你做的事情是甚麼你從 P data 這個 distribution 裡面 sample m 筆 example
GAN_Lecture_4_(2018)_-_Basic_Theory-841 你從某一個 prior distribution，它可以是 Gaussian Distribution 可以是 Normal Distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-842 其實沒有那麼重要，你就固定住某一個 prior distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-843 從這個 prior distribution 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-844 去 sample 出 m 個 vector，這些 vector 用 z 來表示
GAN_Lecture_4_(2018)_-_Basic_Theory-845 根據這些 vector z 把它丟到 generator 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-846 他們會產生一堆 image 也就是 x tilde
GAN_Lecture_4_(2018)_-_Basic_Theory-847 所以產生一堆 x tilde
GAN_Lecture_4_(2018)_-_Basic_Theory-848 這邊有一堆 real 的 image
GAN_Lecture_4_(2018)_-_Basic_Theory-849 這邊有一堆 generated 的 image
GAN_Lecture_4_(2018)_-_Basic_Theory-850 接下來要 train discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-851 這個 discriminator 在 training 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-852 它就是 maximize 下面這個 Objective Function
GAN_Lecture_4_(2018)_-_Basic_Theory-853 但我們剛才有講過 maximize 下面這個 Objective Function 其實就等同於
GAN_Lecture_4_(2018)_-_Basic_Theory-854 直接 train 一個 Binary Classifier，把這邊的 x 當作是一類，把這邊的 x tilde 當作是另外一類
GAN_Lecture_4_(2018)_-_Basic_Theory-855 直接 train 一個 Binary Classifier，你的 Binary Classifier 是 minimum Cross Entropy
GAN_Lecture_4_(2018)_-_Basic_Theory-856 就是在 maximize 下面這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-857 train 了一個 discriminator 出來
GAN_Lecture_4_(2018)_-_Basic_Theory-858 我們之前有講過我們 train discriminator 的目的是甚麼
GAN_Lecture_4_(2018)_-_Basic_Theory-859 是為了要 evaluate J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-860 而甚麼時候 discriminator 可以 evaluate J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-861 當它可以讓你的 V 的值最大的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-862 那個 discriminator 才是在 evaluate J-S divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-863 你要讓第一個值被 maximize 他才是在 evaluate J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-864 為了要讓這個 V 的值最大
GAN_Lecture_4_(2018)_-_Basic_Theory-865 你一定要 train 很多次train 到收斂為止
GAN_Lecture_4_(2018)_-_Basic_Theory-866 它才能讓 V 的值最大，對不對
GAN_Lecture_4_(2018)_-_Basic_Theory-867 但在實作上你沒有辦法真的 train 很多次train 到收斂為止
GAN_Lecture_4_(2018)_-_Basic_Theory-868 但是你會說，我今天 train d 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-869 我要反覆 k 次，這個參數要 update k 次
GAN_Lecture_4_(2018)_-_Basic_Theory-870 而不是像投影片上面只寫 update 一次而已你可能會 update 三次或五次才停止
GAN_Lecture_4_(2018)_-_Basic_Theory-871 這個步驟是在解這個問題
GAN_Lecture_4_(2018)_-_Basic_Theory-872 找一個 D 它可以 maximize V(G, D)
GAN_Lecture_4_(2018)_-_Basic_Theory-873 但是其實你沒有辦法真的找到一個最好的 D 去 maximize V(G, D)
GAN_Lecture_4_(2018)_-_Basic_Theory-874 你能夠找的其實只是一個 lower bound 而已
GAN_Lecture_4_(2018)_-_Basic_Theory-875 因為這邊通常在實作的時候你沒有辦法真的 train 到收斂
GAN_Lecture_4_(2018)_-_Basic_Theory-876 你沒有辦法真的一直 train，train 到說可以讓 V(G, D) 變的最大
GAN_Lecture_4_(2018)_-_Basic_Theory-877 通常就是 train 幾步然後就停下來
GAN_Lecture_4_(2018)_-_Basic_Theory-878 就算我們退一萬步說這邊可以一直 traintrain 到收斂
GAN_Lecture_4_(2018)_-_Basic_Theory-879 你其實也未必真的能夠 maximize 這個 Objective Function
GAN_Lecture_4_(2018)_-_Basic_Theory-880 因為在 train 的時候，D 的 capacity 並不是無窮大的
GAN_Lecture_4_(2018)_-_Basic_Theory-881 你會 train train train 然後就卡在一個 Local Minimum 然後就結束了
GAN_Lecture_4_(2018)_-_Basic_Theory-882 你並不真的可以 maximize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-883 再退一萬步說假設沒有 Local Maximum、Local Minimum 的問題
GAN_Lecture_4_(2018)_-_Basic_Theory-884 你可以直接解這個問題
GAN_Lecture_4_(2018)_-_Basic_Theory-885 你的 D 它的 capacity 也是有限
GAN_Lecture_4_(2018)_-_Basic_Theory-886 記得我們說過如果要量 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-887 一個假設是 D 可以是任何 function
GAN_Lecture_4_(2018)_-_Basic_Theory-888 事實上 D 是一個 network 所以它也不是任何 function
GAN_Lecture_4_(2018)_-_Basic_Theory-889 所以你沒有辦法真的 maximize 這一項
GAN_Lecture_4_(2018)_-_Basic_Theory-890 你能夠找到的只是一個 lower bound 而已但我們就假設你可以 maximize 這一項就是了
GAN_Lecture_4_(2018)_-_Basic_Theory-891 接下來要 train generator
GAN_Lecture_4_(2018)_-_Basic_Theory-892 我們說 train discriminator 是為了量 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-893 train generator 的時候是為了要 minimize J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-894 為了要減少 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-895 前面這個步驟是在量出 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-896 接下來下一個步驟是在減少 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-897 怎麼做
GAN_Lecture_4_(2018)_-_Basic_Theory-898 就先一樣 sample m 個 vector 出來從 prior distribution sample m 個 vector 出來
GAN_Lecture_4_(2018)_-_Basic_Theory-899 接下來就要解下面這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-900 本來 discriminator 是要 minimize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-901 generator 要 minimize 一個一模一樣的式子
GAN_Lecture_4_(2018)_-_Basic_Theory-902 下面這個式子裡面你會發現第一項跟 generator 是沒有關係的
GAN_Lecture_4_(2018)_-_Basic_Theory-903 因為第一項只跟 discriminator 有關
GAN_Lecture_4_(2018)_-_Basic_Theory-904 它跟 generator 沒有關係
GAN_Lecture_4_(2018)_-_Basic_Theory-905 所以要 train generator 去 minimize 這個式子的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-906 第一項是可以不用考慮它的
GAN_Lecture_4_(2018)_-_Basic_Theory-907 所以把第一項拿掉只去 minimize 第二項式子
GAN_Lecture_4_(2018)_-_Basic_Theory-908 這個第二個步驟就是在 train generator
GAN_Lecture_4_(2018)_-_Basic_Theory-909 剛才有講過 generator 不能夠 train 太多
GAN_Lecture_4_(2018)_-_Basic_Theory-910 因為一旦 train 太多的話discriminator 就沒有辦法 evaluate J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-911 所以 generator 不能 train 太多
GAN_Lecture_4_(2018)_-_Basic_Theory-912 你只能夠少量的 update 它的參數而已
GAN_Lecture_4_(2018)_-_Basic_Theory-913 所以通常 generator update 一次就好
GAN_Lecture_4_(2018)_-_Basic_Theory-914 你可以 update discriminator 很多次
GAN_Lecture_4_(2018)_-_Basic_Theory-915 但是 generator update 一次就好
GAN_Lecture_4_(2018)_-_Basic_Theory-916 用 discriminator 算出 J-S Divergence 以後你只
GAN_Lecture_4_(2018)_-_Basic_Theory-917 用 generator 少少的 update 一次讓你的 J-S Divergence 變小，但你不能走太多，你 update 太多
GAN_Lecture_4_(2018)_-_Basic_Theory-918 量出來 J-S Divergence 就不對了你就不是在量 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-919 所以這邊就不能夠 update 太多
GAN_Lecture_4_(2018)_-_Basic_Theory-920 到目前為止講說 train generator 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-921 你要去 minimize 的式子長這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-922 在 Ian Goodfellow 原始的 paper 裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-923 從有 GAN 以來，從開天闢地以來
GAN_Lecture_4_(2018)_-_Basic_Theory-924 他就不是在 minimize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-925 因為他在 paper 加了一小段，他說這個式子 log( 1 - D(x)) 它長的是這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-926 而我們一開始在做 training 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-927 你的 D(x) 的值通常是很小的
GAN_Lecture_4_(2018)_-_Basic_Theory-928 因為 discriminator 會知道 generator 產生出來的 image 它是 fake 的，所以他會給他很小的值
GAN_Lecture_4_(2018)_-_Basic_Theory-929 所以一開始 D(x) 的值會落在這個地方
GAN_Lecture_4_(2018)_-_Basic_Theory-930 它的微分是很小的，所以在 training 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-931 會造成你在 training 的一些問題
GAN_Lecture_4_(2018)_-_Basic_Theory-932 所以他說我們把它改成這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-933 沒有為甚麼，我們把它改成這個樣子本來是 log( 1 - D(x))
GAN_Lecture_4_(2018)_-_Basic_Theory-934 把它換成 -log(D(x))
GAN_Lecture_4_(2018)_-_Basic_Theory-935 -log(D(x)) 它長的是這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-936 這兩個式子的趨勢是一樣的
GAN_Lecture_4_(2018)_-_Basic_Theory-937 在這邊最大的東西在這邊還是最大在這邊最小的東西在這邊還是最小
GAN_Lecture_4_(2018)_-_Basic_Theory-938 他們的趨勢是一樣的
GAN_Lecture_4_(2018)_-_Basic_Theory-939 但是他們在同一個位置的斜率就變得不一樣
GAN_Lecture_4_(2018)_-_Basic_Theory-940 在一開始 D(x) 還很小的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-941 算出來的微分會比較大
GAN_Lecture_4_(2018)_-_Basic_Theory-942 所以 Ian Goodfellow 覺得這樣子 training 是比較容易的
GAN_Lecture_4_(2018)_-_Basic_Theory-943 其實你再從另外一個實作的角度來看
GAN_Lecture_4_(2018)_-_Basic_Theory-944 如果你是要 maximize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-945 你會發現你需要改 code 有點麻煩
GAN_Lecture_4_(2018)_-_Basic_Theory-946 如果你再仔細想想如果你是要 minimize 這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-947 其實你要改 code
GAN_Lecture_4_(2018)_-_Basic_Theory-948 如果你是 minimize 這個式子你可以不用改 code
GAN_Lecture_4_(2018)_-_Basic_Theory-949 剛才不是說在 train discriminator 就是在 train 一個 Binary Classifier 嗎
GAN_Lecture_4_(2018)_-_Basic_Theory-950 它就分辨兩個不同的類別
GAN_Lecture_4_(2018)_-_Basic_Theory-951 如果你是要 minimize 下面這個式子的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-952 你其實只是把 Binary Classifier 的 label 換過來本來是說從 data sample 出來的是 class 1
GAN_Lecture_4_(2018)_-_Basic_Theory-953 從 generator sample 出來的是 class 2
GAN_Lecture_4_(2018)_-_Basic_Theory-954 把它 label 換過來，把 generator sample 出來的改標成 label 1，然後用同樣的 code 跑下去就可以了
GAN_Lecture_4_(2018)_-_Basic_Theory-955 我認為 Ian Goodfellow 它只是懶得改 code 而已
GAN_Lecture_4_(2018)_-_Basic_Theory-956 所以就胡亂編一個理由應該要用下面這個式子
GAN_Lecture_4_(2018)_-_Basic_Theory-957 但實際上後來有人試了比較這兩種不同的方法
GAN_Lecture_4_(2018)_-_Basic_Theory-958 發現都可以 train 得起來，performance 也是差不多的
GAN_Lecture_4_(2018)_-_Basic_Theory-959 不知道為甚麼 Ian Goodfellow 一開始就選了這個
GAN_Lecture_4_(2018)_-_Basic_Theory-960 我覺得只是因為它 implement 下去才發現弄錯了
GAN_Lecture_4_(2018)_-_Basic_Theory-961 然後又懶得改 code 所以他就用這個東西繼續做下去
GAN_Lecture_4_(2018)_-_Basic_Theory-962 這些東西是有名字的，因為這些方法有人就會說這個叫 Original GAN 這個叫 Modify GAN
GAN_Lecture_4_(2018)_-_Basic_Theory-963 然後都搞不清楚甚麼是甚麼，所以後來 Ian Goodfellow 還寫了另外一篇文章
GAN_Lecture_4_(2018)_-_Basic_Theory-964 他說我們應該把這些 GAN 正名
GAN_Lecture_4_(2018)_-_Basic_Theory-965 上面這個叫做 Minimax GAN 就是 MMGAN
GAN_Lecture_4_(2018)_-_Basic_Theory-966 下面這個叫做 Non-saturating GAN 就是 NSGAN
GAN_Lecture_4_(2018)_-_Basic_Theory-967 在下課之前，剛才講了很多數學，現在講一些比較直觀的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-968 所以按照 Ian Goodfellow 的講法今天這個 generator和 discriminator 他們之間的關係是甚麼樣呢
GAN_Lecture_4_(2018)_-_Basic_Theory-969 假設這個是 data 的 distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-970 這個是 generator 所產生出來的 data distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-971 現在要 learn 一個 discriminator，discriminator 要給綠色的點比較高的分數
GAN_Lecture_4_(2018)_-_Basic_Theory-972 給藍色的點比較低的分數
GAN_Lecture_4_(2018)_-_Basic_Theory-973 這個 discriminator 的 Objective Value 就是這兩堆 data 的 J-S Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-974 或某種其他的 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-975 有了這個 discriminator 以後
GAN_Lecture_4_(2018)_-_Basic_Theory-976 這些 generated 的點，因為在 train 的時候，generator 會希望它產生出來的東西通過 discriminator 得到的分數
GAN_Lecture_4_(2018)_-_Basic_Theory-977 越大越好，所以這些點就會往右移動
GAN_Lecture_4_(2018)_-_Basic_Theory-978 可能會一下子跑太多就跑到藍色的點的右邊去
GAN_Lecture_4_(2018)_-_Basic_Theory-979 但沒有關係，接下來你會再 train 一次你的 discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-980 因為現在藍色的點根綠色的點比較近
GAN_Lecture_4_(2018)_-_Basic_Theory-981 所以 discriminator 他的 loss 就比較大代表這兩種點的 J-S Divergence 是比較小的
GAN_Lecture_4_(2018)_-_Basic_Theory-982 接下來這些藍色的點會順著 discriminator 給的 gradient 的方向往左移
GAN_Lecture_4_(2018)_-_Basic_Theory-983 最後藍色 distribution 跟綠色 distribution 就會越來越近
GAN_Lecture_4_(2018)_-_Basic_Theory-984 當這兩種 distribution 完全一模一樣的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-985 train 一個 discriminator，這個 discriminator 會完全沒有辦法分別這兩種 data
GAN_Lecture_4_(2018)_-_Basic_Theory-986 因為對 discriminator 來說這兩種 data 是一模一樣的不管它怎麼努力
GAN_Lecture_4_(2018)_-_Basic_Theory-987 都沒有辦法分別這兩種 data所以最後 discriminator 會壞掉
GAN_Lecture_4_(2018)_-_Basic_Theory-988 對它來說每一個地方都是一樣的
GAN_Lecture_4_(2018)_-_Basic_Theory-989 每一個地方 P data 跟 PG 出現的機率都是一樣的
GAN_Lecture_4_(2018)_-_Basic_Theory-990 所以 discriminator 就會變成一條水平的線它沒有辦法再做任何事情
GAN_Lecture_4_(2018)_-_Basic_Theory-991 這個 demo 是這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-992 綠色的點是 P data
GAN_Lecture_4_(2018)_-_Basic_Theory-993 藍色的點是 PG
GAN_Lecture_4_(2018)_-_Basic_Theory-994 綠色的點是你的目標藍色的點是 generator 產生出來的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-995 背景的顏色是 discriminator 的值
GAN_Lecture_4_(2018)_-_Basic_Theory-996 discriminator 會 assign 給每一個 space 上的 x 一個值
GAN_Lecture_4_(2018)_-_Basic_Theory-997 背景的這個顏色是 discriminator 的值
GAN_Lecture_4_(2018)_-_Basic_Theory-998 這個 demo 看起來是這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-999 你就會發現這個 discriminator 就把 PG 產生出來藍色的點趕來趕去
GAN_Lecture_4_(2018)_-_Basic_Theory-1000 直到最後藍色的點跟綠色的點重合在一起的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-1001 對 discriminator 來說它就會壞掉它完全沒有辦法分辨 PG 跟 P data 之間的差別
GAN_Lecture_4_(2018)_-_Basic_Theory-1002 最後 discriminator 就會壞掉，因為完全沒有辦法分辨 generator 跟 discriminator 之間的差別
GAN_Lecture_4_(2018)_-_Basic_Theory-1003 講到這邊有沒有問題要問
GAN_Lecture_4_(2018)_-_Basic_Theory-1004 一般在做的時候，在 train 一個 classifier 的時候其實會害怕 data imbalance 的問題
GAN_Lecture_4_(2018)_-_Basic_Theory-1005 今天在這個 task 裡面，data 是自己 sample 出來的
GAN_Lecture_4_(2018)_-_Basic_Theory-1006 我們不會給自己製造 data imbalance 的問題
GAN_Lecture_4_(2018)_-_Basic_Theory-1007 所以兩種 task 會 sample 一樣的數目
GAN_Lecture_4_(2018)_-_Basic_Theory-1008 假設從 generator 裡面 generate 256 筆 data
GAN_Lecture_4_(2018)_-_Basic_Theory-1009 那你今天從你的 sample 的 database 裡面你也會 sample 256 筆 data
GAN_Lecture_4_(2018)_-_Basic_Theory-1010 如果你沒有問題要問的話其實我有一個問題要問
GAN_Lecture_4_(2018)_-_Basic_Theory-1011 我的問題是這個樣子
GAN_Lecture_4_(2018)_-_Basic_Theory-1012 你不覺得今天講的跟上週講的是有點矛盾的嗎
GAN_Lecture_4_(2018)_-_Basic_Theory-1013 如果按照 Ian Goodfellow 的講法上面這個圖是 Ian Goodfellow paper 上的圖
GAN_Lecture_4_(2018)_-_Basic_Theory-1014 他說最後 discriminator train 到後來它就會爛掉變成一個水平線
GAN_Lecture_4_(2018)_-_Basic_Theory-1015 但我們上周說 discriminator 其實就是 evaluation function
GAN_Lecture_4_(2018)_-_Basic_Theory-1016 也就是說 discriminator 的值代表它想要評斷這個 object、generate 出來的東西它到底是好還是不好
GAN_Lecture_4_(2018)_-_Basic_Theory-1017 如果 discriminator 是一條水平線
GAN_Lecture_4_(2018)_-_Basic_Theory-1018 他就不是一個 evaluation function
GAN_Lecture_4_(2018)_-_Basic_Theory-1019 對他來說所有的東西都是一樣好
GAN_Lecture_4_(2018)_-_Basic_Theory-1020 或者是一樣壞
GAN_Lecture_4_(2018)_-_Basic_Theory-1021 這個是 Ian Goodfellow 畫的圖
GAN_Lecture_4_(2018)_-_Basic_Theory-1022 右上角是 Yann LeCun 畫的圖
GAN_Lecture_4_(2018)_-_Basic_Theory-1023 Yann LeCun 在畫那個圖的時候這個它也是在講 GAN
GAN_Lecture_4_(2018)_-_Basic_Theory-1024 這個圖就是 discriminator 的圖
GAN_Lecture_4_(2018)_-_Basic_Theory-1025 這個綠色的點就是 real data 分布
GAN_Lecture_4_(2018)_-_Basic_Theory-1026 你發現他在畫的時候，在他的想像裡面
GAN_Lecture_4_(2018)_-_Basic_Theory-1027 他的 discriminator 並沒有爛掉變成一個水平線
GAN_Lecture_4_(2018)_-_Basic_Theory-1028 而是有 data 分布的地方他會得到比較小的值
GAN_Lecture_4_(2018)_-_Basic_Theory-1029 而沒有 data 分布的地方他會得到比較大的值
GAN_Lecture_4_(2018)_-_Basic_Theory-1030 跟我上課講的是相反我上課講有 data 分布的地方值比較大
GAN_Lecture_4_(2018)_-_Basic_Theory-1031 沒有 data 分布的地方值比較小他就講正好相反但意思完全是一樣的
GAN_Lecture_4_(2018)_-_Basic_Theory-1032 有 data 分布的地方值是比較小其他地方值是比較大的
GAN_Lecture_4_(2018)_-_Basic_Theory-1033 跟 Ian Goodfellow 講的是有一些矛盾的
GAN_Lecture_4_(2018)_-_Basic_Theory-1034 這個就是神奇的地方
GAN_Lecture_4_(2018)_-_Basic_Theory-1035 因為這個都是上代發展中的理論
GAN_Lecture_4_(2018)_-_Basic_Theory-1036 所以有很多的問題是未知的也許明年再開同樣的課的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-1037 我又會有完全不一樣的講法也說不定
GAN_Lecture_4_(2018)_-_Basic_Theory-1038 以前在 train Deep Learning 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-1039 我們都要用 Restricted Boltzmann Machine
GAN_Lecture_4_(2018)_-_Basic_Theory-1040 過去我們都相信沒有 Restricted Boltzmann Machine 是 train 不起來的
GAN_Lecture_4_(2018)_-_Basic_Theory-1041 像我們之前第一次開 MLDS 的時候其實就會花兩周講 Restricted Boltzmann Machine
GAN_Lecture_4_(2018)_-_Basic_Theory-1042 但今天如果再講 Restricted Boltzmann Machine 我相信你就生氣了覺得是在拖台錢
GAN_Lecture_4_(2018)_-_Basic_Theory-1043 因為根本就用不上這個技術
GAN_Lecture_4_(2018)_-_Basic_Theory-1044 所以這個變化是非常快的也許明年再來講同樣東西的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-1045 就會有截然不同的講法也說不定
GAN_Lecture_4_(2018)_-_Basic_Theory-1046 你如果問我到底是哪一種的話假設你硬要我給你一個答案
GAN_Lecture_4_(2018)_-_Basic_Theory-1047 告訴你到底應該是 Ian Goodfellow 講得比較對還是 Yann LeCun 講得比較對
GAN_Lecture_4_(2018)_-_Basic_Theory-1048 我的感覺是首先可以從實驗上來看看
GAN_Lecture_4_(2018)_-_Basic_Theory-1049 如果你真的 train 完你的 GAN
GAN_Lecture_4_(2018)_-_Basic_Theory-1050 然後去 evaluate 一下 discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-1051 他的感覺好像是介於這兩個 case 中間他絕對不是爛掉
GAN_Lecture_4_(2018)_-_Basic_Theory-1052 絕對不是變成一個完全爛掉的 discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-1053 你自己回去做做看，幾時 train 出這樣的結果
GAN_Lecture_4_(2018)_-_Basic_Theory-1054 雖然是這種簡單的例子你也 train 不出這個結果的
GAN_Lecture_4_(2018)_-_Basic_Theory-1055 就算是一維的例子也都做不出這個結果
GAN_Lecture_4_(2018)_-_Basic_Theory-1056 所以不太像是 Ian Goodfellow 講的這樣
GAN_Lecture_4_(2018)_-_Basic_Theory-1057 但是 discriminator 也不完全反映了 data distribution
GAN_Lecture_4_(2018)_-_Basic_Theory-1058 感覺是介於這兩個 case 之間
GAN_Lecture_4_(2018)_-_Basic_Theory-1059 這些觀點到底對我們了解 GAN 有甚麼幫助
GAN_Lecture_4_(2018)_-_Basic_Theory-1060 也許 GAN 的 algorithm 就是一樣，那演算法就是那個樣子，就是 train generator、train discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-1061 iterative train，也許他的 algorithm 是不會隨著你的觀點不同
GAN_Lecture_4_(2018)_-_Basic_Theory-1062 但是你用不同的觀點來看待 GAN
GAN_Lecture_4_(2018)_-_Basic_Theory-1063 你其實在設計 algorithm 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-1064 空間會有些微妙的差別，也許這些微妙的差別導致最後 training 的結果會是很不一樣的
GAN_Lecture_4_(2018)_-_Basic_Theory-1065 我覺得也許 Yann LeCun 的這個講法
GAN_Lecture_4_(2018)_-_Basic_Theory-1066 我上週講的
GAN_Lecture_4_(2018)_-_Basic_Theory-1067 discriminator 是在 evaluate 一個 object 的好還是不好
GAN_Lecture_4_(2018)_-_Basic_Theory-1068 他是在反映了 data distribution 這件事
GAN_Lecture_4_(2018)_-_Basic_Theory-1069 也許更接近現實
GAN_Lecture_4_(2018)_-_Basic_Theory-1070 為甚麼我會這麼說
GAN_Lecture_4_(2018)_-_Basic_Theory-1071 首先，你在文獻上會看到很多人會做甚麼事情
GAN_Lecture_4_(2018)_-_Basic_Theory-1072 他會把 discriminator 當作 classifier 來用
GAN_Lecture_4_(2018)_-_Basic_Theory-1073 所以先 train 好一個 GAN
GAN_Lecture_4_(2018)_-_Basic_Theory-1074 然後把 discriminator 拿來做其他事情
GAN_Lecture_4_(2018)_-_Basic_Theory-1075 假設 discriminator train 到最後
GAN_Lecture_4_(2018)_-_Basic_Theory-1076 按照 Ian Goodfellow 猜想會爛掉的話
GAN_Lecture_4_(2018)_-_Basic_Theory-1077 拿它來當作 pre-training 根本就沒有意義
GAN_Lecture_4_(2018)_-_Basic_Theory-1078 但很多人會拿它當作 pre-training
GAN_Lecture_4_(2018)_-_Basic_Theory-1079 也顯示他是有用的
GAN_Lecture_4_(2018)_-_Basic_Theory-1080 所以他不太可能真的 train 到後來就壞掉
GAN_Lecture_4_(2018)_-_Basic_Theory-1081 這個是第一個 evidence
GAN_Lecture_4_(2018)_-_Basic_Theory-1082 另外一個 evidence 是你想想看你在 train GAN 的時候
GAN_Lecture_4_(2018)_-_Basic_Theory-1083 你並不是每一次都重新 train discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-1084 我們並不是每次都重 train discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-1085 而是會拿前一個 iteration 的 discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-1086 當作下一個 iteration 的 initialize 的參數
GAN_Lecture_4_(2018)_-_Basic_Theory-1087 如果你的 discriminator 是想要衡量兩個 data distribution 的 Divergence 的話
GAN_Lecture_4_(2018)_-_Basic_Theory-1088 你其實沒有必要把前一個 iteration 的東西拿來用
GAN_Lecture_4_(2018)_-_Basic_Theory-1089 因為 generator 已經變了
GAN_Lecture_4_(2018)_-_Basic_Theory-1090 保留前一個 iteration 的東西有甚麼意義呢
GAN_Lecture_4_(2018)_-_Basic_Theory-1091 這樣感覺是不太合理的
GAN_Lecture_4_(2018)_-_Basic_Theory-1092 也有人可能會說因為 generator update 的參數
GAN_Lecture_4_(2018)_-_Basic_Theory-1093 他 update 的量是比較小的
GAN_Lecture_4_(2018)_-_Basic_Theory-1094 所以也許把前一個 time step 得到的 generator
GAN_Lecture_4_(2018)_-_Basic_Theory-1095 當作下一個 time step 的 initialization
GAN_Lecture_4_(2018)_-_Basic_Theory-1096 可以加快 discriminator 訓練的速度
GAN_Lecture_4_(2018)_-_Basic_Theory-1097 也說不定這個理由感覺也是成立的
GAN_Lecture_4_(2018)_-_Basic_Theory-1098 不過在文獻上我看到有人在 train GAN 的時候他有一招
GAN_Lecture_4_(2018)_-_Basic_Theory-1099 每次 train 的時候他不只拿現在的 generator 去 sample data
GAN_Lecture_4_(2018)_-_Basic_Theory-1100 他也會拿過去的 generator 也 sample data
GAN_Lecture_4_(2018)_-_Basic_Theory-1101 然後把這些各個不同 generator sample 的 data 通通集合起來
GAN_Lecture_4_(2018)_-_Basic_Theory-1102 再去 train discriminator
GAN_Lecture_4_(2018)_-_Basic_Theory-1103 可以得到的 performance 會是比較好的
GAN_Lecture_4_(2018)_-_Basic_Theory-1104 如果 discriminator 是在 evaluate 現在的 generator
GAN_Lecture_4_(2018)_-_Basic_Theory-1105 跟 data distribution 的差異的話
GAN_Lecture_4_(2018)_-_Basic_Theory-1106 好像做這件事情也沒有太大的意義
GAN_Lecture_4_(2018)_-_Basic_Theory-1107 因為現在量 generator 跟 data 之間的差異
GAN_Lecture_4_(2018)_-_Basic_Theory-1108 拿過去 generator 產生的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-1109 有甚麼用，沒什麼用為甚麼要這麼做，但是
GAN_Lecture_4_(2018)_-_Basic_Theory-1110 在實作上發現拿過去 generator 產生的東西
GAN_Lecture_4_(2018)_-_Basic_Theory-1111 再去訓練 discriminator 是可以得到比較好的成果
GAN_Lecture_4_(2018)_-_Basic_Theory-1112 所以這樣看起來，也許這是另外一個 support 支持也許 discriminator 在做的事情
GAN_Lecture_4_(2018)_-_Basic_Theory-1113 並不見得是在 evaluate 兩個 distribution 之間的 Divergence
GAN_Lecture_4_(2018)_-_Basic_Theory-1114 不過至少 Ian Goodfellow 一開始是這麼說的所以我們把 GAN 最開始的理論告訴大家
GAN_Lecture_5_(2018)_-_General_Framework-0 我們要講一個東西，叫做 fGAN，那這一段是這個樣子，這一段數學比較多
GAN_Lecture_5_(2018)_-_General_Framework-1 如果你聽不懂的話，不用太在意，就算了，因為這個東西有點用不上
GAN_Lecture_5_(2018)_-_General_Framework-2 為什麼，我們這邊要講的是什麼？
GAN_Lecture_5_(2018)_-_General_Framework-3 我們現在要講的是說，我們說
GAN_Lecture_5_(2018)_-_General_Framework-4 我們定某種 objective function，就是在量  js divergences
GAN_Lecture_5_(2018)_-_General_Framework-5 那我們能不能夠量其他的divergence 呢？
GAN_Lecture_5_(2018)_-_General_Framework-6 fGAN 就是要告訴我們怎麼量其他的 divergences
GAN_Lecture_5_(2018)_-_General_Framework-7 那我之所以會說這招不太有用的關係，原因就是，用不同的 divergences
GAN_Lecture_5_(2018)_-_General_Framework-8 就 fGAN 可以讓你用不同的 f divergences 來量你 generated 的 example 跟 real example 的差距
GAN_Lecture_5_(2018)_-_General_Framework-9 但是用不同的 x divergences 的結果
GAN_Lecture_5_(2018)_-_General_Framework-10 是差不多的，所以這一招好像沒什麼特別有用的地方
GAN_Lecture_5_(2018)_-_General_Framework-11 但是我們還是跟大家介紹一下
GAN_Lecture_5_(2018)_-_General_Framework-12 因為這個在數學上，他感覺非常的屌這樣子
GAN_Lecture_5_(2018)_-_General_Framework-13 但是在實作上，好像沒什麼特別的不同
GAN_Lecture_5_(2018)_-_General_Framework-14 我們來講一下 fGAN，fGAN 想要告訴我們的是說
GAN_Lecture_5_(2018)_-_General_Framework-15 其實不只是用 js divergence，任何的f-divergence
GAN_Lecture_5_(2018)_-_General_Framework-16 你都可以放到 GAN 的架構裡面去
GAN_Lecture_5_(2018)_-_General_Framework-17 那我們就先來介紹一下，什麼是 f-divergence
GAN_Lecture_5_(2018)_-_General_Framework-18 f-divergence 是說我們現在假設有兩個 distribution，p and q
GAN_Lecture_5_(2018)_-_General_Framework-19 p of x 代表 x 從 p 這個 distribution sample 出來的機率
GAN_Lecture_5_(2018)_-_General_Framework-20 q of x 代表 x 從 q 這個 distribution sample 出來的機率
GAN_Lecture_5_(2018)_-_General_Framework-21 那 p 跟 q 這兩個 distribution 的 f-divergence 它長什麼樣子呢？
GAN_Lecture_5_(2018)_-_General_Framework-22 他的長相就是，我們可以把它寫成像這邊這個式子
GAN_Lecture_5_(2018)_-_General_Framework-23 p 是一個 distribution，q 是一個 distribution
GAN_Lecture_5_(2018)_-_General_Framework-24 然後你對 x 做積分，然後把 q of x 乘上 f of (p of x 除以 q of x)
GAN_Lecture_5_(2018)_-_General_Framework-25 那 f 這個 function，假設這個東西要是一個 f-divergence 的話
GAN_Lecture_5_(2018)_-_General_Framework-26 那 f 這個 function 它必須是 convex 的
GAN_Lecture_5_(2018)_-_General_Framework-27 這是第一個條件，第二個條件是，f of 1 必須要等於 0
GAN_Lecture_5_(2018)_-_General_Framework-28 有這兩個條件的話，這一個式子
GAN_Lecture_5_(2018)_-_General_Framework-29 就是某一種 f-divergence
GAN_Lecture_5_(2018)_-_General_Framework-30 你這個 f 放不同的 function，它就是不同的 divergence
GAN_Lecture_5_(2018)_-_General_Framework-31 然後會看到說你 f 放某一個 function
GAN_Lecture_5_(2018)_-_General_Framework-32 就是 KL divergence
GAN_Lecture_5_(2018)_-_General_Framework-33 放另外一個 function 就變成 inverse 的 KL divergence
GAN_Lecture_5_(2018)_-_General_Framework-34 那首先我們想要跟大家說明的是，p 跟 q
GAN_Lecture_5_(2018)_-_General_Framework-35 就為什麼這個式子，它可以看作是在衡量 p 跟 q 的差異呢？
GAN_Lecture_5_(2018)_-_General_Framework-36 為什麼這個式子，可以看作是 p 跟 q 的 divergence 呢？
GAN_Lecture_5_(2018)_-_General_Framework-37 首先第一個我們要跟大家說明的是
GAN_Lecture_5_(2018)_-_General_Framework-38 如果 p 跟 q 這兩個 distribution 是一模一樣的
GAN_Lecture_5_(2018)_-_General_Framework-39 那這個值會是 0，因為這個東西
GAN_Lecture_5_(2018)_-_General_Framework-40 這個 divergence 等於是在衡量 p 跟 q 兩個 distribution 之間的距離
GAN_Lecture_5_(2018)_-_General_Framework-41 所以如果這兩個 distribution 是一樣的，它們的距離應該為 0
GAN_Lecture_5_(2018)_-_General_Framework-42 所以我們現在先來看看，假設 p 跟 q 是一樣的
GAN_Lecture_5_(2018)_-_General_Framework-43 他們的距離是不是應該為 ０呢？
GAN_Lecture_5_(2018)_-_General_Framework-44 假設 p 跟 q 是一樣的，會發生什麼事呢？
GAN_Lecture_5_(2018)_-_General_Framework-45 假設 p 跟 q 是一樣的，這一項的值是 1
GAN_Lecture_5_(2018)_-_General_Framework-46 那我們說 f-divergence 是一個條件式
GAN_Lecture_5_(2018)_-_General_Framework-47 f 這個式子帶 1 要是 0，所以這邊就變成 0
GAN_Lecture_5_(2018)_-_General_Framework-48 這邊變成 0 意味著什麼？意味著這一整項都是 0
GAN_Lecture_5_(2018)_-_General_Framework-49 所以當 p 跟 q 是一樣的時候，它們距離會是 0
GAN_Lecture_5_(2018)_-_General_Framework-50 接下來要跟大家說明的地方是
GAN_Lecture_5_(2018)_-_General_Framework-51 0 是這個式子可以達到的最小的距離
GAN_Lecture_5_(2018)_-_General_Framework-52 也就是說如果 p 跟 q 有些不同，不是完全一模一樣
GAN_Lecture_5_(2018)_-_General_Framework-53 它們算出來的 f-divergence，就會大於 0
GAN_Lecture_5_(2018)_-_General_Framework-54 怎麼說呢？因為 f 它是一個 convex 的 function
GAN_Lecture_5_(2018)_-_General_Framework-55 這是 f-divergence 訂的條件，你的 f 必須要是一個 convex function
GAN_Lecture_5_(2018)_-_General_Framework-56 如果是一個 convex function，這個式子
GAN_Lecture_5_(2018)_-_General_Framework-57 可以寫成這樣，這步跳的有點快，反正你自己回去 check 一下，反正就是這麼回事就對了
GAN_Lecture_5_(2018)_-_General_Framework-58 因為 f 是 convex 的關係，所以這個式子會大於等於這個式子
GAN_Lecture_5_(2018)_-_General_Framework-59 這個式子會是他的一個 lower bound
GAN_Lecture_5_(2018)_-_General_Framework-60 接下來，你把 q，消掉，然後對 p of x 做積分
GAN_Lecture_5_(2018)_-_General_Framework-61 你就得到 f of 1，然後會得到 0，所以今天 p 跟 q
GAN_Lecture_5_(2018)_-_General_Framework-62 它們如果一模一樣的時候，你的 f-divergence 算出來是 0
GAN_Lecture_5_(2018)_-_General_Framework-63 如果 p 跟 q，略有不同，它們一定會大於等於 0
GAN_Lecture_5_(2018)_-_General_Framework-64 因為 p 跟 q 是，p 跟 q 的值一定會大於等於 0
GAN_Lecture_5_(2018)_-_General_Framework-65 所以 p 跟 q，略有不同的時候，他們的值就會大於等於 0
GAN_Lecture_5_(2018)_-_General_Framework-66 所以我們可以說這個式子，它可以拿來量 p 跟 q 之間的差異
GAN_Lecture_5_(2018)_-_General_Framework-67 那接下來，我想告訴大家說，假設你 x 帶不同的式子，你就得到不同的 divergence
GAN_Lecture_5_(2018)_-_General_Framework-68 舉例來說，如果你的 f，是 x log x
GAN_Lecture_5_(2018)_-_General_Framework-69 那帶進去以後，把這個 f 用 x log x 帶進去以後
GAN_Lecture_5_(2018)_-_General_Framework-70 你算出來就是 KL divergence，如果你 x 是 -log x
GAN_Lecture_5_(2018)_-_General_Framework-71 如果你把這個 -log x 帶進去，你算出來的就是 reverse KL divergence
GAN_Lecture_5_(2018)_-_General_Framework-72 如果你現在 f of x，帶 (x-1)^2
GAN_Lecture_5_(2018)_-_General_Framework-73 你算出來的就是 chi square 的 divergence
GAN_Lecture_5_(2018)_-_General_Framework-74 你可以帶不同的東西，你就得到各式各樣的 f-divergence 的 measure 就是了
GAN_Lecture_5_(2018)_-_General_Framework-75 那接下來要講一個你可能沒有聽過的東西，叫做  Fenchel Conjugate
GAN_Lecture_5_(2018)_-_General_Framework-76 這個要講的意思是說呢，這邊要講的東西是說
GAN_Lecture_5_(2018)_-_General_Framework-77 每一個 convex 的 function 的 f，他都有另外一個 conjugate 的 function
GAN_Lecture_5_(2018)_-_General_Framework-78 叫做 f *，就每一個 f 他都有一個對應的夥伴，叫做 f*
GAN_Lecture_5_(2018)_-_General_Framework-79 這個 f 跟 f* 之間的關係
GAN_Lecture_5_(2018)_-_General_Framework-80 寫成一個這樣看起來很可怕的式子，看了你就有點頭痛，然後等一下你就會跟不上了
GAN_Lecture_5_(2018)_-_General_Framework-81 你看就覺得有點頭痛這樣，這個到底是什麼東西？
GAN_Lecture_5_(2018)_-_General_Framework-82 這個東西是這個樣子，這個東西是，我們有一個 f
GAN_Lecture_5_(2018)_-_General_Framework-83 那它的 conjugate 的 function，叫做 f*
GAN_Lecture_5_(2018)_-_General_Framework-84 這個 f* 他長得是這個樣子，它是由 f of x 所導出來的
GAN_Lecture_5_(2018)_-_General_Framework-85 f* 如果我們帶 t 進去，要怎麼算出他的值呢？
GAN_Lecture_5_(2018)_-_General_Framework-86 你就窮舉所有的 t，看哪一個 t 可以讓 (xt - f of x) 的值最大
GAN_Lecture_5_(2018)_-_General_Framework-87 就是我們要的值
GAN_Lecture_5_(2018)_-_General_Framework-88 這樣講可能有點抽象，所以我們就實際上舉一個例子
GAN_Lecture_5_(2018)_-_General_Framework-89 f* of t1 是多少？
GAN_Lecture_5_(2018)_-_General_Framework-90 你就把 t1 帶進去，然後窮舉各種不同 x 的值，看看哪一個值，可以讓 t1 最大
GAN_Lecture_5_(2018)_-_General_Framework-91 舉例來說，如果你 x 帶 x1，你得到 x1t1 - f of x1
GAN_Lecture_5_(2018)_-_General_Framework-92 你 x 帶 x2，你得到 x2t1 - f of x2
GAN_Lecture_5_(2018)_-_General_Framework-93 你 x 帶 x3，你得到 x3t1 - f of x3
GAN_Lecture_5_(2018)_-_General_Framework-94 在看這些不同的 x 誰最大，發現 x1 最大
GAN_Lecture_5_(2018)_-_General_Framework-95 那我們得到的 f* of t1，就是這個值
GAN_Lecture_5_(2018)_-_General_Framework-96 同理呢，假設你想要知道，f* of t2 的值是多少
GAN_Lecture_5_(2018)_-_General_Framework-97 那你就把不同的 x 呢，你就把 t2 先帶進去
GAN_Lecture_5_(2018)_-_General_Framework-98 然後把不同的 x 也帶進去，你帶 x1 進去，得到這個值
GAN_Lecture_5_(2018)_-_General_Framework-99 帶 x2 進去，得到這個值
GAN_Lecture_5_(2018)_-_General_Framework-100 帶 x3 進去，得到這個值，看誰最大
GAN_Lecture_5_(2018)_-_General_Framework-101 最大的值就是 f* of t2
GAN_Lecture_5_(2018)_-_General_Framework-102 所以如果你今天要知道，f* of t 長什麼樣子
GAN_Lecture_5_(2018)_-_General_Framework-103 你就每一點，從 t0 到 t100，每一點，通通這個方法去算
GAN_Lecture_5_(2018)_-_General_Framework-104 你就可以把 f* of t1 描繪出來
GAN_Lecture_5_(2018)_-_General_Framework-105 那這樣有點麻煩，另外一個方法是說
GAN_Lecture_5_(2018)_-_General_Framework-106 我們現在把 (xt - f of x)，把它畫出來，我們帶不同的 x1
GAN_Lecture_5_(2018)_-_General_Framework-107 但是把 xt-f of x 畫出來，假設 x 是 x1 的話
GAN_Lecture_5_(2018)_-_General_Framework-108 長這樣，假設 x 是 x2 的話
GAN_Lecture_5_(2018)_-_General_Framework-109 長這樣，這個都是一直線嘛，因為假設
GAN_Lecture_5_(2018)_-_General_Framework-110 你的 t 是你唯一的變數，x 是固定的
GAN_Lecture_5_(2018)_-_General_Framework-111 xt - f of x，x 是固定的
GAN_Lecture_5_(2018)_-_General_Framework-112 要嘛是 x1， 要嘛是 x2，他都是固定的
GAN_Lecture_5_(2018)_-_General_Framework-113 這時候 xt - f of x 他都是直線
GAN_Lecture_5_(2018)_-_General_Framework-114 你帶不同的 x 進去，他就是不同的直線
GAN_Lecture_5_(2018)_-_General_Framework-115 帶 x1 進去是這樣，帶 x2 進去是這樣，帶 x3 進去是這樣
GAN_Lecture_5_(2018)_-_General_Framework-116 帶不同的值進去，他就是不同的直線
GAN_Lecture_5_(2018)_-_General_Framework-117 接下來，我們要找最大的那一個
GAN_Lecture_5_(2018)_-_General_Framework-118 給定一個 t，我們要找最大的那一個
GAN_Lecture_5_(2018)_-_General_Framework-119 最大的那個是什麼呢？
GAN_Lecture_5_(2018)_-_General_Framework-120 假設給定 t1，最大的那個就是在這個地方
GAN_Lecture_5_(2018)_-_General_Framework-121 假設給定 t2，最大的那個就是在這個地方
GAN_Lecture_5_(2018)_-_General_Framework-122 所以今天你要把 f* of t 畫出來
GAN_Lecture_5_(2018)_-_General_Framework-123 就是把所有不同的 x1 所造成的直線，通通畫出來
GAN_Lecture_5_(2018)_-_General_Framework-124 然後再取它們的 upper bound，再把他們的 upper bound 找出來
GAN_Lecture_5_(2018)_-_General_Framework-125 這個就是 f* of t
GAN_Lecture_5_(2018)_-_General_Framework-126 所以今天你會發現 f* of t 它一定是 convex 的
GAN_Lecture_5_(2018)_-_General_Framework-127 所以今天如果你畫很多條直線，隨便亂畫，隨便亂畫，不管你怎麼隨便畫
GAN_Lecture_5_(2018)_-_General_Framework-128 最後你只要找的是 upper bound
GAN_Lecture_5_(2018)_-_General_Framework-129 你得到的 function 都是 convex 的
GAN_Lecture_5_(2018)_-_General_Framework-130 所以今天 f of x 是 convex，f* of t，其實也一定是convex
GAN_Lecture_5_(2018)_-_General_Framework-131 那這邊是想要舉一個例子跟大家說明一下我們剛才講的是什麼？
GAN_Lecture_5_(2018)_-_General_Framework-132 假設 f of x 是 xlogx
GAN_Lecture_5_(2018)_-_General_Framework-133 今天如果我們 x 帶 0.1，我們會得到這樣一條直線
GAN_Lecture_5_(2018)_-_General_Framework-134 x 帶 1，我們會得到這樣一條直線
GAN_Lecture_5_(2018)_-_General_Framework-135 x 帶 10，我們會得到這樣一條直線
GAN_Lecture_5_(2018)_-_General_Framework-136 把這些直線的 upper bound 通通串起來，我們會得到一個看起來像是這樣的直線
GAN_Lecture_5_(2018)_-_General_Framework-137 這不是直線，看起來像是這樣的曲線，所以這 x 要帶不同的值
GAN_Lecture_5_(2018)_-_General_Framework-138 你從 0.1 帶進去，得到一條線
GAN_Lecture_5_(2018)_-_General_Framework-139 0.1001 帶進去，也是一條線
GAN_Lecture_5_(2018)_-_General_Framework-140 0.1002 帶進去，也是一條線，通通帶進去，你得到無窮無盡的線
GAN_Lecture_5_(2018)_-_General_Framework-141 把這些所有的線 upper bound 都找出來，就是紅色這一條線
GAN_Lecture_5_(2018)_-_General_Framework-142 那你會發現說這條紅色的線，它看起來像是 exponential，沒錯！
GAN_Lecture_5_(2018)_-_General_Framework-143 這一條紅色的線，它是 exp(t-1)
GAN_Lecture_5_(2018)_-_General_Framework-144 所以 f of x 它的 conjugate，就是 exp(t-1)
GAN_Lecture_5_(2018)_-_General_Framework-145 這邊是舉一個很直觀的例子啦
GAN_Lecture_5_(2018)_-_General_Framework-146 那下一頁是一個 proof，我們也許就可以把它跳過去，反正你就
GAN_Lecture_5_(2018)_-_General_Framework-147 怒算一波以後會發現說，假設 f of x 是 xlog x
GAN_Lecture_5_(2018)_-_General_Framework-148 怒算一波以後就會知道它是 exp(t-1)，它的 conjugate 是 exp(t-1)
GAN_Lecture_5_(2018)_-_General_Framework-149 那講這麼多，好像都跟 GAN 沒半毛錢的關係
GAN_Lecture_5_(2018)_-_General_Framework-150 到底為什麼要講這些呢？
GAN_Lecture_5_(2018)_-_General_Framework-151 接下來，我們就要進入跟 GAN 有關的內容了
GAN_Lecture_5_(2018)_-_General_Framework-152 我們剛才知道說，f of x，有一個 conjugate
GAN_Lecture_5_(2018)_-_General_Framework-153 就是 f* of t
GAN_Lecture_5_(2018)_-_General_Framework-154 其實 f* of t 它本身的 conjugate
GAN_Lecture_5_(2018)_-_General_Framework-155 也就是 f of x，所以 f of x 跟 f* of t
GAN_Lecture_5_(2018)_-_General_Framework-156 他們其實是互為 conjugate
GAN_Lecture_5_(2018)_-_General_Framework-157 所以 f* of t 跟 f of x 之間的關係，可以寫成這樣的式子
GAN_Lecture_5_(2018)_-_General_Framework-158 所以接下來呢，假設你有一個convex function，叫做 f of x
GAN_Lecture_5_(2018)_-_General_Framework-159 你就可以把這個 convex function f of x，換成右邊這樣的式子
GAN_Lecture_5_(2018)_-_General_Framework-160 你可能會想說，這不是把本來簡單的問題變複雜了嗎？到底在搞什麼這樣子
GAN_Lecture_5_(2018)_-_General_Framework-161 這邊告訴你說，這件事情跟 GAN 有關係的
GAN_Lecture_5_(2018)_-_General_Framework-162 所以假設我們有一個 f-divergence 的 function
GAN_Lecture_5_(2018)_-_General_Framework-163 f-divergence 的 function 你是對 x 做積分，q of x 乘上 f of (p/q)
GAN_Lecture_5_(2018)_-_General_Framework-164 那這個 f 是一個 convex function 對不對？
GAN_Lecture_5_(2018)_-_General_Framework-165 那我們說 convex function， f of x
GAN_Lecture_5_(2018)_-_General_Framework-166 就可以換成右邊這個看起來比較複雜的樣子
GAN_Lecture_5_(2018)_-_General_Framework-167 所以我們就把它換掉，把簡單的問題弄得更複雜，把他換掉
GAN_Lecture_5_(2018)_-_General_Framework-168 所以本來 x 是 p/q，x 這邊是 p/q，是 p/q，所以將 p/q 帶進去，p/q 帶進去
GAN_Lecture_5_(2018)_-_General_Framework-169 所以得到這樣的一個式子
GAN_Lecture_5_(2018)_-_General_Framework-170 所以本來 f of (p/q) 就變成了
GAN_Lecture_5_(2018)_-_General_Framework-171 p/q 乘上 t，再減掉 f* of t，f* of t 是 f 的 conjugate
GAN_Lecture_5_(2018)_-_General_Framework-172 然後再對所有的 t 取一個 max，你就得到這一個值
GAN_Lecture_5_(2018)_-_General_Framework-173 接下來我要告訴你說，我們現在 learn 一個 D
GAN_Lecture_5_(2018)_-_General_Framework-174 他其實就是 discriminator，他這個 D 這個 function
GAN_Lecture_5_(2018)_-_General_Framework-175 它就是 input 一個 x，它 output 一個 scalar
GAN_Lecture_5_(2018)_-_General_Framework-176 它 output 的這個 scalar，就是這邊這個 t
GAN_Lecture_5_(2018)_-_General_Framework-177 所以我們把這個 t 用 Ｄof x 取代掉
GAN_Lecture_5_(2018)_-_General_Framework-178 所以我們希望說，我們可以 learn 出一個 function，這個 function
GAN_Lecture_5_(2018)_-_General_Framework-179 我們不要解這個 max 的 problem，本來應該是說
GAN_Lecture_5_(2018)_-_General_Framework-180 給你 p of x，給你 q of x，給你  f* 長什麼樣
GAN_Lecture_5_(2018)_-_General_Framework-181 窮舉所有的 t，看哪一個 t 可以讓這個值最大
GAN_Lecture_5_(2018)_-_General_Framework-182 那這個就是你在小括號裡面的值
GAN_Lecture_5_(2018)_-_General_Framework-183 希望大家聽得懂這樣子
GAN_Lecture_5_(2018)_-_General_Framework-184 那接下來，我們要找一個 discriminator
GAN_Lecture_5_(2018)_-_General_Framework-185 這個 discriminator 幫我們解這個 max 的 problem
GAN_Lecture_5_(2018)_-_General_Framework-186 這個 discriminator 怎麼幫我們解這個 max 的problem 呢？
GAN_Lecture_5_(2018)_-_General_Framework-187 他就是 input 一個 x
GAN_Lecture_5_(2018)_-_General_Framework-188 它告訴我們說，你現在 input 這個 x
GAN_Lecture_5_(2018)_-_General_Framework-189 以後到底哪一個 t，可以讓這個值最大
GAN_Lecture_5_(2018)_-_General_Framework-190 它這個 D 就是要做這件事
GAN_Lecture_5_(2018)_-_General_Framework-191 但是因為假設 D 的 capacity 是有限的
GAN_Lecture_5_(2018)_-_General_Framework-192 那你今天把這個 t 換成 p of x
GAN_Lecture_5_(2018)_-_General_Framework-193 這一項就會變成是 f- divergence 的一個 lower bound
GAN_Lecture_5_(2018)_-_General_Framework-194 然後接下來，你再把它展開
GAN_Lecture_5_(2018)_-_General_Framework-195 這邊 q 可以刪掉
GAN_Lecture_5_(2018)_-_General_Framework-196 所以變成 p of x 乘上 D of x 減掉 q of x 乘上 f* of (D of x)
GAN_Lecture_5_(2018)_-_General_Framework-197 那你就把這個式子列上來
GAN_Lecture_5_(2018)_-_General_Framework-198 因為我先說，我們要找一個 D
GAN_Lecture_5_(2018)_-_General_Framework-199 如果你隨便找一個 D
GAN_Lecture_5_(2018)_-_General_Framework-200 它會比這個 f-divergence 的值還要小
GAN_Lecture_5_(2018)_-_General_Framework-201 如果你找一個最好的 D
GAN_Lecture_5_(2018)_-_General_Framework-202 他預測出來的 p 是最準的
GAN_Lecture_5_(2018)_-_General_Framework-203 你就可以去逼近 f-divergence
GAN_Lecture_5_(2018)_-_General_Framework-204 所以我們找一個 D，它可以去 maximize 後面這一項
GAN_Lecture_5_(2018)_-_General_Framework-205 它就可以去逼近 f-divergence
GAN_Lecture_5_(2018)_-_General_Framework-206 所以 f-divergence 的這個式子，會等於找一個 D
GAN_Lecture_5_(2018)_-_General_Framework-207 它可以 maximize 後面這一項，後面這一項是什麼呢？
GAN_Lecture_5_(2018)_-_General_Framework-208 後面這一項是對所有 x 做積分 p of x 乘上 q of x
GAN_Lecture_5_(2018)_-_General_Framework-209 減掉對所有 x 做積分 q of x 乘上 f* of (D of x)
GAN_Lecture_5_(2018)_-_General_Framework-210 接下來我就是要跟你說它跟 GAN 是有什麼關係
GAN_Lecture_5_(2018)_-_General_Framework-211 這一個式子我們可以寫成是對 p of x 就是一個機率嘛
GAN_Lecture_5_(2018)_-_General_Framework-212 對 p of x 做積分乘上 D of x，就等於是用 p 這個 distribution
GAN_Lecture_5_(2018)_-_General_Framework-213 對 D of x 取期望值
GAN_Lecture_5_(2018)_-_General_Framework-214 這邊是用 f* of (D of x) 這個值
GAN_Lecture_5_(2018)_-_General_Framework-215 然後用這個 Q 的distribution
GAN_Lecture_5_(2018)_-_General_Framework-216 對 f* of (D of x) 取期望值，接下來
GAN_Lecture_5_(2018)_-_General_Framework-217 這邊我們只是換了一下名字
GAN_Lecture_5_(2018)_-_General_Framework-218 我們把 p 改成 p data，我們把 Q 改成 PG
GAN_Lecture_5_(2018)_-_General_Framework-219 所以，今天 p data 跟 PG 之間的 f-divergence
GAN_Lecture_5_(2018)_-_General_Framework-220 就可以寫成後面這個式子
GAN_Lecture_5_(2018)_-_General_Framework-221 你的 f-divergence 是什麼？
GAN_Lecture_5_(2018)_-_General_Framework-222 就會影響到這個 f* 你放的是什麼？
GAN_Lecture_5_(2018)_-_General_Framework-223 所以今天假如你的 f-divergence 是 KL divergence
GAN_Lecture_5_(2018)_-_General_Framework-224 那你就看 KL-divergence f* 是什麼？
GAN_Lecture_5_(2018)_-_General_Framework-225 KL divergence f 是 x logx
GAN_Lecture_5_(2018)_-_General_Framework-226 它的 f* 是 exp(t-1)，所以這個 f* 就帶 exp(t-1)，就這樣子
GAN_Lecture_5_(2018)_-_General_Framework-227 可是你看這個式子，跟 GAN 看起來的式子
GAN_Lecture_5_(2018)_-_General_Framework-228 怎麼看起來這麼像呢？
GAN_Lecture_5_(2018)_-_General_Framework-229 你想想看我們今天在 train 一個 generator 的時候
GAN_Lecture_5_(2018)_-_General_Framework-230 我們要做的事情，就是去 minimize 某一個 divergence
GAN_Lecture_5_(2018)_-_General_Framework-231 某一個 f-divergence, KL divergence, js divergence, reverse KL divergence 等等
GAN_Lecture_5_(2018)_-_General_Framework-232 而這個 divergence，我們就可以把它寫成這個式子
GAN_Lecture_5_(2018)_-_General_Framework-233 隨著你要用什麼divergence，你這 f* 就換不同的式子
GAN_Lecture_5_(2018)_-_General_Framework-234 你就是在量不同的 divergence
GAN_Lecture_5_(2018)_-_General_Framework-235 而這個東西就是我們說在 train GAN 的時候
GAN_Lecture_5_(2018)_-_General_Framework-236 你要用 discriminator 去 maximize 你的 generator
GAN_Lecture_5_(2018)_-_General_Framework-237 要去 minimize 的 objective function V of (G,D)
GAN_Lecture_5_(2018)_-_General_Framework-238 只是 V of (G,D) 的定義不同
GAN_Lecture_5_(2018)_-_General_Framework-239 你就是在量不同的 divergence
GAN_Lecture_5_(2018)_-_General_Framework-240 你設某種樣子，他就是在量 js divergence
GAN_Lecture_5_(2018)_-_General_Framework-241 你設另外一種樣子，就是在量 KL divergence
GAN_Lecture_5_(2018)_-_General_Framework-242 這邊就是從 paper 上面的圖，它就告訴你說
GAN_Lecture_5_(2018)_-_General_Framework-243 各種不同的 divergence 的 objective function
GAN_Lecture_5_(2018)_-_General_Framework-244 都幫你列好了，選一個你自己喜歡的
GAN_Lecture_5_(2018)_-_General_Framework-245 那可以 optimize 不同的 divergence，到底有什麼厲害的地方呢？
GAN_Lecture_5_(2018)_-_General_Framework-246 它厲害的地方是
GAN_Lecture_5_(2018)_-_General_Framework-247 也許這一招可以解決一個長期以來困擾大家的問題是
GAN_Lecture_5_(2018)_-_General_Framework-248 當你 train GAN 的時候
GAN_Lecture_5_(2018)_-_General_Framework-249 你會遇到一個現象叫做，Mode Collapse
GAN_Lecture_5_(2018)_-_General_Framework-250 所以 Mode Collapse 的意思是說
GAN_Lecture_5_(2018)_-_General_Framework-251 你的 real data 的 distribution 是比較大的
GAN_Lecture_5_(2018)_-_General_Framework-252 但是你 generate 出來的 example
GAN_Lecture_5_(2018)_-_General_Framework-253 它的 distribution 非常的小
GAN_Lecture_5_(2018)_-_General_Framework-254 舉例來說，你在做二次元人物生成的時候
GAN_Lecture_5_(2018)_-_General_Framework-255 如果你 update 的 iteration 太多
GAN_Lecture_5_(2018)_-_General_Framework-256 你得到的結果可能會是這個樣子
GAN_Lecture_5_(2018)_-_General_Framework-257 你會發現，某一張特定的人臉
GAN_Lecture_5_(2018)_-_General_Framework-258 它就開始蔓延了，它就開始蔓延
GAN_Lecture_5_(2018)_-_General_Framework-259 變得到處都是這樣，但它這些人臉，其實是略有不同的，這個是比較偏黃
GAN_Lecture_5_(2018)_-_General_Framework-260 這個是比較偏紅，但是他們都是看起來就像是同一張人臉
GAN_Lecture_5_(2018)_-_General_Framework-261 也就是說你今天產生出來的 distribution
GAN_Lecture_5_(2018)_-_General_Framework-262 它會越來越小，它會越來越小
GAN_Lecture_5_(2018)_-_General_Framework-263 而最後會發現同一張人臉
GAN_Lecture_5_(2018)_-_General_Framework-264 不斷的反覆出現，這個 case，叫做 Model collapse
GAN_Lecture_5_(2018)_-_General_Framework-265 那有另外一個 case 比 mode collapse 稍微輕微一點
GAN_Lecture_5_(2018)_-_General_Framework-266 叫做 Mode dropping
GAN_Lecture_5_(2018)_-_General_Framework-267 意思是說你的 distribution 其實有很多個 mode
GAN_Lecture_5_(2018)_-_General_Framework-268 假設你 real distribution 是兩群
GAN_Lecture_5_(2018)_-_General_Framework-269 但是你的 generator 只會產生同一群而已
GAN_Lecture_5_(2018)_-_General_Framework-270 他沒有辦法產生兩群不同的 data
GAN_Lecture_5_(2018)_-_General_Framework-271 舉例來說，你可能 train 一個人臉產生的系統
GAN_Lecture_5_(2018)_-_General_Framework-272 你會發現說，它產生出來的人臉，長得是這個樣子
GAN_Lecture_5_(2018)_-_General_Framework-273 你仔細一看覺得，看起來都還可以，也許沒什麼太大的問題
GAN_Lecture_5_(2018)_-_General_Framework-274 但你發現你在 update 一次參數以後，在 update 一次 generator 參數以後
GAN_Lecture_5_(2018)_-_General_Framework-275 產生出來的 image，變成這樣
GAN_Lecture_5_(2018)_-_General_Framework-276 所以發現說，其實剛才在前面產生的 image
GAN_Lecture_5_(2018)_-_General_Framework-277 他沒有產生黃皮膚的人
GAN_Lecture_5_(2018)_-_General_Framework-278 他只有產生膚色比較偏白的人，他沒產生黃皮膚的人
GAN_Lecture_5_(2018)_-_General_Framework-279 但是你今天 update 一次
GAN_Lecture_5_(2018)_-_General_Framework-280 它就變成產生黃皮膚的人，就沒產生白皮膚的人
GAN_Lecture_5_(2018)_-_General_Framework-281 再 update 一次，它就變成產生黑皮膚的人
GAN_Lecture_5_(2018)_-_General_Framework-282 他每次都只能產生某一種膚色的人
GAN_Lecture_5_(2018)_-_General_Framework-283 你的感覺好像是不太 ok 的
GAN_Lecture_5_(2018)_-_General_Framework-284 那為什麼會發生這種現象呢？
GAN_Lecture_5_(2018)_-_General_Framework-285 一個遠古的猜測是
GAN_Lecture_5_(2018)_-_General_Framework-286 也許是因為我們 divergence 選得不好
GAN_Lecture_5_(2018)_-_General_Framework-287 如果今天你的 data 的 distribution
GAN_Lecture_5_(2018)_-_General_Framework-288 是藍色這樣子的分佈
GAN_Lecture_5_(2018)_-_General_Framework-289 你的 generator 的 distribution
GAN_Lecture_5_(2018)_-_General_Framework-290 它只能有一個 mixture
GAN_Lecture_5_(2018)_-_General_Framework-291 它是綠色的虛線分佈
GAN_Lecture_5_(2018)_-_General_Framework-292 如果你選不同的 divergence
GAN_Lecture_5_(2018)_-_General_Framework-293 你最後 optimize 的結果
GAN_Lecture_5_(2018)_-_General_Framework-294 你最後選出來可以 minimize divergence 的那個 generator distribution
GAN_Lecture_5_(2018)_-_General_Framework-295 你會發現就是不一樣
GAN_Lecture_5_(2018)_-_General_Framework-296 假設你現在去 minimize KL divergence
GAN_Lecture_5_(2018)_-_General_Framework-297 你用 maximum likelihood 的方法，去 minimize KL divergence
GAN_Lecture_5_(2018)_-_General_Framework-298 那你的 generator 最後認為最好的那個 distribution
GAN_Lecture_5_(2018)_-_General_Framework-299 長得是這個樣子
GAN_Lecture_5_(2018)_-_General_Framework-300 那你發現說假設你的 generator distribution 長的是這個樣子
GAN_Lecture_5_(2018)_-_General_Framework-301 你從它裡面去 sample data
GAN_Lecture_5_(2018)_-_General_Framework-302 你 sample 在 mixture-mixture 之間
GAN_Lecture_5_(2018)_-_General_Framework-303 結果反而會是差的
GAN_Lecture_5_(2018)_-_General_Framework-304 所以這個可以解釋為什麼，過去沒有 GAN 的時候
GAN_Lecture_5_(2018)_-_General_Framework-305 我們是在 minimize KL divergence
GAN_Lecture_5_(2018)_-_General_Framework-306 我們是在 maximize likelihood，我們產生的圖片會那麼模糊
GAN_Lecture_5_(2018)_-_General_Framework-307 也許就是因為我們產生的 distribution 是這個樣子的
GAN_Lecture_5_(2018)_-_General_Framework-308 我們在 sample 的時候
GAN_Lecture_5_(2018)_-_General_Framework-309 其實並不是真的在 data density 很高的地方 sample
GAN_Lecture_5_(2018)_-_General_Framework-310 而是會 sample 到 data density 很低的地方
GAN_Lecture_5_(2018)_-_General_Framework-311 所以這地方就對應到模糊的圖片
GAN_Lecture_5_(2018)_-_General_Framework-312 那有人就說，如果你覺得是，KL divergence 所造成的
GAN_Lecture_5_(2018)_-_General_Framework-313 那如果你換別的 divergence
GAN_Lecture_5_(2018)_-_General_Framework-314 比如說你換 reverse KL divergence
GAN_Lecture_5_(2018)_-_General_Framework-315 那你就會發現說，對 generator 來說
GAN_Lecture_5_(2018)_-_General_Framework-316 最好的 distribution 是
GAN_Lecture_5_(2018)_-_General_Framework-317 完全跟某個 mode 一模一樣
GAN_Lecture_5_(2018)_-_General_Framework-318 就因為如果你看這個 reverse KL divergence 的式子，你就會發現說
GAN_Lecture_5_(2018)_-_General_Framework-319 對它來說，如果他產生出來的 data
GAN_Lecture_5_(2018)_-_General_Framework-320 是原來的 distribution
GAN_Lecture_5_(2018)_-_General_Framework-321 藍色 distribution 沒有涵蓋它 penalty 比較大
GAN_Lecture_5_(2018)_-_General_Framework-322 所以如果你今天選擇的是 reverse KL divergence
GAN_Lecture_5_(2018)_-_General_Framework-323 那你的那個 generator
GAN_Lecture_5_(2018)_-_General_Framework-324 它就會選擇集中在某一個 mode 就好
GAN_Lecture_5_(2018)_-_General_Framework-325 而不是分散在不同的 mode
GAN_Lecture_5_(2018)_-_General_Framework-326 而我們傳統的 GAN 的那個 js divergence
GAN_Lecture_5_(2018)_-_General_Framework-327 它比較接近 reverse KL divergence
GAN_Lecture_5_(2018)_-_General_Framework-328 這也許解釋了，為什麼你 train 一下 GAN 的時候
GAN_Lecture_5_(2018)_-_General_Framework-329 你會產生出來，你會有 mode collapse
GAN_Lecture_5_(2018)_-_General_Framework-330 或者是 mode dropping 的情形
GAN_Lecture_5_(2018)_-_General_Framework-331 因為對你的 generator 來說
GAN_Lecture_5_(2018)_-_General_Framework-332 產生這種 mode collapse 或 mode dropping 的情形
GAN_Lecture_5_(2018)_-_General_Framework-333 其實反而是比較 optimal 的，
GAN_Lecture_5_(2018)_-_General_Framework-334 所以今天 fGAN 厲害的地方就是
GAN_Lecture_5_(2018)_-_General_Framework-335 如果你覺得是 js divergence 的問題
GAN_Lecture_5_(2018)_-_General_Framework-336 那現在你可以換了，你可以換 KL divergence
GAN_Lecture_5_(2018)_-_General_Framework-337 但結果就是，換不同的 divergence，mode dropping 的 case
GAN_Lecture_5_(2018)_-_General_Framework-338 狀況還是一樣，所以看起來不是 mode dropping 或 mode collapse 的問題
GAN_Lecture_5_(2018)_-_General_Framework-339 並不完全是選擇不同的 divergence 所造成的
GAN_Lecture_5_(2018)_-_General_Framework-340 那你可能會問說，那我要怎麼解決 mode collapse 的問題呢？
GAN_Lecture_5_(2018)_-_General_Framework-341 你在做作業的時候，你很可能會遇到 mode collapse 的問題
GAN_Lecture_5_(2018)_-_General_Framework-342 我不是大家要產生 25 張圖嗎？
GAN_Lecture_5_(2018)_-_General_Framework-343 你的 generator 可能會產生出來的圖通通都是一樣的
GAN_Lecture_5_(2018)_-_General_Framework-344 那你這樣就會被扣分了，那要怎麼避免這個情形呢？
GAN_Lecture_5_(2018)_-_General_Framework-345 這邊有一個其實做得好的同學都會這麼做的，就是做 Ensemble
GAN_Lecture_5_(2018)_-_General_Framework-346 什麼意思呢？今天要你產生25 張圖片，你就 train 25 個 generator
GAN_Lecture_5_(2018)_-_General_Framework-347 然後你的每一個 generator 也許它都 mode collapse
GAN_Lecture_5_(2018)_-_General_Framework-348 也許你的 generator 1 只會產生這樣的圖
GAN_Lecture_5_(2018)_-_General_Framework-349 gnerator 2 只會產生這樣的圖
GAN_Lecture_5_(2018)_-_General_Framework-350 但沒有關係，你有 25 個 generator
GAN_Lecture_5_(2018)_-_General_Framework-351 所以實際上助教在 run 你的 code 的時候
GAN_Lecture_5_(2018)_-_General_Framework-352 其實是跑了 25 個 generator
GAN_Lecture_5_(2018)_-_General_Framework-353 每個 generator 會產生一張 image
GAN_Lecture_5_(2018)_-_General_Framework-354 但是對使用者來說，使用者並不知道你有很多個 generator
GAN_Lecture_5_(2018)_-_General_Framework-355 那所以你產生出來的結果，看起來就會 diverse
GAN_Lecture_5_(2018)_-_General_Framework-356 這是一個我覺得最有效可以避免 mode collapse 的問題
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-0 要講的就是 WGAN
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-1 我們之前已經有講到說，假設我們在 train GAN 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-2 最原始的 GAN，他量的是 generated data 跟 real data 之間的 js divergence
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-3 但是今天用 js divergence 來衡量的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-4 其實有一個非常嚴重的問題，這個嚴重的問題，他的根源是什麼呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-5 他的根源是，你的 generator 產生出來的 data distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-6 跟你的 real data 的 distribution，往往是沒有任何重疊的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-7 為什麼 generate 出來的 data，跟 real 的 data，往往是沒有重疊的呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-8 一個理由是，data 本質上的問題
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-9 因為我們通常相信說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-10 image 是一個 high dimensional 的高維空間中的低維的 manifold
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-11 也就是說 image 它實際上在高維空間中的分佈
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-12 其實是低維的一個 manifold，就是你把在三維空間中
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-13 然後把一個二維的平面折到三維的空間中
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-14 這樣就是一個低維的 manifold
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-15 或是說你有一個二維的平面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-16 如果 image 在這二維平面上的分佈可能就像是一條曲線一樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-17 那你今天在一個高維空間中的兩個低維的 manifold
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-18 它們的 overlap 的地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-19 幾乎是可以忽略的，你有兩條曲線，在一個二維的平面上
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-20 你有兩條曲線，他們中間重疊的地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-21 幾乎是可以忽略的，你的 Pdata 跟 PG
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-22 幾乎是可以忽略的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-23 那這是第一個理由
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-24 有人可能會說，我不相信
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-25 我覺得 image 不是高維空間中的低維的 manifold
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-26 那我從另外一個角度來說服你
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-27 我們實際上在衡量 PG 跟 Pdata 的 divergence 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-28 我們是先做 sample 對不對
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-29 我們從兩個 data distribution 裡面做一些 sample 得到兩堆 data
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-30 再用 discriminator 去量他們之間的 divergence
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-31 那所以我們現在就算你的 PG 跟 Pdata 這兩個 distribution 是有 overlap 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-32 但是你是先從這兩個 distribution 裡面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-33 做一些 sample，而且 sample 的時候，你也不會 sample 太多
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-34 也就是從紅色 distribution sample 一些點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-35 從藍色 distribution 再 sample 一些點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-36 你 sample 了兩堆點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-37 這兩堆點，它們的 overlap
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-38 幾乎是不會出現的，除非你 sample 真的很多
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-39 不然這兩堆點其實完全就可以視為是兩個沒有任何交集的 distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-40 所以就算是，本質上實際上 Pdata 跟 PG 有 overlap
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-41 但你在量 divergence 的時候你是 sample 少量的 data 出來
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-42 才量 divergence，那在你 sample 出來的少量 data 裡面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-43 PG 跟 Pdata，看起來就是沒有重合的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-44 那怎麼辦？會遇到什麼樣的問題呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-45 當 PG 跟 Pdata 沒有重合的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-46 你用 js divergence 來衡量 PG 跟 Pdata 之間的距離
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-47 他們 PG 跟 Pdata 之間的差異
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-48 會對你 training 的時候，造成很大的障礙
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-49 什麼樣的障礙呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-50 因為 js divergence 它的特性是這樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-51 如果兩個 distribution 沒有任何的重合，算出來就是 log 2
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-52 不管這兩個 distribution 實際上是不是有接近
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-53 只要沒有重合，沒有 overlap
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-54 算出來就是 log 2
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-55 所以今天假設你的 Pdata 是紅色這一條線
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-56 你有一個 G0，generator 0 號，G0 generate 出來的 distribution 是這樣一條線
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-57 你用 js divergence 去量，它們的 js divergence 是 log 2
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-58 這一個 case，你有一個 G1，你算 G1 跟 Pdata 的 divergence
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-59 它也是 log 2
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-60 雖然實際上 G1 其實是比 G0 好的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-61 因為 G1 產生出來的 data，其實相較於 G0
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-62 它更接近 real data distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-63 所以實際上，我們知道 G1 比 G0 好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-64 但從 js divergence 看起來，G1 和 G0 是一樣差的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-65 除非說現在你的 G100 跟 Pdata 完全重合
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-66 這時候 js divergence，算出來才會是 0
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-67 只要沒有重合，他們就算是非常的靠近
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-68 你算出來也是 log2
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-69 所以這樣子會對你的 training 造成問題
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-70 因為我們知道說我們實際上 training 的時候，generator 要做的事情
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-71 就是想要去 minimize 你的 divergence
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-72 你用 discriminator 量出 divergence，量出 js divergence，或其他 divergence 以後
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-73 generator 要做的事情是 minimize 你的 divergence
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-74 那對 generator 來說，PG0 跟 PG1，PG1 就是唱嘻哈的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-75 這講過了，PG0 跟 PG1 他們其實就是一樣的，他們其實是一樣差的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-76 或是一樣好的，他們其實是一樣差的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-77 所以對 generator 來說，他根本就不會把 PG0 update 成 PG1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-78 因為對他來說 PG0 跟 PG1 是一樣差的，所以你根本沒有辦法把 PG0 update 到 PG1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-79 你最後也沒有辦法 update 到 PG100，因為在 PG0 的地方就卡住了，他沒有辦法 update 到 PG1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-80 所以你今天是用 js divergence 來衡量兩個 distribution，而恰好這兩個 distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-81 又沒有太多重疊，他們重疊幾乎可以無視的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-82 你會發現，你 train 起來是有問題的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-83 那為什麼這兩個 distribution 沒有重疊
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-84 我們剛才試著用兩個不同面向來說服你說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-85 data 的 distribution 跟 generator 的 distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-86 非常可能就是沒有重疊
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-87 那假如你說你對 js divergence 不熟，你不知道說 js divergence 只要沒有重疊，算出來就是 log2
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-88 那我們從另外一個直覺的方向來告訴你說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-89 為什麼今天只要兩個 distribution 沒有重合
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-90 他們算出來的 loss，他們量出來的 divergence 就會一樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-91 因為你想想看
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-92 我們今天實際上在量 js divergence 的時候， 我們做的事情是什麼？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-93 我們是說，我們有兩群 data
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-94 把它視為是兩個 class
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-95 learn 一個 discriminator，learn 一個 binary 的 classifier
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-96 你用 minimize cross entropy 當成你的 loss function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-97 你 learn 一個 binary 的 classifier
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-98 去分別出這兩組 data 之間的差異
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-99 但假設你 learn 的是一個 binary 的 classifier
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-100 其實只要這兩堆 data，沒有重合
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-101 它的 loss 就是一樣的，對不對？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-102 因為假設你 learn 一個 binary 的 classifier，它可以完全的分辨
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-103 這兩堆 data，它可以完全的分辨，這兩堆 data，只要沒有重合
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-104 binary 的 classifier，假如它 capacity 是無窮大
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-105 它就可以分辨這兩堆 data
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-106 在這兩堆 data 都可以分辨，在這兩堆 data 都可以分辨的前提之下
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-107 你算出來的 loss，其實會是一樣大或者是一樣小的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-108 對不對，那我們所以在 train binary classifier 的時候，你 train 到最後
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-109 你得到的那個 loss，或是你得到的那個 objective value
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-110 其實就是你的 js divergence
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-111 今天如果你的 binary 的 classifier
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-112 不管在這 case，還是在這個 case
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-113 它都可以完全把兩堆 data 分開
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-114 它算出來的 objective 都是一樣大，它算出來的 loss 都是一樣小的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-115 那意味著，你量出來的 divergence，就是一樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-116 總之這邊要告訴大家的事情是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-117 在原始的 GAN 裡面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-118 當你 train 的是一個 binary classifier 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-119 你會發現，你是比較難 train 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-120 因為對你的 GAN 來說，這個 case 和這個 case
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-121 其實是一樣差的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-122 或者是這邊是用另外一個直觀的方法來說明
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-123 假設這是你 real data 的 distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-124 假設這是你 fake data 的 distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-125 那我們今天要 learn 一個 binary classifier
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-126 這個 binary classifier 會給這些藍色的點 0 分
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-127 給這些綠色的點 1 分
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-128 那我們知道我們的 binary classifier 它的 output
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-129 是 sigmoid function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-130 所以它在接近 1 這邊特別平
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-131 它在接近 0 這邊特別平
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-132 那你 train 好這個 classifier 以後
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-133 本來我們是期待說，你 train 一個 generator
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-134 這個 generator 會帶領這些藍色的點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-135 順著這個紅色的線的 gradient
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-136 就 generator 會順著 discriminator 給我們的 gradient
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-137 去移動它的，去改變它的 generated distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-138 所以我們本來是期待 generator
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-139 會順著這個紅色線的 gradient
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-140 把藍色的點往右移
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-141 但實際上你會發現說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-142 這些藍色的點是不動的，為什麼
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-143 因為在這藍色的點附近的 gradient 都是 0
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-144 如果你今天是 train 一個binary 的 classifier，它的 output 有一個 sigmoid function 的話
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-145 他在藍色的點附近，它是非常平的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-146 你會發現說他的微分幾乎都是 0
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-147 你根本就 train 不動它
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-148 所以你真的實際上去 train 一個 binary classifier
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-149 你直接 train GAN，然後 train 一個 binary classifier 的話
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-150 你很容易遇到這樣子的狀況
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-151 過去的一個解法是說，不要把那個 binary 的 classifier train 的太好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-152 就不要 train 的太好，因為如果你 train 的太好的話
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-153 它把這些藍色的點，都給他 0
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-154 這邊就會變得很平
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-155 綠色點都給它 1，就會變得很平
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-156 不要讓它 train 的太好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-157 不要 update 太多次
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-158 讓它在這邊仍然保有一些斜率
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-159 不要讓它 train 的太好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-160 那這樣的問題就是，什麼叫做不要 train 的太好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-161 你就會很痛苦這樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-162 你搞不清楚什麼叫做不要 train 的太好，你不能夠
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-163 在 train discriminator 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-164 當然太小力不行，太小力沒辦法分別 real 跟 fake data
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-165 太大力也不行，太大力的話
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-166 你就會陷入這個狀況，你會陷入這個微分是 0
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-167 沒有辦法 train 的狀況
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-168 但是什麼叫做不要太大力，不要太小力
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-169 你就會很難控制
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-170 那在早年還沒有我們剛才講的種種 tip 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-171 GAN 其實不太容易 train 起來，所以你 train 的時候通常就是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-172 你一邊 update discriminator
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-173 然後你就一邊吃飯，然後你就看他 output 的結果
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-174 每 10 個 iteration 就 output 一次結果，我要看它好不好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-175 如果發現結果不好的話，就重做這樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-176 就一邊吃飯，一邊看那個結果
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-177 所以後來就有一個方法，叫做 Least Square GAN (LSGAN)
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-178 那 LSGAN 做的事情，就是把 sigmoid 換成 linear
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-179 那這樣子是怎麼回事呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-180 這樣子你就不會有這種在某些地方特別平坦的情形
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-181 因為你現在的 output 是linear 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-182 那我們本來是一個 classification problem
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-183 現在把 output 換成了 linear 以後呢
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-184 它就變成一個 regression 的 problem
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-185 這個 regression 的 problem 是怎麼樣呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-186 這 regression problem 是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-187 如果是 positive 的 example
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-188 我們就讓它的值越接近 1 越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-189 如果是 negative example，我們就讓它的值越接近 0 越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-190 但其實跟原來 train binary classifier 是非常像的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-191 只是我們把 sigmoid 拔掉，把它變成 linear
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-192 那今天很多人都會用的一個技術，叫做 WGAN
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-193 Wassertein Distance GAN，這邊有一個冷知識，就是唸法問題
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-194 WGAN 是什麼呢？在 WGAN 裡面我們做的事情是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-195 我們換了另外一種 evaluation 的 measure
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-196 來衡量 Pdata 跟 PG
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-197 我們之前說在原來的 GAN 裡面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-198 要衡量 Pdata 跟 PG 的差異
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-199 我們用的是 js divergence
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-200 在我們講 fGAN 的時候我們說，你不一定要用 js divergence，你其實可以用任何其他的 f divergence
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-201 那在 WGAN 裡面用的是 Earth Mover's Distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-202 或者又叫 Wassertein Distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-203 來衡量兩個distribution 的差異，那他其實不是 f divergence 的一種
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-204 所以在 fGAN 那個 table，其實是沒有 WGAN 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-205 所以這邊是另外不一樣的方法
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-206 同樣的地方是，就是你換了一個 divergence
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-207 來衡量你的 generated data 和 real data 之間的差異
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-208 那我們先來介紹一下，什麼是 Earth Mover's Distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-209 Earth Mover's Distance 的意思是這樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-210 假設你有兩堆 data，這兩個 distribution 叫做 P and Q
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-211 那 Earth Mover's Distance 的意思是説，你就想像成你是在開一台推土機
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-212 那你的土從 P 的地方剷到 Q 的地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-213 就 P 的地方是一堆土
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-214 Q 的地方是你準備要把土移過去的位置
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-215 然後你看你那推土機
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-216 把 P 的土鏟到 Q 那邊，所走的平均的距離
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-217 就叫做 Earth Mover's Distance，就叫做Wasserstein Distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-218 那這個 W Distance 怎麼定義呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-219 如果是在這個非常簡單的 case，我們假設 P 的 distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-220 就集中在一維空間中的某一個點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-221 Q 的 distribution，也集中在一維空間中的某一個點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-222 如果你要開一台推土機把 P 的土
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-223 挪到 Q 的地方去
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-224 那假設 P 跟 Q 它們之間的距離是 d
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-225 那你的 W Distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-226 P 這個 distribution 跟 Q distribution 的 W Distance 就等於 d
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-227 但是實際上你可能會遇到一個更複雜的狀況
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-228 假設你 P distribution 是長這個樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-229 假設你 Ｑ distribution 是長這個樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-230 那如果你今天要衡量這兩個 distribution 之間的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-231 Earth Mover's Distance，假設你要衡量他們之間的 W Distance，怎麼辦呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-232 為什麼會造成問題呢？因為你會發現說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-233 當你要把 P 的土鏟到 Q 的位置的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-234 其實有很多組不同的鏟法
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-235 舉例來說你可以說我把這邊的土挪到這邊來
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-236 把這邊的土挪到這邊來，想辦法把 P 變成 Q，
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-237 是你也可以說我捨近求遠，我故意把這個土鏟到這邊來
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-238 我把這個土鏟到這邊來，一樣也可以變成 Q
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-239 但是你的推土機走的平均距離是不一樣的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-240 這樣就會變成說同樣的兩個 distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-241 推土機走的距離不一樣，你不知道哪個才是 W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-242 我們說 W distance 就是你把某一堆土
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-243 鏟到你目標的位置去，平均所走的距離就是，W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-244 但現在你的問題就是，鏟土的方法有很多種
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-245 到底哪一個才是 W distance 呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-246 所以今天 w distance 實際上的定義是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-247 窮舉所有可能鏟土的方法
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-248 每種鏟土的方法，我們就叫它一個
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-249 moving plan，叫它一個鏟土的計畫
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-250 窮舉出所有鏟土的計畫，有的可能是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-251 比較有效的，有的可能是捨近求遠的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-252 窮舉出所有鏟土的計畫
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-253 每一個鏟土的計畫，推土機平均要走的距離通通都算出來
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-254 看哪一個距離最小，就是 W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-255 那今天在這個例子裡面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-256 其實最好的剷土的方法，是像這個圖上所示這個樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-257 那我們把同樣一堆土，就用同樣的顏色來表示它
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-258 所以你把這堆土，挪到這個地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-259 你把這堆土，挪到這個地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-260 你把這堆土，挪到這個地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-261 粉紅色這邊土，也挪一點到這個地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-262 這樣你用這一個 moving plan 來挪土的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-263 你的推土機平均走的距離是最短的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-264 這個平均走的距離就是 W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-265 那這邊是一個更正式的定義
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-266 如果你要把 Q distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-267 應該是把 P 挪到 Q，但是其實意思是一樣的，它是對稱的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-268 所以 P 挪到 Q，Q 挪到 P，你算出來的距離是一樣的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-269 不過我們剛才都是講 P 挪到 Q 的樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-270 假設你要把這個 P 的圖挪到  Q 這邊
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-271 那首先你要訂一個 moving plan
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-272 那什麼是一個 moving plan 呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-273 moving plan 其實你要表現它的話，你可以把它化做是一個 matrix
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-274 把它化做是一個矩陣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-275 今天這個矩陣，就是某一個 moving plan
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-276 我們把它叫做 gamma
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-277 那在這個矩陣上的每一個 element
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-278 就代表說，我們要從縱座標的這個位置
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-279 挪多少土到橫座標的這個位置
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-280 今天的每一個 element 的值，就代表說我們要從這個地方挪多少土
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-281 到這個位置上面去
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-282 這邊的值越亮，就代表說，我們挪的土越多
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-283 所以你會發現說，我們要從這邊挪很多的土到這邊...
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-284 如果是這一塊土，我們要挪一些到這裡
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-285 我們要挪一些到這裡，我想大家應該會了解我的意思
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-286 那因為這是一個 moving plan
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-287 所以實際上你會發現在它的 column
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-288 你把 column 這些值合起來就會變成這個 bar 的高度
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-289 如果你把 row 的這些值合起來
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-290 就會變成這個 bar 的高度，因為這邊所有的土
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-291 都會挪到這邊，這邊所有的土都會分配到這個位置
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-292 所以這邊合起來就是這個 bar 的高度
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-293 這邊合起來就是這個 bar 的高度
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-294 接下來你要做的事情是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-295 假設給你一個 moving plan
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-296 這個 moving plan 叫做 gamma
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-297 你會不會算用這個 moving plan 挪土的時候要走多少距離呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-298 那它很簡單嘛，你就算說假設這邊每一個縱軸的值
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-299 叫做 xp
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-300 橫軸的每一個值，叫做 xq
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-301 那你先算說從 xp 的這個位置，你就窮舉所有的xp 跟 xq 的組合
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-302 然後再算說從 xp 你要挪多少的土到 xq 去
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-303 那這個土要挪多少是受這個 gamma 決定的，是這個moving plan 決定的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-304 然後你再算 xp 跟 xq 之間的距離
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-305 再 summation over 所有 xp 跟 xq 的 pair，這個就是 moving plan
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-306 就是給一個 moving plan gamma 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-307 你算出來挪土的 average distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-308 那接下來這個 W distance 或 Earth mover's distance，它做的事情就是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-309 窮舉所有的 gamma
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-310 窮舉所有可能的 gamma
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-311 看哪一個gamma 他算出來的距離最小
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-312 這個最小的距離就是 W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-313 所以你會發現說 W distance，它是一個很神奇的 distance，為什麼？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-314 今天一般的 distance 就是直接套一個公式運算出來你就得到結果，對不對？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-315 但 W distance，你要算它的話
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-316 你要解一個 optimization problem，很麻煩
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-317 因為你要窮舉所有的 gamma
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-318 看哪一個 gamma 可以算出來距離最小，那個才是 W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-319 所以今天給你兩個 distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-320 要算 W distance 是很麻煩的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-321 因為你要解一個 optimization problem
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-322 才算得出 W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-323 那我們等一下再講說怎麼用 discriminator 來衡量 W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-324 但我們現在先來看說如果用 W distance 來衡量兩個 distribution 的距離
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-325 有什麼樣的好處，那我們之前有看到說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-326 假設你今天是用 js divergence
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-327 這一個 G0 跟 data 的距離
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-328 G50 跟 data 之間的距離
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-329 對 js divergence 來說，根本就是一樣的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-330 除非你今天可以把 G0 一步跳到 G100
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-331 然後讓 G100 正好跟 Pdata 重疊
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-332 不然 machine 在 update 你的 generator 參數的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-333 它根本沒有辦法從 G0 update 到 G50
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-334 因為在這個 case，js divergence 其實是一樣大
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-335 那這個其實就讓我想到一個演化上的例子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-336 我們知道說人眼是非常的複雜
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-337 是一個非常複雜的器官
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-338 有人就會想說，憑藉著天擇的力量，不段的突變
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-339 到底怎麼可能讓生物突然產生人眼呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-340 那也許天擇的假說並不是正確的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-341 但是實際上今天生物是怎麼從完全沒有眼睛，變到有眼睛呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-342 他並不是一步就產生眼睛
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-343 而是透過不斷微小的突變的累積
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-344 才產生眼睛這麼複雜的器官
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-345 比如說在一開始，生物只是在皮膚上面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-346 產生一些感光的細胞
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-347 那透過突變，某一些細胞具有感光的能力
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-348 也許是做得到的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-349 接下來呢，感光細胞所在的那個皮膚
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-350 就凹陷下去，凹陷的好處是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-351 光線從不同方向進來，就不同的感光細胞會受到刺激
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-352 那生物就可以判斷光線進來的方向
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-353 接下來因為有凹洞的關係所就會容易堆灰塵
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-354 就在裡面放了一些液體，然後免得灰塵跑進去
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-355 然後再用一個蓋子把它蓋起來，最後就變成眼睛這個器官
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-356 但是你要直接從皮膚就突然突變
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-357 變異產生出眼睛是不可能的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-358 所以就像人，沒有辦法一下子就長出翅膀變成一個鳥人一樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-359 天擇只能做小小的變異
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-360 而每一個變異都必須是有好處的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-361 那才能夠把這些變異累積起來，最後才能夠產生巨大的變異
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-362 所以從產生感光細胞，到皮膚凹陷下去
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-363 到產生體液把蓋子蓋起來等等
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-364 每一個小小步驟對生物的生存來說都是有利的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-365 所以演化才會由左往右走，生物才會產生眼睛
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-366 那如果要產生翅膀可能就比較困難，因為假設你一開始
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-367 產生很小的翅膀，沒有辦法飛的話
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-368 那就沒有佔到什麼優勢
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-369 所以人就沒有辦法一下子突變產生翅膀變成一個鳥人
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-370 那對這個 generator 來說也是一樣的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-371 它如果說 G50 並沒有比 G0 好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-372 你就沒有辦法從 G0，變到 G50
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-373 然後慢慢累積變化變到 G100
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-374 但是如果你用 W distance 就不一樣了
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-375 因為對 W distance 來說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-376 這兩個 distribution 他們之間的差異是 d0
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-377 這兩個 distribution 之間的差異是 d50
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-378 d50 是比 d0 還要小的，所以從 W distance 來說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-379 這個 distribution 是比這個 distribution 好的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-380 所以對 generator 來說，它就可以 update 參數
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-381 把 distribution 從這個地方挪到這個地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-382 直到最後你 generator 的 output 可以和 data 真正的重合
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-383 那接下來要問的問題就是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-384 我們現在要量 PG 和 Pdata 之間的差異
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-385 我們現在要量 PG 和 Pdata 之間的 W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-386 我們要怎麼去改 discriminator
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-387 讓他可以衡量 PG 和 Pdata 的 W distance 呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-388 這邊就是直接告訴大家結果，這個推論的過程
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-389 其實是非常複雜的，這個證明過程其實很複雜，所以我們就直接告訴大家結果
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-390 怎麼樣設計一個 discriminator，它 train 完以後
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-391 它的 objective function 的值，就是 W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-392 就像下面這個樣子，式子就像下面這個樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-393 這個式子其實看起來很單純，其實是也很容易理解的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-394 這個式子的意思是什麼？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-395 意思是說如果今天 x 是從 Pdata 裡面 sample 出來的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-396 讓它的 discriminator 的 output 越大越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-397 如果  x 是從 PG 裡面 sample 出來的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-398 讓它的 discriminator 的 output 越小越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-399 但是你不能夠光只讓從 data 裡面 sample 出來的 x
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-400 和從 PG 裡面 sample 出來的 x
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-401 你不能夠只讓這些從 Pdata sample 出來的 x，它的值越大越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-402 但 PG 裡面 sample 出來的 x，他的值越小越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-403 你還要有一個 constrain，這 constrain 是這樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-404 discriminator 必須要是一個 1-Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-405 那可能大家不見得知道 Lipschitz function 是什麼
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-406 等一下會給大家定義，你先記得說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-407 所謂的 1-Lipschitz function 意思是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-408 這個 discriminator，他是很平滑的， 他是很 smooth 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-409 這個就叫做 1-Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-410 為什麼這個 1-Lipschitz function 是必要的呢？當然你可以說根據證明
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-411 就是要這麼做，算出來才是 W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-412 但是你也可以非常的直觀地了解這件事
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-413 怎麼樣非常的直觀地了解這件事呢
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-414 你的直觀的了解方法可以是這樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-415 這個是 generator 產生出來的 sample
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-416 這個是從 real data 裡面 sample 出來的 samples
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-417 如果我們不考慮這個 constrain
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-418 我們只說要讓 discriminator 的分數越大越好，這些 data
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-419 帶到discriminator 裡面分數越大越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-420 這些 data 帶到discriminator 裡面分數越小越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-421 那你 train 的時候 discriminator 就會知道說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-422 這邊的分數要讓他一直拉高一直拉高
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-423 這邊的分數要讓他一直壓低一直壓低
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-424 如果你的這兩堆 data 是沒有 overlap 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-425 我們講過 real data 跟 generated data
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-426 它很有可能是沒有 overlap 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-427 如果這兩堆 data 是沒有 overlap 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-428 今天如果只是 discriminator 一味的要讓這些 data
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-429 值越來越高，這邊 data 值越來越小
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-430 它就崩潰了
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-431 因為這個 training 永遠不會收斂
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-432 他這個值可以越來越大直到無限大
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-433 他這個值可以越來越小直到無限小
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-434 你的 training 永遠不會停止
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-435 所以怎麼辦，你必須要有一個額外的限制
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-436 這個額外的限制是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-437 你今天的 discriminator，必須要是夠平滑的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-438 因為如果你今天的 discriminator 沒有給任何限制
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-439 那它可能 real data 這邊就跑到無窮大
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-440 這邊就會跑到無窮小
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-441 但是假設這邊跑到無窮大，這邊跑到無窮小
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-442 這一段距離之間的差距
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-443 就會變得很大，對不對，它們就會拉得很開
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-444 那你的這個 discriminator 它就不平滑了
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-445 那我們說，我們給一個限制是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-446 discriminator 必須是平滑的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-447 這樣就可以強迫你在 learn 這個 discriminator 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-448 不會 learn 到說這邊一直上升，這邊一直下降
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-449 永遠不會停下來，那最終還是會停下來的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-450 那實際上什麼是 1-Lipschitz function 呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-451 一個 Lipschitz function 它的定義是這個樣子的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-452 你有一個 function 叫做 f
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-453 如果它是一個 Lipschitz function 的話呢
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-454 他滿足下面的這個條件
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-455 你把 x1 帶到 f 裡面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-456 你把 x2 帶到 f 裡面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-457 然後把 f of x1 減掉 f of x2
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-458 這個差距必須小於等於 K 倍的 x1 跟 x2 之間的距離
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-459 所以這個 Lipschitz function 它的意思到底是什麼？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-460 他的意思是說，這邊是 output 的 change
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-461 這邊是 input 的 change，就是你的 input 從 x1 改到 x2
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-462 然後你的 output 會從 f of x1 變到 f of x2
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-463 那接下來這個 input 的差距
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-464 乘上 K 倍，要大於等於 output 的差距
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-465 意思就是說，當你 input 有一個變化的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-466 output 的變化不能太大
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-467 才能夠讓 input 的差距乘上 K 倍
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-468 大於等於 output 的差距
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-469 也就是說你 output 的差距不能夠太大
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-470 不能夠比 input 的差距大很多
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-471 那所謂的 1-Lipschitz function 意思就是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-472 把 k 設為 0，就是  1-Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-473 當你把 K 設為 1 的時候是  1-Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-474 當把 K 設為 1 的時候，意味著說，你 output 的變化
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-475 總是比 input 的變化要小的，那意味著說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-476 這個 function 它是一個 smooth 的 function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-477 舉例來說現在假設有兩個 function，一個是綠色的 function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-478 一個是藍色的 function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-479 那像藍色的 function 它變化這麼劇烈，它變化這麼劇烈
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-480 在某些地方，它的 output 的變化
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-481 可能會大過 input 的變化
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-482 所以那就不是 1-Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-483 那像藍色這個 function，他很平滑，它的變化很小
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-484 它在每一個地方，output 的變化都小於 input 的變化
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-485 那它就是一個 1-Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-486 那如果這個你記不起來也沒有關係，反正你就記得說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-487 一個 function 只要很 smooth，它就是一個 1-Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-488 只要很平滑它就是一個 1-Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-489 接下來問題是，怎麼解這個 optimization problem?
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-490 如果我們把這個 constrain
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-491 這個給 discriminator 的 constrain 拿掉
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-492 你就用 gradient ascent 去 maximize 它就好了，對不對
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-493 完全沒有任何的問題
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-494 對不對，你就用 gradient ascent 你就可以
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-495 maximize 大括號裡面的這個式子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-496 但現在問題是你的 discriminator 是有 constrain 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-497 我們一般在做 gradient decent 的時候，我們並不會給我們的參數 constrain
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-498 你會發現說如果你要給參數 constrain 的話
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-499 在 learning 的時候，還蠻困難的，你會
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-500 不太清楚應該要怎麼做
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-501 所以你今天要給 discriminator constrain
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-502 是蠻困難的，但實際上到底是怎麼做的呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-503 在最原始的 W GAN 裡面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-504 他的作法就是 weight clipping
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-505 weight clipping 的想法很簡單，它的想法是這樣子，它說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-506 我們今天一樣用原來的 gradient decent
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-507 或者說用 gradient ascent，因為現在要 maximize 你的 objective function，所以其實是 gradient ascent
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-508 用 gradient ascent 去 train 你的 model，去 train 你的 discriminator
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-509 但是 train 完之後，如果你發現你的 weight
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-510 大過某一個你事先設好的常數 c，就把它設為 c
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-511 如果小於 -c 就把它設為 -c，結束
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-512 那他希望說透過這個 weight clipping 的技術
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-513 可以讓你 learn 出來的 discriminator，它是比較平滑的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-514 因為你限制著它 weight 的大小
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-515 所以可以讓這個 discriminator 它在 output 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-516 沒有辦法產生很劇烈的變化
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-517 這個 discriminator 可以是比較平滑的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-518 但是你可能會問說，加了這個限制就可以讓他變成 1-Lipschitz function 嗎？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-519 答案就是不行，就是這樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-520 因為一開始也不知道要怎麼解這個問題，所以就胡亂想一招這樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-521 能動再說，那我覺得有時候做研究就是這樣子嘛
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-522 不需要一次解決所有的問題，在 W GAN 的第一篇原始 paper 裡面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-523 他就 propose 這個想法，他就 propose 説
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-524 如果 D 是 1-Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-525 那我們就可以有量 W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-526 但他不知道要怎麼真的 optimize 這個 problem，沒關係先胡亂提一個
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-527 擋著先，先propose，先把 paper publish 出去，再慢慢想這樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-528 所以先做一個方法叫做 weight clipping
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-529 那 weight clipping 就是非常簡單
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-530 那我們說在作業裡面你可以選擇實作 weight clipping
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-531 那很容易，這個就是秒做
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-532 那當然它的 performance 不見得是最好的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-533 因為你用這個方法他並沒有真的讓 D 限制在 1-Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-534 它就只是希望透過這個限制
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-535 可以讓你的 D 是比較 smooth 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-536 那這個是 W GAN 最原始的版本，用的是 weight clipping
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-537 後來就有一個新的招數，他不是用 weight clipping，它是用
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-538 gradient 的 penalty
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-539 那這個技術叫做 improved WGAN
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-540 或者是又叫做 WGAN GP
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-541 那 WGAN GP 這邊想要講的是什麼呢？他是說他的觀察是這個樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-542 如果今天一個 function 它是 1-Lipschitz function，if and only if
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-543 保證它呢它在每一個位置的 gradient 的 norm
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-544 都必須要小於等於 1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-545 這兩件事情其實是等價的，一個 discriminator 它是 1-Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-546 等價於，如果你對所有可能的 input x
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-547 都拿去對 discriminator 求他的 gradient 的話
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-548 這 gradient 的 norm 總是會小於等於 1 的，這兩件事情是等價的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-549 所以今天假設你不知道怎麼弄它，那你能不能弄這一項呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-550 你不知道怎麼限制你的 discriminator，是 1-Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-551 你能不能限制你的 discriminator 對所有的 input x
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-552 去算他的 gradient 的時候，它的 norm
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-553 都要小於等於 1 呢？這件事顯然是有辦法 approximate 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-554 要怎麼 approximate 呢？這個 approximate 方法就是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-555 在原來的這項後面，再加一個 penalize 的項
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-556 這一項的作用有點像是 regularization
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-557 這一項的作用是說，它對所有的 x 做積分
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-558 然後呢，取一個 max 0 然後 gradient 的 norm 減 1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-559 也就是說如果 gradient 的，如果這 gradient 的 norm 大於 1 的話
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-560 這邊就有值，就有 penalty，如果這個 gradient norm 小於 1 的話
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-561 那就沒有 penalty，如果 gradient norm &gt; 1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-562 這一項就會有值，就會有 penalty
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-563 所以今天在 train 這個 discriminator 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-564 今天在 training 的時候會儘量希望呢
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-565 這個 discriminator 它的 gradient norm，小於等於 1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-566 但實際上這麼做會有一個問題，因為你不可能對所有的 x 都做積分
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-567 我們說一個 function 是 Lipschitz function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-568 它的 if and only if 的條件是對所有的 x
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-569 這件事情都要滿足
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-570 但是你無法真的去 check 説
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-571 不管你是在 train 還是在 check 的時候，你都無法做到說 sample 所有的 x
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-572 讓他們通通滿足這個條件
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-573 這件事情你根本做不到
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-574 x 代表是所有可能的 image 喔，那個 space 這麼大
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-575 你根本無法 sample 所有的 x
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-576 保證這件事情成立，所以怎麼辦？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-577 這邊做的另外一個 approximation 是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-578 假設 x 是從某一個事先定好的 distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-579 這個事先定好的 distribution，叫做 P penalty
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-580 這個 x 是從 P penalty 那個 distribution sample 出來的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-581 我們只保證說在 P penalty 那個 distribution 裡面的 x
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-582 它的 gradient norm 小於等於 1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-583 其他部分就管不了那麼多，就無視他
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-584 管不了那麼多，你不可能讓所有的 x 它的  gradient norm 都小於 1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-585 我們現在只能確保說，我們從一個 distribution 叫 P penalty
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-586 裡面去 sample 出 x
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-587 在那個 P penalty distribution 被涵蓋的範圍內
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-588 我們讓 x gradient norm 小於 1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-589 這個 P penalty 長什麼樣子呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-590 在 W GAN GP 裡面，它 P penalty 是長這個樣子的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-591 它說從 Pdata 裡面 sample 一個點出來
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-592 從 PG 裡面 sample 一個點出來
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-593 把這兩個點相連，然後在這兩個點中間
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-594 做一個 random 的 sample，就在這兩個點所連成的直線間
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-595 做一個 random 的 sample，sample 出來的 x 就當作是從 P penalty sample 出來的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-596 這個紅色的點可以是 P data 裡面 sample 出來的任何點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-597 這個黃色的點可以是 PG 裡面 sample 出來的任何點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-598 從這兩個點連起來
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-599 從這個連線中間去 sample，就是 P penalty
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-600 所以 P penalty 的分佈大概就是在 PG 和 P data 中間
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-601 就是藍色的這一個範圍
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-602 那你可能會問說，為什麼會是這樣子呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-603 為什麼我們本來應該對，整個 space 整個 image 的 space 所有的 x
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-604 通通去給它 penalty
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-605 但為什麼只在藍色的部分給 penalty 是可以的呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-606 在原始的 improved WGAN paper 它是這樣寫的，他說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-607 剛才講過很多次了，給每個地方都給它 gradient penalty 是不可能的，他說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-608 就是說實驗做起來，這樣就是好的這樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-609 實驗做起來，這樣看起來是 ok 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-610 但是你從直覺上也可以了解說這麼做是 make sense 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-611 怎麼說這麼做是 make sense 的呢
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-612 因為我們今天在 train GAN 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-613 我們不是要 update 那個 generator
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-614 然後讓 generator 順著 discriminator 給我們的 gradient 的方向
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-615 挪到 P data 的位置去嗎，也就是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-616 我們要讓 generator 的這些點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-617 慢慢往作左移，往左移，在這個例子裡面 generator 的點，要慢慢往左移，挪到 P data 的位置去
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-618 那所以 generator 在挪動它的位置的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-619 在 update 參數的時候，它看的就是 discriminator gradient
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-620 所以應該只有在 generator output 的 distribution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-621 跟 real data 的 distribution，中間的連線這個區域
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-622 才會真的影響你最後的結果，對不對
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-623 因為今天這個 PG 是看著這個地方的 gradient，這個地方的斜率
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-624 去挪動它的參數，去 update 它的參數的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-625 所以只有 PG 和 P data 之間的區域
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-626 你需要去考慮你的 discriminator 的 shape*** 長什麼樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-627 其他這些地方，反正你的 generator 也走不到
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-628 那你就不需要去考慮 discriminator 的 shape 長什麼樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-629 所以我覺得在 PG 和 Pdata 中間做 sample 也是有道理的，也算是 make sense 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-630 接下來要再做另外一個 approximation
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-631 實際上在 WGAN GP 裡面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-632 他真的做的事情是這樣子，他說本來我們是希望這個 gradient norm
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-633 如果大過 1 給它 penalty，小於 1 不用 penalty
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-634 因為我們就是要 gradient norm 小於 1 嘛，沒有說小於 1 會怎麼樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-635 小於 1 是不用懲罰的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-636 但實際上在 WGAN 的 implementation 裡面，他說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-637 我們實際上 training 的時候，我們是希望 gradient 越接近 1 越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-638 本來理論上我們只需要 gradient &lt; 1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-639 大過 1 給他懲罰，小於 1 沒有關係
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-640 但實作的時候說，gradient norm 必須離 1 越接近越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-641 gradient norm &gt; 1 有懲罰，&lt; 1 也有懲罰
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-642 為什麼會這樣呢，在 paper 裡面說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-643 實驗上這麼做的 performance 是比較好的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-644 當然這個 improved WGAN 也不會是最終的 solution
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-645 實際上你很直覺的會覺得，它是有一些問題的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-646 舉例來說我這邊舉一個例子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-647 假設紅色的曲線是你的 data
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-648 你在 data 上 sample 一個點是紅色的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-649 你在黃色的是你的 distribution，這邊sample 一個點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-650 你說把他們兩個連起來，然後給這邊的這些線 constrain
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-651 你不覺得其實是不 make sense 的嘛
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-652 因為如果我們今天照理說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-653 我們只考慮黃色的點，要如何挪到紅色的點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-654 所以照理說，我們應該在紅色的這個地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-655 sample 一個點跟黃色是最近的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-656 然後比如說 sample 在這邊
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-657 然後只 penalize 這個地方跟黃色的點之間的 gradient
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-658 這個才 make sense 嘛，對不對，因為
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-659 到時候黃色的點，其實它要挪動的話，他也是走走走就走到最近的地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-660 他不會跨過這些已經有紅色點的地方跑到這裡來
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-661 這個是有點奇怪的，我認為他會走這個方向
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-662 而不是走這樣的方向
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-663 所以你 gradient penalty penalize 在這個地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-664 是有點奇怪的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-665 那其實 improved WGAN 後面還有很多其他的變形，大家可以自己找一下
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-666 其實像今年的 ICLR 2018
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-667 就有一個 improved WGAN 的變形，叫做
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-668 improved 的 improved WGAN 這樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-669 那 improved 的 improved WGAN 他一個很重要的不同是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-670 它的 gradient penalty 不是只放在 Pdata 跟 PG 之間
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-671 他覺得要放在這個紅色的區塊
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-672 而且還有另外一個也很像 WGAN 的變形叫做那個 dragon
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-673 就是龍的意思，paper title 就是 How to train your dragon？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-674 我發現它最開始的版本叫做 How to train your dragon？後來 submit 到 conference 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-675 沒有很討喜，所以後來名字就通通改了這樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-676 他在 Archive 上名字是有換過，本來叫 How to train your dragon？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-677 它已經改成一個正常的名字
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-678 那篇 paper 也是 propose 説，今天你要放 gradient penalty，應該放在 Pdata 的地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-679 是有幫助的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-680 其實還有另外一招叫做 spectrum norm
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-681 spectrum norm 是什麼呢？spectrum norm 是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-682 剛才那個那個什麼 WGAN 什麼都是一堆 approximation 嘛
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-683 你聽了也覺得不太舒服這樣，就他除了很多 experimentally 就是這個樣子，你聽了也不太舒服
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-684 spectrum norm 是這樣，他 propose 了一個方法，這個方法
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-685 真的可以限制你的 discriminator
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-686 在每一個位置的 gradient norm
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-687 都是小於 1 的，本來 WGAN GP 它只是 penalize 某一個區域的 gradient norm &lt; 1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-688 但是 spectrum norm 這個方法可以讓你的 discriminator learn 完以後
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-689 它在每一個位置的 gradient norm都是小於 1
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-690 那細節，這個也是最近的 paper ICLR 2018 的 paper
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-691 那細節我們就不提，大家就自己去看看，看看你作業裡面有沒有辦法 implement 這一個
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-692 這個我們還是說一下這個 GAN 的 algorithm
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-693 我們看一下怎麼從 GAN 改成 WGAN
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-694 那原來 GAN algorithm 大家都很熟
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-695 就 sample 一堆 image，從原來的 data 裡面 sample 一堆 image
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-696 然後 sample 一堆 vector
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-697 根據這些 vector 產生 generated 的 image x tilde
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-698 然後 learn 一個 discriminator
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-699 learn 的 objective function 是這個樣子，我們說這 objective function 就是量 js divergence
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-700 根據這個 objective function 去 update  你的 discriminator，然後這邊你通常會跑 k 次
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-701 接下來呢你再從你的某個 distribution 裡面 sample 一些 noise
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-702 從 prior distribution 裡面 sample 一些 noise
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-703 然後去 update 你的 generator
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-704 如果你要把原來的 GAN 改成 WGAN 的話
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-705 你只需要做這幾個改變，第一個改變是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-706 首先你要把 log D of x 改成 D of x
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-707 你要把 log (1- D of x) 改成 D of x，結束，這個就是秒改
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-708 那這邊要注意的地方是，在原來的 GAN 裡面
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-709 你的 discriminator 有 sigmoid
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-710 這些是不是必要的就有那個 sigmoid 你算出來才會是 js divergence
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-711 總之它是有那個 sigmoid 的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-712 但是在 WGAN 裡面，你要把 sigmoid 拔掉
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-713 讓它的 output 是 linear 的，算出來才會是 W distance
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-714 所以你要把它的 output 拔掉
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-715 接下來你在 update 你的 discriminator，在 train 你的 discriminator 的時候呢
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-716 要注意一下就是你要加上 weight clipping
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-717 或者是加上 gradient penalty，不然這個 training 可能是不會收斂的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-718 它會一直 train 下去，你要加上 weight clipping 或 gradient penalty
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-719 在 train generator 的時候也很容易
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-720 你就把本來是 log (1 - D of (G of z)) 改成 D of (G of z)，結束了
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-721 所以你總共只要改 4 個地方，一個地方是改這個 objective function
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-722 把 sigmoid 拿掉
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-723 然後把 weight clipping 加進去
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-724 然後改一下 generator update 的 objective function，就結束了
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-725 其實我們還有時間稍微講一下，還有一個東西叫做 Energy based GAN，就是 EBGAN
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-726 其實 EBGAN 還有另外一個變形叫做，把 EB 改過來就變成 BEGAN 這樣子
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-727 另外一個變形，那我們就不講，我們就講Energy based GAN，EBGAN 就好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-728 EBGAN 是什麼，EBGAN 他唯一跟一般的 GAN 不同的地方是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-729 它改了 discriminator 的 network 架構
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-730 本來 discriminator 是一個 binary 的 classifier 對不對
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-731 它現在把它改成 auto encoder，就這樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-732 generator 不要動它，但你的 discriminator 是一個 auto encoder
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-733 你的 discriminator 是這樣，input 一張 image
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-734 有一個 encoder，把它變成 code，然後有一個 decoder 把它解回來
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-735 接下來你算那個 auto encoder 的 reconstruction error
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-736 把 reconstruction error 乘一個負號，就變成你的 discriminator 的 output
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-737 所以 Energy based GAN 的意思就是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-738 你這個 discriminator 跟一般的 discriminator，其實也是一樣，input 一張 image
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-739 output 就是一個 scalar，那這個 scalar 是怎麼來的？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-740 這個 scalar 是從 auto encoder 算出來的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-741 本來你是說你有一個 binary classifier，他的 output 直接就是一個 scalar
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-742 現在變成說，你的 discriminator 是一個 auto encoder
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-743 auto encoder 丟一張 image 進去
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-744 它的 reconstruction error 就是你的 discriminator 的 output
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-745 也就是說這個 energy based GAN 它的假設就是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-746 假設某一張 image 它可以被 reconstruction 的越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-747 它的 reconstruction error 越低
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-748 代表它是一個 high quality 的 image
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-749 如果它很難被 reconstruct，它的 reconstruction error 很大
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-750 代表它是一個 low quality 的 image
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-751 那這種 EBGAN 他到底有什麼樣的好處呢？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-752 我覺得他最大的好處就是，你可以 pre train 你的 discriminator
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-753 你知道 auto encoder 在 train 的時候，不需要 negative example
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-754 你在 train 你的 discriminator 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-755 你需要 negative example，對不對
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-756 你在 train 你的 discriminator 的時候，它是一個 binary classifier
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-757 所以你要從你的 data 裡面去 sample 一些 real image
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-758 再用你的 generator sample 一堆 fake image
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-759 然後再去 train 你的 discriminator，去分別 real 跟 fake image
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-760 所以這個東西無法 pre trained
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-761 對不對，你沒有辦法只拿 positive example 去 train 一個 binary classifier
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-762 你沒辦法 pre train 它，所以這邊會造成的問題是什麼？會造成的問題是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-763 一開始你的 generator 很弱
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-764 所以它 sample 出來的 negative example 也很弱
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-765 用很弱的 negative example 你 learn 出來就是一個很弱的 discriminator
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-766 那 discriminator 必須要等 generator 慢慢變強以後，它才會越來越強
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-767 所以你 discriminator 一開始不會很厲害
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-768 你要 train 很久，才會讓 discriminator 變得比較厲害
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-769 但是 energy base GAN 就不一樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-770 discriminator 是一個 auto encoder
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-771 auto encoder 是可以 pre trained，auto encoder 不需要 negative example
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-772 你只要給它 positive example
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-773 讓它去 minimize reconstruction error 就好了
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-774 所以你真的要用 energy base GAN 的時候
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-775 你要先 pre-train 好你的 discriminator
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-776 先拿你手上的那些 real 的 image，去把你的 auto encoder 先 train 好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-777 所以你一開始的 discriminator，會很強
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-778 所以因為你的 discriminator 一開始就很強
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-779 所以你的 generator 一開始就可以 generate 很好的 image
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-780 所以如果你今天是用 energy base GAN，你會發現說你前面幾個 epoch
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-781 你就可以還蠻清楚的 image
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-782 那這個就是 energy base GAN 一個厲害的地方
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-783 那 energy based GAN 實際上在 train 的時候，還有一個細節你是要注意的，就是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-784 今天在 train energy based GAN 的時候，你要讓 real 的 example 它得到的分數越大越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-785 也就是 real example 它的 reconstruction error 越小越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-786 但是要注意你並不是要讓 generated 的 example 它的分數越小越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-787 或它的 reconstruction error 越大越好，為什麼？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-788 因為建設是比較難的破壞是比較容易的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-789 reconstruction error 要讓它變小很難，因為
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-790 你必須要 input 一張 image 把它變成 code
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-791 再 output 同樣一張 image，這件事很難
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-792 但是如果你要讓 input 跟 output 非常不像
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-793 這件事太簡單了，input 一張 image
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-794 你要讓它 reconstruction error 很大，不就 output 一個 noise 就很大了嗎？
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-795 所以如果你今天太專注於說要 maximize 這些 generated image 的 reconstruction error
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-796 那你的 discriminator，到時候就學到說看到什麼 image 都 output 那個 noise
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-797 就這邊反正就犧牲掉就好，反正這個也增加不了多少
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-798 這邊儘量壓低都 output noise，故意把它壓低
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-799 這個時候你的 discriminator 的 loss 可以把它變得很小，但這個不是我們要的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-800 所以實際上在做的時候，你會設一個 margin 説
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-801 今天 generator 的 reconstruction loss 只要小於某一個 threshold 就好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-802 當然 threshold 這個 margin 是你要手調的
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-803 所以就多一個參數要調就是了
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-804 這個 margin 意思是說 generator loss 只要小於 margin 就好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-805 不用再小，小於 margin 就好，不用讓它再更小
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-806 那今天其實還有另外一個東西也是有用到 margin 的概念，叫做
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-807 Loss-Sensitive GAN
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-808 這個時候要講這個它有一個很有趣的地方就是，它也是
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-809 LSGAN 這樣
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-810 我們有一個 least square GAN，這邊還有一個 Loss-Sensitive GAN
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-811 那 LSGAN 它也有用到 margin 的概念
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-812 我們之前在做 WGAN 的時候是說
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-813 如果是 positive example，就讓他的值越大越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-814 negative example，就讓他的值越小越好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-815 但是假設你已經有些 image，其實已經很 realistic
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-816 你讓它的值越小越好，其實也不 make sense 對不對
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-817 所以今天在 LSGAN 裡面它的概念就是，他加了一個叫做 margin 的東西
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-818 就是你需要先有一個方法，去 evaluate 說你現在產生出來的 image 有多好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-819 可能是把你產生出來的 image 呢
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-820 這邊有一個錯又忘了改，x double prime
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-821 如果今天這個 x double prime 跟 x
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-822 已經很像了
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-823 那它們的 margin 就小一點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-824 如果 x prime 跟 x 很不像，它們 margin 就大一點
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-825 所以你會希望 x prime 的分數\
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-826 被壓得很低，x double prime 的分數只要壓低過 margin 就好
GAN_Lecture_6_(2018)_-_WGAN,_EBGAN-827 不需要壓得太低
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-0 今天的計畫是這樣今天會講四件事
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-1 會講一些用 GAN 和 Feature Extraction 有關的事情
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-2 然後會講 Cycle GAN，或者是用 GAN 做 Unsupervised Conditional Generation
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-3 用 GAN 做 Unsupervised Conditional Generation 是作業 3-3 要做的事情
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-4 接下來會講 WGAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-5 最後會講一些 GAN 的應用怎麼用 GAN 來做一個智能的 Photoshop
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-6 最後助教會來講作業 3-3
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-7 第一堂課就來講一下一些和用 GAN 做 Feature Extraction 有關的事情
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-8 我想先跟大家講的是 InfoGAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-9 大家在作業 1 裡面或作業 3-1 3-2 也都會 train 一個 GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-10 我們知道 GAN 會 random input 一個 vector
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-11 然後 output 一個你要的 object
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-12 我們通常期待 input 的那個 vector 它的每一個 dimension 代表了某種 specific 的 characteristic
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-13 你改了 input 的某個 dimension
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-14 output 就會有一個對應的變化
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-15 然後你可以知道每一個 dimension 它做的事情是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-16 但是實際上未必有那麼容易
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-17 如果真的 train 了一個 GAN 你會發現
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-18 input 的 dimension 跟 output 的關係有甚麼觀察不到甚麼關係
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-19 這邊這是一個文獻上的例子
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-20 假設 train 了一個 GAN這個 GAN 做的事情
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-21 是手寫數字的生成
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-22 你會發現你改了 input 的某一個維度
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-23 對 output 來說這邊這個橫軸
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-24 代表說改變了 input 的某一個維度
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-25 output 的變化是看不太出規律的比如說這邊的 7
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-26 突然中間寫了一橫也不知道是甚麼意思搞不清楚說
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-27 改變了某一維度到底對 output 的結果
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-28 起了什麼樣的作用
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-29 為甚麼會這樣呢
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-30 因為我們原先期待說假設現在這個投影片上這個二維平面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-31 代表 generator input 的 random vector 的 space
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-32 假設 input 的 vector 只有兩維
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-33 我們通常期待在這個 latent 的 space 上面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-34 不同的 characteristic 的 object 它的分布是有某種規律性的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-35 我們這邊用不同的顏色來代表假設你在這個區塊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-36 你使用這個區塊的 vector 當作 generator 的 input
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-37 它 output 會有藍色的特徵，這個區塊會有橙色的特徵這個區塊會有黃色的特徵，這個區塊會有綠色的特徵
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-38 本來的假設是這些不同的特徵他們在 latent space 上的分布是有某種規律性的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-39 但是實際上也許它的分布是非常不規則的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-40 我們本來期待如果改變了 input vector 的某一個維度
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-41 它就會從綠色變到黃色再變到橙色再變到藍色它有一個固定的變化
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-42 但是實際上也許它的分布長的這個樣子
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-43 也許 latent space 跟你要生成的那個 object 之間的關係是非常繁複的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-44 所以當你改變某一個維度的時候你從藍色變到綠色再變到黃色又再變回藍色
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-45 你就覺得說不知道在幹嘛
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-46 所以 InfoGAN 就是想要解決這個問題
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-47 InfoGAN 的概念是這樣這邊是一個原來的 GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-48 我們知道原來的 GAN 就是有一個 generator有一個 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-49 在 InfoGAN 裡面你會把 input 的 vector 分成兩個部分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-50 比如說假設 input vector 是二十維就說前十維把它叫作 c
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-51 後十維我們把它叫作 z'
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-52 在 InfoGAN 裡面你會 train 一個 classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-53 這個 classifier 工作是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-54 這個 classifier 工作是他看 generator 的 output
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-55 然後決定根據這個 generator 這個 output 去預測現在 generator input 的 c 是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-56 所以這個 generator 吃這個 vector
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-57 產生了 x，classifier 要能夠從 x 裡面反推原來 generator 輸入的 c 是什麼樣的東西
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-58 在這個 InfoGAN 裡面你可以把 classifier 視為一個 decoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-59 這個 generator 視為一個 encoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-60 這個 generator 跟 classifier 合起來可以把它看作是一個 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-61 它跟傳統的 Autoencoder 做的事情是正好相反的所以這邊加一個雙引號
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-62 因為我們知道傳統的 Autoencoder 做的事情是給一張圖片，他把它變成一個 code
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-63 再把 code 解回原來的圖片
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-64 但是在 InfoGAN 裡面這個 generator 和 classifier 所組成的 Autoencoder 做的事情
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-65 跟我們所熟悉的 Autoencoder 做的事情是正好相反的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-66 在 InfoGAN 裡面，generator 是一個 code 產生一張 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-67 然後 classifier 要根據這個 image 決定那個原來的 code 是甚麼樣的東西
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-68 當然如果只有 train generator 跟 classifier 是不夠的這個 discriminator 一定要存在
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-69 為甚麼 discriminator 一定要存在
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-70 假設沒有 discriminator 的話
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-71 對 generator 來說因為 generator 想要幫助 classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-72 讓 classifier 能夠成功的預測x 是從什麼樣的 c 弄出來的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-73 如果沒有 discriminator 的話對 generator 來說
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-74 最容易讓 classifier 猜出 c 的方式就是直接把 c 貼在這個圖片上，結束
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-75 就把 c 貼在圖片的中間
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-76 然後 classifier 只要知道他去讀這個圖片中間的數值就知道 c 是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-77 那這樣就完全沒有意義不是我們要的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-78 所以這邊一定要有一個 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-79 discriminator 會檢查這張 image 看起來像不像是一個 real image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-80 如果 generator 為了要讓 classifier 猜出 c 是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-81 而刻意地把 c 原本的數值
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-82 我們期待是 generator 根據 c 所代表的資訊去產生對應的 x
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-83 但 generator 它可能就直接把 c 原封不動貼到這個圖片上
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-84 但是如果只是把 c 原封不動貼到這個圖片上
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-85 discriminator 就會發現這件事情不對
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-86 發現這看起來不像是真的圖片
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-87 所以 generator 並不能夠直接把 c 放在圖片裡面透露給 classifier 知道
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-88 InfoGAN 在實作上 discriminator 跟 classifier 往往會 share 參數
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-89 因為他們都是吃同樣的 image 當作 input
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-90 不過他們 output 的地方不太一樣一個是 output scalar，一個是 output 一個 code、vector
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-91 不過通常你可以讓他們的一些參數是 share 的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-92 為甚麼用 InfoGAN 這個架構為甚麼加上這個 classifier 以後
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-93 會有甚麼好處我們說我們剛才想要解的問題就是
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-94 input feature 它對 output 的影響不明確這件事
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-95 InfoGAN 怎麼解決 input feature 對 output 影響不明確這件事呢
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-96 InfoGAN 的想法是這個樣子
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-97 為了要讓 classifier 可以成功地從 image x 裡面知道原來的 input c 是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-98 generator 要做的事情就是
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-99 他必須要讓 c 的每一個維度
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-100 對 output 的 x 都有一個明確的影響
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-101 如果 generator 可以學到 c 的每一個維度對 output 的 x 都有一個非常明確的影響
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-102 那 classifier 就可以輕易地根據 output 的 image 反推出原來的 c 是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-103 如果 generator 沒有學到讓 c 對 output 有明確影響就像剛看到那個例子
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-104 改了某一個 dimension 對 output 影響是很奇怪的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-105 classifier 就會無法從 x 反推原來的 c 是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-106 在原來的 InfoGAN 裡面他把 input z 分成兩塊一塊是 c 一塊是 z'
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-107 這個 c 他代表了某些特徵也就是 c 的每一個維度代表圖片某些特徵
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-108 他對圖片是會有非常明確影響
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-109 如果你是做手寫數字生成，那 c 的某一個維度可能就代表了那個數字筆畫有多粗
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-110 那另外一個維度可能代表寫的數字的角度是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-111 其實在 generator input 裡面還有一個 z'在原始的 InfoGAN 裡面他還加一個 z'
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-112 z' 代表的是純粹隨機的東西代表的是那些無法解釋的東西
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-113 那有人可能會問這個 c 跟 z' 到底是怎麼分的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-114 我們怎麼知道前十維這個 feature 是應該對 output 有影響的，後十維這個 feature 他是屬於 z'
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-115 對 output 的影響是隨機的呢你不知道
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-116 但是這邊的道理是這個 c 並不是因為它代表了某些特徵
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-117 而被歸類為 c
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-118 而是因為他被歸類為 c 所以他會代表某些特徵
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-119 大家可以聽得懂這個意思嗎
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-120 這是個充滿哲理的話我不知道大家聽不聽得懂我的意思
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-121 並不是因為他代表某些特徵所以我們把他設為 c
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-122 而是因為他被設為 c 以後根據 InfoGAN 的 training
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-123 使得他必須具備某種特徵
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-124 希望大家聽得懂我的意思
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-125 這個是 InfoGAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-126 這個是文獻上的結果
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-127 第一張圖是 learn 了 InfoGAN 以後
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-128 他改了 c 的第一維，然後發現甚麼事
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-129 發現 c 的第一維代表了 digit
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-130 這個很神奇，改了 c 的第一維以後
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-131 更動他的數值就從 0 跑到 9
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-132 這個 b 是原來的結果，他有做普通的 GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-133 output 結果是很奇怪的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-134 改第二維的話你產生的數字的角度就變了
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-135 改第三維的話你產生的數字就從筆劃很細變到筆劃很粗
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-136 這個就是 InfoGAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-137 另外一個跟大家介紹的叫作 VAE-GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-138 VAE-GAN 是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-139 VAE-GAN 可以看作是用 GAN 來強化 VAE
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-140 也可以看作是用 VAE 來強化 GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-141 VAE 在 ML 有講過的就是 Autoencoder 的變形
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-142 這個 Variational Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-143 Autoencoder 大家都很熟就是有一個 encoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-144 有一個 decoder，encoder input x output 是一個 z
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-145 decoder 吃那個 z output 原來的 x
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-146 你要讓 input 跟 output 越近越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-147 這個是 Variational Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-148 如果是 Variational Autoencoder 你還會給 z 一個 constrain，希望 z 的分布像是一個 Normal Distribution
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-149 只是在這邊圖上沒有把它畫出來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-150 那 VAE-GAN 的意思是在原來的 encoder decoder 之外 再加一個 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-151 這個 discriminator 工作就是 check 這個 decoder 的 output x 看起來像不像是真的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-152 如果看前面的 encoder 跟 decoder 他們合起來是一個 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-153 如果看後面的這個 decoder 跟 discriminator在這邊 decoder 他扮演的腳色其實是 generator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-154 我們看這個 generator 跟 discriminator 他們合起來是一個 GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-155 在 train VAE-GAN 的時候，一方面 encoder decoder 要讓這個 Reconstruction Error 越小越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-156 但是同時 decoder 也就是這個 generator 要做到另外一件事
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-157 他會希望他 output 的 image 越 realistic 越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-158 如果從 VAE 的角度來看
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-159 原來我們在 train VAE 的時候
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-160 是希望 input 跟 output 越接近越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-161 但是對 image 來說
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-162 如果單純只是讓 input 跟 output 越接近越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-163 VAE 的 output 不見得會變得 realistic
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-164 他通常產生的東西就是很模糊的如果你實際做過 VAE 生成的話
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-165 因為根本不知道怎麼算 input 跟 output 的 loss
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-166 如果 loss 是用 L1 L2 norm那 machine 學到的東西就會很模糊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-167 那怎麼辦，就加一個 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-168 你就會迫使 Autoencoder 在生成 image 的時候不是只是 minimize Reconstruction Error
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-169 同時還要產生比較 realistic image讓 discriminator 覺得是 realistic
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-170 所以從 VAE 的角度來看
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-171 加上 discriminator 可以讓他的 output 更加 realistic
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-172 如果從 GAN 的角度來看
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-173 前面這邊 generator 加 discriminator 合起來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-174 是一個 GAN 然後在前面放一個 encoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-175 從 GAN 的角度來看，原來在 train GAN 的時候
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-176 你是隨機 input 一個 vector你希望那個 vector 最後可以變成一個 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-177 對 generator 來說他從來沒有看過真正的 image 長甚麼樣子
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-178 他要花很多力氣，你需要花很多的時間去調參數
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-179 才能夠讓 generator 真的學會產生真正的 image，知道 image 長甚麼樣子
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-180 但是如果加上 Autoencoder 的架構
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-181 在學的時候 generator 不是只要騙過 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-182 他同時要 minimize Reconstruction Error
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-183 generator 在學的時候他不是只要騙過 discriminator 他還有一個目標
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-184 他知道真正的 image 長甚麼樣子
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-185 他想要產生一張看起來像是 encoder input 的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-186 他在學習的時候有一個目標不是只看 discriminator 的 feedback
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-187 不是只看 discriminator 傳來那邊的 gradient
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-188 所以 VAE-GAN 學起來會比較穩一點
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-189 在 VAE-GAN 裡面，encoder 要做的事情就是要 minimize Reconstruction Error
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-190 同時希望 z 它的分布接近 Normal Distribution
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-191 對 generator 來說他也是要 minimize Reconstruction Error，同時他想要騙過 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-192 對 discriminator 來說他就要分辨一張 image 是真正的 image 還是生成出來的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-193 跟一般的 discriminator 是一樣的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-194 假如你對 VAE-GAN 有興趣的話這邊也是列一下 algorithm
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-195 這 algorithm 是這樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-196 有三個東西，一個 encoder 一個 decoder 一個 discriminator，他們都是 network
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-197 所以先 initialize 他們的參數這樣就是他的 algorithm
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-198 這 algorithm 是這樣說的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-199 我們要先 sample M 個 real image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-200 接下來再產生這 M 個 image 的 code把這個 code 寫作 z tilde
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-201 把 x 丟到 encoder 裡面產生 z tilde他們是真正的 image 的 code
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-202 接下來再用 decoder 去產生 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-203 你把真正的 image 的 code z tilde把 z tilde 丟到 decoder 裡面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-204 decoder 就會產生 reconstructed image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-205 就邊寫作 x tilde，x tilde 是 reconstructed image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-206 接下來 sample M 的 z這個現在 z 不是從某一張 image 生成的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-207 他們是從一個 Normal Distribution 生成的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-208 這邊的 z 是從某一張 image 生成的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-209 這邊這個 z 是從一個 Normal Distribution sample 出來的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-210 用這些從 Normal Distribution sample 出來的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-211 再丟到 encoder 裡面再產生 image 這邊叫做 x hat
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-212 現在總共有三種 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-213 一種是真的從 database 裡面 sample 出來的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-214 一個是從 database sample 出來的 image 做 encode
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-215 變成 z tilde 以後再用 decoder 再還原出來叫做 x tilde
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-216 還有一個是 generator 自己生成的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-217 他不是看 database 裡面任何一張 image 生成的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-218 他是自己根據一個 Normal Distribution sample 所生成出來的 image，這邊寫成 x hat
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-219 再來在 training 的時候，你先 train encoder，encoder 目標是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-220 他要 minimize Autoencoder Reconstruction Error
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-221 所以要讓真正的 image xi 跟 reconstruct 出來的 image x tilde 越接近越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-222 這應該是 decoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-223 這個不是 encoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-224 這個是一個 decoder，等一下告訴你叫甚麼名字
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-225 這是一個 decoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-226 這邊要講現在你有原來 input 的 image 跟 reconstructed image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-227 encoder 目的是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-228 他希望原來 input 的 image 跟 reconstructed imagex 跟 x tilde 越接近越好，這第一件他要做的事
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-229 第二件他要做的事情是他希望這個 x 產生出來的 z tilde 跟 Normal Distribution 越接近越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-230 這事本來 VAE 要做的事情
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-231 接下來 decoder 要做的事情是他同時要 minimize Reconstruction Error
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-232 他有另外一個工作是他希望他產生出來的東西
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-233 可以騙過 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-234 他希望他產生出來的東西discriminator 會給他高的分數
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-235 而現在 decoder 其實會產生兩種東西
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-236 一種是 x tilde 是 reconstructed image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-237 通常 reconstructed image 就是會看起來整個結構比較好但是比較模糊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-238 這個 x tilde 產生一個 reconstructed image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-239 這個 reconstructed image 就到 discriminator 裡面分數要越大越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-240 那你把 x hat 就是 machine 自己生出來的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-241 不是 reconstructed image 是自己生出來的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-242 丟到 discriminator 裡面，希望值越大越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-243 最後輪到 discriminator，discriminator 要做的事情是如果是一個 real image 給他高分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-244 如果是 faked image，faked image 有兩種
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-245 一種是 reconstruct 出來的，一種是自己生成出來的，都要給他低分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-246 這是 VAE-GAN 的作法
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-247 但其實我還看過另外一個做法是 discriminator 他不是一個 Binary Classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-248 我們之前看到 discriminator 都是一個 Binary Classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-249 他就是要鑑別一張 image 是 real 還是 fake
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-250 其實還有另外一個做法是
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-251 discriminator 其實是一個三個 class 的 classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-252 給他一張 image 他要鑑別他是 real 還是 generated 還是 reconstructed
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-253 因為 generated image 跟 reconstructed image 他們本質上看起來頗不像的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-254 在右邊的 algorithm 裡面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-255 是把 generated 跟 reconstructed 視為是同一個 class就是 fake 的 class，都當作 fake 的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-256 但是我有看過做法是把 generate 出來的 image 跟 reconstruct 出來的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-257 視為兩種不同的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-258 discriminator 必須去學著鑑別這兩種的差異
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-259 generator 在學的時候
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-260 這是 discriminator，discriminator 要把一張 image 分成
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-261 input 一張 image，discriminator 要把它分成三類
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-262 但對 generator 來說他有可能產生 generated image他也有可能產生 reconstructed image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-263 他都要試著讓這兩種 image discriminator 都誤判認為他是 real 的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-264 這個是 VAE-GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-265 VAE-GAN 是去修改了 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-266 還有另外一個技術叫做 BiGAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-267 BiGAN 他也是修改了 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-268 他怎麼做呢我們知道在 Autoencoder 裡面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-269 有一個 encoder，有一個 decoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-270 在 Autoencoder 裡面是把 encoder 的 output 丟給 decoder 去做 reconstruction
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-271 但是在 BiGAN 裡面不是
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-272 在 BiGAN 裡面就有一個 encoder 有一個 decoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-273 但是他們的 input output 不是接在一起的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-274 encoder 吃一張 image 他就變成一個 code
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-275 decoder 是從一個 Normal Distribution 裡面 sample 一個 z 出來丟進去，他就產生一張 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-276 但是我們並不會把 encoder 的輸出丟給 decoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-277 並不會把 decoder 的輸出丟給 encoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-278 這兩個是分開的有一個 encoder 有一個 decoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-279 這兩個是分開的那他們怎麼學呢
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-280 在 Autoencoder 裡面可以學是因為收集了一大堆 image 要讓 Autoencoder 的 input 等於 Autoencoder output
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-281 現在 encoder 跟 decoder 各自都只有一邊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-282 encoder 只有 input 他不知道 output target 是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-283 decoder 他只有 input他不知道 output 的 image 應該長甚麼樣子
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-284 怎麼學這個 encoder 跟 decoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-285 這邊的做法是再加一個 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-286 這個 discriminator 他是吃 encoder 的 input 加 output
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-287 他吃 decoder 的 input 加 output他同時吃一個 code z 跟一個 image x，一起吃進去
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-288 然後它要做的事情是鑑別 x 跟 z 的 pair 他們是從 encoder 來的還是從 decoder 來的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-289 所以它要鑑別一個 pair 他是從 encoder 來的還是從 decoder 來的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-290 其實 BiGAN 還有另外一個技術
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-291 跟他非常地相近
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-292 其實不只是相近根本是一模一樣，叫做 ALI
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-293 BiGAN 跟 ALI 如果沒記錯的話是同時發表在 ICLR 2017 上面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-294 有甚麼差別？就是沒有任何差別
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-295 不同的兩群人居然想出了一模一樣的方法
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-296 而且我發現 BiGAN 的 citation 比較高
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-297 我想原因就是因為他有 GAN然後 ALI 他沒有用到 GAN 這個字眼，citation 就少一點
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-298 我們先講一下 BiGAN 的 algorithm 然後再告訴你為甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-299 BiGAN 這樣做到底是有什麼樣的道理先講一下他的 algorithm
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-300 現在有一個 encoder 有一個 decoder 有一個 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-301 這個跟剛才講 VAE-GAN 他有一個 encoder 有一個 decoder 有一個 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-302 不過這邊 BiGAN 的運作方式跟 VAE-GAN 是非常不一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-303 BiGAN 有一個 encoder 有一個 decoder 有一個 discriminator，但每一個 iteration 裡面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-304 你會 sample M 個 realistic image 出來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-305 你會 sample M 個 code 出來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-306 這邊 encoder 會做的事情是
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-307 剛才講錯，這個不是 sample 出來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-308 先從 database 裡面 sample 出 M 張真的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-309 然後把這些真的 image 丟到 encoder 裡面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-310 encoder 會 output code 就得到了 M 組 code得到了 M 個 z tilde
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-311 這個是用 encoder 生出來的東西
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-312 接下來用 decoder 生東西
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-313 decoder 怎麼生東西sample M 個 code
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-314 這個從一個 Normal Distribution sample 出來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-315 把這些 code 丟到 decoder 裡面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-316 decoder 就產生他自己生成的 image x tilde
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-317 所以這邊沒有 tilde 的東西都是真的有 tilde 的東西都是生成的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-318 這邊有 M 個 real image 生成出 M 個 code
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-319 這邊有 M 個 code 生成出 M 個 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-320 接下來要 learn 一個 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-321 discriminator 工作是給他 encoder 的 input 跟 output 給他高分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-322 給它 decoder 的 input 跟 output 給它低分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-323 如果這個 pair 是 encoder 的 input 跟 output給他高分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-324 如果這個 pair 是 decoder 的 input 跟 output就給他低分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-325 有人會問為甚麼是 encoder 會給高分decoder 會給低分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-326 其實反過來講你也會問同樣的問題
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-327 不管是你要讓 encoder 高分 decoder 低分還是 encoder 低分 decoder 高分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-328 是一樣的，意思是完全一模一樣的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-329 learn 出來結果也會是一樣的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-330 它並沒有甚麼差別只是選其中一個方法來做就是了
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-331 encoder 跟 decoder 要做的事情就是去騙過 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-332 如果 discriminator 要讓 encoder 的 input output 高分decoder 的 input output 低分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-333 encoder decoder 他們就要聯手起來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-334 讓 encoder 的 input output 讓 discriminator 給它低分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-335 讓 decoder 的 input outputdiscriminator 給他高分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-336 所以 discriminator 要做甚麼事
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-337 encoder 跟 decoder 就要聯手起來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-338 去騙過 discriminator 就對了
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-339 到底要讓 encoder 高分還是 decoder 高分是無關緊要的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-340 這個是 BiGAN 的 algorithm
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-341 BiGAN 這麼做到底是有甚麼道理
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-342 我們知道 GAN 做的事情
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-343 這個 discriminator 做的事情就是在 evaluate 兩組 sample 出來的 data
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-344 到底他們接不接近
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-345 我們上週講過從 real database 裡面 sample 一堆 image 出來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-346 用 generator sample 一堆 image出來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-347 一個 discriminator 做的事情其實就是在量這兩堆 image 的某種 divergence 到底接不接近
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-348 這個道理是一樣的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-349 可以把 encoder 的 input output 合起來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-350 當作是一個 Joint Distribution
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-351 encoder input 跟 output 合起來有一個 Joint Distribution 寫成 P(x, z)
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-352 decoder input 跟 output 合起來也是另外一個 Joint Distribution Q(x, z)
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-353 discriminator 要做的事情就是去衡量這兩個 distribution 之間的差異
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-354 然後希望透過 discriminator 的引導讓這兩個 distribution 之間越近越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-355 希望在原來的 GAN 裡面，我們希望 PG generator 生成出來的 data distribution
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-356 跟 P data 越接近越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-357 這邊的道理是完全一模一樣的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-358 discriminator 希望 encoder input output 所組成的 Joint Probability
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-359 跟 decoder input output 所組成的 Joint Probability這兩個 Data Distribution 越接近越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-360 P 跟 Q 這兩個 distribution 越接近越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-361 所以 eventually 在理想的狀況下
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-362 應該會學到 P 這個 distribution也就是 encoder 的 input 跟 output 所組成的 distribution
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-363 跟 Q 這個 distribution，這兩個 distribution
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-364 他們是一模一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-365 如果最後他們 learn 到一模一樣的時候會發生甚麼事情
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-366 你可以輕易的證明這個沒什麼好特別證明的你用直觀想一下其實就是這個樣子
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-367 你可以輕易的知道如果 P 跟 Q 的 distribution是一模一樣的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-368 你把一個 image x' 丟到 encoder 裡面讓它給你一個 code z'
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-369 再把 z' 丟到 decoder 裡面讓它給你一個 image x'
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-370 x' 會等於原來的 input x'
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-371 你把 x' 丟進去它會產生 z'
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-372 你把 z' 丟到 decoder 裡面它會產生原來的 x'
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-373 你把 z'' 丟到 decoder 裡面讓它產生 x''
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-374 你就把 x'' 丟到 encoder 裡面那它就會產生 z''
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-375 所以 encoder 的 input 產生一個 output
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-376 再把 output 丟到 decoder 裡面會產生原來 encoder 的 input
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-377 decoder 給它一個 input 它產生一個 output
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-378 再把它的 output 丟到 encoder 裡面它會產生一模一樣的 input
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-379 雖然說實際上在 training 的時候encoder 跟 decoder 並沒有接在一起
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-380 但是透過 discriminator 會讓 encoder decoder 最終在理想上達成以下的特性
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-381 所以有人會問這樣 encoder 跟 decoder 做的事情是不是就好像是 learn 了一個 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-382 這個 Autoencoder input 一張 image 它變成一個 code
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-383 再把 code 用 decoder 解回原來一樣的 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-384 你還會 learn 一個反向的 Autoencoder所謂的反向的 Autoencoder 的意思是
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-385 decoder 吃一個 code 它產生一張 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-386 再從這個 image 還原回原來的 code
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-387 要讓 input 跟 output 越像越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-388 你要讓 encoder input 跟 decoder output 越像越好你要讓 decoder input 跟 encoder output 越像越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-389 假設在理想狀況下
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-390 BiGAN 它可以 learn 到 optimal 的結果
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-391 確實會跟同時 learn 這樣子一個 encoder 跟 Autoencoder 得到的結果是一樣的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-392 那有人就會問為甚麼不 learn 這樣子一個 encoder 跟一個 inverse Autoencoder 就好了呢
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-393 為甚麼還要引入 GAN這樣聽起來感覺上是畫蛇添足
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-394 我覺得如果用 BiGAN learn 的話
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-395 得到的結果還是會不太一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-396 這邊想要表達的意思是
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-397 learn 一個 BiGAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-398 跟 learn 一個下面這個 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-399 他們在最佳的 solution他們的 optimal solution 是一樣的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-400 但它他們的 Error Surface 是不一樣的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-401 如果這兩個 model 都 train 到 optimal 的 case
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-402 得到的結果會是一樣的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-403 但是實際上不可能 train 到 optimal 的 caseBiGAN 無法真的 learn 到 P 跟 Q 的 distribution 一模一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-404 Autoencoder 無法 learn 到 input 跟 output 真的一模一樣這件事情是不可能發生的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-405 所以不會真的收斂到 optimal solution
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-406 但不是收斂到 optimal solution 的狀況下這兩種方法 learn 出來的結果就會不一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-407 到底有甚麼不一樣，這邊沒有把文獻上的圖片列出來如果你看一下文獻上的圖片的話
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-408 一般的 Autoencoder learn 完以後input 一張 image 它就是 reconstruct 另外一張 image
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-409 跟原來的 input 很像
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-410 然後比較模糊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-411 這個大家應該都知道 Autoencoder 就是這麼回事
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-412 但是如果用 BiGAN 的話，其實也是 learn 出了一個 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-413 learn 了一個 encoder 一個 decoder他們合起來就是一個 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-414 但是當你把一張 image 丟到這個 encoder再從 decoder 輸出出來的時候
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-415 其實你可能會得到的 output 跟 input 是非常不像的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-416 它會比較清晰
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-417 但是非常不像
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-418 比如說你把一隻鳥丟進去
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-419 它 output 還是會是一隻鳥但是是另外一隻鳥
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-420 這個就是 BiGAN 的特性你可以去看一下它的 paper
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-421 如果跟 Autoencoder 比起來他們的最佳的 solution 是一樣的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-422 但是實際上 learn 出來的結果會發現這兩種 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-423 就是用這種 minimize Reconstruction Error 方法 learn 了一個 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-424 還是用 BiGAN learn 的 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-425 他們的特性其實是非常不一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-426 BiGAN 的 Autoencoder 它比較能夠抓到語意上的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-427 就像剛才說的你 input 一隻鳥
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-428 它知道牠是一隻鳥
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-429 它 reconstruct 出來的結果decoder output 也是一隻鳥
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-430 但是不是同一隻鳥
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-431 這就是一個還滿神奇的結果
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-432 這個是 BiGAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-433 有 BiGAN，Bi 就是二的意思
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-434 那就有三，就是 Triple GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-435 我沒有打算要仔細講 Triple GAN，但是你之後會知道為甚麼要這邊要特別放一個 Triple GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-436 我們可以非常快的跟大家解釋一下 Triple GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-437 Triple GAN 裡面有三個東西
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-438 一個 discriminator 一個 generator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-439 一個 classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-440 如果先不要管 classifier 的話
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-441 Triple GAN 本身就是一個 Conditional GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-442 Conditional GAN 上週講過了 input 一個東西，output 一個東西
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-443 比如說 input 一個文字然後就 output 一張圖片
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-444 generator 就是吃一個 condition這邊 condition 寫成 Y
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-445 然後產生一個 x
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-446 它把 x 跟 y 的 pair 丟到 discriminator 裡面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-447 discriminator 要分辨出 generator 產生出來的東西是 fake 的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-448 然後 real database sample 出來的東西就是 true
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-449 所以 generator 跟 discriminator 合起來就是一個 Conditional GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-450 這邊是沒有甚麼特別的地方
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-451 都是上周就已經講過的東西
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-452 這邊再加一個 classifier 是甚麼意思
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-453 這邊再加一個 classifier 意思是 BiGAN 是一個 Semi-supervised Learning 的做法 ( 講錯了 )
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-454 不是 BiGAN 一個 Semi-supervised Learning說錯了
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-455 Triple GAN 是一個 Semi-supervised Learning 的做法
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-456 在 Triple GAN setup 裡面假設有少量的 labeled data
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-457 但是大量的 unlabeled data
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-458 也就是說你有少量的 X 跟 Y 的 pair
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-459 有大量的 X 跟 Y 他們是沒有被 pair 在一起
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-460 所以 Triple GAN 它主要的目標
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-461 是想要去學好一個 classifier，這 classifier 可以 input X
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-462 然後就 output Y
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-463 你可以用 labeled data 去訓練 classifier你可以從有 label 的 data 的 set 裡面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-464 去 sample X Y 的 pair
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-465 去 train classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-466 但是同時也可以根據 generatorgenerator 會吃一個 Y 產生一個 X
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-467 把 generator 產生出來的 X Y 的 pair
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-468 也丟給這個 classifier 去學
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-469 它的用意就是增加 training data
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-470 本來有 labeled 的 X Y 的 pair 很少
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-471 但是有一大堆的 X 跟 Y 是沒有 pair 的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-472 所以用 generator 去給他吃一些 Y 讓它產生 X
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-473 得到更多 X Y 的 pair 去 train classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-474 這個 classifier 它也可以吃 X
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-475 然後去產生 Y
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-476 discriminator 會去鑑別這 classifier input 跟 output 之間的關係
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-477 看起來跟真正 X Y 的 pair 有沒有像
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-478 所以 Triple GAN 是一個 Semi-supervised Learning 的做法，這邊就不特別再仔細地說它
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-479 只是告訴大家有 Triple GAN 這個東西有 BiGAN 就要有 Triple GAN
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-480 最後我要很快地複習一下 Domain-adversarial training
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-481 因為等一下在講 Unsupervised Conditional Generation 的時候，我們用得上這個技術
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-482 這個技術在 ML 有講過，所以這邊就只是再複習一下
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-483 這個 Domain-adversarial training 要做的事情是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-484 就是要 learn 一個 generator這個 generator 工作就是抽 feature
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-485 假設要做影像的分類
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-486 這個 generator 工作就是吃一張圖片 output 一個 feature
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-487 在做 Machine Learning 的時候
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-488 很害怕遇到一個問題是
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-489 training data 跟 testing data 不 match
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-490 假設 training data 是黑白的 MNIST
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-491 testing data 是彩色的圖片
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-492 是彩色的 MNIST
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-493 你可能會以為你在這個 training data 上 train 起來apply 到這個 testing data 上
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-494 搞不好也 work，因為 machine 搞不好可以學到反正 digit 就是跟顏色無關
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-495 考慮形狀就好了
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-496 所以他在黑白圖片上 learn 的東西也可以 apply 到彩色圖片
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-497 但是事實上事與願違，machine 就是很笨
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-498 實際上 train 下去，train 在黑白圖片上apply 彩色圖片上
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-499 雖然你覺得 machine 只要學到把彩色圖片自己在某個 layer 轉成黑白的，應該就可以得到正確結果
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-500 但是實際上不是，它很笨，它就是會答錯
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-501 怎麼辦？我們希望有一個好的 generator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-502 這個 generator 做的事情是 training set 跟 testing set 的 data 不 match 沒有關係
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-503 透過 generator 幫你抽出 feature
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-504 然後在 training set 跟 testing set 雖然他們不 match他們的 domain 不一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-505 但是透過 generator 抽出來的 feature
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-506 他們有同樣的 distribution，他們是 match
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-507 這個就是 Domain-adversarial training
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-508 怎麼做呢這個圖在 Machine Learning 那門課有看過了
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-509 就 learn 一個 generator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-510 其實就是 feature extractor
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-511 它吃一張 image 它會 output 一個 feature
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-512 有一個 Domain Classifier 其實就是 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-513 這個 discriminator 是要判斷現在這個 feature 來自於哪個 domain
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-514 假設有兩個 domain，domain x 跟 domain y你要 train 在 domain x 上面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-515 apply 在 domain y 上面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-516 有兩個 domain，domain x 跟 domain y
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-517 要 train 在 domain x 上面 apply 在 domain y 上面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-518 然後這個時候 Domain Classifier 要做的事情是分辨這個 feature 來自於 domain x 還是 domain y
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-519 在這邊同時你又要有另外一個 classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-520 這個 classifier 工作是根據這個 feature 判斷假設現在是數字的分類
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-521 要根據這個 feature 判斷它屬於哪個 class
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-522 它屬於哪個數字
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-523 這三個東西是一起 learn 的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-524 但是實際上在真正 implement 的時候不一定要一起 learn
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-525 在原始 Domain-adversarial training 的 paper 裡面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-526 它就是一起 learn 的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-527 這三個 network 就是一起 learn
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-528 只是這個 Domain Classifier 它的 gradient 在 back propagate 的時候在這邊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-529 在進入 Feature Extractor 之前
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-530 會乘一個負號
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-531 但是實際上真的在 implement 的時候你不一定要同時一起 train
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-532 你可以 iterative train 就像 GAN 一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-533 在 GAN 裡面也不是同時 train generator 跟 discriminator你是 iterative 的去 train
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-534 有人可能會問能不能夠同時 train generator 跟 discriminator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-535 其實是可以的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-536 如果你去看 f-GAN 那篇 paper 的話
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-537 它其實就 propose 一個方法，它的 generator 跟 discriminator 是 simultaneously 同時 train 的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-538 就跟原始的 Domain-adversarial training 的方法是一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-539 我們有同學試過類似的作法，但發現同時 train 比較不穩
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-540 如果是 iterative train 其實比較穩
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-541 如果先 train Domain Classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-542 再 train Feature Extractor
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-543 先 train discriminator 再 train generator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-544 iterative 的去 train
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-545 它的結果會是比較穩的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-546 這個是 Domain-adversarial training
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-547 用類似這樣的技術可以做一件事情這件事情叫做 Feature Disentangle
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-548 Feature Disentangle 是甚麼意思
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-549 用語音來做一下舉例
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-550 在別的 domain 上比如說 image processing
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-551 或者是 video processing
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-552 這樣的技術也是用得非常多
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-553 用語音來做例子
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-554 假設 learn 一個語音的 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-555 learn 一個 sequence to sequence 的 Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-556 learn 一個 Autoencoder 它 input 是一段聲音訊號
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-557 把這段聲音訊號壓成 code
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-558 再把這段 code 透過 decoder 解回原來的聲音訊號
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-559 你希望 input 跟 output 越接近越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-560 就要 learn 這樣一個 sequence to sequence Autoencoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-561 它中間你的 encoder 會抽出一個 latent representation
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-562 現在你的期待是 latent representation 可以代表發音的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-563 但是你發現你實際 train 這樣 sequence to sequence Autoencoder 的時候
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-564 你抽出來未必能讓中間的 latent representation 代表發音的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-565 為甚麼？因為中間的 latent representation 它可能包含了很多各式各樣不同的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-566 因為 input 一段聲音訊號，這段聲音訊號裡面不是只有發音的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-567 它還有語者的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-568 還有環境的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-569 對 decoder 來說
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-570 這個 feature 裡面一定必須要同時包含各種的資訊包含發音的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-571 包含語者的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-572 包含環境的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-573 這個 decoder 根據所有的資訊合起來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-574 才可以還原出原來的聲音
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-575 我們希望做的事情是
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-576 知道在這個 vector 裡面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-577 到底哪些維度代表了發音的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-578 那些維度代表了語者的資訊或者是其他的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-579 這邊就需要用到一個叫做 Feature Disentangle 的技術
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-580 這種技術就有很多的用處因為你可以想像
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-581 假設你可以 learn 一個 encoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-582 它的 output 你知道那些維是跟發音有關的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-583 那些維是跟語者有關的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-584 你可以只把發音有關的部分
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-585 丟到語音辨識系統裡面去做語音辨識
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-586 只跟語者有關的部分丟到語者的 speaker 的 verification
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-587 speaker verification 中文怎麼翻
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-588 語者識別嗎
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-589 這個有很多很多的應用，比如說你打電話去花旗銀行的時候
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-590 聲紋比對
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-591 這個技術通常叫作聲紋比對
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-592 你就把有關語者的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-593 丟到聲紋比對的系統裡面去
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-594 然後它就會知道現在是不是某個人說的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-595 所以像這種 Feature Disentangle 技術有很多的應用
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-596 怎麼做到 Feature Disentangle 這件事
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-597 現在假設要 learn 兩個 encoder
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-598 一個 encoder 它的 output 就是發音的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-599 另外一個 encoder 它的 output 就是語者的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-600 然後 decoder 吃發音的資訊加語者的資訊合起來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-601 還原出原來的聲音訊號
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-602 接下來就可以把抽發音資訊的 encoder 拔出來
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-603 把它的 output 去接語音辨識系統
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-604 因為在做語音辨識的時候
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-605 常會遇到的問題是兩個不同的人說同一句話
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-606 它聽起來不太一樣，在聲音訊號上不太一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-607 如果這個 encoder 可以把語者的 variation、語者所造成的差異 remove 掉
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-608 對語音辨識系統來說辨識就會比較容易
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-609 對聲紋比對也是一樣，同一個人說不同的句子
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-610 他的聲音訊號也是不一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-611 如果可以把這種發音的資訊、content 的資訊、跟文字有關的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-612 把它濾掉，只抽出語者的特徵的話
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-613 對後面聲紋比對的系統也是非常有用
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-614 這件事怎麼做
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-615 怎麼讓機器自動學到這個 encoder，如果這三個東西 joint learn
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-616 當然沒有辦法保證它的 output 一定要是發音的資訊它的 output 一定要是語者的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-617 怎麼辦就需要加一些額外的 constrain
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-618 比如說對語者的地方
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-619 你可能可以假設現在 input 一段聲音訊號在訓練的時候
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-620 我們知道那些聲音訊號是同一個人說的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-621 這個假設其實也還滿容易達成的因為可以假設同一句話就是同一個人說的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-622 同一句話把它切成很多個小塊每一個小塊就是同一個人說的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-623 所以對 Speaker Encoder 來說，給它同一個人說的聲音訊號
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-624 雖然他們的聲音訊號可能不太一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-625 但是 output 的 vector、output 的 embedding 要越接近越好
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-626 同時假設 input 的兩段聲音訊號是不同人說的那 output 的 embedding 就不可以太像
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-627 他們要有一些區別
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-628 就算是這樣做，你只能夠讓 Speaker Encoder 的 output 考慮語者的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-629 沒有辦法保證 Phonetic Encoder output 一定是發音的資訊，因為也許語者的資訊也會被藏在綠色的 vector 裡
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-630 所以怎麼辦？這邊就可以用到 Domain Adversarial Training 的概念
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-631 再另外去 train 一個 Speaker Classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-632 這 Speaker Classifier 作用是甚麼
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-633 這 Speaker Classifier 作用是給它兩個 vector
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-634 它去判斷這兩個 vector 到底是同一個人說的還是不同的人說的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-635 Phonetic Encoder 要做的事情就是去想辦法騙過Speaker Classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-636 Speaker Classifier 要盡力去判斷給他兩個 vector 到底是同一個人說的還是不同人說的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-637 Phonetic Encoder 要想盡辦法去騙過 classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-638 這個其實就是個 GAN，後面就是 discriminator，前面就是 generator
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-639 如果 Phonetic Encoder 可以騙過 Speaker Classifier
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-640 Speaker Classifier 完全無法從這些 vector 判斷到底是不是同一個人說的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-641 那就意味著 Phonetic Encoder 它可以濾掉所有跟語者有關的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-642 只保留和語者無關的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-643 這個就是 Feature Disentangle 的技術這邊就是一些真正的實驗結果
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-644 training 是 train 在一個叫做 LibriSpeech 的 corpus 上面
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-645 它就是蒐集很多有聲書給機器去學
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-646 這邊的結果左邊是 Phonetic Encoder 的 output
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-647 右邊是 Speaker Encoder 的 output
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-648 上面兩個圖每一個點就代表一段聲音訊號
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-649 這邊不同顏色的點代表聲音訊號背後對應的詞彙是不一樣的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-650 但他們都是不同的人講的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-651 如果看 Phonetic Embedding 的 output 就會發現同樣的詞彙它是被聚集在一起的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-652 雖然他們是不同人講的但是 Phonetic Encoder 知道它會把語者的資訊濾掉
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-653 知道不同人講的聲音訊號不太一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-654 但是這些都是同一個詞彙
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-655 如果看 Speaker Encoder output 就會發現 Speaker Encoder output 很明顯就分成兩群
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-656 不同的詞彙發音雖然不太一樣
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-657 但是因為現在 Speaker Encoder 已經把發音的資訊都濾掉只保留語者的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-658 就會發現不同的詞彙都是混在一起的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-659 下面是兩個不同顏色的點代表兩個不同的 speaker兩個不同的語者他們所發出來的聲音訊號
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-660 就會發現如果是看 Phonetic Embedding看發音上面的資訊
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-661 兩個不同的人他們很有可能會說差不多的內容
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-662 他們說出來的可能就是那幾個詞彙
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-663 所以你會發現如果看 Phonetic Encoder 這兩個人的東西
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-664 是重疊在一起的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-665 這兩個 embedding 重疊在一起
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-666 如果看 Speaker Encoding 就會發現這兩個人的聲音是很明顯的分成兩群的
GAN_Lecture_7_(2018)_-_Info_GAN,_VAE-GAN,_BiGAN-667 這個就是 Feature Disentangle，這邊是舉語音做例子但是它也可以用在影像等等其他 application 上
GAN_Lecture_8_(2018)_-_Photo_Editing-0 修圖的例子，先來看一個 demo這個 demo 是這樣，右邊有很多的 bar
GAN_Lecture_8_(2018)_-_Photo_Editing-1 把這個 bar 調整一下以後
GAN_Lecture_8_(2018)_-_Photo_Editing-2 這些相片裡面的臉就會換
GAN_Lecture_8_(2018)_-_Photo_Editing-3 這 NVIDIA 做的
GAN_Lecture_8_(2018)_-_Photo_Editing-4 這個是變禿頭
GAN_Lecture_8_(2018)_-_Photo_Editing-5 再反過來就是頭髮會變多
GAN_Lecture_8_(2018)_-_Photo_Editing-6 變黑頭髮
GAN_Lecture_8_(2018)_-_Photo_Editing-7 變金頭髮
GAN_Lecture_8_(2018)_-_Photo_Editing-8 然後戴眼鏡
GAN_Lecture_8_(2018)_-_Photo_Editing-9 變男的
GAN_Lecture_8_(2018)_-_Photo_Editing-10 反過來就是變女的
GAN_Lecture_8_(2018)_-_Photo_Editing-11 這個是出現鬍子
GAN_Lecture_8_(2018)_-_Photo_Editing-12 讓大家笑了
GAN_Lecture_8_(2018)_-_Photo_Editing-13 反過來就生氣了
GAN_Lecture_8_(2018)_-_Photo_Editing-14 藍皮膚是嗎
GAN_Lecture_8_(2018)_-_Photo_Editing-15 就年輕有一個 attractive
GAN_Lecture_8_(2018)_-_Photo_Editing-16 變白這樣子
GAN_Lecture_8_(2018)_-_Photo_Editing-17 變小鼻子就自己調一下
GAN_Lecture_8_(2018)_-_Photo_Editing-18 你說有變藍皮膚嗎我沒有發現
GAN_Lecture_8_(2018)_-_Photo_Editing-19 我有時候再仔細看看有沒有變藍皮膚
GAN_Lecture_8_(2018)_-_Photo_Editing-20 接下來就講一下這個東西是怎麼做的
GAN_Lecture_8_(2018)_-_Photo_Editing-21 這個就是以後可以拿來自動修圖就是了
GAN_Lecture_8_(2018)_-_Photo_Editing-22 這怎麼做的？我們知道假設 train 一個人臉的 generator
GAN_Lecture_8_(2018)_-_Photo_Editing-23 會 input 一個 random vector 然後就 output 一個人臉這作業大家 3-1 的時候已經有做了
GAN_Lecture_8_(2018)_-_Photo_Editing-24 在一開始講 GAN 的時候跟大家說過 input vector 的每一個 dimension 其實可能對應了某一種類的特徵
GAN_Lecture_8_(2018)_-_Photo_Editing-25 只是問題是我們並不知道每一個 dimension 對應的特徵是甚麼
GAN_Lecture_8_(2018)_-_Photo_Editing-26 現在要講的是怎麼去反推出現在 input 的這個 vector 每一個 dimension 它對應的特徵是甚麼
GAN_Lecture_8_(2018)_-_Photo_Editing-27 現在的問題是這個樣子
GAN_Lecture_8_(2018)_-_Photo_Editing-28 你其實可以收集到大量的 image
GAN_Lecture_8_(2018)_-_Photo_Editing-29 你可以收集到這些 image 的 labellabel 說這張 image 裡面的人
GAN_Lecture_8_(2018)_-_Photo_Editing-30 是金頭髮的、是男的
GAN_Lecture_8_(2018)_-_Photo_Editing-31 是年輕的等等
GAN_Lecture_8_(2018)_-_Photo_Editing-32 你可以得到 image，你也可以得到 image 的特徵你也可以得到 image 的 label
GAN_Lecture_8_(2018)_-_Photo_Editing-33 但現在的問題是會搞不清楚這張 image 它到底應該是由甚麼 vector 所生成的
GAN_Lecture_8_(2018)_-_Photo_Editing-34 假設你可以知道生成這張 image 的 vector 長甚麼樣子
GAN_Lecture_8_(2018)_-_Photo_Editing-35 你就可以知道 vector 跟 label 之間的關係
GAN_Lecture_8_(2018)_-_Photo_Editing-36 因為現在有 image，然後有 image 的 label
GAN_Lecture_8_(2018)_-_Photo_Editing-37 你有 image 跟它特徵的 label
GAN_Lecture_8_(2018)_-_Photo_Editing-38 假設可以知道某一張 image 它可以用什麼樣的 random vector 丟到 generator 就可以產生這張 image
GAN_Lecture_8_(2018)_-_Photo_Editing-39 你就可以把這個 vector 跟 label 的特徵把它 link 起來
GAN_Lecture_8_(2018)_-_Photo_Editing-40 現在的問題就是給你一張 image
GAN_Lecture_8_(2018)_-_Photo_Editing-41 你其實並不知道什麼樣的 random vector 可以產生這張 image
GAN_Lecture_8_(2018)_-_Photo_Editing-42 所以這邊要做的第一件事情是
GAN_Lecture_8_(2018)_-_Photo_Editing-43 假設已經 train 好一個 generator
GAN_Lecture_8_(2018)_-_Photo_Editing-44 這個 generator 給一個 vector z 它可以產生一個 image x
GAN_Lecture_8_(2018)_-_Photo_Editing-45 這邊要做的事情是去做一個逆向的工程
GAN_Lecture_8_(2018)_-_Photo_Editing-46 去反推說如果給你一張現成的 image什麼樣的 z 可以生成這張現成的 image
GAN_Lecture_8_(2018)_-_Photo_Editing-47 做一個逆向的工程
GAN_Lecture_8_(2018)_-_Photo_Editing-48 怎麼做呢？
GAN_Lecture_8_(2018)_-_Photo_Editing-49 這邊的做法是再 learn 另外一個 encoder
GAN_Lecture_8_(2018)_-_Photo_Editing-50 再 learn 一個 encoder，這個 encoder 跟這個 generator 合起來 就是一個 Autoencoder
GAN_Lecture_8_(2018)_-_Photo_Editing-51 在 train 這個 Autoencoder 的時候 input 一張 image x
GAN_Lecture_8_(2018)_-_Photo_Editing-52 它把這個 x 壓成一個 vector z
GAN_Lecture_8_(2018)_-_Photo_Editing-53 希望把 z 丟到 generator 以後它 output 的是原來那張 image
GAN_Lecture_8_(2018)_-_Photo_Editing-54 在 train 的過程中
GAN_Lecture_8_(2018)_-_Photo_Editing-55 generator 的參數是固定不動的generator 是事先已經訓練好的就放在那邊
GAN_Lecture_8_(2018)_-_Photo_Editing-56 我們要做一個逆向工程猜出假設 generator 產生某一張 image x 的時候
GAN_Lecture_8_(2018)_-_Photo_Editing-57 應該 input 什麼樣的 z要作一個反向的工程
GAN_Lecture_8_(2018)_-_Photo_Editing-58 這個怎麼做？就是 learn 一個 encoder
GAN_Lecture_8_(2018)_-_Photo_Editing-59 然後在 train 的時候給 encoder 一張 image
GAN_Lecture_8_(2018)_-_Photo_Editing-60 它把這個 image 變成一個 code z
GAN_Lecture_8_(2018)_-_Photo_Editing-61 再把 z 丟到 generator 裡面讓它產生一張 image x
GAN_Lecture_8_(2018)_-_Photo_Editing-62 希望 input 跟 output 的 image 越接近越好
GAN_Lecture_8_(2018)_-_Photo_Editing-63 在 train 的時候要記得這個 generator 是不動的
GAN_Lecture_8_(2018)_-_Photo_Editing-64 因為我們是要對 generator 做逆向的工程
GAN_Lecture_8_(2018)_-_Photo_Editing-65 我們是要反推它用什麼樣的 z 可以產生什麼樣的 x
GAN_Lecture_8_(2018)_-_Photo_Editing-66 所以這個 generator 是不動的我們只 train encoder 就是了
GAN_Lecture_8_(2018)_-_Photo_Editing-67 在實作上，這個 encoder 因為它跟 discriminator 很像
GAN_Lecture_8_(2018)_-_Photo_Editing-68 所以可以拿 discriminator 的參數來初始化 encoder 的參數，總之這是一個實驗的細節就是了
GAN_Lecture_8_(2018)_-_Photo_Editing-69 接下來假設做了剛才那件事以後就得到一個 encoder
GAN_Lecture_8_(2018)_-_Photo_Editing-70 encoder 做的事情就是給一張 image 它會告訴你這個 image 可以用什麼樣的 vector 來生成
GAN_Lecture_8_(2018)_-_Photo_Editing-71 用什麼樣的 vector 可以生成這張 image
GAN_Lecture_8_(2018)_-_Photo_Editing-72 因為現在你就把 database 裡面的 image 都倒出來
GAN_Lecture_8_(2018)_-_Photo_Editing-73 然後反推出他們的 vector
GAN_Lecture_8_(2018)_-_Photo_Editing-74 就是這個 vector 可以生成這張圖
GAN_Lecture_8_(2018)_-_Photo_Editing-75 這個 vector 生這張圖
GAN_Lecture_8_(2018)_-_Photo_Editing-76 然後我們知道這些 image 他們的特徵是知道的
GAN_Lecture_8_(2018)_-_Photo_Editing-77 這些是短髮的人臉
GAN_Lecture_8_(2018)_-_Photo_Editing-78 這些是長髮的人臉
GAN_Lecture_8_(2018)_-_Photo_Editing-79 把短髮的人臉它的 code 推出來
GAN_Lecture_8_(2018)_-_Photo_Editing-80 再平均就得到一個短髮的人臉的代表
GAN_Lecture_8_(2018)_-_Photo_Editing-81 把這個長髮的人臉的 code 都平均就得到長髮人臉的代表
GAN_Lecture_8_(2018)_-_Photo_Editing-82 再把他們相減就知道在這個 code space 上面做什麼樣的變化，就可以把短髮的臉變成長髮的臉
GAN_Lecture_8_(2018)_-_Photo_Editing-83 你把短髮的臉加上這個向量 z long 它就會變成長髮
GAN_Lecture_8_(2018)_-_Photo_Editing-84 所以這個就講一下 z long 怎麼來
GAN_Lecture_8_(2018)_-_Photo_Editing-85 你就把長髮的 image x 它的 code 都找出來
GAN_Lecture_8_(2018)_-_Photo_Editing-86 把 x 丟到 encoder 裡面，把它 code 都找出來
GAN_Lecture_8_(2018)_-_Photo_Editing-87 然後變平均得到這個 z
GAN_Lecture_8_(2018)_-_Photo_Editing-88 把這個不是長髮的，短髮的 code 都找出來平均得到這個點，這兩個點相減，就得到 z long 這個向量
GAN_Lecture_8_(2018)_-_Photo_Editing-89 接下來在生成 image 的時候
GAN_Lecture_8_(2018)_-_Photo_Editing-90 給你一張短髮，你怎麼把它變長髮呢？
GAN_Lecture_8_(2018)_-_Photo_Editing-91 給你一張短髮 image x，你把 x 這張 image 丟到 encoder 裡面得到它的 code
GAN_Lecture_8_(2018)_-_Photo_Editing-92 再加上 z long 得到新的 vector z'
GAN_Lecture_8_(2018)_-_Photo_Editing-93 再把 z' 丟到 generator 裡面就可以產生一張長髮的圖就這樣子
GAN_Lecture_8_(2018)_-_Photo_Editing-94 所以剛才的 NVIDIA 的 demo 裡面
GAN_Lecture_8_(2018)_-_Photo_Editing-95 它做的事情就是它不是做各種的調整
GAN_Lecture_8_(2018)_-_Photo_Editing-96 它可以變黑頭髮
GAN_Lecture_8_(2018)_-_Photo_Editing-97 變男的、變年輕的，它就只是找出黑頭髮的 vector 長甚麼樣子
GAN_Lecture_8_(2018)_-_Photo_Editing-98 年輕的 vector 長甚麼樣子
GAN_Lecture_8_(2018)_-_Photo_Editing-99 是男的 vector 長甚麼樣子
GAN_Lecture_8_(2018)_-_Photo_Editing-100 給他任何一張 image，它就是把那張 image 變成 code
GAN_Lecture_8_(2018)_-_Photo_Editing-101 再把那個 code 加上你要的特徵的 vector
GAN_Lecture_8_(2018)_-_Photo_Editing-102 然後變成新的 vector 再丟到 generator
GAN_Lecture_8_(2018)_-_Photo_Editing-103 就可以產生新的圖了
GAN_Lecture_8_(2018)_-_Photo_Editing-104 剛才那個 demo 就是這樣做
GAN_Lecture_8_(2018)_-_Photo_Editing-105 那這邊還有另外一個 demo
GAN_Lecture_8_(2018)_-_Photo_Editing-106 這個 demo 是一個智能的 Photoshop
GAN_Lecture_8_(2018)_-_Photo_Editing-107 我突然發現沒有聲音不過沒有聲音你應該還是看得懂
GAN_Lecture_8_(2018)_-_Photo_Editing-108 其實還有另外一個版本
GAN_Lecture_8_(2018)_-_Photo_Editing-109 你發現不同的人幾乎在同樣的時間做差不多的事情
GAN_Lecture_8_(2018)_-_Photo_Editing-110 有另外一個版本的智能的 Photoshop
GAN_Lecture_8_(2018)_-_Photo_Editing-111 這是改人臉
GAN_Lecture_8_(2018)_-_Photo_Editing-112 接下來就講這種智能的 Photoshop 是怎麼做的
GAN_Lecture_8_(2018)_-_Photo_Editing-113 講一下它的作法，這個怎麼做呢
GAN_Lecture_8_(2018)_-_Photo_Editing-114 這個做法是這樣，首先 train 一個 GANtrain 一個 generator
GAN_Lecture_8_(2018)_-_Photo_Editing-115 這個 generator train 好以後
GAN_Lecture_8_(2018)_-_Photo_Editing-116 這個 generator 你從它的 latent space 隨便 sample 一個點
GAN_Lecture_8_(2018)_-_Photo_Editing-117 output 它都會產生一個假設 train 的時候是用商品的圖來 train
GAN_Lecture_8_(2018)_-_Photo_Editing-118 那你在 latent space 上面、在 z 的 space 上面隨便 sample 一個 vector
GAN_Lecture_8_(2018)_-_Photo_Editing-119 丟到 generator 裡面它就 output 一個商品
GAN_Lecture_8_(2018)_-_Photo_Editing-120 你拿不同位子做 sample 會 output 出不同商品
GAN_Lecture_8_(2018)_-_Photo_Editing-121 這個地方 sample 就 output 一個袋子這個地方 sample 就 output 一個鞋子
GAN_Lecture_8_(2018)_-_Photo_Editing-122 那接下來剛才不是看到智能的 Photoshop給一張圖片
GAN_Lecture_8_(2018)_-_Photo_Editing-123 然後在這個圖片上面稍微做一點修改
GAN_Lecture_8_(2018)_-_Photo_Editing-124 結果就會產生一個新的商品
GAN_Lecture_8_(2018)_-_Photo_Editing-125 這件事情是怎麼做的？
GAN_Lecture_8_(2018)_-_Photo_Editing-126 這個做法大概是這個樣子
GAN_Lecture_8_(2018)_-_Photo_Editing-127 先把這張圖片反推出它在 code space 上面的哪一個位子
GAN_Lecture_8_(2018)_-_Photo_Editing-128 然後接下來在 code space 上面做一下小小的移動
GAN_Lecture_8_(2018)_-_Photo_Editing-129 希望產生一張新的圖片
GAN_Lecture_8_(2018)_-_Photo_Editing-130 這張新的圖片一方面跟原來的圖片夠像
GAN_Lecture_8_(2018)_-_Photo_Editing-131 一方面它跟原來的圖片夠像新的圖片跟原來的圖片夠像
GAN_Lecture_8_(2018)_-_Photo_Editing-132 但同時又要滿足使用者給的指示
GAN_Lecture_8_(2018)_-_Photo_Editing-133 比如使用者說這個地方是紅色的
GAN_Lecture_8_(2018)_-_Photo_Editing-134 所以產生出來的圖片在這個地方是紅色的
GAN_Lecture_8_(2018)_-_Photo_Editing-135 但它仍然是一件 T-shirt
GAN_Lecture_8_(2018)_-_Photo_Editing-136 假設 GAN train 的夠好的話
GAN_Lecture_8_(2018)_-_Photo_Editing-137 只要在 code space 上做 sample你在這 code space 上做一些移動
GAN_Lecture_8_(2018)_-_Photo_Editing-138 你的 output 仍然會是一個商品
GAN_Lecture_8_(2018)_-_Photo_Editing-139 假設 GAN train 的夠好的話
GAN_Lecture_8_(2018)_-_Photo_Editing-140 在這 code space 上做 sample
GAN_Lecture_8_(2018)_-_Photo_Editing-141 output 出來的每個東西都是商品，只是有不同的特徵
GAN_Lecture_8_(2018)_-_Photo_Editing-142 所以你已經推出這張 image 對應的 code 就在這個地方
GAN_Lecture_8_(2018)_-_Photo_Editing-143 你把它小小的移動一下
GAN_Lecture_8_(2018)_-_Photo_Editing-144 就可以產生一張新的圖
GAN_Lecture_8_(2018)_-_Photo_Editing-145 然後這張新的圖要符合使用者給你的 constrain
GAN_Lecture_8_(2018)_-_Photo_Editing-146 接下來實際上怎麼做的呢？
GAN_Lecture_8_(2018)_-_Photo_Editing-147 實際上會遇到的第一個問題就是要給一張 image
GAN_Lecture_8_(2018)_-_Photo_Editing-148 你要反推它原來的 code 長甚麼樣子
GAN_Lecture_8_(2018)_-_Photo_Editing-149 怎麼做到這件事？有很多不同的做法
GAN_Lecture_8_(2018)_-_Photo_Editing-150 舉例來說一個可行的做法是你把它當作是一個 optimization 的 problem 來解
GAN_Lecture_8_(2018)_-_Photo_Editing-151 你就在這個 code space 上面想要找到一個 vector z*
GAN_Lecture_8_(2018)_-_Photo_Editing-152 這個 z* 可以產生所有的 image X^T
GAN_Lecture_8_(2018)_-_Photo_Editing-153 所以要解的是這樣一個 optimization problem
GAN_Lecture_8_(2018)_-_Photo_Editing-154 要找一個 z，把這個 z 丟到 generator 以後產生一張 image
GAN_Lecture_8_(2018)_-_Photo_Editing-155 這個 G(z) 代表一張產生出來的 image
GAN_Lecture_8_(2018)_-_Photo_Editing-156 產生出來的 image 要跟原來的圖片 X^T 越接近越好
GAN_Lecture_8_(2018)_-_Photo_Editing-157 L 是一個 Loss Function，它代表的是要衡量這個 G(z) 這張圖片跟 X^T 之間的差距
GAN_Lecture_8_(2018)_-_Photo_Editing-158 至於怎麼衡量他們之間的差距有很多不同的方法比如說你用 Pixel-wise 的方法
GAN_Lecture_8_(2018)_-_Photo_Editing-159 直接衡量 G(z) 這張圖片跟 X^T 的 L1 或 L2  的 loss
GAN_Lecture_8_(2018)_-_Photo_Editing-160 也有人會說它是用 Perception Loss
GAN_Lecture_8_(2018)_-_Photo_Editing-161 所謂 Perception Loss 是拿一個 pretrain 好的 classifier 出來
GAN_Lecture_8_(2018)_-_Photo_Editing-162 這個 pretrain 好的 classifier 就吃這張圖片得到一個 embedding
GAN_Lecture_8_(2018)_-_Photo_Editing-163 再吃 X^T 得到一個 embedding
GAN_Lecture_8_(2018)_-_Photo_Editing-164 希望 G(z) 根據 pretrain 的 classifier比如說 VGG 得到 embedding，跟 X^T 得到 embedding
GAN_Lecture_8_(2018)_-_Photo_Editing-165 他們越接近越好
GAN_Lecture_8_(2018)_-_Photo_Editing-166 找一個 z，這個 z 丟到 generator 以後
GAN_Lecture_8_(2018)_-_Photo_Editing-167 它跟你目標的圖片 X^T 越接近越好
GAN_Lecture_8_(2018)_-_Photo_Editing-168 就得到了 z*，這是一個方法
GAN_Lecture_8_(2018)_-_Photo_Editing-169 解這個問題可以用 Gradient Descent 來解
GAN_Lecture_8_(2018)_-_Photo_Editing-170 第二個方法就是我們剛才在講前一個 demo 的時候用的方法
GAN_Lecture_8_(2018)_-_Photo_Editing-171 learn 一個 encoder，這個 encoder 要把一張 image 變成一個 code z
GAN_Lecture_8_(2018)_-_Photo_Editing-172 這個 z 丟到 generator 要產生回原來的 image
GAN_Lecture_8_(2018)_-_Photo_Editing-173 這是個 Autoencoder，你希望 input 跟 output 越接近越好
GAN_Lecture_8_(2018)_-_Photo_Editing-174 還有一個方法，就是把第一個方法跟第二個方法做結合
GAN_Lecture_8_(2018)_-_Photo_Editing-175 怎麼做結合？因為第一個方法要用 Gradient Descent
GAN_Lecture_8_(2018)_-_Photo_Editing-176 Gradient Descent 可能會遇到一個問題就是 Local Minimum 的問題
GAN_Lecture_8_(2018)_-_Photo_Editing-177 所以在不同的地方做 initialization給 z 不同的 initialization 找出來的結果是不一樣的
GAN_Lecture_8_(2018)_-_Photo_Editing-178 你先用方法 2 得到一個 z
GAN_Lecture_8_(2018)_-_Photo_Editing-179 用方法 2 得到的 z 當作方法 1 的 initialization
GAN_Lecture_8_(2018)_-_Photo_Editing-180 再去 fine tune 你的結果可能得到的結果會是最好的
GAN_Lecture_8_(2018)_-_Photo_Editing-181 總之有不同方法可以從 x 反推 z
GAN_Lecture_8_(2018)_-_Photo_Editing-182 你可以從 x 反推 z 以後
GAN_Lecture_8_(2018)_-_Photo_Editing-183 接下來要解另外一個 optimization problem
GAN_Lecture_8_(2018)_-_Photo_Editing-184 這個 optimization problem 是要找一個 z
GAN_Lecture_8_(2018)_-_Photo_Editing-185 這個 z 可以做到甚麼事情？
GAN_Lecture_8_(2018)_-_Photo_Editing-186 這個 z 一方面，你把 z 丟到 generator 產生一張 image 以後
GAN_Lecture_8_(2018)_-_Photo_Editing-187 這個 image 要符合人下的 constrain
GAN_Lecture_8_(2018)_-_Photo_Editing-188 人下的 constrain 舉例來說這個地方要是紅色的
GAN_Lecture_8_(2018)_-_Photo_Editing-189 這個 U 代表有沒有符合 constrain
GAN_Lecture_8_(2018)_-_Photo_Editing-190 那至於甚麼樣叫做符合 constrain 這個就要自己去定義
GAN_Lecture_8_(2018)_-_Photo_Editing-191 這個寫智能 Photoshop 的 developer 要自己去定義
GAN_Lecture_8_(2018)_-_Photo_Editing-192 你用 G(z) 產生一張 image
GAN_Lecture_8_(2018)_-_Photo_Editing-193 接下來用 U 這個 function 去算這張 image 有沒有符合人定的 constrain
GAN_Lecture_8_(2018)_-_Photo_Editing-194 這是第一個要 minimize 的東西
GAN_Lecture_8_(2018)_-_Photo_Editing-195 第二個要 minimize 的東西是你希望新找出來的 z
GAN_Lecture_8_(2018)_-_Photo_Editing-196 跟原來的 z，假設原來是一隻鞋子
GAN_Lecture_8_(2018)_-_Photo_Editing-197 原來這隻鞋子，你反推出它的 z 就是 z0
GAN_Lecture_8_(2018)_-_Photo_Editing-198 你希望做一下修改以後，新的 z 跟原來的 z0 越接近越好
GAN_Lecture_8_(2018)_-_Photo_Editing-199 因為你不希望本來是一張鞋子
GAN_Lecture_8_(2018)_-_Photo_Editing-200 然後你畫一筆希望變紅色的鞋子但它變成一件衣服
GAN_Lecture_8_(2018)_-_Photo_Editing-201 不希望這個樣子，你希望它仍然是一隻鞋子
GAN_Lecture_8_(2018)_-_Photo_Editing-202 所以希望新的 vector z 跟舊的 z0 他們越接近越好
GAN_Lecture_8_(2018)_-_Photo_Editing-203 最後還可以多加一個 constrain這個 constrain 是來自於 discriminator
GAN_Lecture_8_(2018)_-_Photo_Editing-204 discriminator 會看你把 z 丟到 generator 裡面再產生出來的 image 丟到 D 裡面
GAN_Lecture_8_(2018)_-_Photo_Editing-205 把 generator output 再丟到 discriminator 裡面
GAN_Lecture_8_(2018)_-_Photo_Editing-206 discriminator 去 check 這個結果是好還是不好
GAN_Lecture_8_(2018)_-_Photo_Editing-207 你要找一個 z 同時滿足這三個條件
GAN_Lecture_8_(2018)_-_Photo_Editing-208 你要找一個 z 它產生出來的 image 符合使用者下的 constrain
GAN_Lecture_8_(2018)_-_Photo_Editing-209 你要找一個 z 它跟原來的 z 不要差太多
GAN_Lecture_8_(2018)_-_Photo_Editing-210 因為你希望 generate 出來的東西跟原來的東西仍然是同個類型的
GAN_Lecture_8_(2018)_-_Photo_Editing-211 希望找一個 z 它可以騙過 discriminator
GAN_Lecture_8_(2018)_-_Photo_Editing-212 discriminator 覺得你產生出來的結果是好的
GAN_Lecture_8_(2018)_-_Photo_Editing-213 就解這樣一個 optimization problem可能用 Gradient Descent 來解
GAN_Lecture_8_(2018)_-_Photo_Editing-214 就找到一個 z*
GAN_Lecture_8_(2018)_-_Photo_Editing-215 這個就可以做到剛才講的智能的 Photoshop
GAN_Lecture_8_(2018)_-_Photo_Editing-216 就是這個做出來的
GAN_Lecture_8_(2018)_-_Photo_Editing-217 GAN 在影像上還有很多其他的應用比如說它可以做 Super-resolution
GAN_Lecture_8_(2018)_-_Photo_Editing-218 我想你完全可以想像怎麼做 Super-resolution它就是一個 Conditional GAN 的 problem
GAN_Lecture_8_(2018)_-_Photo_Editing-219 input 模糊的圖 output 就是清晰的圖
GAN_Lecture_8_(2018)_-_Photo_Editing-220 在作業 3-2 我們做過 input 文字 output 圖現在只是 input 不是文字，input 是模糊的圖
GAN_Lecture_8_(2018)_-_Photo_Editing-221 output 是清晰的圖就結束了
GAN_Lecture_8_(2018)_-_Photo_Editing-222 要 train 的時候要蒐集很多模糊的圖跟清晰的圖的 pair
GAN_Lecture_8_(2018)_-_Photo_Editing-223 要蒐集這種 pair 很簡單，就把清晰的圖故意弄模糊就行了
GAN_Lecture_8_(2018)_-_Photo_Editing-224 實作就是這麼做
GAN_Lecture_8_(2018)_-_Photo_Editing-225 清晰的圖弄模糊比較容易，模糊弄清晰比較難
GAN_Lecture_8_(2018)_-_Photo_Editing-226 這個是文獻上的結果
GAN_Lecture_8_(2018)_-_Photo_Editing-227 這個是還滿知名的圖，如果你有看過 GAN 的介紹通常都會引用這組圖
GAN_Lecture_8_(2018)_-_Photo_Editing-228 這個是傳統的、不是用 network 的方法得到的結果
GAN_Lecture_8_(2018)_-_Photo_Editing-229 產生出來的圖是比較模糊的
GAN_Lecture_8_(2018)_-_Photo_Editing-230 這個是用 network 的方法產生出來的圖
GAN_Lecture_8_(2018)_-_Photo_Editing-231 這個是原圖，這個沒有用 GAN
GAN_Lecture_8_(2018)_-_Photo_Editing-232 這個有用 GAN，這是用 GAN 產生出來的圖
GAN_Lecture_8_(2018)_-_Photo_Editing-233 你會發現如果用 network 雖然比較清楚
GAN_Lecture_8_(2018)_-_Photo_Editing-234 但是在一些細節的地方，比如說衣領的地方
GAN_Lecture_8_(2018)_-_Photo_Editing-235 這個頭飾的地方還是有些模糊的
GAN_Lecture_8_(2018)_-_Photo_Editing-236 但是如果看這個 GAN 的結果的話
GAN_Lecture_8_(2018)_-_Photo_Editing-237 在衣領和頭飾的地方，花紋都是滿清楚的
GAN_Lecture_8_(2018)_-_Photo_Editing-238 有趣的地方你發現衣領的花紋雖然清楚
GAN_Lecture_8_(2018)_-_Photo_Editing-239 但衣領的花紋跟原圖的花紋其實不一樣
GAN_Lecture_8_(2018)_-_Photo_Editing-240 頭飾的花紋跟原圖的花紋是不一樣
GAN_Lecture_8_(2018)_-_Photo_Editing-241 機器自己創造出清晰的花紋反正能騙過 discriminator 就好
GAN_Lecture_8_(2018)_-_Photo_Editing-242 未必要跟原來的花紋是一樣的
GAN_Lecture_8_(2018)_-_Photo_Editing-243 這是 image 的 Super-resolution
GAN_Lecture_8_(2018)_-_Photo_Editing-244 現在還會做的一個事情是 Image Completion
GAN_Lecture_8_(2018)_-_Photo_Editing-245 Image Completion 就是給一張圖片
GAN_Lecture_8_(2018)_-_Photo_Editing-246 然後它某個地方挖空
GAN_Lecture_8_(2018)_-_Photo_Editing-247 機器自己把挖空的地方補上去
GAN_Lecture_8_(2018)_-_Photo_Editing-248 這個怎麼做？這個就是 Conditional GAN
GAN_Lecture_8_(2018)_-_Photo_Editing-249 就是給機器一張有挖空的圖
GAN_Lecture_8_(2018)_-_Photo_Editing-250 它 output 一張填進去的圖就結束了
GAN_Lecture_8_(2018)_-_Photo_Editing-251 怎麼產生這樣的 training data？它非常容易產生
GAN_Lecture_8_(2018)_-_Photo_Editing-252 就找一堆圖片，中間故意挖空就得到這種 training data pair
GAN_Lecture_8_(2018)_-_Photo_Editing-253 然後就結束了
GAN_Lecture_8_(2018)_-_Photo_Editing-254 這個是這樣，就是它可以把圖片補回去
GAN_Lecture_8_(2018)_-_Photo_Editing-255 假設是 previous approach，也就是前人的做法結果是怎樣
GAN_Lecture_8_(2018)_-_Photo_Editing-256 把涼亭的柱子挖掉，柱子就不見了就變成樹一樣
GAN_Lecture_8_(2018)_-_Photo_Editing-257 它的方法就好很多
GAN_Lecture_8_(2018)_-_Photo_Editing-258 把人塗掉
GAN_Lecture_8_(2018)_-_Photo_Editing-259 就沒有人了
GAN_Lecture_8_(2018)_-_Photo_Editing-260 不戴眼鏡
GAN_Lecture_8_(2018)_-_Photo_Editing-261 其實就這樣子
GAN_Lecture_8_(2018)_-_Photo_Editing-262 你說他怎麼知道這邊應該有頭髮
GAN_Lecture_8_(2018)_-_Photo_Editing-263 GAN learn 出來就是這個樣子
GAN_Lecture_8_(2018)_-_Photo_Editing-264 大多數人都是有頭髮的，GAN learn 出來就覺得那邊應該是要有頭髮
GAN_Lecture_9_(2018)_-_Sequence_Generation-0 我們今天要講的是用 GAN 來 improve sequence generation
GAN_Lecture_9_(2018)_-_Sequence_Generation-1 那  sequence generation 的 task 它有非常多的應用
GAN_Lecture_9_(2018)_-_Sequence_Generation-2 那我們先講一下怎麼用 GAN 來 improve 這個 conditional sequence generation
GAN_Lecture_9_(2018)_-_Sequence_Generation-3 然後接下來我們會講說
GAN_Lecture_9_(2018)_-_Sequence_Generation-4 今天假設有了 GAN 的技術以後
GAN_Lecture_9_(2018)_-_Sequence_Generation-5 其實我們可以做到 unsupervised conditional generation
GAN_Lecture_9_(2018)_-_Sequence_Generation-6 我們上次有講過，如果有 Unsupervised conditional generation 的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-7 你可以做比如說 image 的 style transfer
GAN_Lecture_9_(2018)_-_Sequence_Generation-8 把 image 在不同風格間作轉換
GAN_Lecture_9_(2018)_-_Sequence_Generation-9 或者是做 voice style transfer 也就是 speaker conversion
GAN_Lecture_9_(2018)_-_Sequence_Generation-10 把 A 的聲音轉成 B 的聲音等等
GAN_Lecture_9_(2018)_-_Sequence_Generation-11 那今天我們要講，有的 GAN 技術以後
GAN_Lecture_9_(2018)_-_Sequence_Generation-12 我們不只可以用 GAN 來 improve  conditional sequence generation
GAN_Lecture_9_(2018)_-_Sequence_Generation-13 我們還可以做到 Unsupervised conditional generation
GAN_Lecture_9_(2018)_-_Sequence_Generation-14 那我們先講一下 conditional sequence generation 指的是什麼
GAN_Lecture_9_(2018)_-_Sequence_Generation-15 那其實只要是要產生一個 sequence 的 task
GAN_Lecture_9_(2018)_-_Sequence_Generation-16 都是 conditional sequence generation
GAN_Lecture_9_(2018)_-_Sequence_Generation-17 舉例來說語音辨識可以看作是一個 conditional sequence generation 的 task
GAN_Lecture_9_(2018)_-_Sequence_Generation-18 你需要的是一個 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-19 input 是聲音訊號，output 就是語音辨識的結果
GAN_Lecture_9_(2018)_-_Sequence_Generation-20 就是這一段聲音訊號所對應到的文字
GAN_Lecture_9_(2018)_-_Sequence_Generation-21 或者是說假設你要做翻譯，你要做 translation 的話呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-22 那你的 input，假設你要中翻英，你的 input 是中文
GAN_Lecture_9_(2018)_-_Sequence_Generation-23 那你的 output 就是翻譯過的結果
GAN_Lecture_9_(2018)_-_Sequence_Generation-24 是一串 word sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-25 或者是說我們在作業二裡面有做一個 chatbot，那其實 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-26 也是一個 conditional sequence generation 的 task
GAN_Lecture_9_(2018)_-_Sequence_Generation-27 它的 input 是一個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-28 它的  output 是另外一個 sequence，是另外一個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-29 其實你可以用今天我們在這一堂課學到的技術
GAN_Lecture_9_(2018)_-_Sequence_Generation-30 來 improve 你在作業二的時候，所學出來的 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-31 那我們之前有講過說，其實這些 task
GAN_Lecture_9_(2018)_-_Sequence_Generation-32 語音辨識，翻譯或 chatbot，你是怎麼解它的呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-33 你都是用 sequence to sequence 的 model 來解它的
GAN_Lecture_9_(2018)_-_Sequence_Generation-34 所以實際上這邊這個圖上所畫的 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-35 它們都是 sequence to sequence 的 model
GAN_Lecture_9_(2018)_-_Sequence_Generation-36 只是我們原來在 train seq2seq model 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-37 我們有講過說怎麼 train 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-38 大家在作業二都已經 train 了 seq2seq model
GAN_Lecture_9_(2018)_-_Sequence_Generation-39 所以你都知道要怎麼 train
GAN_Lecture_9_(2018)_-_Sequence_Generation-40 那今天要講的是用一個不一樣的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-41 用 GAN 的技術來 train 一個 seq2seq model
GAN_Lecture_9_(2018)_-_Sequence_Generation-42 那為什麼我們會要用到 GAN 的技術或其他的技術來 train seq2seq model 呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-43 我們先來看看在作業二裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-44 我們 train seq2seq model 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-45 有什麼不足的地方
GAN_Lecture_9_(2018)_-_Sequence_Generation-46 那我們都知道在作業 2-2 裡面，我們就 train 了一個 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-47 一個 chatbot 它是一個 seq2seq model，它裡面有一個 encoder
GAN_Lecture_9_(2018)_-_Sequence_Generation-48 有一個 decoder
GAN_Lecture_9_(2018)_-_Sequence_Generation-49 那在這邊這個 seq2seq model 就是我們的 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-50 那這個 encoder 會吃一個 input 的句子，這邊用 c 來表示
GAN_Lecture_9_(2018)_-_Sequence_Generation-51 那它會 output 另外一個句子 x
GAN_Lecture_9_(2018)_-_Sequence_Generation-52 encoder 吃一個句子，之於 decoder 會output 一個句子 x
GAN_Lecture_9_(2018)_-_Sequence_Generation-53 那我們知道説要 train 這樣子的 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-54 你需要收集一些 training data
GAN_Lecture_9_(2018)_-_Sequence_Generation-55 所謂的 training data 就是人的對話
GAN_Lecture_9_(2018)_-_Sequence_Generation-56 所以你今天告訴 chatbot 説，在這個 training data 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-57 A 説 How are you 的時候，B 的回應是 I'm good
GAN_Lecture_9_(2018)_-_Sequence_Generation-58 所以 chatbot 必須學到說
GAN_Lecture_9_(2018)_-_Sequence_Generation-59 當 input 的句子是 How are you 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-60 它 output 這個 I'm good  的 likelihood
GAN_Lecture_9_(2018)_-_Sequence_Generation-61 應該越大越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-62 或者假設你不知道 maximum likelihood 是什麼的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-63 這邊的意思就是說
GAN_Lecture_9_(2018)_-_Sequence_Generation-64 今天假設正確答案是 I'm good
GAN_Lecture_9_(2018)_-_Sequence_Generation-65 那你在用 decoder 產生句子的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-66 第一個 timestamp 產生 I'm
GAN_Lecture_9_(2018)_-_Sequence_Generation-67 假設 I'm 算是一個 word，產生 I'm 的機率要越大越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-68 那在第二個 timestamp 產生 good 的機率要越大越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-69 那這麼做顯然有一個非常大的問題
GAN_Lecture_9_(2018)_-_Sequence_Generation-70 就是我們看兩個可能的 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-71 假設今天有一個 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-72 它 input How are you 的時候，它 output 是 Not bad
GAN_Lecture_9_(2018)_-_Sequence_Generation-73 有另外一個 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-74 它 input How are you 的時候，它 output 是 I'm John
GAN_Lecture_9_(2018)_-_Sequence_Generation-75 那如果從人的觀點來看
GAN_Lecture_9_(2018)_-_Sequence_Generation-76 Not bad 是一個比較合理的 answer
GAN_Lecture_9_(2018)_-_Sequence_Generation-77 I'm John 是一個比較奇怪的 answer
GAN_Lecture_9_(2018)_-_Sequence_Generation-78 但是如果從我們 training 的 criteria 來看
GAN_Lecture_9_(2018)_-_Sequence_Generation-79 從我們在 train 這個 chatbot 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-80 希望 chatbot 要 maximize 的 object，希望 chatbot 學到的結果來看
GAN_Lecture_9_(2018)_-_Sequence_Generation-81 事實上 I'm John 是一個比較好的結果
GAN_Lecture_9_(2018)_-_Sequence_Generation-82 為什麼呢？因為 I'm John 跟 I'm bad 比起來
GAN_Lecture_9_(2018)_-_Sequence_Generation-83 你至少第一個 word 的還是對的
GAN_Lecture_9_(2018)_-_Sequence_Generation-84 那如果是 Not bad
GAN_Lecture_9_(2018)_-_Sequence_Generation-85 你兩個 word 都是錯的
GAN_Lecture_9_(2018)_-_Sequence_Generation-86 所以從這個 training 的 criteria 來看，假設你 train 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-87 是 maximum likelihood
GAN_Lecture_9_(2018)_-_Sequence_Generation-88 或是其實 maximum likelihood 就是 minimize 每一個 timestamp 的 cross entropy
GAN_Lecture_9_(2018)_-_Sequence_Generation-89 不管，這兩個其實是 equivalent 的東西啦
GAN_Lecture_9_(2018)_-_Sequence_Generation-90 其實 maximum likelihood 就是 minimize cross entropy
GAN_Lecture_9_(2018)_-_Sequence_Generation-91 所以假設有一天
GAN_Lecture_9_(2018)_-_Sequence_Generation-92 這個是一個真實的問題，就有人去某個大公司面試
GAN_Lecture_9_(2018)_-_Sequence_Generation-93 然後人家問他說
GAN_Lecture_9_(2018)_-_Sequence_Generation-94 train 這個 classifier 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-95 有時候我們會說我們是 maximum likelihood
GAN_Lecture_9_(2018)_-_Sequence_Generation-96 有時候我們會說我們是在 minimize cross entropy
GAN_Lecture_9_(2018)_-_Sequence_Generation-97 這兩者有什麼不同呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-98 如果你答一些，這兩個東西有點像
GAN_Lecture_9_(2018)_-_Sequence_Generation-99 但他們中間有微妙的不同，你就錯了這樣，這個時候你就是要說
GAN_Lecture_9_(2018)_-_Sequence_Generation-100 他們兩個就是一模一樣的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-101 所以 maximum likelihood 跟 minimize cross entropy
GAN_Lecture_9_(2018)_-_Sequence_Generation-102 是一模一樣的東西，剛才講的是真正的例子
GAN_Lecture_9_(2018)_-_Sequence_Generation-103 某人去面試某一個大家都知道的
GAN_Lecture_9_(2018)_-_Sequence_Generation-104 全球性的科技公司，是真的被問了這個問題
GAN_Lecture_9_(2018)_-_Sequence_Generation-105 那我們現在來看一下我們今天要講的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-106 我們先講一下怎麼去 improve 這個 seq2seq 的 model
GAN_Lecture_9_(2018)_-_Sequence_Generation-107 那這邊呢會講兩個 improve 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-108 我們其實會先講說，怎麼用 reinforcement learning
GAN_Lecture_9_(2018)_-_Sequence_Generation-109 來 improve conditional generation 這件事情
GAN_Lecture_9_(2018)_-_Sequence_Generation-110 然後接下來我們才會講說
GAN_Lecture_9_(2018)_-_Sequence_Generation-111 怎麼用 GAN 來 improve conditional generation
GAN_Lecture_9_(2018)_-_Sequence_Generation-112 之所以要講 RL，是因為等一下你會發現說
GAN_Lecture_9_(2018)_-_Sequence_Generation-113 用 GAN 來 improve conditional generation 這件事情
GAN_Lecture_9_(2018)_-_Sequence_Generation-114 其實跟 RL 是非常像的
GAN_Lecture_9_(2018)_-_Sequence_Generation-115 你甚至可以說使用 RL
GAN_Lecture_9_(2018)_-_Sequence_Generation-116 來 improve seq2seq 的 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-117 可以看作是 GAN 的一個 special case
GAN_Lecture_9_(2018)_-_Sequence_Generation-118 那等一下我們繼續看下去，你就會比較清楚
GAN_Lecture_9_(2018)_-_Sequence_Generation-119 假設我們今天要 train 一個 seq to seq  的 model
GAN_Lecture_9_(2018)_-_Sequence_Generation-120 你不想要用 train maximum likelihood 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-121 來 train seq to seq model，因為我們剛才講說
GAN_Lecture_9_(2018)_-_Sequence_Generation-122 用 maximum likelihood 的方法，有很明顯的問題
GAN_Lecture_9_(2018)_-_Sequence_Generation-123 那我們現在如果想引入 RL
GAN_Lecture_9_(2018)_-_Sequence_Generation-124 來 train seq to seq model 的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-125 我們等一下都用 chatbot 來做例子，其實我們等一下討論的技術
GAN_Lecture_9_(2018)_-_Sequence_Generation-126 不是只限於 chatbot 而已
GAN_Lecture_9_(2018)_-_Sequence_Generation-127 其實任何 seq to seq model，舉例來說作業 2-1 做的
GAN_Lecture_9_(2018)_-_Sequence_Generation-128 tracation generation 也可以用到等一下討論的技術
GAN_Lecture_9_(2018)_-_Sequence_Generation-129 不過我們等一下舉例的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-130 我們都假設我們是要做 chatbot 就是了
GAN_Lecture_9_(2018)_-_Sequence_Generation-131 那今天假設你要 train 一個 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-132 你不要 maximum likelihood 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-133 你想要 Reinforcement learning 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-134 那你會怎麼做呢？那你的作法可能是這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-135 你就讓這個 chatbot 去胡亂在線上跟人講話
GAN_Lecture_9_(2018)_-_Sequence_Generation-136 就有一個人說 How are you，chatbot 就回答 bye-bye，人就會給 chatbot 一個很糟的評價
GAN_Lecture_9_(2018)_-_Sequence_Generation-137 chatbot 就知道說這樣做是不好的
GAN_Lecture_9_(2018)_-_Sequence_Generation-138 再下一次他跟人對話的時候，人說 Hello
GAN_Lecture_9_(2018)_-_Sequence_Generation-139 chatbot 説 Hi，人就覺得說它的回答是對的，就給它一個 positive 的評價
GAN_Lecture_9_(2018)_-_Sequence_Generation-140 chatbot 就知道說它做的事情是好的
GAN_Lecture_9_(2018)_-_Sequence_Generation-141 那 chatbot 在跟人互動的過程中呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-142 他會得到 reward，那我今天把這個問題想的單純一點
GAN_Lecture_9_(2018)_-_Sequence_Generation-143 就是人說一個句子，然後 chatbot 就做一個回應
GAN_Lecture_9_(2018)_-_Sequence_Generation-144 然後人就會給 chatbot 一個分數
GAN_Lecture_9_(2018)_-_Sequence_Generation-145 那今天 chatbot 要做的事情
GAN_Lecture_9_(2018)_-_Sequence_Generation-146 就是希望透過互動的過程
GAN_Lecture_9_(2018)_-_Sequence_Generation-147 它去學習怎麼 maximize 它可以得到的分數
GAN_Lecture_9_(2018)_-_Sequence_Generation-148 或者是說我們用這頁投影片來說明一下我們現在的問題是什麼樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-149 我們有一個 chatbot，它 input 一個 sentence c
GAN_Lecture_9_(2018)_-_Sequence_Generation-150 他要 output 一個 response x，它就是一個 seq to seq model
GAN_Lecture_9_(2018)_-_Sequence_Generation-151 接下來有一個人，人其實也可以看作是一個 function
GAN_Lecture_9_(2018)_-_Sequence_Generation-152 人這個 function 做的事情是什麼呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-153 人這個 function 做的事情就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-154 input 一個 sentence c
GAN_Lecture_9_(2018)_-_Sequence_Generation-155 還有 input 一個 response x
GAN_Lecture_9_(2018)_-_Sequence_Generation-156 然後給於一個評價，給於一個 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-157 這個 reward 我們就寫成 R(c, x)
GAN_Lecture_9_(2018)_-_Sequence_Generation-158 但如果你熟係 conditional generation 的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-159 就記得我們在講這個我們作業 3-2，就是要做 conditional generation 嘛
GAN_Lecture_9_(2018)_-_Sequence_Generation-160 對不對，要 input 文字，然後 output 對應的圖片
GAN_Lecture_9_(2018)_-_Sequence_Generation-161 那你會發現說這個圖
GAN_Lecture_9_(2018)_-_Sequence_Generation-162 跟用 GAN 做 conditional generation
GAN_Lecture_9_(2018)_-_Sequence_Generation-163 其實是非常像的
GAN_Lecture_9_(2018)_-_Sequence_Generation-164 唯一的不同只是說
GAN_Lecture_9_(2018)_-_Sequence_Generation-165 在原來的 conditional generation 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-166 這一個如果用 GAN 做 conditional generation 的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-167 這個綠色的方塊，它是一個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-168 我們說 discriminator 切記它不要只吃 generator 的 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-169 它要同時吃 generator 的 input 跟 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-170 才能給於評價
GAN_Lecture_9_(2018)_-_Sequence_Generation-171 今天人也是一樣，人來取代那個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-172 人他就不用 train，或者是說你可以說人已經 train 好了
GAN_Lecture_9_(2018)_-_Sequence_Generation-173 人有一個腦，然後在數十年的成長歷程中你其實已經 train 好了， 所以你不用再 train
GAN_Lecture_9_(2018)_-_Sequence_Generation-174 然後給你一個 input sentence c，給你一個 response x，然後你可以給於一個評價
GAN_Lecture_9_(2018)_-_Sequence_Generation-175 我們接下來要做的事情，chatbot 要做的事情就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-176 它希望去調整這個 seq to seq model 裡面內部的參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-177 希望去 maximize 人會給他的評價，這邊寫成 R(c, x)
GAN_Lecture_9_(2018)_-_Sequence_Generation-178 這件事情怎麼做呢？我們要用的技術
GAN_Lecture_9_(2018)_-_Sequence_Generation-179 其實就是 policy gradient
GAN_Lecture_9_(2018)_-_Sequence_Generation-180 policy gradient 我們其實在 machine learning 的最後幾堂課其實是有說過的
GAN_Lecture_9_(2018)_-_Sequence_Generation-181 那也許你記得，也許你忘了，我們這邊以 chatbot 做例子
GAN_Lecture_9_(2018)_-_Sequence_Generation-182 來很快地複習一下，policy gradient 是怎麼做的
GAN_Lecture_9_(2018)_-_Sequence_Generation-183 那我們有一個 seq to seq model，它的 input 是 c output 是 x
GAN_Lecture_9_(2018)_-_Sequence_Generation-184 接下來我們有另外一個 function，這個 function 是人
GAN_Lecture_9_(2018)_-_Sequence_Generation-185 人吃 c 跟 x，然後 output 一個 R
GAN_Lecture_9_(2018)_-_Sequence_Generation-186 那我們現在要做的事情是什麼呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-187 我們要去調 encoder 跟 generator 的參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-188 這個 encoder 跟 generator 合起來是一個 seq to seq model
GAN_Lecture_9_(2018)_-_Sequence_Generation-189 他們合起來的參數，我們叫做 theta
GAN_Lecture_9_(2018)_-_Sequence_Generation-190 我們希望調這個 theta 去 maximize human 這個 function 的 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-191 那怎麼做呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-192 我們先來計算一個東西，這個東西是給定某一組參數 theta 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-193 假設固定 theta，把 theta 固定起來的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-194 這個時候這個 seq to seq model 這個 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-195 會得到的期望的 reward 有多大
GAN_Lecture_9_(2018)_-_Sequence_Generation-196 假設這個 theta 是固定的，然後計算一下這個 seq to seq model
GAN_Lecture_9_(2018)_-_Sequence_Generation-197 它會得到的期望的 reward 是有多大
GAN_Lecture_9_(2018)_-_Sequence_Generation-198 那這個東西怎麼算呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-199 首先我們先 summation over 所有可能的 input c
GAN_Lecture_9_(2018)_-_Sequence_Generation-200 然後乘上每一個 c 出現的機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-201 因為 c 可能有各種不同的 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-202 比如說人可能說 How are you，人可能說 Good morning
GAN_Lecture_9_(2018)_-_Sequence_Generation-203 人可能說 Good evening，你有各種各樣的 input
GAN_Lecture_9_(2018)_-_Sequence_Generation-204 你有各種各樣的 c
GAN_Lecture_9_(2018)_-_Sequence_Generation-205 但是每一個 input 出現的機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-206 可能是不太一樣的，比如說 How are you 相較於其他的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-207 也許它出現的機率是特別大的
GAN_Lecture_9_(2018)_-_Sequence_Generation-208 因為人特別常對 chatbot 說這個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-209 接下來，summation over 所有可能的回應 x
GAN_Lecture_9_(2018)_-_Sequence_Generation-210 當你有一個 c 的時候，當你有一個 input c 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-211 再加上假設這個 chatbot 的參數 theta，我們已經知道的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-212 接下來你就可以算出一個機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-213 這個機率是在 given c，given 這組參數的情況下
GAN_Lecture_9_(2018)_-_Sequence_Generation-214 chatbot 會回答某一個答覆 x 的機率有多少
GAN_Lecture_9_(2018)_-_Sequence_Generation-215 那你說這邊會什麼會是一個機率呢？給一個 input c
GAN_Lecture_9_(2018)_-_Sequence_Generation-216 為什麼 output 會是一個機率呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-217 你想想看喔，我們今天在 train seq to seq model 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-218 每一個 timestamp 我們不是其實要做一個 sampling 嘛
GAN_Lecture_9_(2018)_-_Sequence_Generation-219 我們 train 一個 seq to seq model 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-220 每一次給同樣的 input，它的 output，不見得是一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-221 假設你在做 sampling 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-222 你從那個就是我們的 decoder 的 output 是一個 distribution
GAN_Lecture_9_(2018)_-_Sequence_Generation-223 你要把 distribution 變成一個 token 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-224 如果你是採取 sampling 的方式
GAN_Lecture_9_(2018)_-_Sequence_Generation-225 那你 chatbot 的每一次 output 都會是不一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-226 所以今天給一個 c，每一次 output 的 x
GAN_Lecture_9_(2018)_-_Sequence_Generation-227 其實是不一樣的，所以給一個 c
GAN_Lecture_9_(2018)_-_Sequence_Generation-228 我們其實得到的是一個 x 的機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-229 那你說假設你不是用 sampling 的方式
GAN_Lecture_9_(2018)_-_Sequence_Generation-230 你是用 Arg Max 的方式呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-231 其實這樣也可以，如果是用 Arg Max 的方式
GAN_Lecture_9_(2018)_-_Sequence_Generation-232 給一個 c，那你一定會得到一模一樣的 x
GAN_Lecture_9_(2018)_-_Sequence_Generation-233 但我們可以說，那個 x 出現的機率就是 1
GAN_Lecture_9_(2018)_-_Sequence_Generation-234 其他的 response 出現的機率都是 0
GAN_Lecture_9_(2018)_-_Sequence_Generation-235 其他的 x 出現機率都是 0
GAN_Lecture_9_(2018)_-_Sequence_Generation-236 總之給你一個 c，在參數 x, c theta 知道的情況下
GAN_Lecture_9_(2018)_-_Sequence_Generation-237 你可以把 chatbot 可能的 output 看作是一個 distribution
GAN_Lecture_9_(2018)_-_Sequence_Generation-238 這邊寫成 Ptheta of (x, given c)
GAN_Lecture_9_(2018)_-_Sequence_Generation-239 當 chatbot 回答一個 x 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-240 當給一個 c，chatbot 產生一個 x 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-241 接下來人就會給一個 reward R(c, x)
GAN_Lecture_9_(2018)_-_Sequence_Generation-242 那這一整項，這一整項 summation over 所有的 c
GAN_Lecture_9_(2018)_-_Sequence_Generation-243 summation over 所有的 x
GAN_Lecture_9_(2018)_-_Sequence_Generation-244 這邊乘上 c 的機率，這邊乘上 x 出現的機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-245 在 weighted by 這個 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-246 其實就是 reward 的期望值，對不對
GAN_Lecture_9_(2018)_-_Sequence_Generation-247 這一項就是某一個 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-248 它的參數是 theta，它可以得到的 reward 的期望值
GAN_Lecture_9_(2018)_-_Sequence_Generation-249 那接下來我們要做的事情是什麼呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-250 接下來我們要做的事情就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-251 我們要調這個 theta，要調這個 chatbot 的參數 theta
GAN_Lecture_9_(2018)_-_Sequence_Generation-252 讓 reward 的期望值，越大越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-253 那這件事情怎麼做呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-254 這件事情怎麼做呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-255 我們先把這個 reward 的期望值稍微做一下整理
GAN_Lecture_9_(2018)_-_Sequence_Generation-256 這邊是 summation over c
GAN_Lecture_9_(2018)_-_Sequence_Generation-257 所以我們可以看作是
GAN_Lecture_9_(2018)_-_Sequence_Generation-258 根據 p of c 這個 distribution，取期望值
GAN_Lecture_9_(2018)_-_Sequence_Generation-259 這邊是 summation over x
GAN_Lecture_9_(2018)_-_Sequence_Generation-260 然後乘上 Ptheta of (x, given c)
GAN_Lecture_9_(2018)_-_Sequence_Generation-261 所以這邊可以看作是
GAN_Lecture_9_(2018)_-_Sequence_Generation-262 對這一個 distribution
GAN_Lecture_9_(2018)_-_Sequence_Generation-263 這一個 x 是從這個 distribution sample 出來的
GAN_Lecture_9_(2018)_-_Sequence_Generation-264 然後取期望值，那取期望值的對象是 R(c, x)
GAN_Lecture_9_(2018)_-_Sequence_Generation-265 那接下來這邊這項沒什麼了不起的
GAN_Lecture_9_(2018)_-_Sequence_Generation-266 就把這兩個 sample 放在一起
GAN_Lecture_9_(2018)_-_Sequence_Generation-267 這邊我發現了一個錯，有發現嗎？
GAN_Lecture_9_(2018)_-_Sequence_Generation-268 這邊這個應該是 c
GAN_Lecture_9_(2018)_-_Sequence_Generation-269 其實是這樣子的，我在做投影片的時候，本來都是 h
GAN_Lecture_9_(2018)_-_Sequence_Generation-270 而做到某個地方我突然想把它改成 c
GAN_Lecture_9_(2018)_-_Sequence_Generation-271 然後前面就有地方沒有改到這樣子，這個是一個 c
GAN_Lecture_9_(2018)_-_Sequence_Generation-272 那這邊的意思就是說
GAN_Lecture_9_(2018)_-_Sequence_Generation-273 我們從 P of c 裡面 sample 出一個 c 出來
GAN_Lecture_9_(2018)_-_Sequence_Generation-274 我們從這個機率裡面 sample 出一個 x 出來
GAN_Lecture_9_(2018)_-_Sequence_Generation-275 然後取 R(c, x) 的期望值
GAN_Lecture_9_(2018)_-_Sequence_Generation-276 你應該可以注意找一下
GAN_Lecture_9_(2018)_-_Sequence_Generation-277 我相信有很多地方的 c 都沒有改到
GAN_Lecture_9_(2018)_-_Sequence_Generation-278 然後接下來你的問題就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-279 這個期望值要怎麼算？
GAN_Lecture_9_(2018)_-_Sequence_Generation-280 這個期望值要怎麼算？你要算這個期望值
GAN_Lecture_9_(2018)_-_Sequence_Generation-281 你實際上做法
GAN_Lecture_9_(2018)_-_Sequence_Generation-282 甚至 theoretical 作法
GAN_Lecture_9_(2018)_-_Sequence_Generation-283 你要 summation over 所有的 c
GAN_Lecture_9_(2018)_-_Sequence_Generation-284 summation over 所有的 x
GAN_Lecture_9_(2018)_-_Sequence_Generation-285 但是在實作上
GAN_Lecture_9_(2018)_-_Sequence_Generation-286 你根本無法窮舉所有 input
GAN_Lecture_9_(2018)_-_Sequence_Generation-287 你根本無法窮舉所有可能 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-288 所以實作上是怎麼做的？
GAN_Lecture_9_(2018)_-_Sequence_Generation-289 實作上就是做 sampling
GAN_Lecture_9_(2018)_-_Sequence_Generation-290 假設這個 distribution 你知道，假設這個 distribution 你知道
GAN_Lecture_9_(2018)_-_Sequence_Generation-291 那這兩個 distribution 我們知道嗎？
GAN_Lecture_9_(2018)_-_Sequence_Generation-292 這兩個 distribution 我們知道
GAN_Lecture_9_(2018)_-_Sequence_Generation-293 P of c，人會說什麼句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-294 你就從你的 database 裡面 sample 看看
GAN_Lecture_9_(2018)_-_Sequence_Generation-295 你就從你的 database 的句子裡面 sample，你就知道人常輸入什麼句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-296 那這個機率，你只要知道參數，他就是給定的
GAN_Lecture_9_(2018)_-_Sequence_Generation-297 所以我們根據這兩個機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-298 我們去做一些 sample，我們去 sample 大 N 筆的 c 跟 x 的pair
GAN_Lecture_9_(2018)_-_Sequence_Generation-299 比如說上百筆的 c 跟 x 的pair
GAN_Lecture_9_(2018)_-_Sequence_Generation-300 所以本來這邊應該是要取一個期望值
GAN_Lecture_9_(2018)_-_Sequence_Generation-301 但實際上我們並沒有辦法真的去取期望值
GAN_Lecture_9_(2018)_-_Sequence_Generation-302 我們真正的做法是
GAN_Lecture_9_(2018)_-_Sequence_Generation-303 做一下 sample，sample 出大 N 筆 data
GAN_Lecture_9_(2018)_-_Sequence_Generation-304 這大 N 筆 data，每一筆都去算它的 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-305 把這大 N 筆 data 的 reward 全部平均起來
GAN_Lecture_9_(2018)_-_Sequence_Generation-306 我們用這個東西來 approximate 期望值
GAN_Lecture_9_(2018)_-_Sequence_Generation-307 而這一項就是期望的 reward 的 approximation
GAN_Lecture_9_(2018)_-_Sequence_Generation-308 然後再來你會遇到的問題是說
GAN_Lecture_9_(2018)_-_Sequence_Generation-309 那我們現在要對 theta
GAN_Lecture_9_(2018)_-_Sequence_Generation-310 我們要對 theta 做 optimization
GAN_Lecture_9_(2018)_-_Sequence_Generation-311 我們要找一個 theta 讓 R bar 這一項越大越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-312 那意味著說我們要拿 theta 去對 R bar 算它的 gradient
GAN_Lecture_9_(2018)_-_Sequence_Generation-313 但是問題是在這項裡面，我們說 R bar 就等於這項
GAN_Lecture_9_(2018)_-_Sequence_Generation-314 這項裡面沒有 theta 啊
GAN_Lecture_9_(2018)_-_Sequence_Generation-315 沒有 theta 你根本沒有辦法對 theta 算 gradient，
GAN_Lecture_9_(2018)_-_Sequence_Generation-316 heta 不見了，不知不覺間
GAN_Lecture_9_(2018)_-_Sequence_Generation-317 本來這邊都好像還有 theta
GAN_Lecture_9_(2018)_-_Sequence_Generation-318 這邊有theta，這邊有theta
GAN_Lecture_9_(2018)_-_Sequence_Generation-319 不知不覺間，它就不見了
GAN_Lecture_9_(2018)_-_Sequence_Generation-320 它到哪裡去了呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-321 它被藏到 sampling 的這個 process 裡面去了
GAN_Lecture_9_(2018)_-_Sequence_Generation-322 當你改變 theta 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-323 你會改變 sample 到的東西，但在這式子裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-324 theta 就不見了
GAN_Lecture_9_(2018)_-_Sequence_Generation-325 你根本就不知道要怎麼對這個式子算 theta 的 gradient
GAN_Lecture_9_(2018)_-_Sequence_Generation-326 所以怎麼辦呢？實作上的方法是這個樣子的
GAN_Lecture_9_(2018)_-_Sequence_Generation-327 這一項如果把它 approximate 成這一項的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-328 就會沒有辦法算 gradient 了
GAN_Lecture_9_(2018)_-_Sequence_Generation-329 所以怎麼辦？先把對這一項算 gradient
GAN_Lecture_9_(2018)_-_Sequence_Generation-330 再做 approximation
GAN_Lecture_9_(2018)_-_Sequence_Generation-331 先對這一項算 gradient
GAN_Lecture_9_(2018)_-_Sequence_Generation-332 這一項算 gradient 是怎麼樣呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-333 只有這個 Ptheta of (x, given c)
GAN_Lecture_9_(2018)_-_Sequence_Generation-334 跟 theta 是有關的
GAN_Lecture_9_(2018)_-_Sequence_Generation-335 所以你對 R bar 取 gradient 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-336 那你只需要把 gradient 放到 Ptheta 的前面就好了，因為只有這一項和 theta 是有關係的
GAN_Lecture_9_(2018)_-_Sequence_Generation-337 接下來呢，唯一的 trick 是
GAN_Lecture_9_(2018)_-_Sequence_Generation-338 對這一個式子，他的分子和分母的地方
GAN_Lecture_9_(2018)_-_Sequence_Generation-339 都同乘 Ptheta of (x, given c)，分子分母同乘一樣的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-340 當然對結果是沒有任何影響的
GAN_Lecture_9_(2018)_-_Sequence_Generation-341 那我們知道說，假設你有一個 function f of x
GAN_Lecture_9_(2018)_-_Sequence_Generation-342 dx 分之 d log (f of x) 會等於 (f of x) 分之 1，dx 分之 d (f of x)
GAN_Lecture_9_(2018)_-_Sequence_Generation-343 反正微分的式子告訴我們反正就是這個樣子，所以今天這個式子
GAN_Lecture_9_(2018)_-_Sequence_Generation-344 Ptheta of (x,  given c) 乘上 gradient Ptheta of (x,  given c) 除以 Ptheta of (x,  given c) 其實就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-345 其實這一項
GAN_Lecture_9_(2018)_-_Sequence_Generation-346 就是 gradient Ptheta of (x,  given c) 除以 Ptheta of (x,  given c)
GAN_Lecture_9_(2018)_-_Sequence_Generation-347 其實就是 gradient log Ptheta of (x, given c)
GAN_Lecture_9_(2018)_-_Sequence_Generation-348 這兩項是一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-349 那接下來呢，接下來我們知道說
GAN_Lecture_9_(2018)_-_Sequence_Generation-350 前面這邊，這 summation over c 乘上 P of c
GAN_Lecture_9_(2018)_-_Sequence_Generation-351 可以看作是在取 expectation 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-352 對從 P of c 這個 distribution 裡面去 sample c 出來
GAN_Lecture_9_(2018)_-_Sequence_Generation-353 這邊 summation over x
GAN_Lecture_9_(2018)_-_Sequence_Generation-354 可以看做是從 Ptheta of (x, given c) 這個 distribution 裡面去 sample x 出來
GAN_Lecture_9_(2018)_-_Sequence_Generation-355 那你要取期望值的對象是什麼呢？要取期望值的對象是 R(c, x)
GAN_Lecture_9_(2018)_-_Sequence_Generation-356 R(c, x) 乘上 gradient log Ptheta of (x, given c)
GAN_Lecture_9_(2018)_-_Sequence_Generation-357 乘上 gradient log  Ptheta (x, given c)
GAN_Lecture_9_(2018)_-_Sequence_Generation-358 所以這一項，當你要對 R bar 做 gradient 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-359 你要去 approximate 這一項的話，你是怎麼算的呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-360 實際上你的做法是這樣子的
GAN_Lecture_9_(2018)_-_Sequence_Generation-361 所以這一項怎麼算，這一項就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-362 這個 summation 把它換做 sampling
GAN_Lecture_9_(2018)_-_Sequence_Generation-363 你就 sample 大 N 項
GAN_Lecture_9_(2018)_-_Sequence_Generation-364 每一項都去算 R of (ci, xi)
GAN_Lecture_9_(2018)_-_Sequence_Generation-365 再乘上 gradient log Ptheta of (xi, given ci)
GAN_Lecture_9_(2018)_-_Sequence_Generation-366 把它們平均起來就是這一項 expectation 的 approximation
GAN_Lecture_9_(2018)_-_Sequence_Generation-367 那假設前面那些東西你沒有聽懂的話呢，那就算了，所以我們實際上是怎麼做的呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-368 實際上的做法是，我們今天怎麼 update 我們的參數 theta
GAN_Lecture_9_(2018)_-_Sequence_Generation-369 你 update 的方法是，原來你的參數叫做 theta old
GAN_Lecture_9_(2018)_-_Sequence_Generation-370 然後你用 gradient ascent 去 update 它
GAN_Lecture_9_(2018)_-_Sequence_Generation-371 加上某一個 gradient 的項，你得到新的 model，theta new
GAN_Lecture_9_(2018)_-_Sequence_Generation-372 接下來呢，gradient 這一項怎麼算？
GAN_Lecture_9_(2018)_-_Sequence_Generation-373 gradient 這一項算法就是，去 sample N 個 pair 的 ci 跟 xi 出來
GAN_Lecture_9_(2018)_-_Sequence_Generation-374 去 sample N 個 pair 的 ci 跟 xi 出來
GAN_Lecture_9_(2018)_-_Sequence_Generation-375 然後接下來你把 R of (ci, xi) 去乘上
GAN_Lecture_9_(2018)_-_Sequence_Generation-376 gradient log Ptheta of (xi, given ci)，就結束了
GAN_Lecture_9_(2018)_-_Sequence_Generation-377 那這一項假設前面的推導你聽不懂的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-378 其實這一項它是非常的直覺的
GAN_Lecture_9_(2018)_-_Sequence_Generation-379 怎麼說它非常的直覺呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-380 它的直覺的解釋是這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-381 這一個 gradient 到底代表什麼意思呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-382 這個 gradient 所代表的意思是說
GAN_Lecture_9_(2018)_-_Sequence_Generation-383 假設今天 given ci, xi
GAN_Lecture_9_(2018)_-_Sequence_Generation-384 也就是說有人對 machine 說了 ci 這個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-385 machine 回答 xi 這個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-386 然後人給的 reward 是 positive 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-387 人說如果輸入 ci 回答 xi
GAN_Lecture_9_(2018)_-_Sequence_Generation-388 它是好的，給你一個 positive 的 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-389 那我們就要增加 given ci 的時候，xi 出現的機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-390 這個非常直覺，如果 given ci 產生 xi 你的 reward 是 positive 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-391 given ci 產生 xi 是好的，那你就要增加 given ci 產生 xi 的機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-392 反之如果 R of (ci, xi) 是 negative 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-393 當人對 chatbot 説 ci
GAN_Lecture_9_(2018)_-_Sequence_Generation-394 chatbot 回答 xi，然後得到負面的評價的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-395 這個時候我們就應該調整參數 theta
GAN_Lecture_9_(2018)_-_Sequence_Generation-396 讓這一項機率，也就是 given ci
GAN_Lecture_9_(2018)_-_Sequence_Generation-397 回答 xi 的這個機率呢，越小越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-398 它的精神就是這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-399 所以實作上的時候，實際上如果你要用 policy gradient 這個技術
GAN_Lecture_9_(2018)_-_Sequence_Generation-400 來 implement 一個 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-401 讓它在 reinforcement learning 的情境中
GAN_Lecture_9_(2018)_-_Sequence_Generation-402 可以去學習怎麼和人對話的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-403 實際上你是怎麼做的呢？實際上你的做法是這個樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-404 你有一個 chatbot 它的參數叫做 Theta(t)
GAN_Lecture_9_(2018)_-_Sequence_Generation-405 然後你把你的 chatbot 拿去跟人對話，然後他們就講了很多
GAN_Lecture_9_(2018)_-_Sequence_Generation-406 這個是一個 sampling 的 process
GAN_Lecture_9_(2018)_-_Sequence_Generation-407 你先用 chatbot 跟人對話
GAN_Lecture_9_(2018)_-_Sequence_Generation-408 做一個 sampling 的 process
GAN_Lecture_9_(2018)_-_Sequence_Generation-409 在這個 sampling 的 process 裡面呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-410 當人說 c1 chatbot 回答 x1 的時候，會得到 reward R of (c1, x1)
GAN_Lecture_9_(2018)_-_Sequence_Generation-411 當輸入 c2 回答 x2 的時候，會得到 reward R of (c2, x2)
GAN_Lecture_9_(2018)_-_Sequence_Generation-412 那你會 sample 出 N 筆 data
GAN_Lecture_9_(2018)_-_Sequence_Generation-413 每一筆 data 都會得到一個 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-414 N 筆 data N 個 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-415 那接下呢，接下來你做的事情是這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-416 你有一個參數 Theta(t)，你要 update 這個參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-417 讓它變成 Theta(t+1)
GAN_Lecture_9_(2018)_-_Sequence_Generation-418 那怎麼 update 呢？你要把它加上
GAN_Lecture_9_(2018)_-_Sequence_Generation-419 對這個 R bar 的 gradient
GAN_Lecture_9_(2018)_-_Sequence_Generation-420 那這個 R bar 的 gradient 這一項到底怎麼算呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-421 這一項式子就列在這邊
GAN_Lecture_9_(2018)_-_Sequence_Generation-422 那這個式子的直觀解釋我們剛才講過說
GAN_Lecture_9_(2018)_-_Sequence_Generation-423 如果 R of (ci, xi) 是正的
GAN_Lecture_9_(2018)_-_Sequence_Generation-424 那就增加這一項的機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-425 如果 R of (ci, xi) 是負的，就減少這一項的機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-426 但是你這邊要注意
GAN_Lecture_9_(2018)_-_Sequence_Generation-427 每次你 update 完參數以後
GAN_Lecture_9_(2018)_-_Sequence_Generation-428 你要從頭回去，再去 sample data
GAN_Lecture_9_(2018)_-_Sequence_Generation-429 因為這個 R bar 它是在 given 參數是 theta 的情況下
GAN_Lecture_9_(2018)_-_Sequence_Generation-430 所算出來的結果
GAN_Lecture_9_(2018)_-_Sequence_Generation-431 那你今天一但 update 你的參數，從 theta(t) 變成 theta(t+1)
GAN_Lecture_9_(2018)_-_Sequence_Generation-432 這一項就不對了
GAN_Lecture_9_(2018)_-_Sequence_Generation-433 所以你本來參數 theta(t)，一但你 update theta(t+1) 以後
GAN_Lecture_9_(2018)_-_Sequence_Generation-434 你就要回過頭去再重新收集參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-435 所以這跟一般 train 的 gradient decent 非常不同
GAN_Lecture_9_(2018)_-_Sequence_Generation-436 因為一般 gradient decent，你就算 gradient
GAN_Lecture_9_(2018)_-_Sequence_Generation-437 然後就可以 update 參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-438 然後就可以馬上再算下一次 gradient，再 update 參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-439 但是如果你 apply reinforcement learning 的時候，你的持續做法是
GAN_Lecture_9_(2018)_-_Sequence_Generation-440 每次你 update 完參數以後
GAN_Lecture_9_(2018)_-_Sequence_Generation-441 你就要去跟使用者再互動
GAN_Lecture_9_(2018)_-_Sequence_Generation-442 然後才能再次 update 參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-443 所以每次 update 參數的時間呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-444 需要的 effort 是非常大的
GAN_Lecture_9_(2018)_-_Sequence_Generation-445 每 update 一次參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-446 你就要跟使用者互動 N 次
GAN_Lecture_9_(2018)_-_Sequence_Generation-447 才能 update 下一次參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-448 所以在 policy gradient 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-449 update 參數這件事情
GAN_Lecture_9_(2018)_-_Sequence_Generation-450 是非常寶貴的，就這一步是非常寶貴的
GAN_Lecture_9_(2018)_-_Sequence_Generation-451 絕對不能夠走錯這樣子，你一走錯
GAN_Lecture_9_(2018)_-_Sequence_Generation-452 你就要要你要重新再去跟人互動，才能夠走回來
GAN_Lecture_9_(2018)_-_Sequence_Generation-453 那你也有可能甚至就走不回來，所以
GAN_Lecture_9_(2018)_-_Sequence_Generation-454 下一週會講到一些新的技術呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-455 來讓這一步做得更好
GAN_Lecture_9_(2018)_-_Sequence_Generation-456 不過這是我們下週才要再講的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-457 那這邊是把 reinforcement learning 跟 maximum likelihood 呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-458 做一下比較
GAN_Lecture_9_(2018)_-_Sequence_Generation-459 在做 maximum likelihood 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-460 你有一堆 training data
GAN_Lecture_9_(2018)_-_Sequence_Generation-461 這些 training data 告訴我們說
GAN_Lecture_9_(2018)_-_Sequence_Generation-462 今天假設人說 c1
GAN_Lecture_9_(2018)_-_Sequence_Generation-463 chatbot 最正確的回答是 x1 hat
GAN_Lecture_9_(2018)_-_Sequence_Generation-464 對不對，我們就會有 labeled 的 data 嘛
GAN_Lecture_9_(2018)_-_Sequence_Generation-465 大家都做過作業 2-2 你應該知道怎麼回事
GAN_Lecture_9_(2018)_-_Sequence_Generation-466 就你有 input c1 output x1 hat，
GAN_Lecture_9_(2018)_-_Sequence_Generation-467 input cN 正確答案就是 xN hat
GAN_Lecture_9_(2018)_-_Sequence_Generation-468 這是 training data 告訴我們的
GAN_Lecture_9_(2018)_-_Sequence_Generation-469 在 training 的時候，你就是 maximize 你的 likelihood
GAN_Lecture_9_(2018)_-_Sequence_Generation-470 怎麼樣 maximize 你的 likelihood 呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-471 你希望 input ci 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-472 output xi hat 的機率越大越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-473 input 某個 condition，input 某個 input 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-474 input 某個輸入的句子的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-475 你希望正確的答案出現的機率越大越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-476 那算 gradient 的時候很單純
GAN_Lecture_9_(2018)_-_Sequence_Generation-477 你就把這個 log Ptheta 前面呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-478 加上一個 gradient，你就算 gradient 了
GAN_Lecture_9_(2018)_-_Sequence_Generation-479 這個是 maximum likelihood
GAN_Lecture_9_(2018)_-_Sequence_Generation-480 那我們來看一下 reinforcement learning
GAN_Lecture_9_(2018)_-_Sequence_Generation-481 在做 reinforcement learning 的時候呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-482 你也會得到一堆 c 跟 x 的 pair
GAN_Lecture_9_(2018)_-_Sequence_Generation-483 但這些 c 跟 x 的 pair，它並不是正確的答案
GAN_Lecture_9_(2018)_-_Sequence_Generation-484 這些 x 並不是人去標的答案
GAN_Lecture_9_(2018)_-_Sequence_Generation-485 這些 x 是機器自己產生的
GAN_Lecture_9_(2018)_-_Sequence_Generation-486 就人輸入 c1 到 cN
GAN_Lecture_9_(2018)_-_Sequence_Generation-487 機器自己產生了 x1 到 xN
GAN_Lecture_9_(2018)_-_Sequence_Generation-488 所以有些答案是對的，有些答案有可能是錯的
GAN_Lecture_9_(2018)_-_Sequence_Generation-489 接下來呢我們說
GAN_Lecture_9_(2018)_-_Sequence_Generation-490 我們在做reinforcement learning 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-491 我們是怎麼計算 gradient 的呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-492 我們是怎麼計算 gradient 的呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-493 我們是用這樣的式子來計算 gradient
GAN_Lecture_9_(2018)_-_Sequence_Generation-494 所以我們實際上的作法呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-495 我們這個式子的意思就是把這個 gradient log Ptheta 前面乘上 R(c, x)
GAN_Lecture_9_(2018)_-_Sequence_Generation-496 就如果你比較這兩個式子的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-497 你會發現說他們唯一的差別是
GAN_Lecture_9_(2018)_-_Sequence_Generation-498 在做 reinforcement learning 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-499 你在算 gradient 的時候，每一個 x 跟 c 的 pair 前面都乘上 R(c, x)
GAN_Lecture_9_(2018)_-_Sequence_Generation-500 如果你覺得這個 gradient 算起來不太直觀
GAN_Lecture_9_(2018)_-_Sequence_Generation-501 那沒關係，我們根據這個 gradient
GAN_Lecture_9_(2018)_-_Sequence_Generation-502 反推 objective function
GAN_Lecture_9_(2018)_-_Sequence_Generation-503 我們反推說什麼樣的 objective function
GAN_Lecture_9_(2018)_-_Sequence_Generation-504 在取 gradient 的時候，會變成下面這個式子
GAN_Lecture_9_(2018)_-_Sequence_Generation-505 那如果你反推了以後，你就會知道說
GAN_Lecture_9_(2018)_-_Sequence_Generation-506 什麼樣的 objective function 取 gradient 以後會變成下面這個式子呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-507 你的 objective function 就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-508 summation over 你 sample 到的 data
GAN_Lecture_9_(2018)_-_Sequence_Generation-509 每一筆 sample 到的 data，你都乘上 R (c, x)
GAN_Lecture_9_(2018)_-_Sequence_Generation-510 然後你去計算每一筆 sample 到的 data 的 log 的 likelihood
GAN_Lecture_9_(2018)_-_Sequence_Generation-511 你去計算每一筆 sample 到的 data 的 log Ptheta
GAN_Lecture_9_(2018)_-_Sequence_Generation-512 再把它乘上 R (c, x)，就是你的 objective function
GAN_Lecture_9_(2018)_-_Sequence_Generation-513 把這個 objective function  做 gradient 以後，你就會得到這個式子
GAN_Lecture_9_(2018)_-_Sequence_Generation-514 那我們在做 reinforcement learning 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-515 我們每一個 iteration
GAN_Lecture_9_(2018)_-_Sequence_Generation-516 其實是在 maximize 這樣一個 objective function
GAN_Lecture_9_(2018)_-_Sequence_Generation-517 那如果你把這兩個式子做比較的話，那就非常清楚了
GAN_Lecture_9_(2018)_-_Sequence_Generation-518 今天右邊這個 reinforcement learning 的 case
GAN_Lecture_9_(2018)_-_Sequence_Generation-519 可以想成是每一筆 training data 都是有 weight
GAN_Lecture_9_(2018)_-_Sequence_Generation-520 而在 maximum likelihood case 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-521 每一筆 training data 的 weight 都是一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-522 每一筆 training data 的 weight 都是 1
GAN_Lecture_9_(2018)_-_Sequence_Generation-523 那在 reinforcement learning 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-524 每一筆 training data 都有不同的 weight
GAN_Lecture_9_(2018)_-_Sequence_Generation-525 這一個 weight 就是那一筆 training data 得到的 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-526 也就是說今天輸入一個 ci
GAN_Lecture_9_(2018)_-_Sequence_Generation-527 機器回答一個 xi
GAN_Lecture_9_(2018)_-_Sequence_Generation-528 如果今天機器的回答正好是好的
GAN_Lecture_9_(2018)_-_Sequence_Generation-529 這個 xi 是一個正確的回答
GAN_Lecture_9_(2018)_-_Sequence_Generation-530 那我們在 training 的時候就給那筆 data 比較大的 weight
GAN_Lecture_9_(2018)_-_Sequence_Generation-531 如果今天 xi 是一個不好的回答
GAN_Lecture_9_(2018)_-_Sequence_Generation-532 代表這筆 training data 是錯的
GAN_Lecture_9_(2018)_-_Sequence_Generation-533 我們 even 會給它一個 negative 的 weight
GAN_Lecture_9_(2018)_-_Sequence_Generation-534 這個就是 maximum likelihood
GAN_Lecture_9_(2018)_-_Sequence_Generation-535 和 reinforcement learning 的比較
GAN_Lecture_9_(2018)_-_Sequence_Generation-536 理論上並沒有特別的限制
GAN_Lecture_9_(2018)_-_Sequence_Generation-537 就看你今你 R 想要訂怎麼樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-538 你用 policy gradient，都可以去 maximize R
GAN_Lecture_9_(2018)_-_Sequence_Generation-539 但是在實作上，會有限制
GAN_Lecture_9_(2018)_-_Sequence_Generation-540 我們剛才不是講到說，如果 R 是正的
GAN_Lecture_9_(2018)_-_Sequence_Generation-541 你就要讓機率越大越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-542 那你今天會不會遇到一個問題就是，假設 R 永遠都是正的
GAN_Lecture_9_(2018)_-_Sequence_Generation-543 今天這個 task R 就是正的，你做的最差
GAN_Lecture_9_(2018)_-_Sequence_Generation-544 也只是得到的分數比較小而已，它永遠都是正的
GAN_Lecture_9_(2018)_-_Sequence_Generation-545 那今天不管你採取什麼樣的行為
GAN_Lecture_9_(2018)_-_Sequence_Generation-546 machine 都會說我要讓機率上升，聽請來有點怪怪的
GAN_Lecture_9_(2018)_-_Sequence_Generation-547 但是在理論上這樣未必會有問題，為什麼說理論上這樣未必會有問題呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-548 你想想看喔，你要 maximize 的這一項，是一個機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-549 它是一個機率，它的和是 1
GAN_Lecture_9_(2018)_-_Sequence_Generation-550 所以今天就算是所有不同的 xi
GAN_Lecture_9_(2018)_-_Sequence_Generation-551 他前面乘的 R 是正的
GAN_Lecture_9_(2018)_-_Sequence_Generation-552 他終究是有大有小的
GAN_Lecture_9_(2018)_-_Sequence_Generation-553 對不對，你不可能讓所有的機率都上升
GAN_Lecture_9_(2018)_-_Sequence_Generation-554 因為機率的和是 1，你不可能讓所有機率都上升
GAN_Lecture_9_(2018)_-_Sequence_Generation-555 所以變成說，如果 weight 比較大的
GAN_Lecture_9_(2018)_-_Sequence_Generation-556 就比較 positive 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-557 就上升比較多
GAN_Lecture_9_(2018)_-_Sequence_Generation-558 如果 weight 比較小的，比較 negative 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-559 它就可能反而是會減少的
GAN_Lecture_9_(2018)_-_Sequence_Generation-560 就算他是正的，但如果他值比較小
GAN_Lecture_9_(2018)_-_Sequence_Generation-561 它可能也是會減小，因為 constrain 就是它的和要是 1
GAN_Lecture_9_(2018)_-_Sequence_Generation-562 但是你今天在實作上並沒有那麼容易，因為在實作上會遇到的問題是
GAN_Lecture_9_(2018)_-_Sequence_Generation-563 你不可能 sample 到所有的 x
GAN_Lecture_9_(2018)_-_Sequence_Generation-564 所以到時候就會變成說，假設一筆 data 你沒有 sample 到
GAN_Lecture_9_(2018)_-_Sequence_Generation-565 其他人只要有 sample 到都是 positive 的 reward，沒 sample 到的
GAN_Lecture_9_(2018)_-_Sequence_Generation-566 反而就會機率下降而 sample 到的都會機率上升
GAN_Lecture_9_(2018)_-_Sequence_Generation-567 這個反而不是我們要的
GAN_Lecture_9_(2018)_-_Sequence_Generation-568 所以其實今天在設計那個 reward 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-569 你其實會希望那個 reward 是有正有負的
GAN_Lecture_9_(2018)_-_Sequence_Generation-570 你 train 起來會比較容易
GAN_Lecture_9_(2018)_-_Sequence_Generation-571 那假設你的 task reward 都是正的
GAN_Lecture_9_(2018)_-_Sequence_Generation-572 實際上你會做的一件事情是，把那個 reward 通通都減掉一個 threshold
GAN_Lecture_9_(2018)_-_Sequence_Generation-573 讓它變成是有正有負，這樣你 train 起來會容易很多
GAN_Lecture_9_(2018)_-_Sequence_Generation-574 這個是講了 maximum likelihood 跟 reinforcement learning 的比較
GAN_Lecture_9_(2018)_-_Sequence_Generation-575 但是你知道實作上要做什麼 reinforcement learning 根本就是不太可能的
GAN_Lecture_9_(2018)_-_Sequence_Generation-576 你有沒有看過一篇文章就是講說
GAN_Lecture_9_(2018)_-_Sequence_Generation-577 當有一個人寫一篇網路文章，然後說
GAN_Lecture_9_(2018)_-_Sequence_Generation-578 當有人問他說某一個 task 用 reinforcement learning 好不好的時候，他的回答都是不好這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-579 那多數的時候他都是對的
GAN_Lecture_9_(2018)_-_Sequence_Generation-580 就是你要做 reinforcement learning 一個最大的問題就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-581 機器必須要跟人真的互動很多次
GAN_Lecture_9_(2018)_-_Sequence_Generation-582 才能夠學得起來
GAN_Lecture_9_(2018)_-_Sequence_Generation-583 你不要看今天那些什麼，google 或者是，Deep mind
GAN_Lecture_9_(2018)_-_Sequence_Generation-584 或者是 OpenAI 他們在玩那些什麼 3D 遊戲都玩得嚇嚇叫這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-585 那個 machine 跟環境互動的次數都可能是上千萬次
GAN_Lecture_9_(2018)_-_Sequence_Generation-586 或者是上億次
GAN_Lecture_9_(2018)_-_Sequence_Generation-587 那麼多互動的次數
GAN_Lecture_9_(2018)_-_Sequence_Generation-588 除了在電玩這種 simulated 的 task 以外，在真實的情境
GAN_Lecture_9_(2018)_-_Sequence_Generation-589 幾乎是不可能發生
GAN_Lecture_9_(2018)_-_Sequence_Generation-590 所以如果你要用 reinforcement learning 去 train 一個 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-591 幾乎是不可能的，因為在現實的情境中
GAN_Lecture_9_(2018)_-_Sequence_Generation-592 人沒有辦法花那麼多力氣
GAN_Lecture_9_(2018)_-_Sequence_Generation-593 去跟 chatbot 做互動
GAN_Lecture_9_(2018)_-_Sequence_Generation-594 所以後來就有人就想了一個 Alpha Go style training
GAN_Lecture_9_(2018)_-_Sequence_Generation-595 也就是說我們 learn 兩個 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-596 讓它們去互講這樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-597 就有一個 bot 説 How are you，另外一個說 see you
GAN_Lecture_9_(2018)_-_Sequence_Generation-598 然後它再說 see you，它說 see you
GAN_Lecture_9_(2018)_-_Sequence_Generation-599 然後陷入一個無窮迴圈永遠都跳不出來這樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-600 但它們有時候可能也會說出比較正確的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-601 因為我們知道說機器在回應的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-602 其實是有隨機性的所以問它同一個句子，每次的回答不見得是一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-603 接下來你再去訂一個 evaluation 的 function
GAN_Lecture_9_(2018)_-_Sequence_Generation-604 因為你還是不可能說讓兩個 chatbot 互相對話
GAN_Lecture_9_(2018)_-_Sequence_Generation-605 然後產生一百萬則對話以後，人再去一百萬則對話每一個去給它 feedback 説
GAN_Lecture_9_(2018)_-_Sequence_Generation-606 講得好還是不好
GAN_Lecture_9_(2018)_-_Sequence_Generation-607 你可能會設計一個 evaluation function，這個就是人自己訂好
GAN_Lecture_9_(2018)_-_Sequence_Generation-608 這邊我其實應該換成 c 啦，這邊其實應該換成 c 啦
GAN_Lecture_9_(2018)_-_Sequence_Generation-609 不過我其實覺得也沒差就是了
GAN_Lecture_9_(2018)_-_Sequence_Generation-610 你就訂一個 evaluation function
GAN_Lecture_9_(2018)_-_Sequence_Generation-611 給一則對話，然後看說這則對話好不好
GAN_Lecture_9_(2018)_-_Sequence_Generation-612 這則對話好不好
GAN_Lecture_9_(2018)_-_Sequence_Generation-613 但是這種 evaluation function 是人訂的
GAN_Lecture_9_(2018)_-_Sequence_Generation-614 你其實沒有辦法真的訂出太複雜的 function，就只能定義一些很簡單的，就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-615 比如說陷入無窮迴圈，就是得到負的 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-616 說出 I don't know，就是得到負的 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-617 你根本沒有辦法真的訂出太複雜的 evaluation function
GAN_Lecture_9_(2018)_-_Sequence_Generation-618 所以用這種方法還是有極限的
GAN_Lecture_9_(2018)_-_Sequence_Generation-619 所以接下來要解這個問題，你可以引入 GAN 的概念
GAN_Lecture_9_(2018)_-_Sequence_Generation-620 GAN 和 RL 有什麼不同呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-621 在 RL 裡面，你是人給 feedback
GAN_Lecture_9_(2018)_-_Sequence_Generation-622 在 GAN 裡面，你變成是 discriminator 來給 feedback
GAN_Lecture_9_(2018)_-_Sequence_Generation-623 我們一樣有一個 chatbot，一樣吃一個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-624 output 另外一個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-625 現在有一個 discriminator，這個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-626 其實就是取代了人的角色
GAN_Lecture_9_(2018)_-_Sequence_Generation-627 它吃 chatbot 的 input 跟 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-628 然後吐出一個分數
GAN_Lecture_9_(2018)_-_Sequence_Generation-629 那這個跟 typical 的 conditional GAN 就是一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-630 我們知道說， 就算是別的 task
GAN_Lecture_9_(2018)_-_Sequence_Generation-631 什麼 image 的生成，你做的事情也是一樣的，你就是有一個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-632 它吃你的 generator 的 input 跟 output，接下來給你一個評價
GAN_Lecture_9_(2018)_-_Sequence_Generation-633 那在 chatbot 裡面也是一樣，你有一個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-634 它吃 chatbot input 的 sentence  跟 output sentence
GAN_Lecture_9_(2018)_-_Sequence_Generation-635 然後給予一個評價
GAN_Lecture_9_(2018)_-_Sequence_Generation-636 那這個 discriminator 呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-637 你要給他大量人類的對話
GAN_Lecture_9_(2018)_-_Sequence_Generation-638 讓他知道說真正的人類的對話
GAN_Lecture_9_(2018)_-_Sequence_Generation-639 真正的當這個chatbot 換成一個人的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-640 他的 c 跟 x 長什麼樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-641 那這個 discriminator 就會學著鑑別說
GAN_Lecture_9_(2018)_-_Sequence_Generation-642 這個 c 跟 x 的 pair
GAN_Lecture_9_(2018)_-_Sequence_Generation-643 是來自於人類，還是來自於 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-644 然後 discriminator 會把他學到的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-645 feedback 給 chatbot，或者是說 chatbot 要想辦法騙過這個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-646 那這跟我們之前上週講的 conditional GAN
GAN_Lecture_9_(2018)_-_Sequence_Generation-647 還是上上週講的 conditional GAN
GAN_Lecture_9_(2018)_-_Sequence_Generation-648 反正就是一模一樣的事情就是了
GAN_Lecture_9_(2018)_-_Sequence_Generation-649 那這個 algorithm 是什麼樣子呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-650 其實這個 discriminator 的 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-651 就可以想成是人在給 reward，你要把這個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-652 想成其實就是一個人
GAN_Lecture_9_(2018)_-_Sequence_Generation-653 只是這個 discriminator 和人不一樣的地方是
GAN_Lecture_9_(2018)_-_Sequence_Generation-654 它不是完美的，所以他也是要去更新它自己的參數的
GAN_Lecture_9_(2018)_-_Sequence_Generation-655 那整個 algorithm 其實就跟傳統的 GAN
GAN_Lecture_9_(2018)_-_Sequence_Generation-656 是一樣的，傳統 conditional GAN 是一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-657 你有 training data，這些 training data
GAN_Lecture_9_(2018)_-_Sequence_Generation-658 就是一大堆的正確的 c 跟 x 的 pair
GAN_Lecture_9_(2018)_-_Sequence_Generation-659 然後你一開始你就 initialize 一個 G
GAN_Lecture_9_(2018)_-_Sequence_Generation-660 其實你的 G 就是你的 generator 你的 chatbot，然後 initialize 你的 discriminator D
GAN_Lecture_9_(2018)_-_Sequence_Generation-661 在每一個 training 的 iteration 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-662 你從你的 training data 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-663 sample 出正確的 c 跟 x 的 pair
GAN_Lecture_9_(2018)_-_Sequence_Generation-664 你從你的 training data 裡面 sample 出一個 c prime
GAN_Lecture_9_(2018)_-_Sequence_Generation-665 然後把這個 c prime 丟到你的 generator 也就是 chatbot 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-666 讓它回一個句子 x tilde
GAN_Lecture_9_(2018)_-_Sequence_Generation-667 那這個 c prime, x tilde 就是一個 native 的一個 example
GAN_Lecture_9_(2018)_-_Sequence_Generation-668 接下來discriminator 要學著說
GAN_Lecture_9_(2018)_-_Sequence_Generation-669 看到正確的 c 跟 x
GAN_Lecture_9_(2018)_-_Sequence_Generation-670 給它比較高的分數
GAN_Lecture_9_(2018)_-_Sequence_Generation-671 看到錯誤的 c prime 跟 x tilde
GAN_Lecture_9_(2018)_-_Sequence_Generation-672 給它比較低的分數
GAN_Lecture_9_(2018)_-_Sequence_Generation-673 至於怎麼 train 這個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-674 你可以用傳統的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-675 量 js divergence 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-676 你完全也可以套用 WGAN，都可以這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-677 都可以，你這邊可以用 WGAN，都是沒有問題的
GAN_Lecture_9_(2018)_-_Sequence_Generation-678 那接下來的問題是說
GAN_Lecture_9_(2018)_-_Sequence_Generation-679 我們知道在 GAN 裡面你 train discriminator 以後
GAN_Lecture_9_(2018)_-_Sequence_Generation-680 接下來你就要 train 你的 chatbot，也就是 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-681 那 train generator 他的目標是什麼呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-682 你要 train 你的 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-683 這個 generator 的目標呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-684 就是要去 update 參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-685 然後呢你 generator 產生出來的 c 跟 x 的 pair
GAN_Lecture_9_(2018)_-_Sequence_Generation-686 能讓 discriminator 的 output 越大越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-687 那這個就是 generator 要做的事情
GAN_Lecture_9_(2018)_-_Sequence_Generation-688 這邊要做的事情，跟我們之前看到的 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-689 跟我們之前看到的 conditional GAN，其實是一模一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-690 我們說 generator 要做的事情
GAN_Lecture_9_(2018)_-_Sequence_Generation-691 其實就是要去騙過 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-692 但是這邊我們會遇到一個問題
GAN_Lecture_9_(2018)_-_Sequence_Generation-693 什麼樣的問題呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-694 如果你仔細想一想
GAN_Lecture_9_(2018)_-_Sequence_Generation-695 你的 chatbot 的 network 的架構的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-696 我們的 chatbot 的 network 的架構它是一個 seq to seq 的 model
GAN_Lecture_9_(2018)_-_Sequence_Generation-697 它是一個 RNN 的 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-698 我們看 chatbot 在 generate 一個 sequence 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-699 它 generate sequence  的 process 是這樣子的
GAN_Lecture_9_(2018)_-_Sequence_Generation-700 那這個我們在講 seq to seq model 的時候我們已經講過了
GAN_Lecture_9_(2018)_-_Sequence_Generation-701 他是怎麼generate 的呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-702 一開始你給它一個 condition
GAN_Lecture_9_(2018)_-_Sequence_Generation-703 這個 condition 可能是從 attention based model 來的
GAN_Lecture_9_(2018)_-_Sequence_Generation-704 給它一個 condition，然後呢它 output 一個 distribution
GAN_Lecture_9_(2018)_-_Sequence_Generation-705 那根據這個 distribution 它會去做一個 sample
GAN_Lecture_9_(2018)_-_Sequence_Generation-706 就 sample 出一個 token
GAN_Lecture_9_(2018)_-_Sequence_Generation-707 sample 出一個 word
GAN_Lecture_9_(2018)_-_Sequence_Generation-708 然後接下來你會把這個 sample 出來的 word
GAN_Lecture_9_(2018)_-_Sequence_Generation-709 當作下一個 timestamp 的 input
GAN_Lecture_9_(2018)_-_Sequence_Generation-710 再產生新的 distribution，再做 sample
GAN_Lecture_9_(2018)_-_Sequence_Generation-711 再當做下一個 timestamp 的 input，再產生 distribution
GAN_Lecture_9_(2018)_-_Sequence_Generation-712 那大家在作業 2 已經 implement 過這種 model
GAN_Lecture_9_(2018)_-_Sequence_Generation-713 所以我相信這個對你來說並不陌生
GAN_Lecture_9_(2018)_-_Sequence_Generation-714 然後我們說我們要把 generator 的 output 呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-715 丟給 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-716 你對這個 discriminator 的架構，你也是自己設計
GAN_Lecture_9_(2018)_-_Sequence_Generation-717 反正只要可以吃兩個 sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-718 注意一下這個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-719 剛才前一頁的圖，我只有畫說它吃這個
GAN_Lecture_9_(2018)_-_Sequence_Generation-720 chatbot 的 output，但它其實不能夠只吃 chatbot 的 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-721 它是同時吃 chatbot 的 input 和 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-722 我想大家應該了解這個意思吧，在做 conditional GAN 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-723 你的 discriminator 要同時吃你的 generator 的 input 和 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-724 所以其實這個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-725 是同時吃了這個 chatbot 的 input 跟 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-726 就是兩個 word sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-727 那至於這個 discriminator network 架構要長什麼樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-728 這個就是看你高興，看你高興
GAN_Lecture_9_(2018)_-_Sequence_Generation-729 你可以說你就 learn 一個 RNN
GAN_Lecture_9_(2018)_-_Sequence_Generation-730 然後你把 chatbot input 跟 output 把它接起來
GAN_Lecture_9_(2018)_-_Sequence_Generation-731 變成一個很長的 sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-732 然後 discriminator 把這個很長的 sequence 就讀過
GAN_Lecture_9_(2018)_-_Sequence_Generation-733 然後就吐出一個數值
GAN_Lecture_9_(2018)_-_Sequence_Generation-734 這樣也是可以的
GAN_Lecture_9_(2018)_-_Sequence_Generation-735 有人說我可以用 CNN，反正只要吃兩個 sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-736 可以吐出一個分數
GAN_Lecture_9_(2018)_-_Sequence_Generation-737 怎麼樣都是可以的
GAN_Lecture_9_(2018)_-_Sequence_Generation-738 那反正 discriminator 就吃一個 word sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-739 接下來他吐出一個分數
GAN_Lecture_9_(2018)_-_Sequence_Generation-740 當我們知道說假設我們今天要 train generator 去騙過 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-741 我們要做的事情是，update generator 的參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-742 update 這個 chatbot seq to seq model 的參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-743 讓 discriminator 的 output 的 scalar 越大越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-744 這件事情你仔細想一下，你有辦法做嗎？
GAN_Lecture_9_(2018)_-_Sequence_Generation-745 你可以想說這個很簡單啊
GAN_Lecture_9_(2018)_-_Sequence_Generation-746 就是把 generator 跟 discriminator 串起來就變成一個巨大的 network
GAN_Lecture_9_(2018)_-_Sequence_Generation-747 然後我們要做的事情就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-748 調這個巨大的 network 的前面幾個 layer 讓
GAN_Lecture_9_(2018)_-_Sequence_Generation-749 這個 network 最後的 output 越大越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-750 但是你會遇到的問題是
GAN_Lecture_9_(2018)_-_Sequence_Generation-751 你發現這個 network 其實是沒有辦法微分的
GAN_Lecture_9_(2018)_-_Sequence_Generation-752 為什麼它沒有辦法微分？
GAN_Lecture_9_(2018)_-_Sequence_Generation-753 你想想看這整個 network 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-754 有一個 sampling 的 process，
GAN_Lecture_9_(2018)_-_Sequence_Generation-755 這跟我們之前在講 image 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-756 是不一樣的，那我覺得這個其實是當你用文字
GAN_Lecture_9_(2018)_-_Sequence_Generation-757 你要用 GAN 來做 natural language 的 processing
GAN_Lecture_9_(2018)_-_Sequence_Generation-758 跟你用 GAN 來做 image processing 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-759 一個非常不一樣的地方
GAN_Lecture_9_(2018)_-_Sequence_Generation-760 在 image 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-761 當你用 GAN 來產生一張影像的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-762 你可以直接把產生的影像
GAN_Lecture_9_(2018)_-_Sequence_Generation-763 丟到 discriminator 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-764 所以你可以把 generator 跟 discriminator 合起來
GAN_Lecture_9_(2018)_-_Sequence_Generation-765 看作是一個巨大的 network
GAN_Lecture_9_(2018)_-_Sequence_Generation-766 但是今天在做文字的生成的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-767 你生成出一個 sentence
GAN_Lecture_9_(2018)_-_Sequence_Generation-768 這個 sentence 是一串 sequence，是一串 token
GAN_Lecture_9_(2018)_-_Sequence_Generation-769 你把這串 token 丟到 discriminator 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-770 這中間
GAN_Lecture_9_(2018)_-_Sequence_Generation-771 你要得到這個 token 的時候這中間有一個 sampling 的 process
GAN_Lecture_9_(2018)_-_Sequence_Generation-772 當一整個 network 裡面有一個 sampling 的 process 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-773 它是沒有辦法微分的
GAN_Lecture_9_(2018)_-_Sequence_Generation-774 為什麼呢，一個簡單的解釋是
GAN_Lecture_9_(2018)_-_Sequence_Generation-775 你想想看所謂的微分的意思是什麼？
GAN_Lecture_9_(2018)_-_Sequence_Generation-776 微分的意思是你把某一個參數小小的變化一下
GAN_Lecture_9_(2018)_-_Sequence_Generation-777 看它對最後的 output 的影響有多大
GAN_Lecture_9_(2018)_-_Sequence_Generation-778 這兩個相除，就是微分
GAN_Lecture_9_(2018)_-_Sequence_Generation-779 那今天假設一個 network 裡面有 sampling 的 process
GAN_Lecture_9_(2018)_-_Sequence_Generation-780 你把裡面的參數做一下小小的變化
GAN_Lecture_9_(2018)_-_Sequence_Generation-781 對 output 的影響是不確定的
GAN_Lecture_9_(2018)_-_Sequence_Generation-782 因為中間有個 sampling 的 process
GAN_Lecture_9_(2018)_-_Sequence_Generation-783 所以你每次得到的 output 是不一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-784 你今天對你整個 network 做一個小小的變化的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-785 它對 output 的影響是不確定的
GAN_Lecture_9_(2018)_-_Sequence_Generation-786 所以你根本就沒有辦法算微分出來
GAN_Lecture_9_(2018)_-_Sequence_Generation-787 或者是另外一個更簡單的解釋就是，你回去用
GAN_Lecture_9_(2018)_-_Sequence_Generation-788 TensorFlow 或 PyTorch implement 一下
GAN_Lecture_9_(2018)_-_Sequence_Generation-789 看看如果 network 裡面有一個 sampling 的 process
GAN_Lecture_9_(2018)_-_Sequence_Generation-790 你跑不跑得動這樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-791 你應該是會得到一個 error 這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-792 但是你應該是跑不動的
GAN_Lecture_9_(2018)_-_Sequence_Generation-793 結果就是這樣，反正無論如何
GAN_Lecture_9_(2018)_-_Sequence_Generation-794 今天你把這個 seq to seq model
GAN_Lecture_9_(2018)_-_Sequence_Generation-795 跟你的 discriminator 接起來的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-796 你是沒有辦法微分的
GAN_Lecture_9_(2018)_-_Sequence_Generation-797 所以接下來真正的難點就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-798 怎麼解這個問題
GAN_Lecture_9_(2018)_-_Sequence_Generation-799 那我在文獻上看到，大概有三類的解法
GAN_Lecture_9_(2018)_-_Sequence_Generation-800 一個是 Gumbel-softmax
GAN_Lecture_9_(2018)_-_Sequence_Generation-801 一個就是給 discriminator continuous input
GAN_Lecture_9_(2018)_-_Sequence_Generation-802 然後另外一個方法就是做 reinforcement learning
GAN_Lecture_9_(2018)_-_Sequence_Generation-803 那 Gumbel-softmax 我們就不解釋，不解釋
GAN_Lecture_9_(2018)_-_Sequence_Generation-804 那它其實 implement 其實也是蠻簡單的，但是我發現用在 GAN 上目前沒有那麼多
GAN_Lecture_9_(2018)_-_Sequence_Generation-805 所以我們就不解釋，總之 Gumbel-softmax 就是想了一個 trick
GAN_Lecture_9_(2018)_-_Sequence_Generation-806 讓本來不能微分的東西，somehow 變成可以微分
GAN_Lecture_9_(2018)_-_Sequence_Generation-807 如果你有興趣的話，你再自己研究 G-S 是怎麼做的
GAN_Lecture_9_(2018)_-_Sequence_Generation-808 那另外一個很簡單的方法就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-809 給 discriminator continuous 的 input
GAN_Lecture_9_(2018)_-_Sequence_Generation-810 你說今天如果問題是在這一個 sampling 的 process
GAN_Lecture_9_(2018)_-_Sequence_Generation-811 那我們何不就避開 sampling process 呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-812 我們今天給這個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-813 這一個 distribution
GAN_Lecture_9_(2018)_-_Sequence_Generation-814 這 discriminator 不是吃這些 word sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-815 不是吃這個 discrete token
GAN_Lecture_9_(2018)_-_Sequence_Generation-816 來得到分數，而是吃這些 word distribution
GAN_Lecture_9_(2018)_-_Sequence_Generation-817 來得到分數
GAN_Lecture_9_(2018)_-_Sequence_Generation-818 那今天如果我們把這一個 seq to seq model
GAN_Lecture_9_(2018)_-_Sequence_Generation-819 跟這個 discriminator 串在一起
GAN_Lecture_9_(2018)_-_Sequence_Generation-820 你就會發現說它變成一個是可以微分的 network 了
GAN_Lecture_9_(2018)_-_Sequence_Generation-821 因為現在沒有那一個 sampling process 了
GAN_Lecture_9_(2018)_-_Sequence_Generation-822 問題就解決了
GAN_Lecture_9_(2018)_-_Sequence_Generation-823 但是實際上問題並沒有這麼簡單，因為你仔細想想看
GAN_Lecture_9_(2018)_-_Sequence_Generation-824 當你今天給你的 discriminator 一個 continuous input 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-825 你會發生什麼樣的問題，你會發生的問題是這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-826 Discriminator 不是會看 real data 跟 fake data
GAN_Lecture_9_(2018)_-_Sequence_Generation-827 去決定說，它會看 real data 跟 fake data
GAN_Lecture_9_(2018)_-_Sequence_Generation-828 然後去給它一筆新的 data 的時候，它會決定它是 real 還是 fake 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-829 當你今天給 discriminator word distribution 的時候，你會發現說
GAN_Lecture_9_(2018)_-_Sequence_Generation-830 real data 跟 fake data 它在本質上就是不一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-831 因為對 real data 來說
GAN_Lecture_9_(2018)_-_Sequence_Generation-832 它是 discrete token
GAN_Lecture_9_(2018)_-_Sequence_Generation-833 或者是說每一個 discrete token
GAN_Lecture_9_(2018)_-_Sequence_Generation-834 我們其實是用一個 1 one-hot 的 vector 來表示它
GAN_Lecture_9_(2018)_-_Sequence_Generation-835 我想 1 one-hot vector 大家應該都知道
GAN_Lecture_9_(2018)_-_Sequence_Generation-836 所以對一個 discrete token
GAN_Lecture_9_(2018)_-_Sequence_Generation-837 我們是用 1 one-hot vector 來表示它
GAN_Lecture_9_(2018)_-_Sequence_Generation-838 所以一個真正的 sentence
GAN_Lecture_9_(2018)_-_Sequence_Generation-839 一個 real sentence
GAN_Lecture_9_(2018)_-_Sequence_Generation-840 對 discriminator 來說，它看到的 real sentence 就長這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-841 而對 generator 來說
GAN_Lecture_9_(2018)_-_Sequence_Generation-842 它每次只會 output 一個 word distribution
GAN_Lecture_9_(2018)_-_Sequence_Generation-843 它每次 output 的都是一個 distribution
GAN_Lecture_9_(2018)_-_Sequence_Generation-844 當它把它的 output 丟給 discriminator 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-845 discriminator 看到的結果是這樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-846 所以對 discriminator 來說
GAN_Lecture_9_(2018)_-_Sequence_Generation-847 要分辨今天的 input 是 real 還是 fake 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-848 太容易了，他完全不需要管這個句子的語義
GAN_Lecture_9_(2018)_-_Sequence_Generation-849 都不用管，它完全不管句子的語義，它只要一看說
GAN_Lecture_9_(2018)_-_Sequence_Generation-850 是不是 one-hot
GAN_Lecture_9_(2018)_-_Sequence_Generation-851 就知道說它是 real 還是 fake 的，就結束了
GAN_Lecture_9_(2018)_-_Sequence_Generation-852 所以如果你直接用這個方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-853 來 train GAN 的話，你會發現會遇到什麼問題呢？你會發現說
GAN_Lecture_9_(2018)_-_Sequence_Generation-854 generator 很快就會發現說 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-855 判斷一筆 data 是 real 還是 fake 的準則
GAN_Lecture_9_(2018)_-_Sequence_Generation-856 是看說今天你的每一個 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-857 是不是 one-hot 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-858 所以 generator 唯一會學到的事情就是，迅速的變成 one-hot
GAN_Lecture_9_(2018)_-_Sequence_Generation-859 它會想辦法趕快把某一個
GAN_Lecture_9_(2018)_-_Sequence_Generation-860 隨便選一個 element 誰都好，也不要在意語意了
GAN_Lecture_9_(2018)_-_Sequence_Generation-861 因為就算你考慮語意
GAN_Lecture_9_(2018)_-_Sequence_Generation-862 也很快會被 discriminator 發現
GAN_Lecture_9_(2018)_-_Sequence_Generation-863 因為 discriminator 就是要看說是不是 one-hot
GAN_Lecture_9_(2018)_-_Sequence_Generation-864 所以今天隨便選一個 element，想辦法趕快把它的值變到 1
GAN_Lecture_9_(2018)_-_Sequence_Generation-865 其他都趕快壓成 0
GAN_Lecture_9_(2018)_-_Sequence_Generation-866 然後產生的句子完全不 make sense，然後就結束了
GAN_Lecture_9_(2018)_-_Sequence_Generation-867 你會發現所以今天直接讓 discriminator 吃 continuous input 是不夠的
GAN_Lecture_9_(2018)_-_Sequence_Generation-868 是沒有辦法真的解決這個問題
GAN_Lecture_9_(2018)_-_Sequence_Generation-869 那其實還有一個解法是說，也許用一般的 GAN
GAN_Lecture_9_(2018)_-_Sequence_Generation-870 train 不起來，但是你可以試試看用 WGAN
GAN_Lecture_9_(2018)_-_Sequence_Generation-871 為什麼在這個 case 用 WGAN，是有希望的呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-872 因為今天呢 WGAN 我們不是說在 train 的時候，你會給你的 model 一個 constrain
GAN_Lecture_9_(2018)_-_Sequence_Generation-873 你要去 constrain 説你的 discriminator，一定要是 1-Lipschitz function
GAN_Lecture_9_(2018)_-_Sequence_Generation-874 因為你有這個 constrain，所以你的 discriminator 它的
GAN_Lecture_9_(2018)_-_Sequence_Generation-875 手腳會被綁住，所以它就沒有辦法馬上分別出 real sentence
GAN_Lecture_9_(2018)_-_Sequence_Generation-876 跟 generated sentence 的差別
GAN_Lecture_9_(2018)_-_Sequence_Generation-877 他的視線是比較模糊的，它是比較看不清楚的
GAN_Lecture_9_(2018)_-_Sequence_Generation-878 因為它有一個 1-Lipschitz function constrain
GAN_Lecture_9_(2018)_-_Sequence_Generation-879 所以他是比較 fuzzy 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-880 所以它就沒有辦法馬上分別這兩者的差別
GAN_Lecture_9_(2018)_-_Sequence_Generation-881 所以今天假設你要做這個 conditional generation 的時候呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-882 如果你是要做這種 sequence generation
GAN_Lecture_9_(2018)_-_Sequence_Generation-883 然後你要用的方法是讓 discriminator 吃 continuous input
GAN_Lecture_9_(2018)_-_Sequence_Generation-884 WGAN 是一個可以的選擇
GAN_Lecture_9_(2018)_-_Sequence_Generation-885 如果你沒有用 WGAN 的話，應該是很難把它做起的
GAN_Lecture_9_(2018)_-_Sequence_Generation-886 因為 generator 其實學不到語意相關的東西，它只學到說
GAN_Lecture_9_(2018)_-_Sequence_Generation-887 output 必須要像是 one-hot，才能夠騙過 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-888 所以這個是第二個 solution，給它 continuous input
GAN_Lecture_9_(2018)_-_Sequence_Generation-889 第三個 solution 呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-890 就是套用 RL
GAN_Lecture_9_(2018)_-_Sequence_Generation-891 我們剛才已經講過說，假設這個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-892 換成一個人的話，你知道怎麼去調你 chatbot 的參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-893 去 maximize 人會給予 chatbot 的 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-894 那今天把人換成 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-895 solution 其實是一模一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-896 怎麼解這個問題呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-897 也就是說現在呢，discriminator 就是一個 human
GAN_Lecture_9_(2018)_-_Sequence_Generation-898 我們說人其實就是一個 function 嘛，然後看 chatbot 的 input output 給予分數
GAN_Lecture_9_(2018)_-_Sequence_Generation-899 所以 discriminator 就是我們的人
GAN_Lecture_9_(2018)_-_Sequence_Generation-900 它的 output，它的 output 那個 scalar
GAN_Lecture_9_(2018)_-_Sequence_Generation-901 discriminator output 的那個數值，就是 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-902 然後今天你的 chatbot 要去調它的參數，去 maximize discriminator 的 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-903 也就是說本來人的 output 是 R(c, x)
GAN_Lecture_9_(2018)_-_Sequence_Generation-904 那我們只是把它換成 discriminator 的 output D of (c, x)，就結束了
GAN_Lecture_9_(2018)_-_Sequence_Generation-905 接下來怎麼 maximize D of (c, x)
GAN_Lecture_9_(2018)_-_Sequence_Generation-906 你在 RL 怎麼做，在這邊就怎麼做，結束，就這樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-907 所以呢，我們說在這個 RL 裡面是怎麼做的呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-908 你讓 theta 去跟人互動，然後得到很多 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-909 接下來套右邊這個式子，你就可以去 train 你的 model
GAN_Lecture_9_(2018)_-_Sequence_Generation-910 現在我們唯一做的事情，是把人呢，換成另外一個機器
GAN_Lecture_9_(2018)_-_Sequence_Generation-911 就是換成 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-912 本來是人給 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-913 現在換成 discriminator 給 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-914 我們唯一做的事情，就是把 R 換成 D
GAN_Lecture_9_(2018)_-_Sequence_Generation-915 所以右邊也是一樣，把 R 換成 D
GAN_Lecture_9_(2018)_-_Sequence_Generation-916 當然這樣跟人互動還是不一樣，因為人跟機器互動很花時間嘛
GAN_Lecture_9_(2018)_-_Sequence_Generation-917 那如果是 discriminator，它要跟 generator 互動多少次
GAN_Lecture_9_(2018)_-_Sequence_Generation-918 反正都是機器，你就可以讓他們真的互動非常多次
GAN_Lecture_9_(2018)_-_Sequence_Generation-919 但是這邊只完成了 GAN 的其中一個 step 而已
GAN_Lecture_9_(2018)_-_Sequence_Generation-920 我們知道說在 GAN 的每一個 iteration 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-921 你要 train generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-922 你要 train discriminator 再 train generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-923 再 train discriminator，再 train generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-924 今天這個 RL 的 step 只是 train 了 generator 而已
GAN_Lecture_9_(2018)_-_Sequence_Generation-925 接下來你還要 train discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-926 怎麼 train discriminator 呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-927 你就給 discriminator 很多人真正的對話
GAN_Lecture_9_(2018)_-_Sequence_Generation-928 你給 discriminator 很多現在你的這個 generator 產生出來的對話
GAN_Lecture_9_(2018)_-_Sequence_Generation-929 你給 discriminator 很多 generator 產生出來的對話
GAN_Lecture_9_(2018)_-_Sequence_Generation-930 給很多人的對話
GAN_Lecture_9_(2018)_-_Sequence_Generation-931 然後 discriminator 就會去學著分辨說這個對話是 real 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-932 是真正人講的，還是 generator 產生的
GAN_Lecture_9_(2018)_-_Sequence_Generation-933 那你就可以學出一個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-934 那你學完 discriminator 以後
GAN_Lecture_9_(2018)_-_Sequence_Generation-935 再重頭去 train
GAN_Lecture_9_(2018)_-_Sequence_Generation-936 因為你的 discriminator 不一樣了
GAN_Lecture_9_(2018)_-_Sequence_Generation-937 這邊給的分數當然也不一樣了
GAN_Lecture_9_(2018)_-_Sequence_Generation-938 你 train 好 discriminator 以後
GAN_Lecture_9_(2018)_-_Sequence_Generation-939 再回頭去 train generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-940 再回頭去 train discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-941 這兩個 step 就反覆地進行
GAN_Lecture_9_(2018)_-_Sequence_Generation-942 這個就是用 GAN 來 train seq to seq model 的方法，用 GAN 來 train chatbot 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-943 那其實還有很多的 tip，那這邊也稍跟大家講一下
GAN_Lecture_9_(2018)_-_Sequence_Generation-944 那如果我們看這個式子的話，你會發現有一個問題
GAN_Lecture_9_(2018)_-_Sequence_Generation-945 什麼樣的問題呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-946 這個式子跟剛才那個 RL 看到的式子是一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-947 我們只是把 R 換成了 D
GAN_Lecture_9_(2018)_-_Sequence_Generation-948 那你問這邊這個式子有什麼樣的問題呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-949 今天假設 ci 是 what is your name
GAN_Lecture_9_(2018)_-_Sequence_Generation-950 然後 xi 是 I don't know，這可能不是一個很好的回答
GAN_Lecture_9_(2018)_-_Sequence_Generation-951 所以你得到的 discriminator 給它的分數是負的
GAN_Lecture_9_(2018)_-_Sequence_Generation-952 當 discriminator 給它的分數是負的的時候，我們希望調整我們的參數 theta
GAN_Lecture_9_(2018)_-_Sequence_Generation-953 讓 log Ptheta of (xi, given ci) 的值越小越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-954 讓 log Ptheta of (xi, given ci) 的值變小
GAN_Lecture_9_(2018)_-_Sequence_Generation-955 那我們再想想看，Ptheta (xi, given ci)
GAN_Lecture_9_(2018)_-_Sequence_Generation-956 到底是什麼樣的東西呢？它其實是一大堆 term 的連成
GAN_Lecture_9_(2018)_-_Sequence_Generation-957 對不對，它其實是一大堆 term 的連成
GAN_Lecture_9_(2018)_-_Sequence_Generation-958 也就是說，我們今天實際上在做 generation 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-959 我們每次只會 generate 一個 word 而已，對不對
GAN_Lecture_9_(2018)_-_Sequence_Generation-960 我們假設 I don't know 這邊有三個 word，第一個 word 是 x1
GAN_Lecture_9_(2018)_-_Sequence_Generation-961 第二個 word 是 x2，第三個 word 是 x3
GAN_Lecture_9_(2018)_-_Sequence_Generation-962 這個 Ptheta of (xi, given ci)
GAN_Lecture_9_(2018)_-_Sequence_Generation-963 實際上是 P of 在 given condition ci 的前提下
GAN_Lecture_9_(2018)_-_Sequence_Generation-964 產生 xi 的機率，乘上 given condition 跟 xi 產生 x2 的機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-965 再乘上 given condition x1, x2 我用 x1:2 代表 x1,  x2
GAN_Lecture_9_(2018)_-_Sequence_Generation-966 given condition x1, x2 產生 x3 的機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-967 你把這三項機率相乘，就會 Ptheta of (xi, given ci)
GAN_Lecture_9_(2018)_-_Sequence_Generation-968 那這邊取 log，取log，取log，取log，所以變成相加
GAN_Lecture_9_(2018)_-_Sequence_Generation-969 這個大家 ok 吧，實際上這個機率的算法
GAN_Lecture_9_(2018)_-_Sequence_Generation-970 是這個樣子的，那你說讓這個機率下降
GAN_Lecture_9_(2018)_-_Sequence_Generation-971 意思就是你希望這一項也下降，你希望這一項也下降
GAN_Lecture_9_(2018)_-_Sequence_Generation-972 你希望這一項也下降，你希望他們每一項都下降
GAN_Lecture_9_(2018)_-_Sequence_Generation-973 但是我們看看 P of (ci, given x1) 是什麼，P of (ci, given x1) 是
GAN_Lecture_9_(2018)_-_Sequence_Generation-974 given condition 是 what is your name 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-975 產生 I 的機率
GAN_Lecture_9_(2018)_-_Sequence_Generation-976 那如果輸入 what is your name?
GAN_Lecture_9_(2018)_-_Sequence_Generation-977 一個好的答案其實可能是比如說 I am John 或 I am Mary
GAN_Lecture_9_(2018)_-_Sequence_Generation-978 所以今天問 What is your name? 的時候，給你的 condition: What is your name? 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-979 你其實回答 I 當作句子的開頭
GAN_Lecture_9_(2018)_-_Sequence_Generation-980 是好的，但是你在 training 的時候，你卻告訴 chatbot 説
GAN_Lecture_9_(2018)_-_Sequence_Generation-981 看到 What is your name? 的時候，回答 I 這個機率，應該是下降的
GAN_Lecture_9_(2018)_-_Sequence_Generation-982 看到 What is your name?  你已經產生 I
GAN_Lecture_9_(2018)_-_Sequence_Generation-983 產生 don't 的機率要下降，這項是合理的
GAN_Lecture_9_(2018)_-_Sequence_Generation-984 產生 I don't  再產生 know 的機率要下降是合理的
GAN_Lecture_9_(2018)_-_Sequence_Generation-985 但是 given What is your name? 產生 I 的機率，其實是不合理的
GAN_Lecture_9_(2018)_-_Sequence_Generation-986 那你可能會這樣覺得說，那這個 training 不是有問題嗎？
GAN_Lecture_9_(2018)_-_Sequence_Generation-987 理論上這個 training 不會有問題，為什麼？
GAN_Lecture_9_(2018)_-_Sequence_Generation-988 因為今天你的 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-989 其實是一個 sampling 的 process
GAN_Lecture_9_(2018)_-_Sequence_Generation-990 所以今天在另外一個 case
GAN_Lecture_9_(2018)_-_Sequence_Generation-991 當你輸入 What is your name 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-992 機器的回答可能是 I am John
GAN_Lecture_9_(2018)_-_Sequence_Generation-993 這個時候機器就會得到一個 positive 的 reward
GAN_Lecture_9_(2018)_-_Sequence_Generation-994 也就是 discriminator 會給機器一個 positive 的評價
GAN_Lecture_9_(2018)_-_Sequence_Generation-995 這個時候 model 要做的事情就是 update 它的參數
GAN_Lecture_9_(2018)_-_Sequence_Generation-996 去 increase log Ptheta (xi, given ci)
GAN_Lecture_9_(2018)_-_Sequence_Generation-997 它要去 increase  Ptheta (xi, given ci)
GAN_Lecture_9_(2018)_-_Sequence_Generation-998 那  Ptheta (xi, given ci)，是這三個項的乘
GAN_Lecture_9_(2018)_-_Sequence_Generation-999 是這三個項的相乘，而第一項是 P of (I, given ci)
GAN_Lecture_9_(2018)_-_Sequence_Generation-1000 那今天我們會希望 P of (I, given ci) 的值越大越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1001 當你輸入 What is your name? sample 到 I don't know 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1002 P of given ci 產生 I 的機率，要偏小
GAN_Lecture_9_(2018)_-_Sequence_Generation-1003 當你 sample 到 I am John 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1004 你希望這個機率上升
GAN_Lecture_9_(2018)_-_Sequence_Generation-1005 那如果你今天 sample 的次數夠多
GAN_Lecture_9_(2018)_-_Sequence_Generation-1006 這兩項就會抵消，那就沒事了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1007 但問題就是在實作上，你永遠 sample 不到夠多的次數
GAN_Lecture_9_(2018)_-_Sequence_Generation-1008 所以在實作上這個方法是會造成一些問題的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1009 所以怎麼辦呢？今天的 solution 是這個樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1010 我們今天希望當輸入 What is your name? sample 到 I don't know 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1011 machine 可以自動知道說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1012 在這三個機率裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-1013 雖然 I don't know 整體而言是不好的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1014 但是造成 I don't know 不好的原因
GAN_Lecture_9_(2018)_-_Sequence_Generation-1015 並不是因為在開頭 sample 到了 I
GAN_Lecture_9_(2018)_-_Sequence_Generation-1016 在開頭 sample 到 I，是沒有問題的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1017 是因為之後你產生了 don't 跟 know，所以才做得不好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1018 所以希望機器可以自動學到說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1019 今天這個句子不好，到底是哪裡不好，是因為產生這兩個 word 不好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1020 而不是產生第一個  word 不好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1021 那所以你今天會改寫你的式子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1022 本來你的式子是這樣，對一整個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1023 given 一個 ci，產生一個完整的句子 xi，他會有一個分數 D of (ci, xi)
GAN_Lecture_9_(2018)_-_Sequence_Generation-1024 現在你給每一個 generation step
GAN_Lecture_9_(2018)_-_Sequence_Generation-1025 都不同的分數
GAN_Lecture_9_(2018)_-_Sequence_Generation-1026 今天在給定 condition ci，已經產生 x1 到 t-1
GAN_Lecture_9_(2018)_-_Sequence_Generation-1027 已經產生前 t-1 個 word 的情況下
GAN_Lecture_9_(2018)_-_Sequence_Generation-1028 產生的 word xt， 他到底有多好或多不好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1029 我們換另外一個 measure 叫做 Q
GAN_Lecture_9_(2018)_-_Sequence_Generation-1030 來取代 D
GAN_Lecture_9_(2018)_-_Sequence_Generation-1031 這個 Q 它是對每一個 timestamp 去做 evaluation
GAN_Lecture_9_(2018)_-_Sequence_Generation-1032 它對這邊每一次 generation 的 timestamp 去做 evaluation
GAN_Lecture_9_(2018)_-_Sequence_Generation-1033 而不是對整個句子去做 evaluation
GAN_Lecture_9_(2018)_-_Sequence_Generation-1034 那接下來問題就是， 這件事情要怎麼做呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1035 我們就沒打算要細講
GAN_Lecture_9_(2018)_-_Sequence_Generation-1036 那你可以自己去查一下文件，因為反正我們作業裡面沒這個東西，所以大概你也沒興趣知道就是了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1037 你如果想知道的話，你就自己查一下文獻
GAN_Lecture_9_(2018)_-_Sequence_Generation-1038 那有不同的作法，那這其實是一個還可以尚待研究中的問題
GAN_Lecture_9_(2018)_-_Sequence_Generation-1039 一個作法就是做 Monte Carlo
GAN_Lecture_9_(2018)_-_Sequence_Generation-1040 跟 Alpha Go 的方法非常地像
GAN_Lecture_9_(2018)_-_Sequence_Generation-1041 你就想成是在做 Alpha Go，你去 sample 接下來會發生到的狀況
GAN_Lecture_9_(2018)_-_Sequence_Generation-1042 然後去估測每一個 generation
GAN_Lecture_9_(2018)_-_Sequence_Generation-1043 每一個 generation 就像是在棋盤上下一個子一樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-1044 可以估測每一個 generation 在棋盤上落一個子的勝率
GAN_Lecture_9_(2018)_-_Sequence_Generation-1045 那這個方法最大的問題就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-1046 它需要的運算量太大，所以在實作上你會很難做
GAN_Lecture_9_(2018)_-_Sequence_Generation-1047 那有另外一個運算量比較小的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1048 這個方法他的縮寫叫做 REGS，不過反正這個方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1049 在文獻上看到的結果就是他不如 MC
GAN_Lecture_9_(2018)_-_Sequence_Generation-1050 那我們自己也有實作過，覺得他確實不如 MC
GAN_Lecture_9_(2018)_-_Sequence_Generation-1051 但 MC 的問題就是，他的運算量太大了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1052 所以這個仍然是一個目前可以研究的問題
GAN_Lecture_9_(2018)_-_Sequence_Generation-1053 那還有另外一個技術可以improve 你的 training，這個方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1054 叫做 RankGAN，那我們今天就不大算講 RankGAN
GAN_Lecture_9_(2018)_-_Sequence_Generation-1055 那你可能說不打算講為什麼要放在這邊呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1056 之後你就知道了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1057 那這邊是講一些我們自己的觀察啦
GAN_Lecture_9_(2018)_-_Sequence_Generation-1058 講一些我們自己的觀察
GAN_Lecture_9_(2018)_-_Sequence_Generation-1059 今天到底當你把 maximum likelihood
GAN_Lecture_9_(2018)_-_Sequence_Generation-1060 換到 GAN 的時候，有什麼樣的不同呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1061 因為大家都已經做過作業 2-2
GAN_Lecture_9_(2018)_-_Sequence_Generation-1062 所以你已經用過 End to End 的技術 train 過 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-1063 事實上如果你有 train 過 chatbot 的話，你會知道說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1064 今天 train 完以後，chatbot 非常喜歡回答一些沒有很常
GAN_Lecture_9_(2018)_-_Sequence_Generation-1065 然後非常 general 的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1066 通常它的回答要嘛就是 I'm sorry
GAN_Lecture_9_(2018)_-_Sequence_Generation-1067 要嘛就是 I don't know，這樣講來講去都是那幾句
GAN_Lecture_9_(2018)_-_Sequence_Generation-1068 那我們實際統計一下
GAN_Lecture_9_(2018)_-_Sequence_Generation-1069 我們用一個 benchmark corpus 叫做 Open subtitle
GAN_Lecture_9_(2018)_-_Sequence_Generation-1070 來 train 一個 end to end 的 chatbot 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1071 其實有 1/10 的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1072 他都會回答 I don't know 或是 I'm sorry
GAN_Lecture_9_(2018)_-_Sequence_Generation-1073 這聽起來其實是沒有非常 make sense
GAN_Lecture_9_(2018)_-_Sequence_Generation-1074 那如果你要解這個問題，我覺得 GAN 就可以派上用場
GAN_Lecture_9_(2018)_-_Sequence_Generation-1075 為什麼今天會回答 I'm sorry 或 I don't know 呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1076 我的猜測是，這些  I'm sorry 或 I don't know  這些句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1077 對應到影像上，就是那些模糊的影像
GAN_Lecture_9_(2018)_-_Sequence_Generation-1078 我們有講過說，為什麼我們今天在做影像生成的時候要用 GAN
GAN_Lecture_9_(2018)_-_Sequence_Generation-1079 而不是傳統的 supervised learning 的方法，是因為
GAN_Lecture_9_(2018)_-_Sequence_Generation-1080 今天在做影像的生成的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1081 你可能同樣的 condition
GAN_Lecture_9_(2018)_-_Sequence_Generation-1082 你有好多不同的對應的 image
GAN_Lecture_9_(2018)_-_Sequence_Generation-1083 比如說火車有很多不同的樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1084 那機器學習的時候，它是會產生所有火車的平均
GAN_Lecture_9_(2018)_-_Sequence_Generation-1085 然後看起來是一個模糊的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-1086 那今天對一般的 training 來說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1087 假設你沒有用 GAN 去 train 一個 chatbot 來說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1088 也是一樣的，因為輸入同一個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1089 在你的 training data 裡面，有好多個不同的答案
GAN_Lecture_9_(2018)_-_Sequence_Generation-1090 對 machine 來說他學習的結果就是希望去同時 maximize 所有不同答案的 likelihood
GAN_Lecture_9_(2018)_-_Sequence_Generation-1091 但是同時 maximize 所有答案的 likelihood 的結果
GAN_Lecture_9_(2018)_-_Sequence_Generation-1092 就是產生一些奇怪的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1093 那我認為這就是導致為什麼 machine
GAN_Lecture_9_(2018)_-_Sequence_Generation-1094 今天用 end to end 的方法，用 maximum likelihood 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1095 train 完一個 chatbot 以後它特別喜歡說 I'm sorry
GAN_Lecture_9_(2018)_-_Sequence_Generation-1096 或者是 I don't know
GAN_Lecture_9_(2018)_-_Sequence_Generation-1097 那用 GAN 的話，一個非常明顯你可以得到的結果是，用 GAN 來 train 你的 chatbot 以後，他比較喜歡講長的句子，那它講的句子會比較有內容，就這件事情算是蠻明顯的，那一個比較不明顯的地方是我們其實不確定說，產生比較長的句子以後，是不是一定就是比較好的對話，但是蠻明顯可以觀察到説，當你把原來 MLE 換成 GAN 的時候，它會產生比較長的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1098 那其實各種不同的 seq to seq model 包括
GAN_Lecture_9_(2018)_-_Sequence_Generation-1099 在作業 2-1 做的 video capturing generation，其實你都可以用上 GAN 的技術，所以這邊的 technical message，就前面的東西假設你都沒有聽到的話，那今天這堂課要講的東西就是，如果你今天在 train seq to seq model 的時候，你其實可以考慮加上 GAN，看看 train 的會不會比較好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1100 剛才講個 conditional sequence generation，那還是 supervised 的，你要有 seq to seq model 的 input 跟 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-1101 接下來要講 Unsupervised conditional sequence generation
GAN_Lecture_9_(2018)_-_Sequence_Generation-1102 那這邊會舉三個例子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1103 那我們先講 Text style transformation
GAN_Lecture_9_(2018)_-_Sequence_Generation-1104 那我們今天已經看過滿坑滿谷的例子是做 image style transformation
GAN_Lecture_9_(2018)_-_Sequence_Generation-1105 把什麼梵谷的畫，轉成
GAN_Lecture_9_(2018)_-_Sequence_Generation-1106 把風景照轉成梵谷的畫風轉成浮世繪的畫風等等
GAN_Lecture_9_(2018)_-_Sequence_Generation-1107 就作業 3-3 大家要做的事情
GAN_Lecture_9_(2018)_-_Sequence_Generation-1108 那其實在文字上，你也可以做 style 的 transformation
GAN_Lecture_9_(2018)_-_Sequence_Generation-1109 什麼叫做文字的 style 呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1110 這邊舉一個簡單的例子是說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1111 我們可以把正面的句子，算做是一種 style
GAN_Lecture_9_(2018)_-_Sequence_Generation-1112 負面的句子，算做是另一種 style
GAN_Lecture_9_(2018)_-_Sequence_Generation-1113 接下來你只要 apply cycle GAN 的技術
GAN_Lecture_9_(2018)_-_Sequence_Generation-1114 把兩種不同 style 的句子，當作兩個 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1115 你就可以用 unsupervised 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1116 你並不需要兩個 domain 的文字句子的 pair
GAN_Lecture_9_(2018)_-_Sequence_Generation-1117 你並不需要知道說這個 positive 的句子應該對應到哪一個 negative 的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1118 你不需要這個資訊，你只需要兩堆句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1119 一堆 positive，一堆 negative
GAN_Lecture_9_(2018)_-_Sequence_Generation-1120 你就可以直接 train 一個 style transformation
GAN_Lecture_9_(2018)_-_Sequence_Generation-1121 那我們知道說其實要做這種你要知道一個句子是不是 positive 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1122 其實還蠻容易的，因為我們在 ML 的作業 5 裡面
GAN_Lecture_9_(2018)_-_Sequence_Generation-1123 你就會 train 一個 RNN
GAN_Lecture_9_(2018)_-_Sequence_Generation-1124 那你就把你 train 過那個 RNN 拿出來，然後給他一堆句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1125 然後如果很 positive，就放一堆，很 negative 就放一堆
GAN_Lecture_9_(2018)_-_Sequence_Generation-1126 你就自動有 positive 跟 negative 的句子了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1127 那這個技術怎麼做呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1128 我們就完全不需要多講，這個是我們上週就看過的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1129 cycle GAN 的圖
GAN_Lecture_9_(2018)_-_Sequence_Generation-1130 那你今天要把 image style transformation
GAN_Lecture_9_(2018)_-_Sequence_Generation-1131 換成 text style transfer
GAN_Lecture_9_(2018)_-_Sequence_Generation-1132 唯一做的事情就是影像換成文字
GAN_Lecture_9_(2018)_-_Sequence_Generation-1133 所以我們就把 positive 的句子算是一個 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1134 negative 的句子算是另外一個 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1135 用 cycle GAN 的方法 train 下去就結束了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1136 那你這邊可能會遇到一個問題是，我們剛才有講到說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1137 如果今天你的 generator 的 output，是 discrete 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1138 你沒有辦法直接做 training
GAN_Lecture_9_(2018)_-_Sequence_Generation-1139 假設你今天你的 generator output 是一個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1140 句子是一個 discrete 的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-1141 你用一個 sampling 的 process
GAN_Lecture_9_(2018)_-_Sequence_Generation-1142 你才能夠產生那個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1143 當你把這兩個 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-1144 跟這個 discriminator 全部串在一起的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1145 你沒辦法一起 train
GAN_Lecture_9_(2018)_-_Sequence_Generation-1146 那怎麼辦呢？有很多不同的解法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1147 我們剛才就講說有三個解法，一個是
GAN_Lecture_9_(2018)_-_Sequence_Generation-1148 用 Gumbel-softmax
GAN_Lecture_9_(2018)_-_Sequence_Generation-1149 一個是給 discriminator continuous 的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-1150 第三個是用 RL，那就看你愛用哪一種，
GAN_Lecture_9_(2018)_-_Sequence_Generation-1151 在我們的實驗裡面，我們是用 continuous 的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-1152 怎麼做呢？其實就是把每一個 word
GAN_Lecture_9_(2018)_-_Sequence_Generation-1153 用它的 word embedding 來取代
GAN_Lecture_9_(2018)_-_Sequence_Generation-1154 你把每一個 word，用它的 word embedding 來取代以後
GAN_Lecture_9_(2018)_-_Sequence_Generation-1155 每一個句子，就是一個 vector 的 sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-1156 那 word embedding 它並不是 one-hot，它是continuous 的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-1157 現在你的 generator，是 output continuous 的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-1158 這個 discriminator 跟這個 generator 就可以吃這個 continuous 的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-1159 當作 input
GAN_Lecture_9_(2018)_-_Sequence_Generation-1160 所以你只要把 word 換成 word embedding
GAN_Lecture_9_(2018)_-_Sequence_Generation-1161 你就可以解這個 discrete 的問題
GAN_Lecture_9_(2018)_-_Sequence_Generation-1162 你可能會問說這個東西有什麼用
GAN_Lecture_9_(2018)_-_Sequence_Generation-1163 這個東西具體而言就是沒有什麼用
GAN_Lecture_9_(2018)_-_Sequence_Generation-1164 正向會轉負向喔，就是會 learn 兩個 network
GAN_Lecture_9_(2018)_-_Sequence_Generation-1165 就是在做 cycle GAN 的時候你會 learn 兩個 network
GAN_Lecture_9_(2018)_-_Sequence_Generation-1166 一個就是正向轉負向
GAN_Lecture_9_(2018)_-_Sequence_Generation-1167 但我們只拿負向轉正向這個出來 demo
GAN_Lecture_9_(2018)_-_Sequence_Generation-1168 就沒有拿正向轉負向出來這個 demo
GAN_Lecture_9_(2018)_-_Sequence_Generation-1169 因為我不希望讓你的人生變得更黑暗這樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1170 那我們上次講到說這種 unsupervised 的 transformation 有兩個做法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1171 一個就是 cycle GAN 系列的做法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1172 那我們剛才看到哪個 Text style transfer
GAN_Lecture_9_(2018)_-_Sequence_Generation-1173 是用 cycle GAN 系列的做法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1174 那也可以有另外一個系列的做法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1175 就是你把不同 domain 的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-1176 都 project 到同一個 space
GAN_Lecture_9_(2018)_-_Sequence_Generation-1177 然後再用不同 domain 的 decoder，把它解回來
GAN_Lecture_9_(2018)_-_Sequence_Generation-1178 Text style transfer 也可以用這樣子的做法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1179 你唯一做的事情
GAN_Lecture_9_(2018)_-_Sequence_Generation-1180 就只是把本來你的 x domain 跟 y domain 可能是真人的頭像
GAN_Lecture_9_(2018)_-_Sequence_Generation-1181 跟二次元人物的頭像，把他們換成正面的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1182 跟負面的句子，就結束了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1183 當然我們有說，今天如果是產生文字的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1184 你會遇到一些特別的問題就是因為，文字是 discrete 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1185 所以今天這個 discriminator，沒有辦法吃 discrete 的 input
GAN_Lecture_9_(2018)_-_Sequence_Generation-1186 如果它吃  discrete 的 input 的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-1187 它會沒有辦法跟 decoder jointly trained
GAN_Lecture_9_(2018)_-_Sequence_Generation-1188 所以怎麼解呢？在文獻上我們看過的一個作法是
GAN_Lecture_9_(2018)_-_Sequence_Generation-1189 當然你可以用 RL，Gumbel-softmax 等等不同的解法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1190 但我在文獻上看到一個有趣的解法是
GAN_Lecture_9_(2018)_-_Sequence_Generation-1191 有人說這個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-1192 這個是 MIT CSAIL lab 做的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1193 我如果沒記錯的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-1194 這 discriminator 不要吃 decoder output 的 word
GAN_Lecture_9_(2018)_-_Sequence_Generation-1195 它吃 decoder 的 hidden state
GAN_Lecture_9_(2018)_-_Sequence_Generation-1196 就 decoder 也是一個 RNN 嘛
GAN_Lecture_9_(2018)_-_Sequence_Generation-1197 那 RNN 每一個 timestamp 就會有一個 hidden vector
GAN_Lecture_9_(2018)_-_Sequence_Generation-1198 這個 decoder 不吃最終的 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-1199 它吃 hidden vector，hidden vector 是 continuous 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1200 所以就沒有那個 discrete 的問題，這是一個解法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1201 然後我們說這個今天你要讓這兩個不同的 encoder
GAN_Lecture_9_(2018)_-_Sequence_Generation-1202 可以把不同 domain 的東西 project 到同一個 space
GAN_Lecture_9_(2018)_-_Sequence_Generation-1203 你需要下一些 constrain，那在上週我們講了很多各式各樣不同的 constrain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1204 那我發現說那些各式各樣不同的 constrain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1205 還沒有被 apply 到文字的領域
GAN_Lecture_9_(2018)_-_Sequence_Generation-1206 所以這是一個未來可以做的事情
GAN_Lecture_9_(2018)_-_Sequence_Generation-1207 我現在看到唯一做的技術只有說有人 train 了一個 classifier
GAN_Lecture_9_(2018)_-_Sequence_Generation-1208 那這個 classifier，就吃這兩個 encoder 的 output
GAN_Lecture_9_(2018)_-_Sequence_Generation-1209 那這兩個 encoder 要盡量去騙過這個 classifier
GAN_Lecture_9_(2018)_-_Sequence_Generation-1210 這個 classifier 要從這個 vector 判斷說這個 vector 是來自於哪一個 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1211 我把文獻放在這邊給大家參考
GAN_Lecture_9_(2018)_-_Sequence_Generation-1212 那接下來我要講的是說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1213 用 GAN 的技術來做 Unsupervised Abstractive summarization
GAN_Lecture_9_(2018)_-_Sequence_Generation-1214 那這個怎麼做呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1215 那怎麼 train 一個 summarizer 呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1216 怎麼 train 一個 network 它可以幫你做摘要呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1217 那所謂做摘要的意思是說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1218 假設你收集到一些文章
GAN_Lecture_9_(2018)_-_Sequence_Generation-1219 那你有沒有時間看，你就把那些文章直接丟給 network
GAN_Lecture_9_(2018)_-_Sequence_Generation-1220 希望它讀完這個文章以後，自動地幫你生成出摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1221 當然做摘要這件事，從來不是一個新的問題
GAN_Lecture_9_(2018)_-_Sequence_Generation-1222 因為這個顯然是一個非常有應用價值的東西，所以他從來不是一個新的問題，50-60 年前就開始有人在做了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1223 只是在過去的時候，machine learning 的技術還沒有那麼強
GAN_Lecture_9_(2018)_-_Sequence_Generation-1224 所以過去你要讓機器學習做摘要的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1225 通常機器學做的事情是
GAN_Lecture_9_(2018)_-_Sequence_Generation-1226 extracted summarization
GAN_Lecture_9_(2018)_-_Sequence_Generation-1227 這邊 title 寫的是 abstractive summarization
GAN_Lecture_9_(2018)_-_Sequence_Generation-1228 還有另外一種作摘要的方法叫做 extracted summarization
GAN_Lecture_9_(2018)_-_Sequence_Generation-1229 extracted summarization 的意思就是說，給機器一篇文章
GAN_Lecture_9_(2018)_-_Sequence_Generation-1230 那每一篇文章機器做的事情就是判斷這篇文章的這個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1231 是重要的還是不重要的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1232 接下來他把所有判斷為重要的句子接起來
GAN_Lecture_9_(2018)_-_Sequence_Generation-1233 就變成一則摘要了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1234 那你可能會說用這樣的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1235 可以產生好的摘要嗎？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1236 那這種方法雖然很簡單
GAN_Lecture_9_(2018)_-_Sequence_Generation-1237 你就是 learn 一個 binary classifier 決定一個句子是重要的還是不重要的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1238 但是你沒有辦法用這個方法，產生真的非常好的摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1239 因為為什麼呢？因為就像我們國小老師都有告訴我們說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1240 你今天在寫課文摘要的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1241 你要用自己的話，來寫摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1242 你不能夠把課文裡面的句子就直接抄出來
GAN_Lecture_9_(2018)_-_Sequence_Generation-1243 當作摘要，你要自己 understanding 這個課文以後
GAN_Lecture_9_(2018)_-_Sequence_Generation-1244 看懂這個課文以後，用自己的話，來寫出摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1245 那過去 extracted summarization
GAN_Lecture_9_(2018)_-_Sequence_Generation-1246 做不到這件事，但是今天多數我們都可以做 abstractive summarization，怎麼做？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1247 learn 一個 seq2seq model
GAN_Lecture_9_(2018)_-_Sequence_Generation-1248 收集一大堆的文章，每一篇文章都有人標的摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1249 然後 seq2seq model 硬 train 下去，就像你 train 一個 chatbot
GAN_Lecture_9_(2018)_-_Sequence_Generation-1250 或 train 一個 video capturing generator 一樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-1251 train 下去就結束了，給它一個新的文章
GAN_Lecture_9_(2018)_-_Sequence_Generation-1252 它就會產生一個摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1253 而且這個摘要是機器用自己的話說出來的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1254 不見得是文章裡面現有的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1255 但是這整套技術最大的問題就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-1256 你要 train 這個 seq2seq model
GAN_Lecture_9_(2018)_-_Sequence_Generation-1257 你顯然需要非常大量的資料
GAN_Lecture_9_(2018)_-_Sequence_Generation-1258 到底要多少資料才夠呢？很多同學會想要自己 train 一個 summarizer
GAN_Lecture_9_(2018)_-_Sequence_Generation-1259 然後他去網路上收集比如說 10 萬篇文章
GAN_Lecture_9_(2018)_-_Sequence_Generation-1260 10 萬篇文章它通通有標註摘要，他覺得已經很多了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1261 train 下去結果整個壞掉，為什麼呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1262 這時候我就會告訴你說，你要 train 一個 abstractive summarization 系統
GAN_Lecture_9_(2018)_-_Sequence_Generation-1263 通常至少要 1 百萬個 examples，才做得起來
GAN_Lecture_9_(2018)_-_Sequence_Generation-1264 沒有一百萬個 examples
GAN_Lecture_9_(2018)_-_Sequence_Generation-1265 機器可能連產生符合文法的句子都做不到
GAN_Lecture_9_(2018)_-_Sequence_Generation-1266 但是如果有上百萬個 examples
GAN_Lecture_9_(2018)_-_Sequence_Generation-1267 對機器來說，要產生合文法的句子，其實不是一個問題
GAN_Lecture_9_(2018)_-_Sequence_Generation-1268 但是這個 abstractive summarization 最大的問題就是
GAN_Lecture_9_(2018)_-_Sequence_Generation-1269 要收集大量的資料，才有辦法去訓練
GAN_Lecture_9_(2018)_-_Sequence_Generation-1270 所以怎麼辦呢？我們就想要提出一些新的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1271 我們其實可以把文章視為是一種 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1272 把摘要視為是另外一種 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1273 現在如果我們有了 GAN 的技術
GAN_Lecture_9_(2018)_-_Sequence_Generation-1274 我們可以在兩個 domain 間
GAN_Lecture_9_(2018)_-_Sequence_Generation-1275 直接用 unsupervised 的方法互轉
GAN_Lecture_9_(2018)_-_Sequence_Generation-1276 我們並不需要兩個 domain 間的東西的 pair
GAN_Lecture_9_(2018)_-_Sequence_Generation-1277 所以今天假設我們把文章視為一個 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1278 摘要視為另外一個 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1279 我們不需要文章和摘要的 pair，只要收集一大堆文章
GAN_Lecture_9_(2018)_-_Sequence_Generation-1280 收集一大堆摘要當作範例告訴機器說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1281 摘要到底長什麼樣子，這些摘要不需要是這些文章的摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1282 只要收集兩堆 data
GAN_Lecture_9_(2018)_-_Sequence_Generation-1283 機器就可以自動在兩個 domain 間互轉
GAN_Lecture_9_(2018)_-_Sequence_Generation-1284 你就可以自動地學會怎麼做摘要這件事
GAN_Lecture_9_(2018)_-_Sequence_Generation-1285 而這個 process 是 unsupervised 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1286 你並不需要標注這些文章的摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1287 你只需要提供機器一些摘要， 作為範例就可以了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1288 那這個技術怎麼做的呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1289 這個技術就跟 cycle GAN 是非常像的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1290 我們 learn 一個 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-1291 這個generator 是一個 seq2seq model
GAN_Lecture_9_(2018)_-_Sequence_Generation-1292 這個 seq2seq model 吃一篇文章
GAN_Lecture_9_(2018)_-_Sequence_Generation-1293 然後 output 一個 word sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-1294 output 一個 比較短的 word sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-1295 但是假設只有這個 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-1296 你沒辦法 train
GAN_Lecture_9_(2018)_-_Sequence_Generation-1297 因為 generator 根本不知道說 output 什麼樣的 word sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-1298 才能當作 input 的文章的摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1299 所以接下來
GAN_Lecture_9_(2018)_-_Sequence_Generation-1300 你就要 learn 一個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-1301 這個 discriminator 的工作是什麼呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1302 這個 discriminator 的工作就是，他看過很多人寫的摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1303 但這些摘要不需要是這些文章的摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1304 Discriminator 看過很多人寫的摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1305 他知道人寫的摘要是什麼樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1306 接下來他就可以給這個 generator feedback
GAN_Lecture_9_(2018)_-_Sequence_Generation-1307 讓 generator output 出來呢 word sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-1308 看起來像是摘要一樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-1309 就跟我們之前講說什麼風景畫轉梵谷畫一樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-1310 你需要一個 discriminator，看説一張圖是不是梵谷的圖
GAN_Lecture_9_(2018)_-_Sequence_Generation-1311 把這個資訊 feedback 給 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-1312 generator 就可以產生看起來像是梵谷的畫作
GAN_Lecture_9_(2018)_-_Sequence_Generation-1313 梵谷 style 的畫作
GAN_Lecture_9_(2018)_-_Sequence_Generation-1314 那這邊其實一樣，你只需要一個 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-1315 一個 discriminator，discriminator 給這個 generator feedback
GAN_Lecture_9_(2018)_-_Sequence_Generation-1316 就可以希望它 output 出來的句子，看起來像是 summary
GAN_Lecture_9_(2018)_-_Sequence_Generation-1317 但是在講 cycle GAN 的時候我們有講過說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1318 光是這樣的架構是不夠的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1319 因為 generator 可能會學到產生看起來像是 summary 的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1320 就人寫的 summary 可能有某些特徵
GAN_Lecture_9_(2018)_-_Sequence_Generation-1321 比如說它都是比較簡短的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1322 也許 generator 可以學到產生一個簡短的句子，但是跟輸入是完全沒有關係的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1323 那怎麼解這個問題呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1324 就跟 cycle GAN 一樣，你要加一個 reconstructor
GAN_Lecture_9_(2018)_-_Sequence_Generation-1325 在做 cycle GAN 的時候我們說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1326 我們把 x domain 的東西轉到 y domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1327 接下來要 learn 一個 generator，把 y domain 的東西轉回來
GAN_Lecture_9_(2018)_-_Sequence_Generation-1328 這樣我們就可以迫使，x domain 跟 y domain 的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-1329 是長得比較像的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1330 我們希望 generator output，跟 input 是有關係的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1331 所以在做 unsupervised abstractive summarization 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1332 我們這邊用的概念，跟 cycle GAN 其實是一模一樣的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1333 你 learn 另外一個 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-1334 我們這邊稱為 reconstructor
GAN_Lecture_9_(2018)_-_Sequence_Generation-1335 他的工作是，吃第一個 generator output 的 word sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-1336 把這個 word sequence，轉回原來的 document
GAN_Lecture_9_(2018)_-_Sequence_Generation-1337 那你在 train 的時候你就希望
GAN_Lecture_9_(2018)_-_Sequence_Generation-1338 原來輸入的文章，被縮短
GAN_Lecture_9_(2018)_-_Sequence_Generation-1339 以後要能被擴寫回原來的 document
GAN_Lecture_9_(2018)_-_Sequence_Generation-1340 這個跟 cycle GAN 用的概念是一模一樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-1341 那你其實可以用另外一個方法來理解這個 model
GAN_Lecture_9_(2018)_-_Sequence_Generation-1342 你說我有一個 generator
GAN_Lecture_9_(2018)_-_Sequence_Generation-1343 這個 generator 把文章變成簡短的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1344 那你有另外一個 reconstructor 它把簡短的句子變回原來的文章
GAN_Lecture_9_(2018)_-_Sequence_Generation-1345 如果這個 reconstructor 可以把簡短的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1346 變回原來的文章，代表說這個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1347 有原來的文章裡面重要的資訊
GAN_Lecture_9_(2018)_-_Sequence_Generation-1348 因為這個句子有原來的文章裡面重要的資訊
GAN_Lecture_9_(2018)_-_Sequence_Generation-1349 所以你就可以把它當作一個摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1350 在 training 的時候，這個 training 的 process 是 unsupervised
GAN_Lecture_9_(2018)_-_Sequence_Generation-1351 因為你只需要文章就好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1352 你只需要輸入和輸出的文章越接近越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1353 所以並不需要給機器摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1354 你只需要提供給機器文章就好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1355 那這個整個 model，這個 generator 跟 reconstructor 合起來
GAN_Lecture_9_(2018)_-_Sequence_Generation-1356 可以看作是一個 seq2seq2seq auto-encoder 這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-1357 一般你都會 train 一個 seq2...，你就一般你 train auto-encoder 就 input 一個東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-1358 把它變成一個 vector，把這個 vector 變回原來的 object
GAN_Lecture_9_(2018)_-_Sequence_Generation-1359 比如說是個 image 等等，那現在是 input 一個 sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-1360 把它變成一個短的 sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-1361 再把它解回原來長的 sequence
GAN_Lecture_9_(2018)_-_Sequence_Generation-1362 這樣是一個 seq2seq2seq auto-encoder
GAN_Lecture_9_(2018)_-_Sequence_Generation-1363 那一般的 auto-encoder 都是用一個 latent vector 來表示你的資訊
GAN_Lecture_9_(2018)_-_Sequence_Generation-1364 那我們現在不是用一個人看不懂的 vector 來表示資訊
GAN_Lecture_9_(2018)_-_Sequence_Generation-1365 我們是用一個句子來表示資訊
GAN_Lecture_9_(2018)_-_Sequence_Generation-1366 這個東西希望是人可以讀的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1367 但是這邊會遇到的問題是，假設你只 train 這個 generator 跟這個 reconstructor
GAN_Lecture_9_(2018)_-_Sequence_Generation-1368 你產生出來的 word sequence 可能是人沒有辦法讀的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1369 他可能是人根本就沒辦法看懂的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1370 因為機器可能會自己發明奇怪的暗語
GAN_Lecture_9_(2018)_-_Sequence_Generation-1371 因為 generator 跟 reconstructor，他們都是 machine 嘛
GAN_Lecture_9_(2018)_-_Sequence_Generation-1372 所以他們可以發明奇怪的暗語，反正只要他們彼此之間看得懂就好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1373 那人看不懂沒有關係，比如說台灣大學
GAN_Lecture_9_(2018)_-_Sequence_Generation-1374 它可能就縮寫成灣學，而不是台大
GAN_Lecture_9_(2018)_-_Sequence_Generation-1375 反正只要 reconstructor 可以把灣學解回台灣大學其實就結束了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1376 所以為了希望 generator 產生出來的句子是人看得懂的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1377 所以我們要加一個 discriminator
GAN_Lecture_9_(2018)_-_Sequence_Generation-1378 這個 discriminator 就可以強迫說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1379 generator 產生出來的句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1380 一方面要是一個 summary
GAN_Lecture_9_(2018)_-_Sequence_Generation-1381 可以對reconstructor 解回原來的文章
GAN_Lecture_9_(2018)_-_Sequence_Generation-1382 同時 generator output 的這個句子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1383 也要是 discriminator 可以看得懂的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1384 覺得像是人類寫的 summary
GAN_Lecture_9_(2018)_-_Sequence_Generation-1385 那這個就是 unsupervised abstractive summarization 的架構
GAN_Lecture_9_(2018)_-_Sequence_Generation-1386 那可以做到什麼樣的程度呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1387 這邊可以跟大家講一下就是說，在 training 的時候，因為這邊 output 是 discrete 的嘛
GAN_Lecture_9_(2018)_-_Sequence_Generation-1388 所以你當然是需要有一些方法來處理這種 discrete output
GAN_Lecture_9_(2018)_-_Sequence_Generation-1389 那我們用的就是 reinforced algorithm 就是了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1390 那我們來看一下可以做到什麼樣的程度，這是一些真正的例子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1391 那有人可能會想說用 unsupervised learning 有什麼好處
GAN_Lecture_9_(2018)_-_Sequence_Generation-1392 因為你用 unsupervised learning 永遠贏不過 supervised learning
GAN_Lecture_9_(2018)_-_Sequence_Generation-1393 supervised learning 就是 unsupervised learning 的 upper bound
GAN_Lecture_9_(2018)_-_Sequence_Generation-1394 但 unsupervised learning 的意義何在
GAN_Lecture_9_(2018)_-_Sequence_Generation-1395 那所以我們以下用這個實驗來說明一下
GAN_Lecture_9_(2018)_-_Sequence_Generation-1396 unsupervised learning 的意義
GAN_Lecture_9_(2018)_-_Sequence_Generation-1397 那我們知道說，那這邊這個縱軸是 ROUGE 的分數
GAN_Lecture_9_(2018)_-_Sequence_Generation-1398 總之就是用來衡量摘要的一個方法啦
GAN_Lecture_9_(2018)_-_Sequence_Generation-1399 值越大，代表我們產生的摘要，越好就是了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1400 那黑色的線是什麼？黑色的線是 supervised learning  的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1401 那今天在做 supervised learning 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1402 需要 380 萬筆 training example
GAN_Lecture_9_(2018)_-_Sequence_Generation-1403 380 萬篇文章跟它的摘要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1404 你才能夠 train 出一個好的 summarization 的系統
GAN_Lecture_9_(2018)_-_Sequence_Generation-1405 那用 380 萬篇文章 train 出來的結果呢
GAN_Lecture_9_(2018)_-_Sequence_Generation-1406 是黑色的這一條線
GAN_Lecture_9_(2018)_-_Sequence_Generation-1407 那這邊我們用了不同的方法來做這個，來 train 這個 GAN
GAN_Lecture_9_(2018)_-_Sequence_Generation-1408 我們有用 WGAN 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1409 有用 reinforcement learning 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1410 分別是藍線跟橙線得到的結果其實是差不多的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1411 看起來 WGAN 是差一點
GAN_Lecture_9_(2018)_-_Sequence_Generation-1412 那橙色的結果，用 reinforcement learning 的結果是比較好的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1413 那今天如果在完全沒有 label 情況下，得到的結果是這個樣子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1414 那當然跟 supervised 的方法，還是差了一截
GAN_Lecture_9_(2018)_-_Sequence_Generation-1415 但是今天你可以用少量的 summary
GAN_Lecture_9_(2018)_-_Sequence_Generation-1416 再去 fine tune unsupervised learning 的 model
GAN_Lecture_9_(2018)_-_Sequence_Generation-1417 就是你先用 unsupervised learning 的方法把你的model 練得很強
GAN_Lecture_9_(2018)_-_Sequence_Generation-1418 再用少量的 label data 去 fine tune
GAN_Lecture_9_(2018)_-_Sequence_Generation-1419 那它的進步就會很快，舉例來說，我們這邊只用 50 萬筆的 data
GAN_Lecture_9_(2018)_-_Sequence_Generation-1420 得到的結果就已經跟 supervised learning 的結果一樣了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1421 所以這邊你只需要原來的 1/6  或者更少的 data
GAN_Lecture_9_(2018)_-_Sequence_Generation-1422 其實就可以跟用全部的 data 得到一樣好的結果
GAN_Lecture_9_(2018)_-_Sequence_Generation-1423 所以 unsupervised learning 帶給我們的好處就是你只需要比較少的 data
GAN_Lecture_9_(2018)_-_Sequence_Generation-1424 比較少的 label data，就可以跟過去大量 label data 的時候得到的結果
GAN_Lecture_9_(2018)_-_Sequence_Generation-1425 也許是一樣好的，那這就是 unsupervised learning 的妙用
GAN_Lecture_9_(2018)_-_Sequence_Generation-1426 這邊舉最後一個例子是 unsupervised machine translation，那至於怎麼做，我們今天就不細講
GAN_Lecture_9_(2018)_-_Sequence_Generation-1427 因為我們今天可以把，不同的語言
GAN_Lecture_9_(2018)_-_Sequence_Generation-1428 就視為是不同的 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1429 就假設你要英文轉法文
GAN_Lecture_9_(2018)_-_Sequence_Generation-1430 你就要把英文視為一個 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1431 法文視為另外一個 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1432 然後就可以用 unsupervised learning 的方法把英文轉成法文，法文轉成英文
GAN_Lecture_9_(2018)_-_Sequence_Generation-1433 那就做到摘要了，就結束了這樣，不是摘要，做到翻譯就結束了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1434 所以你就可以做 unsupervised 的翻譯
GAN_Lecture_9_(2018)_-_Sequence_Generation-1435 那這個方法聽起來還蠻匪夷所思的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1436 真的能夠做得到嗎？其實 facebook 在今年的 ICLR
GAN_Lecture_9_(2018)_-_Sequence_Generation-1437 就發了兩篇這種 paper
GAN_Lecture_9_(2018)_-_Sequence_Generation-1438 看起來還真的是可以的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1439 細節我們就不講，細節你可以想像就很像那個 cycle GAN 這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-1440 就我們前面講過的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1441 只是前面我們有說拿兩種不同 image 當作兩個不同的 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1442 兩種不同的語音當作兩個不同的 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1443 現在只是把兩種語言當作兩個不同的 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1444 然後讓機器去學兩種語言間的對應
GAN_Lecture_9_(2018)_-_Sequence_Generation-1445 硬做看看做不做的起來，這個是文獻上的結果
GAN_Lecture_9_(2018)_-_Sequence_Generation-1446 這個虛線代表 supervised learning 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1447 縱軸是 BLEU score，是拿來衡量摘要好壞的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1448 總之 BLEU 越高，代表摘要做得越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1449 橫軸是訓練資料的量，從 10^4 一直到 10^7
GAN_Lecture_9_(2018)_-_Sequence_Generation-1450 你發現說呢，如果 supervised learning 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1451 這邊是不同語言的翻譯，英文轉法文
GAN_Lecture_9_(2018)_-_Sequence_Generation-1452 法文轉英文，德文轉英文，英文轉德文，四條線
GAN_Lecture_9_(2018)_-_Sequence_Generation-1453 代表四種不同語言的 pair，語言組合間的翻譯
GAN_Lecture_9_(2018)_-_Sequence_Generation-1454 那你發現訓練資料越多，當然結果就越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1455 這個沒有什麼特別稀奇的，橫線是這個橫線是什麼？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1456 橫線是 unsupervised learning 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1457 用 10^7 的 data 去 train 的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1458 unsupervised learning 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1459 但是你並不需要兩個語言間的 pair
GAN_Lecture_9_(2018)_-_Sequence_Generation-1460 做 supervised learning 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1461 你需要兩個語言間的 pair
GAN_Lecture_9_(2018)_-_Sequence_Generation-1462 但做 unsupervised learning 的時候
GAN_Lecture_9_(2018)_-_Sequence_Generation-1463 就是兩堆句子，不需要他們之間的 pair，然後硬做
GAN_Lecture_9_(2018)_-_Sequence_Generation-1464 得到的結果呢，你今天只要
GAN_Lecture_9_(2018)_-_Sequence_Generation-1465 unsupervised learning 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1466 有 10 million 的 sentences
GAN_Lecture_9_(2018)_-_Sequence_Generation-1467 你的 performance，就可以跟 supervised learning 的方法
GAN_Lecture_9_(2018)_-_Sequence_Generation-1468 只用 10 萬筆 data，是一樣好的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1469 所以假設你手上沒有 10 萬筆 data pair
GAN_Lecture_9_(2018)_-_Sequence_Generation-1470 unsupervised 方法其實還可以贏過
GAN_Lecture_9_(2018)_-_Sequence_Generation-1471 supervised learning 的方法，這個結果是我覺得還頗驚人的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1472 接下來我們就想說，既然兩種不同的語言可以做
GAN_Lecture_9_(2018)_-_Sequence_Generation-1473 那語音跟文字間可不可以做呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1474 把語音視為是一個 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1475 把文字視為是另外一個 domain
GAN_Lecture_9_(2018)_-_Sequence_Generation-1476 然後你就可以 apply 類似 GAN 的技術
GAN_Lecture_9_(2018)_-_Sequence_Generation-1477 在這兩個 domain 間，互轉，硬做，硬做
GAN_Lecture_9_(2018)_-_Sequence_Generation-1478 這樣看看機器能不能夠學得起來
GAN_Lecture_9_(2018)_-_Sequence_Generation-1479 如果假設今天機器可以學會說
GAN_Lecture_9_(2018)_-_Sequence_Generation-1480 給它一堆語音給它一堆文字
GAN_Lecture_9_(2018)_-_Sequence_Generation-1481 它就可以自動學會怎麼把聲音轉成文字的話
GAN_Lecture_9_(2018)_-_Sequence_Generation-1482 你就可以做 unsupervised 的語音辨識了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1483 未來機器可能在日常生活中，聽人講話
GAN_Lecture_9_(2018)_-_Sequence_Generation-1484 然後它自己再去網路上
GAN_Lecture_9_(2018)_-_Sequence_Generation-1485 看一下人寫的文章，就自動學會，語音辨識了
GAN_Lecture_9_(2018)_-_Sequence_Generation-1486 有人可能會想說，這個聽起來也是還蠻匪夷所思的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1487 這個東西到底能不能夠做到呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1488 我先覺得是有可能的，如果翻譯可以做到
GAN_Lecture_9_(2018)_-_Sequence_Generation-1489 這件事情也是有機會的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1490 unsupervised 語音辨識也是有機會的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1491 這邊舉一個非常簡單的例子，假如說所有聲音訊號的開頭
GAN_Lecture_9_(2018)_-_Sequence_Generation-1492 都是某個樣子，比如說都有 P1 這個 pattern
GAN_Lecture_9_(2018)_-_Sequence_Generation-1493 我們用 P 代表一個 pattern，就 P1 這個 pattern
GAN_Lecture_9_(2018)_-_Sequence_Generation-1494 那機器在自己去讀文章以後發現説
GAN_Lecture_9_(2018)_-_Sequence_Generation-1495 所有的文章都是 The 開頭
GAN_Lecture_9_(2018)_-_Sequence_Generation-1496 它就可以自動 mapping 到說 P1 這種聲音訊號
GAN_Lecture_9_(2018)_-_Sequence_Generation-1497 這種聲音訊號的 pattern，就是 The 這樣
GAN_Lecture_9_(2018)_-_Sequence_Generation-1498 那這是一個過度簡化的例子
GAN_Lecture_9_(2018)_-_Sequence_Generation-1499 實際上做不做得起來呢？這個是實際上得到的結果
GAN_Lecture_9_(2018)_-_Sequence_Generation-1500 我們用的聲音訊號來自於 TIMIT 這個 corpus
GAN_Lecture_9_(2018)_-_Sequence_Generation-1501 用的文字來自於 WMT 這個 corpus
GAN_Lecture_9_(2018)_-_Sequence_Generation-1502 那這兩個 corpus 是沒有對應關係的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1503 就並不是說這些文字就是這些語音的辨識的結果，不是
GAN_Lecture_9_(2018)_-_Sequence_Generation-1504 這是兩堆不相關的東西，一堆語音講自己的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1505 文字講自己的，兩堆不相關的東西
GAN_Lecture_9_(2018)_-_Sequence_Generation-1506 然後接下來，用類似 cycle GAN 的技術，硬轉
GAN_Lecture_9_(2018)_-_Sequence_Generation-1507 看能不能夠把聲音訊號硬是轉成文字
GAN_Lecture_9_(2018)_-_Sequence_Generation-1508 這是一個實驗的結果，縱軸是辨識的正確率
GAN_Lecture_9_(2018)_-_Sequence_Generation-1509 那其實是 Phoneme recognition，不是辨識出文字
GAN_Lecture_9_(2018)_-_Sequence_Generation-1510 你是辨識出音標而已，辨識出文字還是比較難
GAN_Lecture_9_(2018)_-_Sequence_Generation-1511 直接辨識出音標而已
GAN_Lecture_9_(2018)_-_Sequence_Generation-1512 那這個橫軸代表說訓練資料的量
GAN_Lecture_9_(2018)_-_Sequence_Generation-1513 如果是 supervised learning 的方法，當然訓練資料的量越多
GAN_Lecture_9_(2018)_-_Sequence_Generation-1514 performance 越好
GAN_Lecture_9_(2018)_-_Sequence_Generation-1515 這兩個橫線是什麼呢？
GAN_Lecture_9_(2018)_-_Sequence_Generation-1516 這兩個橫線就是用 unsupervised 的方法硬做得到的結果
GAN_Lecture_9_(2018)_-_Sequence_Generation-1517 那硬做其實有得到 36% 的正確率
GAN_Lecture_9_(2018)_-_Sequence_Generation-1518 你會想 36% 的正確率，這麼低
GAN_Lecture_9_(2018)_-_Sequence_Generation-1519 這個 output 結果應該人看不懂吧，是，人看不懂
GAN_Lecture_9_(2018)_-_Sequence_Generation-1520 但是它是遠比 random 好的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1521 真的是遠比 random 好的
GAN_Lecture_9_(2018)_-_Sequence_Generation-1522 所以就算是在完全 unsupervised 的情況下
GAN_Lecture_9_(2018)_-_Sequence_Generation-1523 只給機器一堆文字，一堆語音，它還是有學到東西了
